{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute chrY z-scores with regards to different distributions, and plot the results.\"\"\"\n",
    "\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, pointless-statement, use-dict-literal, expression-not-assigned, unused-import, too-many-lines, unreachable, duplicate-code\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "from epiclass.core.metadata import Metadata\n",
    "from epiclass.utils.general_utility import get_valid_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSAY = \"assay_epiclass\"\n",
    "SEX = \"harmonized_donor_sex\"\n",
    "TRACK = \"track_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epilap_project = Path.home() / \"Projects/epilap\"\n",
    "\n",
    "metadata = (\n",
    "    epilap_project\n",
    "    / \"input\"\n",
    "    / \"metadata\"\n",
    "    / \"dfreeze-v2\"\n",
    "    / \"hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2.json\"\n",
    ")\n",
    "\n",
    "logs = epilap_project / \"output/logs/epiatlas-dfreeze-v2.1\"\n",
    "chrY_coverage_file = logs / \"chrY_coverage_results\" / \"chrXY_coverage_all.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = Metadata(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chrY = pd.read_csv(chrY_coverage_file, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_chrY.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge metadata and chrY coverage info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.DataFrame.from_records(list(meta.datasets)).set_index(\"md5sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[ASSAY].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-core tracks\n",
    "meta_df = meta_df[~meta_df[ASSAY].str.contains(\"non-core|CTCF\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df_chrY.merge(meta_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_df.shape, df_chrY.shape, merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the merge\n",
    "assert (merged_df.shape[0] == meta_df.shape[0]) and (\n",
    "    df_chrY.shape[1] + meta_df.shape[1] == merged_df.shape[1]\n",
    "), \"not right shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in set(meta_df.index.values) - set(df_chrY.index.values):\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute chrY zscore coverage by assay and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby_columns = [ASSAY, SEX, TRACK]\n",
    "# groupby_columns = [ASSAY, SEX]\n",
    "groupby_columns = [ASSAY, TRACK]\n",
    "\n",
    "chrY_dists = merged_df.groupby(groupby_columns).agg({\"chrY\": [\"mean\", \"std\", \"count\"]})\n",
    "display(chrY_dists)\n",
    "# display(chrY_dists.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_name = \"assay_track\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrY_dists.to_csv(\n",
    "    logs / \"chrY_coverage_results\" / f\"chrY_coverage_distributions_{partial_name}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_name = f\"expected_{partial_name}_chrY_z-score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for row in chrY_dists.iterrows():\n",
    "    index, vals = row\n",
    "    # assay, sex, track = index # type: ignore\n",
    "    # assay, sex = index # type: ignore\n",
    "    # assay = index  # type: ignore\n",
    "    assay, track = index  # type: ignore\n",
    "\n",
    "    mean = vals[\"chrY\"][\"mean\"]\n",
    "    std = vals[\"chrY\"][\"std\"]\n",
    "    count = vals[\"chrY\"][\"count\"]\n",
    "\n",
    "    # partial_df = merged_df[(merged_df[ASSAY] == assay) & (merged_df[SEX] == sex) & (merged_df[TRACK] == track)]\n",
    "    # partial_df = merged_df[(merged_df[ASSAY] == assay) & (merged_df[SEX] == sex)]\n",
    "    # partial_df = merged_df[(merged_df[ASSAY] == assay)]\n",
    "    partial_df = merged_df[(merged_df[ASSAY] == assay) & (merged_df[TRACK] == track)]\n",
    "    partial_df = partial_df[[\"chrY\"]].copy()\n",
    "\n",
    "    partial_df.loc[:, score_name] = (partial_df[\"chrY\"] - mean) / std\n",
    "    partial_df.loc[:, f\"count_{score_name}\"] = count\n",
    "\n",
    "    new_data.append(partial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zscores_df = pd.concat(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df_chrY.merge(\n",
    "    all_zscores_df, how=\"left\", left_index=True, right_index=True, suffixes=(\"\", \"_DROP\")\n",
    ").sort_index()\n",
    "final_df = final_df.drop(columns=[\"chrY_DROP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_path = (\n",
    "    logs / \"chrY_coverage_results\" / f\"chrY_coverage_zscores_{partial_name}.csv\"\n",
    ")\n",
    "# final_df.to_csv(final_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map predictions to chrY z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_pred_dir = Path.home() / \"downloads\" / \"temp\"\n",
    "sex_pred_file = (\n",
    "    sex_pred_dir / \"sex3_oversample_full-10fold-validation_prediction_augmented-all.csv\"\n",
    ")\n",
    "\n",
    "full_chrY_df = pd.read_csv(final_df_path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.read_csv(sex_pred_file, index_col=0, header=0)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pred_df = pred_df.merge(\n",
    "    full_chrY_df, how=\"inner\", left_index=True, right_index=True, suffixes=(\"\", \"_DROP\")\n",
    ").sort_index()\n",
    "merged_pred_df = merged_pred_df.drop(\n",
    "    columns=[column for column in pred_df.columns if column.endswith(\"_DROP\")]\n",
    ")\n",
    "print(merged_pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise UserWarning(\"stop here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chrY z-scores confusion matrix for sex3 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS_DICT = {\"female\": \"red\", \"male\": \"blue\", \"mixed\": \"purple\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_pred_df.to_csv(sex_pred_dir / \"sex3_oversample_full-10fold-validation_prediction_augmented-all_chrY_zscores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([col for col in merged_pred_df.columns if \"z-score\" in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_pred_df = merged_pred_df[\n",
    "#     ~merged_pred_df[ASSAY].str.contains(pat=\"wgb\", case=False)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graphs per assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_label = \"expected_assay_chrY_z-score\"\n",
    "count_label = f\"count_{coverage_label}\"\n",
    "\n",
    "classes = merged_pred_df[\"True class\"].unique()\n",
    "assays = merged_pred_df[ASSAY].unique()\n",
    "\n",
    "matrix_logdir = (\n",
    "    logs\n",
    "    / \"chrY_coverage_results\"\n",
    "    / \"10fold_valid\"\n",
    "    / \"z-score\"\n",
    "    / \"per_assay\"\n",
    "    / \"w-unknown\"\n",
    "    / \"per_assay_graph\"\n",
    ")\n",
    "\n",
    "for assay_label in assays:\n",
    "    assay_df = merged_pred_df[merged_pred_df[ASSAY] == assay_label]\n",
    "\n",
    "    # confusion matrix for chrY z-score\n",
    "    for threshold in [0, 0.7, 0.9]:\n",
    "        row = 1\n",
    "        col = 1\n",
    "        fig = make_subplots(\n",
    "            rows=3,\n",
    "            cols=3,\n",
    "            shared_yaxes=True,\n",
    "            x_title=\"Predicted class (nb of predictions)\",\n",
    "            y_title=\"z-score vs expected assay\",\n",
    "            row_titles=list(classes),\n",
    "            column_titles=list(classes),\n",
    "            vertical_spacing=0.08,\n",
    "            horizontal_spacing=0.01,\n",
    "        )\n",
    "        threshold_df = assay_df[assay_df[\"Max pred\"] >= threshold]\n",
    "\n",
    "        for label in classes:\n",
    "            df_label = threshold_df[threshold_df[\"True class\"] == label]\n",
    "\n",
    "            # Iterate over each target and add a violin plot for it\n",
    "            for target in classes:\n",
    "                sub_df = df_label[df_label[\"Predicted class\"] == target]\n",
    "\n",
    "                if sub_df.shape[0] == 0:\n",
    "                    y_values = [\n",
    "                        threshold_df[coverage_label].mean()\n",
    "                    ]  # Minimal synthetic data\n",
    "                    sample_count = 0\n",
    "                    hovertext = [\"PLACEHOLDER - NO DATA\"]\n",
    "                else:\n",
    "                    y_values = sub_df[coverage_label]\n",
    "                    hovertext = [\n",
    "                        f\"{md5sum, assay}:(z-score={z_score:.3f} (n={int(count)}), pred={pred:.3f})\"\n",
    "                        for md5sum, pred, z_score, count, assay in zip(\n",
    "                            sub_df.index,\n",
    "                            sub_df[\"Max pred\"],\n",
    "                            sub_df[coverage_label],\n",
    "                            sub_df[count_label],\n",
    "                            sub_df[ASSAY],\n",
    "                        )\n",
    "                    ]\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Violin(\n",
    "                        y=y_values,\n",
    "                        name=f\"{target} ({sub_df.shape[0]})\",\n",
    "                        box_visible=True,\n",
    "                        meanline_visible=True,\n",
    "                        points=\"all\",\n",
    "                        line_color=COLORS_DICT[target],\n",
    "                        hovertemplate=\"%{text}\",\n",
    "                        text=hovertext,\n",
    "                    ),\n",
    "                    row=row,\n",
    "                    col=col,\n",
    "                )\n",
    "\n",
    "                # Move to the next subplot position\n",
    "                col += 1\n",
    "                if col > 3:\n",
    "                    col = 1\n",
    "                    row += 1\n",
    "\n",
    "        # Update global layout and traces\n",
    "        fig.update_traces(marker=dict(size=1))\n",
    "        fig.update_yaxes(\n",
    "            range=[\n",
    "                min(assay_df[coverage_label]) - 0.01,\n",
    "                max(assay_df[coverage_label]) + 0.01,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Directly using annotations param does not work with make_subplots\n",
    "        existing_annotations = fig.layout.annotations\n",
    "        new_annotation = dict(\n",
    "            x=1.01,  # Position on the x-axis\n",
    "            y=0.5,  # Position on the y-axis\n",
    "            showarrow=False,  # Do not show arrow\n",
    "            text=\"Reference class\",  # The text you want to display\n",
    "            xref=\"paper\",  # 'x' coordinate is set in relative coordinates\n",
    "            yref=\"paper\",  # 'y' coordinate is set in relative coordinates\n",
    "            xanchor=\"left\",  # Text starts from the left of the x-coordinate\n",
    "            yanchor=\"middle\",  # Middle aligned vertically\n",
    "            font=dict(size=16),\n",
    "            textangle=90,\n",
    "        )\n",
    "        updated_annotations = list(existing_annotations) + [new_annotation]\n",
    "\n",
    "        title = f\"z-score(mean chrY coverage per file):{assay_label} (pred>{threshold})\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"{title} (n={threshold_df.shape[0]})\",\n",
    "            showlegend=False,\n",
    "            annotations=updated_annotations,\n",
    "        )\n",
    "\n",
    "        # fig.show()\n",
    "\n",
    "        # title = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "        # html_file = matrix_logdir / f\"{title}.html\"\n",
    "        # png_file = matrix_logdir / f\"{title}.png\"\n",
    "        # if not png_file.exists():\n",
    "        #     fig.write_image(png_file, scale=2)\n",
    "        # if not html_file.exists():\n",
    "        #     fig.write_html(html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs per assay and track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_df = merged_pred_df[merged_pred_df[SEX].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_name = \"assay_track\"\n",
    "coverage_label = f\"expected_{partial_name}_chrY_z-score\"\n",
    "count_label = f\"count_{coverage_label}\"\n",
    "\n",
    "classes = sex_df[SEX].unique()\n",
    "assays = sex_df[ASSAY].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes, assays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_logdir = (\n",
    "    logs\n",
    "    / \"chrY_coverage_results\"\n",
    "    / \"10fold_valid\"\n",
    "    / \"z-score\"\n",
    "    / \"per_assay_track\"\n",
    "    / \"per_assay_track_confusion_graphs\"\n",
    ")\n",
    "matrix_logdir.mkdir(parents=False, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrices style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for assay_label in assays:\n",
    "    assay_df = merged_pred_df[merged_pred_df[ASSAY] == assay_label]\n",
    "\n",
    "    for track_type in assay_df[TRACK].unique():\n",
    "        assay_track_df = assay_df[assay_df[TRACK] == track_type]\n",
    "\n",
    "        # confusion matrix for chrY z-score\n",
    "        for threshold in [0, 0.7, 0.9]:\n",
    "            row = 1\n",
    "            col = 1\n",
    "            fig = make_subplots(\n",
    "                rows=3,\n",
    "                cols=3,\n",
    "                shared_yaxes=True,\n",
    "                x_title=\"Predicted class (nb of predictions)\",\n",
    "                y_title=\"z-score vs expected assay\",\n",
    "                row_titles=list(classes),\n",
    "                column_titles=list(classes),\n",
    "                vertical_spacing=0.08,\n",
    "                horizontal_spacing=0.01,\n",
    "            )\n",
    "            threshold_df = assay_track_df[assay_track_df[\"Max pred\"] >= threshold]\n",
    "\n",
    "            title = f\"z-score(mean chrY coverage per file):{assay_label},{track_type} (pred>{threshold})\"\n",
    "            filename = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "\n",
    "            html_file = matrix_logdir / f\"{filename}.html\"\n",
    "            png_file = matrix_logdir / f\"{filename}.png\"\n",
    "            if png_file.exists() or html_file.exists():\n",
    "                continue\n",
    "\n",
    "            for label in classes:\n",
    "                df_label = threshold_df[threshold_df[\"True class\"] == label]\n",
    "\n",
    "                # Iterate over each target and add a violin plot for it\n",
    "                for target in classes:\n",
    "                    sub_df = df_label[df_label[\"Predicted class\"] == target]\n",
    "\n",
    "                    if sub_df.shape[0] == 0:\n",
    "                        y_values = [\n",
    "                            threshold_df[coverage_label].mean()\n",
    "                        ]  # Minimal synthetic data\n",
    "                        sample_count = 0\n",
    "                        hovertext = [\"PLACEHOLDER - NO DATA\"]\n",
    "                    else:\n",
    "                        y_values = sub_df[coverage_label]\n",
    "                        hovertext = [\n",
    "                            f\"{md5sum}:(z-score={z_score:.3f} (n={int(count)}), pred={pred:.3f})\"\n",
    "                            for md5sum, pred, z_score, count, assay in zip(\n",
    "                                sub_df.index,\n",
    "                                sub_df[\"Max pred\"],\n",
    "                                sub_df[coverage_label],\n",
    "                                sub_df[count_label],\n",
    "                                sub_df[ASSAY],\n",
    "                            )\n",
    "                        ]\n",
    "\n",
    "                    fig.add_trace(\n",
    "                        go.Violin(\n",
    "                            y=y_values,\n",
    "                            name=f\"{target} ({sub_df.shape[0]})\",\n",
    "                            box_visible=True,\n",
    "                            meanline_visible=True,\n",
    "                            points=\"all\",\n",
    "                            line_color=COLORS_DICT[target],\n",
    "                            hovertemplate=\"%{text}\",\n",
    "                            text=hovertext,\n",
    "                        ),\n",
    "                        row=row,\n",
    "                        col=col,\n",
    "                    )\n",
    "\n",
    "                    # Move to the next subplot position\n",
    "                    col += 1\n",
    "                    if col > 3:\n",
    "                        col = 1\n",
    "                        row += 1\n",
    "\n",
    "            # Update global layout and traces\n",
    "            fig.update_traces(marker=dict(size=1))\n",
    "            fig.update_yaxes(\n",
    "                range=[\n",
    "                    min(assay_track_df[coverage_label]) - 0.01,\n",
    "                    max(assay_track_df[coverage_label]) + 0.01,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Directly using annotations param does not work with make_subplots\n",
    "            existing_annotations = fig.layout.annotations\n",
    "            new_annotation = dict(\n",
    "                x=1.01,  # Position on the x-axis\n",
    "                y=0.5,  # Position on the y-axis\n",
    "                showarrow=False,  # Do not show arrow\n",
    "                text=\"Reference class\",  # The text you want to display\n",
    "                xref=\"paper\",  # 'x' coordinate is set in relative coordinates\n",
    "                yref=\"paper\",  # 'y' coordinate is set in relative coordinates\n",
    "                xanchor=\"left\",  # Text starts from the left of the x-coordinate\n",
    "                yanchor=\"middle\",  # Middle aligned vertically\n",
    "                font=dict(size=16),\n",
    "                textangle=90,\n",
    "            )\n",
    "            updated_annotations = list(existing_annotations) + [new_annotation]\n",
    "\n",
    "            fig.update_layout(\n",
    "                title_text=f\"{title} (n={threshold_df.shape[0]})\",\n",
    "                showlegend=False,\n",
    "                annotations=updated_annotations,\n",
    "            )\n",
    "\n",
    "            # fig.show()\n",
    "            # break\n",
    "            # fig.write_image(png_file, scale=2)\n",
    "            # fig.write_html(html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global assay/track distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_df[ASSAY].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_logdir = (\n",
    "    logs / \"chrY_coverage_results\" / \"10fold_valid\" / \"z-score\" / \"per_assay_track\"\n",
    ")\n",
    "\n",
    "if not matrix_logdir.exists():\n",
    "    matrix_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "# Prepare a subplot grid; one for each assay + track type combination\n",
    "num_assay_track_combinations = sum(\n",
    "    len(sex_df[sex_df[ASSAY] == assay_label][TRACK].unique()) for assay_label in assays\n",
    ")\n",
    "total_cols = num_assay_track_combinations + len(sex_df[TRACK].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_colors_dict = dict(zip(assays, px.colors.qualitative.Dark24))\n",
    "track_colors_dict = dict(zip(sex_df[TRACK].unique(), px.colors.qualitative.Dark24_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### z-score violins - any sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 1\n",
    "row = 1\n",
    "\n",
    "x_title = \"Assay+Track distribution\"\n",
    "fig = make_subplots(\n",
    "    rows=row,\n",
    "    cols=total_cols,\n",
    "    shared_yaxes=True,\n",
    "    x_title=x_title,\n",
    "    y_title=\"z-score\",\n",
    "    horizontal_spacing=0.001,\n",
    ")\n",
    "\n",
    "for assay_label in sorted(assays):\n",
    "    assay_df = merged_pred_df[merged_pred_df[ASSAY] == assay_label]\n",
    "\n",
    "    for track_type in sorted(assay_df[TRACK].unique()):\n",
    "        sub_df = assay_df[assay_df[TRACK] == track_type]\n",
    "\n",
    "        y_values = sub_df[coverage_label]\n",
    "        hovertext = [\n",
    "            f\"{md5sum}:z-score={z_score:.3f} (n={int(count)}), pred={pred:.3f}\"\n",
    "            for md5sum, pred, z_score, count in zip(\n",
    "                sub_df.index,\n",
    "                sub_df[\"Max pred\"],\n",
    "                sub_df[coverage_label],\n",
    "                sub_df[count_label],\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                y=y_values,\n",
    "                name=f\"{assay_label},{track_type} ({sub_df.shape[0]})\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=\"all\",\n",
    "                line_color=assay_colors_dict[assay_label],\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext,\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "        col += 1\n",
    "\n",
    "\n",
    "# global track type distribution (all assays)\n",
    "for track_type in sorted(merged_pred_df[TRACK].unique()):\n",
    "    sub_df = merged_pred_df[merged_pred_df[TRACK] == track_type]\n",
    "\n",
    "    y_values = sub_df[coverage_label]\n",
    "    hovertext = [\n",
    "        f\"{md5sum,assay}:z-score={z_score:.3f} (n={int(count)}), pred={pred:.3f}\"\n",
    "        for md5sum, pred, z_score, count, assay in zip(\n",
    "            sub_df.index,\n",
    "            sub_df[\"Max pred\"],\n",
    "            sub_df[coverage_label],\n",
    "            sub_df[count_label],\n",
    "            sub_df[ASSAY],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Violin(\n",
    "            y=y_values,\n",
    "            name=f\"{track_type} ({sub_df.shape[0]})\",\n",
    "            box_visible=True,\n",
    "            meanline_visible=True,\n",
    "            points=\"all\",\n",
    "            line_color=track_colors_dict[track_type],\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext,\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    col += 1\n",
    "\n",
    "# Final graphing\n",
    "\n",
    "# Update global layout and traces\n",
    "fig.update_traces(marker=dict(size=1))\n",
    "fig.update_yaxes(\n",
    "    range=[\n",
    "        min(merged_pred_df[coverage_label]) - 0.01,\n",
    "        max(merged_pred_df[coverage_label]) + 0.01,\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickangle=50)\n",
    "\n",
    "fig.update_annotations(y=1.5, selector={\"text\": x_title})\n",
    "\n",
    "title = \"z-score(mean chrY coverage per file) distribution per assay+track type\"\n",
    "fig.update_layout(\n",
    "    title_text=f\"{title}\",\n",
    "    showlegend=False,\n",
    "    autosize=True,\n",
    "    width=3000,\n",
    ")\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "# for violin in fig.data:\n",
    "#     print(dir(violin))\n",
    "#     print(violin)\n",
    "#     break\n",
    "\n",
    "# filename = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "# html_file = matrix_logdir / f\"{filename}.html\"\n",
    "# png_file = matrix_logdir / f\"{filename}.png\"\n",
    "\n",
    "# fig.write_image(png_file, scale=3)\n",
    "# fig.write_html(html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### z-score violins - split sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_name = \"assay_track\"\n",
    "coverage_label = f\"expected_{partial_name}_chrY_z-score\"\n",
    "count_label = f\"count_{coverage_label}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_file = (\n",
    "    logs\n",
    "    / \"merged_results\"\n",
    "    / \"epiatlas\"\n",
    "    / \"with_split_nb\"\n",
    "    / \"merged_pred_results_all_2.1_chrY_zscores.csv\"\n",
    ")\n",
    "merged_pred_df = pd.read_csv(all_results_file, index_col=0, header=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cols = num_assay_track_combinations * 2 + len(merged_pred_df[TRACK].unique())\n",
    "\n",
    "to_verify_epirr = [\n",
    "    \"IHECRE00004623\",\n",
    "    \"IHECRE00000171\",\n",
    "    \"IHECRE00001957\",\n",
    "    \"IHECRE00000152\",\n",
    "    \"IHECRE00001531\",\n",
    "    \"IHECRE00000951\",\n",
    "    \"IHECRE00001965\",\n",
    "    \"IHECRE00000099\",\n",
    "    \"IHECRE00000316\",\n",
    "    \"IHECRE00004877\",\n",
    "    \"IHECRE00003706\",\n",
    "    \"IHECRE00001370\",\n",
    "    \"IHECRE00001001\",\n",
    "    \"IHECRE00000954\",\n",
    "    \"IHECRE00004890\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in merged_pred_df.columns if \"epirr\" in str(col).lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pred_df[\"epirr_no_version\"] = merged_pred_df[\"EpiRR\"].str.split(\".\").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pred_df[merged_pred_df[\"epirr_no_version\"].isin(to_verify_epirr)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epirr_cat = \"epirr_no_version\"\n",
    "black = \"#000000\"\n",
    "white = \"#FFFFFF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epirr_colors = dict(zip(to_verify_epirr, px.colors.qualitative.Dark24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_df.harmonized_donor_sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 1\n",
    "row = 1\n",
    "x_title = \"Assay+Track distribution\"\n",
    "fig = make_subplots(\n",
    "    rows=row,\n",
    "    cols=total_cols,\n",
    "    shared_yaxes=True,\n",
    "    x_title=x_title,\n",
    "    y_title=\"z-score\",\n",
    "    horizontal_spacing=0.001,\n",
    ")\n",
    "\n",
    "for assay_label in sorted(assays):\n",
    "    assay_df = sex_df[sex_df[ASSAY] == assay_label]\n",
    "\n",
    "    for track_type in sorted(assay_df[TRACK].unique()):\n",
    "        sub_df = assay_df[assay_df[TRACK] == track_type]\n",
    "\n",
    "        y_values = sub_df[coverage_label]\n",
    "        hovertext = np.array(\n",
    "            [\n",
    "                f\"{md5sum}:z-score={z_score:.3f} (n={int(count)})\"\n",
    "                for md5sum, z_score, count in zip(\n",
    "                    sub_df.index,\n",
    "                    sub_df[coverage_label],\n",
    "                    sub_df[count_label],\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        female_idx = np.argwhere((sub_df[SEX] == \"female\").values).flatten()\n",
    "        male_idx = np.argwhere((sub_df[SEX] == \"male\").values).flatten()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                y=y_values.values[female_idx],\n",
    "                name=f\"{assay_label},{track_type} ({sub_df.shape[0]})\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=\"all\",\n",
    "                line_color=\"red\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[female_idx],\n",
    "                side=\"negative\",\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                y=y_values.values[male_idx],\n",
    "                name=f\"{assay_label},{track_type} ({sub_df.shape[0]})\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=\"all\",\n",
    "                line_color=\"blue\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[male_idx],\n",
    "                side=\"positive\",\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "        col += 1\n",
    "\n",
    "        epirr_scatter_df = sub_df[sub_df[epirr_cat].isin(to_verify_epirr)].sort_values(\n",
    "            by=epirr_cat\n",
    "        )\n",
    "        print(epirr_scatter_df[epirr_cat])\n",
    "        print(epirr_scatter_df[ASSAY].value_counts())\n",
    "\n",
    "        if epirr_scatter_df.shape[1] != 0:\n",
    "            y_values = epirr_scatter_df[coverage_label]\n",
    "\n",
    "            hovertext = np.array(\n",
    "                [\n",
    "                    f\"{md5sum},{epirr},{sex}:z-score={z_score:.3f} (n={int(count)})\"\n",
    "                    for md5sum, z_score, count, epirr, sex in zip(\n",
    "                        epirr_scatter_df.index,\n",
    "                        epirr_scatter_df[coverage_label],\n",
    "                        epirr_scatter_df[count_label],\n",
    "                        epirr_scatter_df[epirr_cat],\n",
    "                        epirr_scatter_df[SEX],\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            unknown_sex_cond = epirr_scatter_df[SEX] == \"unknown\"\n",
    "            unknown_idx = np.argwhere(unknown_sex_cond.values).flatten()\n",
    "            known_idx = np.argwhere((~unknown_sex_cond).values).flatten()\n",
    "\n",
    "            x_pos = list(range(0, len(unknown_idx) + len(known_idx)))\n",
    "\n",
    "            colors = np.array(\n",
    "                [epirr_colors[epirr] for epirr in epirr_scatter_df[epirr_cat]]\n",
    "            )\n",
    "\n",
    "            if len(unknown_idx) != 0:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=x_pos[: len(unknown_idx)],\n",
    "                        y=y_values.values[unknown_idx],\n",
    "                        hovertemplate=\"%{text}\",\n",
    "                        text=hovertext[unknown_idx],\n",
    "                        marker=dict(color=colors[unknown_idx], symbol=\"diamond\"),\n",
    "                        mode=\"markers\",\n",
    "                    ),\n",
    "                    row=row,\n",
    "                    col=col,\n",
    "                )\n",
    "\n",
    "            if len(known_idx) != 0:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=x_pos[len(unknown_idx) :],\n",
    "                        y=y_values.values[known_idx],\n",
    "                        hovertemplate=\"%{text}\",\n",
    "                        text=hovertext[known_idx],\n",
    "                        marker=dict(color=colors[known_idx], symbol=\"circle\"),\n",
    "                        mode=\"markers\",\n",
    "                    ),\n",
    "                    row=row,\n",
    "                    col=col,\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            y_values = [0]\n",
    "            hovertext = [\"NO VALUE\"]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x_pos=[0],\n",
    "                    y=y_values,\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(color=white),\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "        col += 1\n",
    "\n",
    "\n",
    "# global track type distribution (all assays)\n",
    "for track_type in sorted(sex_df[TRACK].unique()):\n",
    "    sub_df = sex_df[sex_df[TRACK] == track_type]\n",
    "\n",
    "    y_values = sub_df[coverage_label]\n",
    "    hovertext = np.array(\n",
    "        [\n",
    "            f\"{md5sum,assay}:z-score={z_score:.3f} (n={int(count)})\"\n",
    "            for md5sum, z_score, count, assay in zip(\n",
    "                sub_df.index,\n",
    "                sub_df[coverage_label],\n",
    "                sub_df[count_label],\n",
    "                sub_df[ASSAY],\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    female_idx = np.argwhere((sub_df[SEX] == \"female\").values).flatten()\n",
    "    male_idx = np.argwhere((sub_df[SEX] == \"male\").values).flatten()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Violin(\n",
    "            y=y_values.values[female_idx],\n",
    "            name=f\"{track_type} ({sub_df.shape[0]})\",\n",
    "            box_visible=True,\n",
    "            meanline_visible=True,\n",
    "            points=\"all\",\n",
    "            line_color=\"red\",\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[female_idx],\n",
    "            side=\"negative\",\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Violin(\n",
    "            y=y_values.values[male_idx],\n",
    "            name=f\"{track_type} ({sub_df.shape[0]})\",\n",
    "            box_visible=True,\n",
    "            meanline_visible=True,\n",
    "            points=\"all\",\n",
    "            line_color=\"blue\",\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[male_idx],\n",
    "            side=\"positive\",\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    col += 1\n",
    "\n",
    "\n",
    "# Final graphing\n",
    "\n",
    "# Update global layout and traces\n",
    "fig.update_traces(marker=dict(size=1))\n",
    "fig.update_traces(marker=dict(size=5), selector=dict(mode=\"markers\"))\n",
    "fig.update_yaxes(\n",
    "    range=[\n",
    "        min(sex_df[coverage_label]) - 0.01,\n",
    "        max(sex_df[coverage_label]) + 0.01,\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickangle=50)\n",
    "\n",
    "fig.update_annotations(y=1.5, selector={\"text\": x_title})\n",
    "\n",
    "title = \"z-score(mean chrY coverage per file) distribution per assay+track type, female/male split\"\n",
    "fig.update_layout(\n",
    "    title_text=f\"{title}\",\n",
    "    showlegend=False,\n",
    "    autosize=True,\n",
    "    width=7000,\n",
    ")\n",
    "\n",
    "# filename = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "# html_file = matrix_logdir / f\"{filename}.html\"\n",
    "# fig.write_html(html_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"stop here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kde graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_graph_df = merged_pred_df[~merged_pred_df[ASSAY].str.contains(\"wgb\")]\n",
    "temp_graph_df = merged_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_title = \"z-score\"\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=2,\n",
    "    shared_yaxes=True,\n",
    "    x_title=x_title,\n",
    "    y_title=\"z-score density\",\n",
    "    horizontal_spacing=0.01,\n",
    ")\n",
    "\n",
    "x_values = np.linspace(min(temp_graph_df[coverage_label]), 8, 300)\n",
    "\n",
    "zscore_classifier_acc = []\n",
    "naive_classifier_answers = []\n",
    "for assay_label in sorted(assays):\n",
    "    assay_df = temp_graph_df[temp_graph_df[ASSAY] == assay_label]\n",
    "\n",
    "    for track_type in sorted(assay_df[TRACK].unique()):\n",
    "        sub_df = assay_df[assay_df[TRACK] == track_type]\n",
    "\n",
    "        z_scores = sub_df[coverage_label]\n",
    "\n",
    "        kde = gaussian_kde(z_scores, bw_method=\"silverman\")\n",
    "        density = kde(x_values)\n",
    "\n",
    "        name = f\"{assay_label},{track_type} (n={len(z_scores)})\"\n",
    "        hovertext = [f\"{assay_label}, {track_type}: {x_val}\" for x_val in x_values]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_values,\n",
    "                y=density,\n",
    "                mode=\"lines+markers\",\n",
    "                name=name,\n",
    "                line_color=assay_colors_dict[assay_label],\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext,\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        if track_type == \"fc\":\n",
    "            minimas_idx = find_peaks(density * -1)[0]\n",
    "            minima = x_values[minimas_idx][0]\n",
    "            sex_sub_df = sub_df[sub_df[SEX].str.contains(\"male\")]\n",
    "\n",
    "            delta = 0.05\n",
    "            lower_bound = minima - delta\n",
    "            upper_bound = minima + delta\n",
    "            print(f\"{name}: {minima:.3f}\" + \"\\u00b1\" + f\"{delta:.3f}\")\n",
    "\n",
    "            predicted_male = sex_sub_df[coverage_label] > upper_bound\n",
    "            predicted_female = sex_sub_df[coverage_label] < lower_bound\n",
    "\n",
    "            naive_classifier_label = f\"naive_sex_pred_delta{0.1}\"\n",
    "            new_df = pd.DataFrame(sex_sub_df[coverage_label])\n",
    "            new_df[naive_classifier_label] = np.nan\n",
    "\n",
    "            # Use `.loc` to update the original DataFrame based on conditions\n",
    "            new_df.loc[\n",
    "                new_df[coverage_label] > upper_bound, naive_classifier_label\n",
    "            ] = \"male\"\n",
    "            new_df.loc[\n",
    "                new_df[coverage_label] < lower_bound, naive_classifier_label\n",
    "            ] = \"female\"\n",
    "\n",
    "            naive_classifier_answers.append(new_df)\n",
    "\n",
    "            # known_male = sex_sub_df[SEX] == \"male\"\n",
    "            # known_female = sex_sub_df[SEX] == \"female\"\n",
    "\n",
    "            # correct_male = sum(predicted_male & known_male)\n",
    "            # correct_female = sum(predicted_female & known_female)\n",
    "            # acc_male = correct_male/known_male.sum()*100\n",
    "            # acc_female = correct_female/known_female.sum()*100\n",
    "            # tot_acc = (correct_male+correct_female)/(known_male.sum() + known_female.sum())*100\n",
    "\n",
    "            # print(f\"acc_male: {acc_male:.2f}%\")\n",
    "            # print(f\"acc_female: {acc_female:.2f}%\")\n",
    "            # nb_excluded = sum(sex_sub_df[coverage_label] > minima) - sum(predicted_male)\n",
    "            # nb_excluded += sum(sex_sub_df[coverage_label] < minima) - sum(predicted_female)\n",
    "\n",
    "            # zscore_classifier_acc.append([name, minima, acc_male, acc_female, tot_acc, nb_excluded])\n",
    "\n",
    "# df = pd.DataFrame(zscore_classifier_acc, columns=[\"name\", \"minima\", \"acc_male\", \"acc_female\", \"acc\", \"nb_excluded\"])\n",
    "# df.to_csv(matrix_logdir / \"chrY_classifier_acc_delta0.1.csv\")\n",
    "\n",
    "naive_classifier_df = pd.concat(naive_classifier_answers).sort_index()\n",
    "naive_classifier_df.to_csv(\n",
    "    matrix_logdir / \"chrY_naive_classifier_acc_delta0.1_predictions.csv\"\n",
    ")\n",
    "\n",
    "# global track type distribution (all assays)\n",
    "\n",
    "# for track_type in sorted(temp_graph_df[TRACK].unique()):\n",
    "#     sub_df = temp_graph_df[temp_graph_df[TRACK] == track_type]\n",
    "\n",
    "#     z_scores = sub_df[coverage_label]\n",
    "\n",
    "#     kde = gaussian_kde(z_scores, bw_method=\"silverman\")\n",
    "#     density = kde(x_values)\n",
    "\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#         x=x_values,\n",
    "#         y=density,\n",
    "#         mode=\"lines+markers\",\n",
    "#         name=f\"{track_type} (n={len(z_scores)})\",\n",
    "#         line_color=track_colors_dict[track_type]\n",
    "#         ),\n",
    "#         row=1,\n",
    "#         col=2,\n",
    "#     )\n",
    "\n",
    "\n",
    "# # Final graphing\n",
    "\n",
    "# fig.update_traces(marker=dict(size=1))\n",
    "\n",
    "# title = \"density(z-score(mean chrY)) per assay+track type\"\n",
    "# fig.update_layout(\n",
    "#     title_text=f\"{title}\",\n",
    "#     showlegend=True,\n",
    "#     autosize=True,\n",
    "#     width=2000,\n",
    "#     height=1000,\n",
    "# )\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "# Save graph\n",
    "# filename = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "# html_file = matrix_logdir / f\"{filename}.html\"\n",
    "# fig.write_html(html_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(naive_classifier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs for all assays mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_label = \"expected_assay_chrY_z-score\"\n",
    "count_label = f\"count_{coverage_label}\"\n",
    "\n",
    "classes = merged_pred_df[\"True class\"].unique()\n",
    "\n",
    "matrix_logdir = chrY_coverage_file = (\n",
    "    logs\n",
    "    / \"chrY_coverage_results\"\n",
    "    / \"10fold_valid\"\n",
    "    / \"z-score\"\n",
    "    / \"per_assay\"\n",
    "    / \"w-unknown\"\n",
    ")\n",
    "\n",
    "\n",
    "# confusion matrix for chrY z-score\n",
    "for threshold in [0, 0.7, 0.9]:\n",
    "    row = 1\n",
    "    col = 1\n",
    "    fig = make_subplots(\n",
    "        rows=3,\n",
    "        cols=3,\n",
    "        shared_yaxes=True,\n",
    "        x_title=\"Predicted class (nb of predictions)\",\n",
    "        y_title=\"z-score for expected assay\",\n",
    "        row_titles=list(classes),\n",
    "        column_titles=list(classes),\n",
    "        vertical_spacing=0.08,\n",
    "        horizontal_spacing=0.01,\n",
    "    )\n",
    "    threshold_df = merged_pred_df[merged_pred_df[\"Max pred\"] >= threshold]\n",
    "\n",
    "    title = (\n",
    "        f\"z-score(mean chrY coverage per file) - (pred>{threshold})<br>w fc/pval, no wgbs\"\n",
    "    )\n",
    "\n",
    "    filename = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "    html_file = matrix_logdir / f\"{filename}.html\"\n",
    "    png_file = matrix_logdir / f\"{filename}.png\"\n",
    "    if png_file.exists() or html_file.exists():\n",
    "        print(f\"{str(html_file)} Already exists, continuing...\")\n",
    "        continue\n",
    "\n",
    "    for label in classes:\n",
    "        df_label = threshold_df[threshold_df[\"True class\"] == label]\n",
    "\n",
    "        # Iterate over each target and add a violin plot for it\n",
    "        for target in classes:\n",
    "            sub_df = df_label[df_label[\"Predicted class\"] == target]\n",
    "\n",
    "            if sub_df.shape[0] == 0:\n",
    "                y_values = [threshold_df[coverage_label].mean()]  # Minimal synthetic data\n",
    "                sample_count = 0\n",
    "                hovertext = [\"PLACEHOLDER - NO DATA\"]\n",
    "            else:\n",
    "                y_values = sub_df[coverage_label]\n",
    "                hovertext = [\n",
    "                    f\"{md5sum, assay}:(z-score={z_score:.3f} (n={int(count)}), pred={pred:.3f})\"\n",
    "                    for md5sum, pred, z_score, count, assay in zip(\n",
    "                        sub_df.index,\n",
    "                        sub_df[\"Max pred\"],\n",
    "                        sub_df[coverage_label],\n",
    "                        sub_df[count_label],\n",
    "                        sub_df[ASSAY],\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    y=y_values,\n",
    "                    name=f\"{target} ({sub_df.shape[0]})\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=\"all\",\n",
    "                    line_color=COLORS_DICT[target],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "            # Move to the next subplot position\n",
    "            col += 1\n",
    "            if col > 3:\n",
    "                col = 1\n",
    "                row += 1\n",
    "\n",
    "    # Update global layout and traces\n",
    "    fig.update_traces(marker=dict(size=1))\n",
    "    fig.update_yaxes(\n",
    "        range=[\n",
    "            min(merged_pred_df[coverage_label]) - 0.01,\n",
    "            max(merged_pred_df[coverage_label]) + 0.01,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Directly using annotations param does not work with make_subplots\n",
    "    existing_annotations = fig.layout.annotations  # type: ignore\n",
    "    new_annotation = dict(\n",
    "        x=1.01,  # Position on the x-axis\n",
    "        y=0.5,  # Position on the y-axis\n",
    "        showarrow=False,  # Do not show arrow\n",
    "        text=\"Reference class\",  # The text you want to display\n",
    "        xref=\"paper\",  # 'x' coordinate is set in relative coordinates\n",
    "        yref=\"paper\",  # 'y' coordinate is set in relative coordinates\n",
    "        xanchor=\"left\",  # Text starts from the left of the x-coordinate\n",
    "        yanchor=\"middle\",  # Middle aligned vertically\n",
    "        font=dict(size=16),\n",
    "        textangle=90,\n",
    "    )\n",
    "    updated_annotations = list(existing_annotations) + [new_annotation]\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{title} (n={threshold_df.shape[0]})\",\n",
    "        showlegend=False,\n",
    "        annotations=updated_annotations,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # fig.write_image(png_file, scale=2)\n",
    "    # fig.write_html(html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with global results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file_dir = logs / \"merged_results\"\n",
    "target_file = target_file_dir / \"merged_pred_results_all_2.1_chrY_zscores.csv\"\n",
    "\n",
    "target_df = pd.read_csv(target_file, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_df[SEX].str.contains(\"male\")\n",
    "sum((target_df[TRACK] == \"fc\") & (target_df[SEX].str.contains(\"male|female\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_target_df = target_df.merge(\n",
    "    naive_classifier_df,\n",
    "    how=\"outer\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    suffixes=(\"\", \"_DROP\"),\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_cols = []\n",
    "for column in updated_target_df.columns:\n",
    "    if column.endswith(\"_DROP\"):\n",
    "        same_cols.append(column[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in same_cols:\n",
    "    assert np.isclose(\n",
    "        0, (updated_target_df[column] - updated_target_df[f\"{column}_DROP\"]).sum()\n",
    "    ), f\"{column} not equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_target_df = updated_target_df.drop(\n",
    "    columns=[column for column in updated_target_df.columns if column.endswith(\"_DROP\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_target_df.to_csv(\n",
    "    target_file_dir / \"merged_pred_results_all_2.1_chrY_zscores_w_naive.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
