{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Top SHAP features: details the following\n",
    "\n",
    "- Features unique to ChIP raw tracks\n",
    "- Features coming from small file subsets (<10)\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=duplicate-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from epiclass.utils.notebooks.paper.paper_utilities import EPIATLAS_16_CT, MetadataHandler\n",
    "from epiclass.utils.shap.subset_features_handling import (\n",
    "    collect_features_from_feature_count_file,\n",
    ")\n",
    "\n",
    "BIOMATERIAL_TYPE = \"harmonized_biomaterial_type\"\n",
    "CELL_TYPE = \"harmonized_sample_ontology_intermediate\"\n",
    "ASSAY = \"assay_epiclass\"\n",
    "SEX = \"harmonized_donor_sex\"\n",
    "CANCER = \"harmonized_sample_cancer_high\"\n",
    "DISEASE = \"harmonized_sample_disease_high\"\n",
    "LIFE_STAGE = \"harmonized_donor_life_stage\"\n",
    "TRACK = \"track_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_label(label: str):\n",
    "    \"\"\"Format strings to lowercase and replaces spaces/hyphen with underscore.\"\"\"\n",
    "    return label.lower().replace(\" \", \"_\").replace(\"-\", \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell type top SHAP features details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_dir = Path.home() / \"projects/epiclass/output/paper\"\n",
    "if not paper_dir.exists():\n",
    "    raise ValueError(f\"{paper_dir} does not exist.\")\n",
    "\n",
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "meta_df = metadata_handler.load_metadata_df(\"v2\", merge_assays=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[CELL_TYPE] = meta_df[CELL_TYPE].apply(format_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cts = [format_label(label) for label in EPIATLAS_16_CT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(meta_df[CELL_TYPE].unique()) & set(cts)) == 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_df = meta_df[meta_df[CELL_TYPE].isin(cts)]\n",
    "print(meta_df.shape, cell_type_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all possible files subsets, and seek folder names with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smallest subsets\n",
    "meta_keys = cell_type_df.groupby([ASSAY, TRACK, CELL_TYPE]).size().index\n",
    "\n",
    "# Need to create keys for the \"ALL\" subsets\n",
    "all_assay_celltype = cell_type_df.groupby([ASSAY, CELL_TYPE]).size().index\n",
    "\n",
    "all_keys_with_all = [\n",
    "    (assay, \"ALL\", cell_type) for (assay, cell_type) in all_assay_celltype\n",
    "]\n",
    "\n",
    "all_keys = list(set(meta_keys) | set(all_keys_with_all))\n",
    "print(len(meta_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map folder_name â†’ all relevant (assay, track, cell_type)\n",
    "folder_map = defaultdict(list)\n",
    "for assay, track, cell_type in all_keys:\n",
    "    if pd.isna(track) or track.upper() == \"ALL\":\n",
    "        folder_name = assay  # No track suffix\n",
    "    else:\n",
    "        folder_name = f\"{assay}_{track}\"\n",
    "    folder_map[folder_name].append((assay, track, cell_type))\n",
    "\n",
    "folder_map[\"mixed_samples\"] = [(\"ALL\", \"ALL\", cell_type) for cell_type in cts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature count per cell type class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.home() / \"scratch/epiclass/join_important_features\"\n",
    "feature_count_general_dir = (\n",
    "    base_path\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / \"harmonized_sample_ontology_intermediate_1l_3000n\"\n",
    "    / \"10fold-oversampling\"\n",
    "    / \"global_shap_analysis\"\n",
    ")\n",
    "\n",
    "top303_dir = feature_count_general_dir / \"top303\"\n",
    "\n",
    "ignore = set([\"fc\", \"raw\", \"pval\", \"mixed_samples\"])\n",
    "\n",
    "\n",
    "all_features_per_class = defaultdict(set)\n",
    "features_per_class = defaultdict(set)\n",
    "for folder in sorted(top303_dir.iterdir()):\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    feature_count_path = folder / \"feature_count.json\"\n",
    "    if not feature_count_path.exists():\n",
    "        print(f\"File {feature_count_path} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    features: Dict[str, List[int]] = collect_features_from_feature_count_file(\n",
    "        feature_count_path, n=8\n",
    "    )\n",
    "\n",
    "    if not features:\n",
    "        print(f\"No features passing threshold found in {folder.name}\")\n",
    "        continue\n",
    "\n",
    "    for class_label, class_features in features.items():\n",
    "        all_features_per_class[class_label].update(class_features)\n",
    "\n",
    "    if any(l in folder.name for l in ignore):\n",
    "        print(f\"Skipping folder {folder.name}\")\n",
    "        continue\n",
    "\n",
    "    print(folder.name)\n",
    "    for class_label, class_features in features.items():\n",
    "        features_per_class[class_label].update(class_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_features_count = pd.DataFrame(dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_label, class_features in all_features_per_class.items():\n",
    "    df_top_features_count.loc[class_label, \"count_all\"] = len(class_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_label, class_features in features_per_class.items():\n",
    "    df_top_features_count.loc[class_label, \"count_limited\"] = len(class_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_features_count.fillna(0, inplace=True)\n",
    "\n",
    "df_top_features_count[\"diff\"] = (\n",
    "    df_top_features_count[\"count_all\"] - df_top_features_count[\"count_limited\"]\n",
    ")\n",
    "display(df_top_features_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_features_count.to_csv(\n",
    "    feature_count_general_dir / \"unique_features_count_union.csv\",\n",
    "    index_label=\"class_label\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features unique to ChIP raw tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_chip_raw_features_per_class = defaultdict(set)\n",
    "chip_raw_features_per_class = defaultdict(set)\n",
    "for folder in sorted(top303_dir.iterdir()):\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    feature_count_path = folder / \"feature_count.json\"\n",
    "    if not feature_count_path.exists():\n",
    "        print(f\"File {feature_count_path} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    features: Dict[str, List[int]] = collect_features_from_feature_count_file(\n",
    "        feature_count_path, n=8\n",
    "    )\n",
    "\n",
    "    if not features:\n",
    "        print(f\"No features passing threshold found in {folder.name}\")\n",
    "        continue\n",
    "\n",
    "    if \"h3\" in folder.name and folder.name.endswith(\"_raw\"):\n",
    "        for class_label, class_features in features.items():\n",
    "            chip_raw_features_per_class[class_label].update(class_features)\n",
    "    else:\n",
    "        for class_label, class_features in features.items():\n",
    "            not_chip_raw_features_per_class[class_label].update(class_features)\n",
    "\n",
    "    print(folder.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_raw_count = defaultdict(int)\n",
    "for class_label in not_chip_raw_features_per_class:\n",
    "    raw_features = set(chip_raw_features_per_class[class_label])\n",
    "    not_raw_features = set(not_chip_raw_features_per_class[class_label])\n",
    "\n",
    "    raw_specific_features = raw_features - not_raw_features\n",
    "    N_unique = len(raw_specific_features)\n",
    "    N_total = len(raw_features | not_raw_features)\n",
    "    print(\n",
    "        f\"{class_label}: {len(raw_specific_features)}/{N_total} = {N_unique/N_total:.2%} features unique to raw tracks\"\n",
    "    )\n",
    "\n",
    "    unique_raw_count[class_label] = N_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associate feature count and subset size for all combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_count_dict = {}\n",
    "\n",
    "for folder in sorted(top303_dir.iterdir()):\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    folder_name = folder.name\n",
    "    if folder_name not in folder_map:\n",
    "        print(f\"Unknown folder: {folder_name}\")\n",
    "        continue\n",
    "\n",
    "    feature_count_path = folder / \"feature_count.json\"\n",
    "    if not feature_count_path.exists():\n",
    "        continue\n",
    "\n",
    "    print(folder_name)\n",
    "    features = collect_features_from_feature_count_file(feature_count_path, n=8)\n",
    "    features = {format_label(k): v for k, v in features.items()}\n",
    "    if not features:\n",
    "        continue\n",
    "\n",
    "    for assay, track, cell_type in folder_map[folder_name]:\n",
    "        count = len(features.get(cell_type, []))\n",
    "        feature_count_dict[(assay, track, cell_type)] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the index is a proper MultiIndex for feature_count_df\n",
    "feature_count_df = pd.DataFrame.from_dict(\n",
    "    feature_count_dict, orient=\"index\", columns=[\"feature_count\"]\n",
    ")\n",
    "\n",
    "# MultiIndex needs to have same names as other df for merge\n",
    "feature_count_df.index = pd.MultiIndex.from_tuples(\n",
    "    feature_count_df.index, names=[ASSAY, TRACK, CELL_TYPE]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build smallest subsets file count df\n",
    "ct_count_df = cell_type_df.groupby([ASSAY, TRACK, CELL_TYPE]).size()\n",
    "\n",
    "# Need to create keys for the track=\"ALL\" subsets\n",
    "all_count_df = cell_type_df.groupby([ASSAY, CELL_TYPE]).size()\n",
    "all_count_df.index = pd.MultiIndex.from_tuples(\n",
    "    [(assay, \"ALL\", cell_type) for assay, cell_type in all_count_df.index],\n",
    "    names=[ASSAY, TRACK, CELL_TYPE],\n",
    ")\n",
    "\n",
    "# Add mixed_samples-style (assay=\"ALL\", track=\"ALL\") rows\n",
    "mixed_count_df = cell_type_df.groupby([CELL_TYPE]).size()\n",
    "mixed_count_df.index = pd.MultiIndex.from_tuples(\n",
    "    [(\"ALL\", \"ALL\", cell_type) for cell_type in mixed_count_df.index],\n",
    "    names=[ASSAY, TRACK, CELL_TYPE],\n",
    ")\n",
    "\n",
    "file_count_df = pd.concat(\n",
    "    [\n",
    "        ct_count_df.rename(\"file_count\").to_frame(),\n",
    "        all_count_df.rename(\"file_count\").to_frame(),\n",
    "        mixed_count_df.rename(\"file_count\").to_frame(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "merged_df = feature_count_df.join(file_count_df, how=\"outer\").fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(feature_count_general_dir / \"feature_count_per_subset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
