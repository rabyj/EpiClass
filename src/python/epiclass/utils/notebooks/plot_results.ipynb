{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create plots of some results.\"\"\"\n",
    "# pylint: disable=import-error,redefined-outer-name, singleton-comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All classifiers performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path.home() / \"downloads\" / \"temp\" / \"all_metrics - Pivot Table 1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_df(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Create a dataframe from the csv file.\"\"\"\n",
    "    metrics_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Fill missing values in the 'classifier' column using forward fill method\n",
    "    metrics_df[\"classifier\"] = metrics_df[\"classifier\"].fillna(method=\"ffill\")\n",
    "\n",
    "    # Rename columns to match the desired format\n",
    "    metrics_df.rename(\n",
    "        columns={\n",
    "            \"classifier\": \"Classifier\",\n",
    "            \"metric\": \"Metric\",\n",
    "            \"AVERAGE of value\": \"Average\",\n",
    "            \"STDEV of value\": \"Std\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifiers_performance(metrics_df: pd.DataFrame) -> None:\n",
    "    \"\"\"Plot the performance of multiple classifiers.\"\"\"\n",
    "    # Set the figure size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Create a bar plot without error bars\n",
    "    barplot = sns.barplot(\n",
    "        data=metrics_df, x=\"Classifier\", y=\"Average\", hue=\"Metric\", errorbar=None\n",
    "    )\n",
    "\n",
    "    # Get the x and y coordinates of the bars\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    for rect in barplot.patches:\n",
    "        x_coords.append(rect.get_x() + rect.get_width() / 2)\n",
    "        y_coords.append(rect.get_height())\n",
    "\n",
    "    # Calculate the number of metrics and classifiers to determine the positions of the error bars\n",
    "    num_metrics = metrics_df[\"Metric\"].nunique()\n",
    "    num_classifiers = metrics_df[\"Classifier\"].nunique()\n",
    "\n",
    "    # Add the error bars\n",
    "    for i in range(num_classifiers):\n",
    "        for j in range(num_metrics):\n",
    "            barplot.errorbar(\n",
    "                x_coords[i * num_metrics + j],\n",
    "                y_coords[i * num_metrics + j],\n",
    "                yerr=metrics_df[\"Std\"][i * num_metrics + j],\n",
    "                color=\"black\",\n",
    "                capsize=3,\n",
    "                fmt=\"none\",\n",
    "            )\n",
    "\n",
    "    # Set the y-axis limits center the value distribution\n",
    "    plt.ylim(min(y_coords) - 0.025, min(max(y_coords) + 0.025, 1))  # type: ignore\n",
    "\n",
    "    # Scale the y-axis to percentage\n",
    "    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "\n",
    "    plt.title(\"Classifier Performance\")\n",
    "    plt.ylabel(\"Performance\")\n",
    "\n",
    "    # Move the legend outside the plot\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple hdf5s versions over a certain classification task - using 10 split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path.home() / \"downloads\" / \"temp\"\n",
    "input_data_path = input_dir / \"cometml_logs_recent.csv\"\n",
    "df = pd.read_csv(input_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_categories = sorted(\n",
    "    list(df[df[\"Server start time\"] > 1.7e12][\"category\"].unique())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in desired_categories:\n",
    "    print(f\"Category: {category}\")\n",
    "    print(df[df[\"category\"] == category][\"output_size\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_SIZES = [11, 5, 3, 2, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertinent_df = df[df[\"category\"].isin(desired_categories)]\n",
    "pertinent_df = pertinent_df[pertinent_df[\"oversampling\"] == True]\n",
    "# pertinent_df = pertinent_df.iloc[:, :-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes: \n",
    "- add 100kb_all_none equivalent tasks\n",
    "- find a way to include the filter information AND the input size.\n",
    "- to get a first idea, just throw everything in with input size + filter_name + resolution, as the label\n",
    "- Then, consider making manual groups using filter names, color by resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_task_metrics(df: pd.DataFrame, category: str, output_dir: Path) -> None:\n",
    "    \"\"\"Graph the metrics of a task.\"\"\"\n",
    "    for metric in [\"val_Accuracy\", \"val_F1Score\"]:\n",
    "        label_order = [\n",
    "            \"all\",\n",
    "            \"global_tasks_union\",\n",
    "            \"random_n4510\",\n",
    "            \"global_tasks_intersection\",\n",
    "            \"random_n118\",\n",
    "        ]\n",
    "        fig = px.box(\n",
    "            df,\n",
    "            x=\"HDF5 filter\",\n",
    "            y=metric,\n",
    "            title=f\"{category}: {metric}\",\n",
    "            points=\"all\",\n",
    "            category_orders={\n",
    "                \"HDF5 filter\": label_order,\n",
    "                \"HDF5 Resolution\": [\"1.0kb\", \"10.0kb\", \"100.0kb\"],\n",
    "            },\n",
    "            color=\"HDF5 Resolution\",\n",
    "            color_discrete_sequence=px.colors.qualitative.Safe,\n",
    "            width=800,\n",
    "            height=800,\n",
    "        )\n",
    "        fig.update_traces(boxmean=True)\n",
    "        fig.write_html(output_dir / f\"{category}_{metric}.html\")\n",
    "        fig.write_image(output_dir / f\"{category}_{metric}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_desired_base_name(x: str, category: str):\n",
    "    \"\"\"Return a representative base name from a full classification task name.\"\"\"\n",
    "    re_str = (\n",
    "        r\"hg38_\\d+kb_(.*)_none-\"\n",
    "        + f\"{category}_1l_3000n\"\n",
    "        + r\"-10fold-oversampl\\w+-split\\d{1}\"\n",
    "    )\n",
    "    m = re.search(re_str, x)\n",
    "    if m is None:\n",
    "        re_str = f\"({category}).*\"\n",
    "        m = re.search(re_str, x)\n",
    "    return m.group(1)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_task_metrics_naive(df: pd.DataFrame, category: str, output_dir: Path) -> None:\n",
    "    \"\"\"Graph the metrics of a task.\"\"\"\n",
    "    df[\"base_name\"] = df[\"Name\"].apply(lambda x: re_desired_base_name(x, category))\n",
    "    df[\"graph_label\"] = df[\"input_size\"].astype(str).str.cat(df[\"base_name\"], sep=\"|\")\n",
    "    label_order = sorted(\n",
    "        df[\"graph_label\"], key=lambda x: (int(x.split(\"|\")[0]), x.split(\"|\")[1])\n",
    "    )\n",
    "\n",
    "    for metric in [\"val_Accuracy\", \"val_F1Score\"]:\n",
    "        fig = px.box(\n",
    "            df,\n",
    "            x=\"graph_label\",\n",
    "            y=metric,\n",
    "            title=f\"{category}: {metric}\",\n",
    "            points=\"all\",\n",
    "            color=\"HDF5 Resolution\",\n",
    "            color_discrete_sequence=px.colors.qualitative.Safe,\n",
    "            category_orders={\n",
    "                \"graph_label\": label_order,\n",
    "                \"HDF5 Resolution\": [\"1.0kb\", \"10.0kb\", \"100.0kb\"],\n",
    "            },\n",
    "            width=800,\n",
    "            height=800,\n",
    "        )\n",
    "        fig.update_traces(boxmean=True)\n",
    "        fig.write_html(output_dir / f\"{category}_{metric}.html\")\n",
    "        fig.write_image(output_dir / f\"{category}_{metric}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path.home() / \"downloads\" / \"temp\" / \"output\" / \"naive\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "for category, output_size in zip(desired_categories, OUTPUT_SIZES):\n",
    "    cat_df = pertinent_df[\n",
    "        (pertinent_df[\"category\"] == category)\n",
    "        & (pertinent_df[\"output_size\"].astype(int) == output_size)\n",
    "    ]\n",
    "    # cat_training_names = cat_df[\"Name\"].tolist()\n",
    "    # if len(cat_training_names) not in [50, 60]:\n",
    "    #     print(category, output_size)\n",
    "    #     print(category, len(cat_training_names))\n",
    "    #     print(cat_training_names)\n",
    "\n",
    "    # assert cat_df[\"Included tracks\"].nunique() == 1\n",
    "\n",
    "    # cat_df[\"base_name\"] = cat_df[\"Name\"].apply(lambda x: x.rsplit(\"-\", 1)[0])\n",
    "    # display(cat_df[\"base_name\"].tolist())\n",
    "    # print(cat_df[\"Name\"].tolist())\n",
    "\n",
    "    graph_task_metrics_naive(cat_df, category, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
