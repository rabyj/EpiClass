{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot accuracy, precision, and subset size for different probability thresholds.\"\"\"\n",
    "\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, pointless-statement, use-dict-literal, expression-not-assigned, unused-import, too-many-lines, too-many-branches, duplicate-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from PIL import ImageColor\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "\n",
    "from epiclass.utils.general_utility import (\n",
    "    get_valid_filename,  # pylint: disable=unused-import\n",
    ")\n",
    "from epiclass.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    BIOMATERIAL_TYPE,\n",
    "    CANCER,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    LIFESTAGE_REMAPPER,\n",
    "    SEX,\n",
    "    SplitResultsHandler,\n",
    "    filter_biomat_LS,\n",
    "    find_target_recall,\n",
    "    format_labels,\n",
    "    merge_life_stages,\n",
    "    rename_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_seq = Union[np.typing.NDArray[np.floating], Sequence[Union[float, np.floating]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "paper_dir = base_dir\n",
    "if not paper_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {paper_dir} does not exist.\")\n",
    "\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "table_dir = paper_dir / \"tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core7_assays = ASSAY_ORDER[:7]\n",
    "core9_assays = ASSAY_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_individual_tasks = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence threshold impact on accuracy\n",
    "\n",
    "Covers Supp Fig 1H,I and Supp Fig 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB: {\"results\": dict, \"other_info\": dict}\n",
    "all_threshold_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing and co. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    threshold: float,\n",
    "    true_col: str,\n",
    "    pred_col: str,\n",
    "    pred_prob_cols: List[str],\n",
    "    target_class: str | None,\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, and subset size for a given probability threshold and class.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the true labels, predicted labels, and predicted probabilities.\n",
    "    threshold (float): The probability threshold for filtering the DataFrame.\n",
    "    true_col (str): The column name containing the true labels.\n",
    "    pred_col (str): The column name containing the predicted labels.\n",
    "    pred_prob_cols (List[str]): List of column names containing the predicted probabilities.\n",
    "    target_class (str|None): The class for which precision is to be calculated. Return np.nan if None.\n",
    "\n",
    "    Considers target class for computations if given, otherwise considers all samples.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, float, float, float]: A tuple containing the threshold, the calculated accuracy (%), the calculated precision (%),\n",
    "                                       and the subset size (%) respectively.\n",
    "    \"\"\"\n",
    "    # Targeting a class or not\n",
    "    if target_class in [None, \"all\"]:\n",
    "        total_size = len(df)\n",
    "    else:\n",
    "        total_size = len(df[true_col] == target_class)\n",
    "\n",
    "    # Filter rows where the max predicted probability is above the threshold\n",
    "    try:\n",
    "        subset_df = df[df[pred_prob_cols].max(axis=1) >= threshold]\n",
    "    except TypeError as e:\n",
    "        print(\n",
    "            f\"Error: Could not filter rows.\\npred_cols: {pred_prob_cols}\\nthreshold: {threshold}\"\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "    if len(subset_df) == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    # Calculate the accuracy for this subset\n",
    "    if target_class in [None, \"all\"]:\n",
    "        correct_preds = np.sum(subset_df[true_col] == subset_df[pred_col])\n",
    "        subset_size = len(subset_df)\n",
    "    else:\n",
    "        correct_preds = np.sum(\n",
    "            (subset_df[true_col] == subset_df[pred_col])\n",
    "            & (subset_df[true_col] == target_class)\n",
    "        )\n",
    "        subset_size = np.sum(subset_df[true_col] == target_class)\n",
    "    accuracy = (correct_preds / subset_size) * 100\n",
    "    subset_size_percent = (subset_size / total_size) * 100\n",
    "\n",
    "    # Calculate precision for the target class\n",
    "    if target_class in [None, \"all\"]:\n",
    "        precision = np.nan\n",
    "        return threshold, accuracy, precision, subset_size_percent\n",
    "\n",
    "    true_positives = np.sum(\n",
    "        (subset_df[true_col] == target_class) & (subset_df[pred_col] == target_class)\n",
    "    )\n",
    "    false_positives = np.sum(\n",
    "        (subset_df[true_col] != target_class) & (subset_df[pred_col] == target_class)\n",
    "    )\n",
    "\n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = np.nan\n",
    "    else:\n",
    "        precision = (true_positives / (true_positives + false_positives)) * 100\n",
    "\n",
    "    return threshold, accuracy, precision, subset_size_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_global(\n",
    "    df: pd.DataFrame,\n",
    "    threshold: float | np.floating,\n",
    "    true_col: str,\n",
    "    pred_col: str,\n",
    "    pred_prob_cols: List[str],\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, and subset size for a given probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the true labels, predicted labels, and predicted probabilities.\n",
    "    threshold (float): The probability threshold for filtering the DataFrame.\n",
    "    true_col (str): The column name containing the true labels.\n",
    "    pred_col (str): The column name containing the predicted labels.\n",
    "    pred_prob_cols (List[str]|str): List of column names containing the predicted probabilities.\n",
    "                                OR a Max PredScore column\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, float, float, float, int]: A tuple containing the threshold, the accuracy (%), the macro f1-score (%) and the subset size (%) respectively.\n",
    "    \"\"\"\n",
    "    total_size = len(df)\n",
    "\n",
    "    # Filter rows where the max predicted probability is above the threshold\n",
    "    # Normally expecting a matrix of probabilities\n",
    "    # But can deal with a Max PredScore column\n",
    "    if isinstance(pred_prob_cols, str):\n",
    "        pred_prob_cols = [pred_prob_cols]\n",
    "    try:\n",
    "        subset_df = df[df[pred_prob_cols].max(axis=1) >= threshold]\n",
    "    except TypeError as e:\n",
    "        print(\n",
    "            f\"Error: Could not filter rows.\\npred_cols: {pred_prob_cols}\\nthreshold: {threshold}\"\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "    N = len(subset_df)\n",
    "    if N == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    # Metrics\n",
    "    existing_labels = sorted(df[true_col].unique())\n",
    "    acc: float = accuracy_score(subset_df[true_col], subset_df[pred_col])  # type: ignore\n",
    "    f1: float = f1_score(subset_df[true_col], subset_df[pred_col], average=\"macro\", labels=existing_labels)  # type: ignore\n",
    "    relative_size = N / total_size\n",
    "\n",
    "    return float(threshold), acc, f1, relative_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCURACY_NAME = \"rec\"\n",
    "PRECISION_NAME = \"prec\"\n",
    "SUBSET_SIZE_NAME = \"sz\"\n",
    "\n",
    "\n",
    "def find_columns(df: pd.DataFrame, verbose: bool = False) -> Dict[str, List[str] | str]:\n",
    "    \"\"\"\n",
    "    Find the columns containing true labels, predicted labels, and predicted probabilities in a DataFrame.\n",
    "    \"\"\"\n",
    "    df_cols = df.columns\n",
    "    df_cols = [col for col in df_cols if str(col) not in [\"TRUE\", \"FALSE\"]]\n",
    "\n",
    "    likely_true_class_cols = [\n",
    "        col for col in df_cols if \"true\" in col.lower() or \"expected\" in col.lower()\n",
    "    ]\n",
    "    likely_pred_class_cols = [col for col in df_cols if \"pred\" in col.lower()]\n",
    "\n",
    "    if not likely_true_class_cols or not likely_pred_class_cols:\n",
    "        raise ValueError(\n",
    "            \"Could not automatically detect 'True class' or 'Predicted class' columns.\"\n",
    "        )\n",
    "\n",
    "    true_col = likely_true_class_cols[0]\n",
    "    pred_col = likely_pred_class_cols[0]\n",
    "    if df[true_col].dtype != object or df[pred_col].dtype != object:\n",
    "        print(f\"{true_col} and {pred_col} are not string columns. Could cause issues.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"True class: {true_col}\")\n",
    "        print(f\"Predicted class: {pred_col}\")\n",
    "\n",
    "    classes = df[true_col].unique().tolist() + [\"all\"]\n",
    "    pred_prob_cols = classes[0:-1]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Classes: {classes}\")\n",
    "        print(f\"Predicted probability columns: {pred_prob_cols}\")\n",
    "\n",
    "    for col in pred_prob_cols:\n",
    "        if df[col].dtype != float:\n",
    "            print(f\"{col} is not a float column ({df[col].dtype}). Could cause issues.\")\n",
    "\n",
    "    return {\n",
    "        \"True\": true_col,\n",
    "        \"Predicted\": pred_col,\n",
    "        \"classes\": classes,\n",
    "        \"pred_prob_cols\": pred_prob_cols,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_thresholds(\n",
    "    df: pd.DataFrame, thresholds: List[float], verbose: bool = False\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy and subset size for different probability thresholds with improved automatic column detection.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe containing true labels and predicted probabilities.\n",
    "    thresholds (list): List of probability thresholds to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe containing the accuracy and subset size for each threshold.\n",
    "    \"\"\"\n",
    "    columns = find_columns(df, verbose=verbose)\n",
    "    true_col: str = columns[\"True\"]  # type: ignore\n",
    "    pred_col: str = columns[\"Predicted\"]  # type: ignore\n",
    "    classes: List[str] = columns[\"classes\"]  # type: ignore\n",
    "    pred_prob_cols: List[str] = columns[\"pred_prob_cols\"]  # type: ignore\n",
    "\n",
    "    # Evaluate each threshold over each class\n",
    "    results_dfs = {}\n",
    "    for class_label in classes:\n",
    "        results = []\n",
    "        filtered_df = (\n",
    "            df\n",
    "            if class_label == \"all\"\n",
    "            else df[(df[true_col] == class_label) | (df[pred_col] == class_label)]\n",
    "        )\n",
    "\n",
    "        for thresh in thresholds:\n",
    "            try:\n",
    "                result = compute_metrics(\n",
    "                    filtered_df,\n",
    "                    thresh,\n",
    "                    true_col,\n",
    "                    pred_col,\n",
    "                    pred_prob_cols,\n",
    "                    target_class=class_label,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Error. Could not compute metric with class {class_label}.\\ntrue_col: {true_col}\\npred_col: {pred_col}\\npred_prob_cols: {pred_prob_cols}\\n\"\n",
    "                )\n",
    "                raise e\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "        # Convert to DataFrame for easier manipulation\n",
    "        short_class_label = class_label[0:10]\n",
    "        results_df = pd.DataFrame(\n",
    "            results,\n",
    "            columns=[\n",
    "                \"Threshold\",\n",
    "                f\"{ACCURACY_NAME}_{short_class_label} (%)\",\n",
    "                f\"{PRECISION_NAME}_{short_class_label} (%)\",\n",
    "                f\"{SUBSET_SIZE_NAME}_{short_class_label} (%) ({filtered_df.shape[0]})\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        results_dfs[class_label] = results_df\n",
    "\n",
    "    return results_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_thresholds_global(\n",
    "    df: pd.DataFrame,\n",
    "    thresholds: float_seq,\n",
    "    verbose: bool = False,\n",
    "    columns: Dict[str, List[str] | str] | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy and subset size for different probability thresholds with improved automatic column detection.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe containing true labels and predicted probabilities.\n",
    "        thresholds (list): List of probability thresholds to evaluate.\n",
    "        verbose (bool): Whether to print verbose information.\n",
    "        columns (dict): A dictionary containing the column names for true labels, predicted labels, and predicted probabilities.\n",
    "                        Expecting entries: \"True\", \"Predicted\", \"pred_prob_cols\"|\"Max pred\".\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing the accuracy and subset size for each threshold.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = find_columns(df, verbose=verbose)\n",
    "        true_col: str = columns[\"True\"]  # type: ignore\n",
    "        pred_col: str = columns[\"Predicted\"]  # type: ignore\n",
    "        pred_prob_cols: List[str] = columns[\"pred_prob_cols\"]  # type: ignore\n",
    "    else:\n",
    "        true_col: str = columns[\"True\"]  # type: ignore\n",
    "        pred_col: str = columns[\"Predicted\"]  # type: ignore\n",
    "        try:\n",
    "            pred_prob_cols: List[str] = columns[\"pred_prob_cols\"]  # type: ignore\n",
    "        except KeyError:\n",
    "            pred_prob_cols: List[str] = [columns[\"Max pred\"]]  # type: ignore\n",
    "\n",
    "    # Evaluate each threshold over each class\n",
    "    results = []\n",
    "    for tresh in thresholds:\n",
    "        try:\n",
    "            result = compute_metrics_global(df, tresh, true_col, pred_col, pred_prob_cols)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error. Could not compute metrics.\\ntrue_col: {true_col}\\npred_col: {pred_col}\\npred_prob_cols: {pred_prob_cols}\\n\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    results_df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\n",
    "            \"Threshold\",\n",
    "            \"Accuracy (%)\",\n",
    "            \"F1-score\",\n",
    "            f\"Subset size (%) ({df.shape[0]})\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thresholds_graph_global_plotly(\n",
    "    metrics_df: pd.DataFrame,\n",
    "    name: str,\n",
    "    xrange: Tuple[float, float] | None = None,\n",
    "    yrange: Tuple[float, float] | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Return graph of the accuracy and subset size at different probability thresholds for global results.\n",
    "\n",
    "    Parameters:\n",
    "    metrics_df (pd.DataFrame): DataFrame with metrics at different probability thresholds.\n",
    "    name (str): Graph title.\n",
    "\n",
    "    Returns:\n",
    "    go.Figure: Plotly figure object with the plotted graph.\n",
    "    \"\"\"\n",
    "    # color-blind friendly\n",
    "    # black, blue, red\n",
    "    colors = [\"#000000\", \"#005AB5\", \"#DC3220\"]\n",
    "    marker1 = \"square-open\"\n",
    "    marker2 = \"cross-open\"\n",
    "    marker3 = \"circle\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    acc_label = metrics_df.filter(like=\"Acc\").columns[0]\n",
    "    f1_score_label = metrics_df.filter(like=\"F1\").columns[0]\n",
    "    subset_size_label = metrics_df.filter(like=\"Subset\").columns[0]\n",
    "\n",
    "    # Plot accuracy\n",
    "    vals = metrics_df[acc_label]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=metrics_df[\"Threshold\"],\n",
    "            y=vals * 100,\n",
    "            name=acc_label,\n",
    "            line=dict(color=colors[2]),\n",
    "            marker_symbol=marker1,\n",
    "            mode=\"lines+markers\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot f1_score\n",
    "    vals = metrics_df[f1_score_label]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=metrics_df[\"Threshold\"],\n",
    "            y=vals * 100,\n",
    "            name=f1_score_label,\n",
    "            line=dict(color=colors[1], dash=\"dot\"),\n",
    "            marker_symbol=marker2,\n",
    "            mode=\"lines+markers\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot subset size on secondary Y-axis\n",
    "    vals = metrics_df[subset_size_label]\n",
    "    min_y2 = vals.min()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=metrics_df[\"Threshold\"],\n",
    "            y=vals * 100,\n",
    "            name=subset_size_label.split(\"(\")[0].strip(),\n",
    "            line=dict(color=colors[0], dash=\"dash\"),\n",
    "            marker_symbol=marker3,\n",
    "            yaxis=\"y2\",\n",
    "            mode=\"lines+markers\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Adjusting the layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Metrics at Different Pred. Score Thresholds<br>{name}\",\n",
    "        xaxis_title=\"Prediction Score Threshold\",\n",
    "        xaxis=dict(\n",
    "            tickvals=np.linspace(0, 1, 11),\n",
    "            ticktext=[f\"{x:.1f}\" for x in np.linspace(0, 1, 11)],\n",
    "        ),\n",
    "        yaxis_title=\"Performance (%)\",\n",
    "        yaxis2=dict(title=\"Subset Size (%)\", overlaying=\"y\", side=\"right\"),\n",
    "        yaxis2_range=[min_y2 * 100 - 0.2, 100.2],\n",
    "    )\n",
    "\n",
    "    # Additional formatting\n",
    "    fig.update_layout(\n",
    "        legend=dict(orientation=\"v\", x=1.1, y=1),\n",
    "        height=600,\n",
    "        width=700,\n",
    "    )\n",
    "\n",
    "    if not xrange:\n",
    "        xrange = (-0.001, 1.001)\n",
    "    fig.update_xaxes(range=xrange)\n",
    "\n",
    "    if yrange:\n",
    "        fig.update_yaxes(range=yrange)\n",
    "\n",
    "    fig.update_traces(line={\"width\": 1})\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thresholds_graph_plotly(threshold_dfs: Dict[str, pd.DataFrame], name: str):\n",
    "    \"\"\"\n",
    "    Return graph of the accuracy and subset size at different probability thresholds for all classes.\n",
    "\n",
    "    Parameters:\n",
    "    threshold_metrics_df (Dict[str, pd.DataFrame]): A dictionary containing dfs with metrics for each class label and the general case.\n",
    "    name (str): Graph title.\n",
    "\n",
    "    Returns:\n",
    "    go.Figure: Plotly figure object with the plotted graph.\n",
    "    \"\"\"\n",
    "    colors = px.colors.qualitative.Dark24\n",
    "    marker1 = \"circle\"\n",
    "    marker2 = \"cross-open\"\n",
    "    marker3 = \"circle-open\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for idx, (_, threshold_metrics) in enumerate(threshold_dfs.items()):\n",
    "        color = colors[idx % len(colors)]\n",
    "\n",
    "        acc_label = threshold_metrics.filter(like=f\"{ACCURACY_NAME}\").columns[0]\n",
    "        acc_subset = threshold_metrics.filter(like=f\"{SUBSET_SIZE_NAME}\").columns[0]\n",
    "        prec_label = threshold_metrics.filter(like=f\"{PRECISION_NAME}\").columns[0]\n",
    "\n",
    "        # Plot accuracy\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=threshold_metrics[\"Threshold\"],\n",
    "                y=threshold_metrics[acc_label],\n",
    "                name=acc_label,\n",
    "                line=dict(color=color),\n",
    "                marker_symbol=marker1,\n",
    "                mode=\"lines+markers\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Plot precision\n",
    "        prec_vals = threshold_metrics[prec_label]\n",
    "        if not prec_vals.isna().all():\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=threshold_metrics[\"Threshold\"],\n",
    "                    y=prec_vals,\n",
    "                    name=prec_label,\n",
    "                    line=dict(color=color, dash=\"dot\"),\n",
    "                    marker_symbol=marker2,\n",
    "                    mode=\"lines+markers\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Plot subset size on secondary Y-axis\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=threshold_metrics[\"Threshold\"],\n",
    "                y=threshold_metrics[acc_subset],\n",
    "                name=acc_subset,\n",
    "                line=dict(color=color, dash=\"dash\"),\n",
    "                marker_symbol=marker3,\n",
    "                yaxis=\"y2\",\n",
    "                mode=\"lines+markers\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Adjusting the layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Accuracy and Subset Size at Different Probability Thresholds<br>{name}\",\n",
    "        xaxis_title=\"Probability Threshold\",\n",
    "        xaxis=dict(\n",
    "            tickvals=np.linspace(0, 1, 11),\n",
    "            ticktext=[f\"{x:.1f}\" for x in np.linspace(0, 1, 11)],\n",
    "        ),\n",
    "        yaxis_title=\"Accuracy (%)\",\n",
    "        yaxis2=dict(title=\"Subset Size (%)\", overlaying=\"y\", side=\"right\"),\n",
    "        legend=dict(orientation=\"v\", x=1.05, y=1),\n",
    "        height=1000,\n",
    "        width=1600,\n",
    "    )\n",
    "    fig.update_xaxes(range=[-0.001, 1.001])\n",
    "    fig.update_traces(line={\"width\": 1})\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds: List[float] = [float(x) for x in np.arange(0, 1, 1 / 20)] + [0.99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP EpiAtlas cross-validation results (Includes Supp Fig 1H,I + Supp Fig 6A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_remapper = {\n",
    "    \"assay\": ASSAY,\n",
    "    \"assay7\": ASSAY,\n",
    "    f\"{ASSAY}_11c\": ASSAY,\n",
    "    ASSAY: ASSAY,\n",
    "    \"sex\": SEX,\n",
    "    \"sex3\": SEX,\n",
    "    SEX: SEX,\n",
    "    \"harmonized_donor_sex_w-mixed\": SEX,\n",
    "    \"cancer\": CANCER,\n",
    "    CANCER: CANCER,\n",
    "    \"biomat\": BIOMATERIAL_TYPE,\n",
    "    BIOMATERIAL_TYPE: BIOMATERIAL_TYPE,\n",
    "}\n",
    "\n",
    "for l in [\"donorlife\", \"lifestage\", LIFE_STAGE]:\n",
    "    category_remapper[l] = LIFE_STAGE\n",
    "    category_remapper[f\"{l}_merged\"] = LIFE_STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    ASSAY,\n",
    "    CELL_TYPE,\n",
    "    SEX,\n",
    "    LIFE_STAGE,\n",
    "    BIOMATERIAL_TYPE,\n",
    "    CANCER,\n",
    "]\n",
    "split_results_handler = SplitResultsHandler()\n",
    "\n",
    "data_dir_100kb = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10-fold oversampling runs\n",
    "all_split_dfs = split_results_handler.general_split_metrics(\n",
    "    results_dir=data_dir_100kb,\n",
    "    merge_assays=False,\n",
    "    include_categories=categories,\n",
    "    exclude_names=[\"reg\", \"no-mixed\", \"chip\", \"16ct\", \"27ct\", \"7c\"],\n",
    "    return_type=\"split_results\",\n",
    "    oversampled_only=True,\n",
    "    verbose=False,\n",
    ")\n",
    "all_split_dfs_concat: Dict = split_results_handler.concatenate_split_results(all_split_dfs, concat_first_level=True)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_life_stages fct was created for fully merged results dataframes (multiple classification tasks)\n",
    "# it does not work here, must do manually\n",
    "lifestage_df = all_split_dfs_concat[LIFE_STAGE]\n",
    "for col in [\"True class\", \"Predicted class\"]:\n",
    "    lifestage_df.loc[:, col] = lifestage_df[col].map(LIFESTAGE_REMAPPER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing all values separately from graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_dfs = {}\n",
    "# for task_name, df in all_split_dfs_concat.items():\n",
    "#     print(\"TASK:\", task_name)\n",
    "#     threshold_dfs[task_name] = evaluate_thresholds(df, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = base_fig_dir / \"threshold_graphs\" / \"100kb_all_none\"\n",
    "# if not output_dir.exists():\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# if plot_individual_tasks:\n",
    "#     for task_name, df in all_split_dfs_concat.items():\n",
    "#         print(\"TASK:\", task_name)\n",
    "#         nb_samples = len(df)\n",
    "#         nb_classes = df[\"True class\"].nunique()\n",
    "\n",
    "#         df = threshold_dfs[task_name]\n",
    "\n",
    "#         # create figure\n",
    "#         name = f\"{task_name} - {nb_classes} classes\"\n",
    "#         fig = create_thresholds_graph_plotly(df, f\"{name} - n={nb_samples}\")\n",
    "#         fig.show()\n",
    "\n",
    "#         # # save\n",
    "#         # filename = f\"threshold_impact_graph_full_{get_valid_filename(name)}\".replace(\n",
    "#         #     \"_-_\", \"-\"\n",
    "#         # )\n",
    "#         # fig.write_image(output_dir / f\"{filename}.png\")\n",
    "#         # fig.write_image(output_dir / f\"{filename}.svg\")\n",
    "#         # fig.write_html(output_dir / f\"{filename}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average/global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_dfs = {}\n",
    "other_info = {}\n",
    "\n",
    "columns_lifestage = {\n",
    "    \"True\": \"True class\",\n",
    "    \"Predicted\": \"Predicted class\",\n",
    "    \"pred_prob_cols\": lifestage_df.columns.tolist()[\n",
    "        2 : 2 + 5\n",
    "    ],  # 5 classes, 3 labels, workaround\n",
    "}\n",
    "\n",
    "for task_name, df in all_split_dfs_concat.items():\n",
    "    print(\"TASK:\", task_name)\n",
    "    nb_samples = len(df)\n",
    "    nb_classes = df[\"True class\"].nunique()\n",
    "\n",
    "    other_info[task_name] = {\n",
    "        \"nb_samples\": nb_samples,\n",
    "        \"nb_classes\": nb_classes,\n",
    "        \"total_possible_classes\": nb_classes,  # all classes present in EpiATLAS, by training definition\n",
    "    }\n",
    "\n",
    "    columns = None\n",
    "    if task_name == LIFE_STAGE:\n",
    "        columns = columns_lifestage\n",
    "\n",
    "    threshold_dfs[task_name] = evaluate_thresholds_global(df, thresholds, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = base_fig_dir / \"threshold_graphs\" / \"100kb_all_none\" / \"EpiATLAS\"\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if plot_individual_tasks:\n",
    "    for task_name, df in all_split_dfs_concat.items():\n",
    "        print(\"TASK:\", task_name)\n",
    "        nb_samples = len(df)\n",
    "        nb_classes = df[\"True class\"].nunique()\n",
    "\n",
    "        df = threshold_dfs[task_name]\n",
    "\n",
    "        # create figure\n",
    "        name = f\"{task_name} - {nb_classes} classes\"\n",
    "        fig = create_thresholds_graph_global_plotly(\n",
    "            df, f\"{name} - n={nb_samples}\", xrange=(0.1, 1.01)\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "        # # save\n",
    "        # filename = f\"threshold_impact_graph_global_{get_valid_filename(name)}\".replace(\n",
    "        #     \"_-_\", \"-\"\n",
    "        # )\n",
    "        # fig.write_image(output_dir / f\"{filename}.png\")\n",
    "        # fig.write_image(output_dir / f\"{filename}.svg\")\n",
    "        # fig.write_html(output_dir / f\"{filename}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename / drop classifier metrics for future graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [f\"{ASSAY}_7c\", \"project\", \"paired_end\"]:\n",
    "    threshold_dfs.pop(label, None)\n",
    "\n",
    "for name in list(threshold_dfs.keys()):\n",
    "    try:\n",
    "        new_name = category_remapper[name]\n",
    "    except KeyError:\n",
    "        # Undesired category for rest\n",
    "        del threshold_dfs[name]\n",
    "        continue\n",
    "\n",
    "    threshold_dfs[new_name] = threshold_dfs.pop(name)\n",
    "    other_info[new_name] = other_info.pop(name)\n",
    "\n",
    "all_threshold_results[\"EpiATLAS\"] = {\"results\": threshold_dfs, \"other_info\": other_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_threshold_results[\"EpiATLAS\"][\"results\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE, ChIP-Atlas and recount3 inference results (Supp Fig 6B-E)\n",
    "\n",
    "Careful, the graphed metrics are overwritten for each subsection (`pred_df` is overwritten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'other'/'unknown' are too undefined, we exclude from life stage predictions\n",
    "cell_line_vals = [\"cell_line\", \"cell line\", \"unknown\", \"other\"]\n",
    "\n",
    "unknown_values = [\"unknown\", \"other\", \"indeterminate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dir = table_dir / \"dfreeze_v2\" / \"predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not apply `life stage classifier` on `cell line` samples because it was not part of the training data,\n",
    "and the notion of life stage for a cell line is dubious. \n",
    "\n",
    "Also, we merge `perinatal stages` public DB inference (embryonic, fetal, newborn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_category_labels(\n",
    "    df: pd.DataFrame, categories: List[str], verbose: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Uniformize class labels for each category labels Uses `format_labels` function.\"\"\"\n",
    "    to_format = []\n",
    "    for col in df.columns:\n",
    "        cond1 = any(category in col.lower() for category in categories)\n",
    "        cond2 = any(l in col.lower() for l in [\"true\", \"expected\", \"predicted\"])\n",
    "        if cond1 and (cond2 or col in categories):\n",
    "            if verbose:\n",
    "                print(f\"Formatting {col}\")\n",
    "            to_format.append(col)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Formatting {len(to_format)} columns.\")\n",
    "\n",
    "    df = format_labels(\n",
    "        df=df,\n",
    "        columns=to_format,\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(df, col_templates, predScore_threshold, category):\n",
    "    \"\"\"Print classification report for a given category.\"\"\"\n",
    "    thresh = round(predScore_threshold, 4)\n",
    "\n",
    "    true_colname = col_templates[\"True\"].format(category)\n",
    "    pred_colname = col_templates[\"Predicted\"].format(category)\n",
    "    max_pred_colname = col_templates[\"Max pred\"].format(category)\n",
    "\n",
    "    sub_df = df[df[max_pred_colname] > thresh]\n",
    "\n",
    "    true = sub_df[true_colname]\n",
    "    pred = sub_df[pred_colname]\n",
    "\n",
    "    report = classification_report(\n",
    "        y_true=true,\n",
    "        y_pred=pred,\n",
    "        labels=sorted(pred.unique()),\n",
    "        digits=4,\n",
    "    )\n",
    "\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChIP-Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = (\n",
    "    predictions_dir / \"ChIP-Atlas_predictions_20240606_merge_metadata_freeze1.csv.xz\"\n",
    ")\n",
    "pred_df = pd.read_csv(preds_path, sep=\",\", low_memory=False, compression=\"xz\")\n",
    "print(pred_df.shape)\n",
    "\n",
    "pred_df.fillna(\"unknown\", inplace=True)\n",
    "pred_df.replace(\"indeterminate\", \"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "    col\n",
    "    for col in pred_df.columns\n",
    "    if any(l in col.lower() for l in [\"disease\", \"assay11\", \"assay13\"])\n",
    "]\n",
    "pred_df = pred_df.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df[pred_df[\"is_EpiAtlas_EpiRR\"].astype(str) == \"0\"]\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df[\n",
    "    ~pred_df[\"core7_DBs_consensus\"].isin(\n",
    "        [\"Ignored - Potential non-core\", \"non-core/CTCF\"]\n",
    "    )\n",
    "]\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[BIOMATERIAL_TYPE] = pred_df[\"expected_biomat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replace = {\n",
    "    \"sex3\": \"sex\",\n",
    "    \"assay7\": \"assay\",\n",
    "    \"donorlife\": \"lifestage\",\n",
    "}\n",
    "pred_df = rename_columns(\n",
    "    df=pred_df,\n",
    "    remapper=to_replace,\n",
    "    exact_match=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_templates = {\n",
    "    \"True\": \"expected_{}\",\n",
    "    \"Predicted\": \"Predicted_class_{}\",\n",
    "    \"Max pred\": \"Max_pred_{}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "categories = [\"assay\", \"sex\", \"cancer\", \"biomat\"]\n",
    "\n",
    "pred_df = format_category_labels(pred_df, categories + [\"lifestage\"], verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = merge_life_stages(\n",
    "    df=pred_df,\n",
    "    lifestage_column_name=\"lifestage\",\n",
    "    column_name_templates=list(col_templates.values()),\n",
    "    verbose=verbose,\n",
    ")\n",
    "\n",
    "to_drop = [template.format(LIFE_STAGE) for template in col_templates.values()]\n",
    "pred_df = pred_df.drop(columns=to_drop, errors=\"ignore\")\n",
    "\n",
    "categories.extend([\"lifestage_merged\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomat_thresh, recall = find_target_recall(\n",
    "    df=pred_df,\n",
    "    category_name=\"biomat\",\n",
    "    class_of_interest=\"cell_line\",\n",
    "    target_recall=0.9,\n",
    "    col_templates=col_templates,\n",
    "    verbose=False,\n",
    "    minimum_threshold=0.5,\n",
    ")  # type: ignore\n",
    "\n",
    "print(f\"Found Threshold: {biomat_thresh:4f}\\nat Recall: {recall:4f}\\n\")\n",
    "biomat_thresh: float = round(biomat_thresh, 4)\n",
    "\n",
    "# sanity check\n",
    "print_report(pred_df, col_templates, biomat_thresh, \"biomat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_dfs = {}\n",
    "other_info = {}\n",
    "\n",
    "for category in categories:\n",
    "    print(\"TASK:\", category)\n",
    "    col_mapper = {k: v.format(category) for k, v in col_templates.items()}\n",
    "\n",
    "    df = pred_df.copy()\n",
    "\n",
    "    # Filter unknown/NA\n",
    "    df = df[~df[col_mapper[\"True\"]].isin(unknown_values)]\n",
    "\n",
    "    if category == \"assay\":\n",
    "        df = pred_df[pred_df[col_mapper[\"True\"]].isin(ASSAY_ORDER[0:7])]\n",
    "\n",
    "    elif category == \"lifestage_merged\":\n",
    "        df = filter_biomat_LS(\n",
    "            df=df,\n",
    "            biomaterial_cat_name=\"biomat\",\n",
    "            col_templates=col_templates,\n",
    "            predScore_threshold=biomat_thresh,\n",
    "            verbose=False,\n",
    "        )\n",
    "        if verbose:\n",
    "            for cats in [\"assay\", \"biomat\", \"lifestage_merged\"]:\n",
    "                print(\n",
    "                    df[col_templates[\"True\"].format(cats)].value_counts(dropna=False),\n",
    "                    \"\\n\",\n",
    "                )\n",
    "\n",
    "    cat_name = category_remapper[category]\n",
    "    print(\"Using category name:\", cat_name)\n",
    "\n",
    "    nb_samples = df.shape[0]\n",
    "    N_true_classes = df[col_mapper[\"True\"]].nunique()\n",
    "    total_N_classes = len(set(df[col_mapper[\"Predicted\"]]) | set(df[col_mapper[\"True\"]]))\n",
    "    other_info[cat_name] = {\n",
    "        \"nb_samples\": nb_samples,\n",
    "        \"nb_classes\": N_true_classes,\n",
    "        \"total_possible_classes\": total_N_classes,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        for col in [col_mapper[\"True\"], col_mapper[\"Predicted\"]]:\n",
    "            print(df[col].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "    threshold_dfs[cat_name] = evaluate_thresholds_global(\n",
    "        df, thresholds, verbose=verbose, columns=col_mapper  # type: ignore\n",
    "    )\n",
    "\n",
    "all_threshold_results[\"ChIP-Atlas\"] = {\"results\": threshold_dfs, \"other_info\": other_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_individual_tasks:\n",
    "    for cat_name in categories:\n",
    "        task_name = category_remapper[cat_name]\n",
    "        print(\"TASK:\", task_name)\n",
    "\n",
    "        df = threshold_dfs[task_name]\n",
    "\n",
    "        nb_samples = other_info[task_name][\"nb_samples\"]\n",
    "        nb_classes = other_info[task_name][\"nb_classes\"]\n",
    "\n",
    "        # create figure\n",
    "        name = f\"{task_name} - {nb_classes} classes\"\n",
    "        fig = create_thresholds_graph_global_plotly(\n",
    "            df,\n",
    "            f\"{name} - n={nb_samples}\",\n",
    "            xrange=(0.1, 1.01),\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENCODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = predictions_dir / \"encode_predictions_merge_metadata_2025-02_freeze1.csv.xz\"\n",
    "\n",
    "pred_df = pd.read_csv(preds_path, sep=\",\", low_memory=False, compression=\"xz\")\n",
    "print(pred_df.shape)\n",
    "\n",
    "pred_df.fillna(\"unknown\", inplace=True)\n",
    "pred_df.replace(\"indeterminate\", \"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "    col\n",
    "    for col in pred_df.columns\n",
    "    if any(\n",
    "        l in col.lower()\n",
    "        for l in [\"disease\", \"assay_epiclass_7c\", \"assay13\", \"biospecimen\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "pred_df = pred_df.drop(columns=to_drop)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(pred_df.columns):\n",
    "    if \"11c\" in col:\n",
    "        new_col = col.replace(\"assay_epiclass_11c\", \"assay_epiclass\")\n",
    "        pred_df = pred_df.rename(columns={col: new_col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df[~pred_df[\"in_epiatlas\"]]\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_templates = {\n",
    "    \"True\": \"{}\",\n",
    "    \"Predicted\": \"Predicted class ({})\",\n",
    "    \"Max pred\": \"Max pred ({})\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = []\n",
    "categories = [ASSAY, SEX, CANCER, BIOMATERIAL_TYPE]\n",
    "\n",
    "pred_df = format_category_labels(pred_df, categories + [LIFE_STAGE], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = merge_life_stages(\n",
    "    df=pred_df,\n",
    "    lifestage_column_name=LIFE_STAGE,\n",
    "    column_name_templates=list(col_templates.values()),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "to_drop = [template.format(LIFE_STAGE) for template in col_templates.values()]\n",
    "pred_df = pred_df.drop(columns=to_drop, errors=\"ignore\")\n",
    "\n",
    "categories.extend([f\"{LIFE_STAGE}_merged\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "threshold_dfs_core = {}\n",
    "threshold_dfs_noncore = {}\n",
    "other_info_core = {}\n",
    "other_info_noncore = {}\n",
    "\n",
    "for category in categories:\n",
    "    print(\"TASK:\", category)\n",
    "    col_mapper = {k: v.format(category) for k, v in col_templates.items()}\n",
    "\n",
    "    df: pd.DataFrame = pred_df.copy()  # type: ignore\n",
    "\n",
    "    # Filter unknown/NA\n",
    "    df = df[~(df[col_mapper[\"True\"]].isin(unknown_values))]\n",
    "\n",
    "    # Merge rna / wgbs pairs\n",
    "    if category == ASSAY:\n",
    "        true, pred = col_mapper[\"True\"], col_mapper[\"Predicted\"]\n",
    "        df.loc[:, [true, pred]] = df.loc[:, [true, pred]].replace(\n",
    "            ASSAY_MERGE_DICT, inplace=False\n",
    "        )\n",
    "    elif category == f\"{LIFE_STAGE}_merged\":\n",
    "        df = df[~df[BIOMATERIAL_TYPE].isin(cell_line_vals)]\n",
    "        if verbose:\n",
    "            print(\"Biomaterial type, lifestage and assay post cell line filter:\")\n",
    "            for col in [BIOMATERIAL_TYPE, ASSAY, f\"{LIFE_STAGE}_merged\"]:\n",
    "                print(\n",
    "                    df[col_templates[\"True\"].format(col)].value_counts(dropna=False),\n",
    "                    \"\\n\",\n",
    "                )\n",
    "\n",
    "    # split core/non-core\n",
    "    df.loc[:, ASSAY] = df.loc[:, ASSAY].replace(ASSAY_MERGE_DICT, inplace=False)\n",
    "    mask = df[ASSAY].isin(core9_assays)\n",
    "\n",
    "    df_core = df[mask]\n",
    "    df_noncore = df[~mask]\n",
    "\n",
    "    # Compute all thresholds\n",
    "    cat_name = category_remapper[category]\n",
    "    for name, container_results, container_other_info, set_df in zip(\n",
    "        [\"core\", \"noncore\"],\n",
    "        [threshold_dfs_core, threshold_dfs_noncore],\n",
    "        [other_info_core, other_info_noncore],\n",
    "        [df_core, df_noncore],\n",
    "    ):\n",
    "        if cat_name == ASSAY and \"ctcf\" in set_df[ASSAY].unique():\n",
    "            if verbose:\n",
    "                print(\"\\nSkipping assay non-core\\n\")\n",
    "            continue\n",
    "\n",
    "        nb_samples = set_df.shape[0]\n",
    "        N_true_classes = set_df[col_mapper[\"True\"]].nunique()\n",
    "        total_N_classes = len(\n",
    "            set(set_df[col_mapper[\"Predicted\"]]) | set(set_df[col_mapper[\"True\"]])\n",
    "        )\n",
    "        container_other_info[cat_name] = {\n",
    "            \"nb_samples\": nb_samples,\n",
    "            \"nb_classes\": N_true_classes,\n",
    "            \"total_possible_classes\": total_N_classes,\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Set: {name}\")\n",
    "            for col in [col_mapper[\"True\"], col_mapper[\"Predicted\"]]:\n",
    "                print(set_df[col].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "        container_results[cat_name] = evaluate_thresholds_global(\n",
    "            set_df, thresholds, verbose=False, columns=col_mapper  # type: ignore\n",
    "        )\n",
    "\n",
    "all_threshold_results[\"ENCODE_core\"] = {\n",
    "    \"results\": threshold_dfs_core,\n",
    "    \"other_info\": other_info_core,\n",
    "}\n",
    "all_threshold_results[\"ENCODE_non-core\"] = {\n",
    "    \"results\": threshold_dfs_noncore,\n",
    "    \"other_info\": other_info_noncore,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_individual_tasks:\n",
    "    for set_name in [\"ENCODE_core\", \"ENCODE_non-core\"]:\n",
    "        print(f\"Set: {set_name}\")\n",
    "\n",
    "        all_dfs = all_threshold_results[set_name][\"results\"]\n",
    "        other_info = all_threshold_results[set_name][\"other_info\"]\n",
    "\n",
    "        for cat_name in categories:\n",
    "            task_name = category_remapper[cat_name]\n",
    "            print(\"TASK:\", task_name)\n",
    "\n",
    "            try:\n",
    "                df = all_dfs[task_name]\n",
    "            except KeyError:\n",
    "                print(f\"Missing {set_name}: {task_name}\")\n",
    "                continue\n",
    "\n",
    "            nb_samples = other_info[task_name][\"nb_samples\"]\n",
    "            nb_classes = other_info[task_name][\"nb_classes\"]\n",
    "\n",
    "            # create figure\n",
    "            name = f\"{task_name} - {nb_classes} classes\"\n",
    "            fig = create_thresholds_graph_global_plotly(\n",
    "                df,\n",
    "                f\"{name} - n={nb_samples}\",\n",
    "                xrange=(max(0, 1.0 / nb_classes - 0.05), 1.001),\n",
    "            )\n",
    "            fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recount3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = predictions_dir / \"recount3_merged_preds_metadata_freeze1.csv.xz\"\n",
    "pred_df = pd.read_csv(preds_path, sep=\",\", low_memory=False, compression=\"xz\")\n",
    "print(pred_df.shape)\n",
    "\n",
    "pred_df.fillna(\"unknown\", inplace=True)\n",
    "pred_df.replace(\"indeterminate\", \"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replace = {\n",
    "    \"expected_lifestage\": LIFE_STAGE,\n",
    "    \"expected_assay\": ASSAY,\n",
    "    \"expected_cancer\": CANCER,\n",
    "    \"expected_biomat\": BIOMATERIAL_TYPE,\n",
    "    \"expected_sex\": SEX,\n",
    "}\n",
    "pred_df = rename_columns(\n",
    "    df=pred_df,\n",
    "    remapper=to_replace,\n",
    "    exact_match=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_templates = {\n",
    "    \"True\": \"{}\",\n",
    "    \"Predicted\": \"Predicted class ({})\",\n",
    "    \"Max pred\": \"Max pred ({})\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    ASSAY,\n",
    "    SEX,\n",
    "    CANCER,\n",
    "    BIOMATERIAL_TYPE,\n",
    "]\n",
    "lifestage_categories = [LIFE_STAGE, f\"{LIFE_STAGE}_merged\"]\n",
    "pred_df = format_category_labels(pred_df, categories + lifestage_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = merge_life_stages(\n",
    "    df=pred_df,\n",
    "    lifestage_column_name=LIFE_STAGE,\n",
    "    column_name_templates=list(col_templates.values()),\n",
    "    verbose=True,\n",
    "    exact_replace=True,\n",
    ")\n",
    "\n",
    "to_drop = [template.format(LIFE_STAGE) for template in col_templates.values()]\n",
    "pred_df = pred_df.drop(columns=to_drop, errors=\"ignore\")\n",
    "\n",
    "categories.extend([f\"{LIFE_STAGE}_merged\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_pred_col = col_templates[\"Predicted\"].format(ASSAY)\n",
    "assay_max_pred_col = col_templates[\"Max pred\"].format(ASSAY)\n",
    "\n",
    "# Only keep \"similar to training\" dsets\n",
    "# Predicted as m/rna-seq by assay classifier with high-pred (>0.6)\n",
    "cond1 = pred_df[assay_pred_col].isin([\"rna_seq\", \"mrna_seq\"])\n",
    "cond2 = pred_df[assay_max_pred_col] > 0.6\n",
    "print(pred_df.shape)\n",
    "filtered_df = pred_df[cond1 & cond2]\n",
    "print(filtered_df.shape)\n",
    "\n",
    "filtered_df_ids = set(filtered_df[\"ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomat_thresh, recall = find_target_recall(\n",
    "    df=filtered_df,\n",
    "    category_name=BIOMATERIAL_TYPE,\n",
    "    class_of_interest=\"cell_line\",\n",
    "    target_recall=0.9,\n",
    "    col_templates=col_templates,\n",
    "    verbose=False,\n",
    "    minimum_threshold=0.7,\n",
    "    iterations=300,\n",
    ")  # type: ignore\n",
    "\n",
    "print(f\"Found Threshold: {biomat_thresh:4f}\\nat Recall: {recall:4f}\\n\")\n",
    "biomat_thresh: float = round(biomat_thresh, 4)\n",
    "print(biomat_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print_report(filtered_df, col_templates, biomat_thresh, BIOMATERIAL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_pred_col = f\"Predicted class ({ASSAY})\"\n",
    "assay_max_pred_col = f\"Max pred ({ASSAY})\"\n",
    "\n",
    "verbose = True\n",
    "\n",
    "threshold_dfs = {}\n",
    "other_info = {}\n",
    "\n",
    "# for category in [f\"{LIFE_STAGE}_merged\"]:\n",
    "for category in categories:\n",
    "    print(\"TASK:\", category)\n",
    "    col_mapper = {k: v.format(category) for k, v in col_templates.items()}\n",
    "\n",
    "    df = pred_df.copy()\n",
    "\n",
    "    # Filter unknown/NA\n",
    "    df = df[~df[col_mapper[\"True\"]].isin(unknown_values)]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Know labels distribution:\")\n",
    "        print(df[col_mapper[\"True\"]].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "    if category == ASSAY:\n",
    "        pred_col = col_mapper[\"Predicted\"]\n",
    "        df.loc[:, pred_col] = df.loc[:, pred_col].replace(ASSAY_MERGE_DICT, inplace=False)\n",
    "\n",
    "        # All supposed to be rna-seq-like assays\n",
    "        true_col = col_mapper[\"True\"]\n",
    "        df.loc[:, true_col] = \"rna_seq\"\n",
    "    else:\n",
    "        new_ids = list(set(df[\"ID\"]) & filtered_df_ids)\n",
    "        df = df[df[\"ID\"].isin(new_ids)]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"All labels distribution after 11c filter:\")\n",
    "        print(df[col_mapper[\"True\"]].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "    if category == f\"{LIFE_STAGE}_merged\":\n",
    "        df = filter_biomat_LS(\n",
    "            df=df,\n",
    "            biomaterial_cat_name=BIOMATERIAL_TYPE,\n",
    "            col_templates=col_templates,\n",
    "            predScore_threshold=biomat_thresh,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    cat_name = category_remapper[category]\n",
    "\n",
    "    nb_samples = df.shape[0]\n",
    "    N_true_classes = df[col_mapper[\"True\"]].nunique()\n",
    "    total_N_classes = len(set(df[col_mapper[\"Predicted\"]]) | set(df[col_mapper[\"True\"]]))\n",
    "    other_info[cat_name] = {\n",
    "        \"nb_samples\": nb_samples,\n",
    "        \"nb_classes\": N_true_classes,\n",
    "        \"total_possible_classes\": total_N_classes,\n",
    "    }\n",
    "\n",
    "    threshold_dfs[cat_name] = evaluate_thresholds_global(\n",
    "        df, thresholds, verbose=verbose, columns=col_mapper  # type: ignore\n",
    "    )\n",
    "\n",
    "all_threshold_results[\"recount3\"] = {\"results\": threshold_dfs, \"other_info\": other_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_individual_tasks:\n",
    "    for cat_name in categories:\n",
    "        task_name = category_remapper[cat_name]\n",
    "\n",
    "        print(\"TASK:\", task_name)\n",
    "        df = threshold_dfs[task_name]\n",
    "        nb_samples = other_info[task_name][\"nb_samples\"]\n",
    "        nb_classes = other_info[task_name][\"nb_classes\"]\n",
    "\n",
    "        # create figure\n",
    "        if \"assay\" in task_name:\n",
    "            xrange = (0.1, 1.001)\n",
    "        else:\n",
    "            xrange = (max(0, 1.0 / nb_classes - 0.05), 1.001)\n",
    "        name = f\"{task_name} - {nb_classes} classes\"\n",
    "        fig = create_thresholds_graph_global_plotly(\n",
    "            df, f\"{name} - n={nb_samples}\", xrange=xrange\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph results for training and inference per database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2hex(r, g, b):\n",
    "    \"\"\"Convert rgb to hex.\"\"\"\n",
    "    return f\"#{r:02x}{g:02x}{b:02x}\"\n",
    "\n",
    "\n",
    "def hex2rgb(hex_str):\n",
    "    \"\"\"Convert hex to rgb.\"\"\"\n",
    "    return ImageColor.getrgb(hex_str)\n",
    "\n",
    "\n",
    "def add_acc_f1(\n",
    "    fig: go.Figure,\n",
    "    df: pd.DataFrame,\n",
    "    row: int,\n",
    "    col: int,\n",
    "    colors: List[str],\n",
    "    show_legend: bool = True,\n",
    "    label_modifier: str = \"\",\n",
    "    color_mod: int = 0,\n",
    ") -> None:\n",
    "    \"\"\"Add accuracy and F1 to the figure.\n",
    "\n",
    "    Args:\n",
    "        fig: The figure to add the traces to.\n",
    "        df: The dataframe containing the data.\n",
    "        row: The row of the subplot. (1 indexed)\n",
    "        col: The column of the subplot. (1 indexd)\n",
    "        colors: The colors to use for the traces (1 for accuracy, 2 for F1).\n",
    "        show_legend: Whether to show the legend.\n",
    "        label_modifier: A string to add to the legend.\n",
    "        color_mod: The RGB amount to modify the color by.\n",
    "    \"\"\"\n",
    "    acc_label = df.filter(like=\"Acc\").columns[0]\n",
    "    f1_label = df.filter(like=\"F1\").columns[0]\n",
    "\n",
    "    color_acc = colors[1]\n",
    "    color_f1 = colors[2]\n",
    "\n",
    "    name_acc = \"Acc\"\n",
    "    name_f1 = \"F1\"\n",
    "\n",
    "    if label_modifier:\n",
    "        # Names\n",
    "        name_acc = f\"{name_acc} {label_modifier}\"\n",
    "        name_f1 = f\"{name_f1} {label_modifier}\"\n",
    "\n",
    "        N = color_mod\n",
    "        # Acc\n",
    "        rgb_color = hex2rgb(color_acc)\n",
    "        rgb_vals = [max(color_val - N, 0) for color_val in rgb_color]\n",
    "        color_acc = rgb2hex(*rgb_vals)\n",
    "\n",
    "        # F1\n",
    "        rgb_color = hex2rgb(color_f1)\n",
    "        rgb_vals = [max(color_val - N, 0) for color_val in rgb_color]\n",
    "        color_f1 = rgb2hex(*rgb_vals)\n",
    "\n",
    "    # Plot accuracy\n",
    "    acc_vals = df[acc_label]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"Threshold\"],\n",
    "            y=acc_vals,\n",
    "            name=name_acc,\n",
    "            line=dict(color=color_acc, dash=\"solid\"),\n",
    "            mode=\"lines\",\n",
    "            showlegend=show_legend,\n",
    "            legendgroup=\"Acc\",\n",
    "            hovertemplate=\"%{y:.2%}\",\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    # Plot F1\n",
    "    prec_vals = df[f1_label]\n",
    "    if not prec_vals.isna().all():\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"Threshold\"],\n",
    "                y=prec_vals,\n",
    "                name=name_f1,\n",
    "                line=dict(color=color_f1, dash=\"dot\"),\n",
    "                mode=\"lines\",\n",
    "                showlegend=show_legend,\n",
    "                legendgroup=\"F1\",\n",
    "                hovertemplate=\"%{y:.4f}\",\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "\n",
    "def add_subset_size(\n",
    "    fig: go.Figure,\n",
    "    df: pd.DataFrame,\n",
    "    row: int,\n",
    "    col: int,\n",
    "    colors: List[str],\n",
    "    show_legend: bool = True,\n",
    "    label_modifier: str = \"\",\n",
    "    color_mod: int = 1,\n",
    ") -> None:\n",
    "    \"\"\"Add file count relative size to the figure.\"\"\"\n",
    "    # Plot subset size on secondary Y-axis\n",
    "    subset_label = df.filter(like=\"Subset\").columns[0]\n",
    "\n",
    "    trace_name = \"Subset %\"\n",
    "\n",
    "    trace_color = colors[0]\n",
    "\n",
    "    if label_modifier:\n",
    "        trace_name = f\"{trace_name} {label_modifier}\"\n",
    "\n",
    "        N = color_mod\n",
    "        rgb_color = hex2rgb(trace_color)\n",
    "        rgb_vals = [min(color_val + N, 255) for color_val in rgb_color]\n",
    "        trace_color = rgb2hex(*rgb_vals)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"Threshold\"],\n",
    "            y=df[subset_label],\n",
    "            name=trace_name,\n",
    "            line=dict(color=trace_color, dash=\"dash\"),\n",
    "            yaxis=\"y2\",\n",
    "            mode=\"lines\",\n",
    "            showlegend=show_legend,\n",
    "            legendgroup=\"Subset\",\n",
    "            hovertemplate=\"%{y:.2%}\",\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "\n",
    "def graph_all_DB_threshold_graphs(\n",
    "    results_dict: Dict[str, Dict],\n",
    "    output_dir: Path | None = None,\n",
    "    filename: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a threshold graph for mutiple DBs and classifiers.\n",
    "\n",
    "    Args:\n",
    "        results_dict: A dictionary containing the results for each DB and classifier.\n",
    "        output_dir: The directory to save the graph to.\n",
    "        name: The name of the graph.\n",
    "\n",
    "    \"\"\"\n",
    "    category_order = [ASSAY, SEX, CANCER, BIOMATERIAL_TYPE, LIFE_STAGE]\n",
    "    DBs_order = [\"EpiATLAS\", \"ENCODE_core\", \"ENCODE_non-core\", \"ChIP-Atlas\", \"recount3\"]\n",
    "    graph_renamer = {\n",
    "        ASSAY: \"Assay\",\n",
    "        SEX: \"Sex\",\n",
    "        CANCER: \"Cancer status\",\n",
    "        LIFE_STAGE: \"Life stage\",\n",
    "        BIOMATERIAL_TYPE: \"Biomaterial type\",\n",
    "    }\n",
    "    db_renamer = {\n",
    "        \"EpiATLAS\": \"EpiATLAS\",\n",
    "        \"ChIP-Atlas\": \"ChIP-Atlas\",\n",
    "        \"ENCODE_core\": \"ENCODE<br>(core)\",\n",
    "        \"ENCODE_non-core\": \"ENCODE<br>(non-core)\",\n",
    "        \"recount3\": \"recount3\",\n",
    "    }\n",
    "    # color-blind friendly\n",
    "    # black, blue, red\n",
    "    colors = [\"#000000\", \"#005AB5\", \"#DC3220\"]\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=5,\n",
    "        cols=5,\n",
    "        row_titles=[db_renamer[DB] for DB in DBs_order],\n",
    "        column_titles=[graph_renamer[category] for category in category_order],\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.025,\n",
    "        horizontal_spacing=0.05,\n",
    "        x_title=\"Prediction Score Threshold\",\n",
    "    )\n",
    "\n",
    "    y_ranges = {\n",
    "        \"EpiATLAS\": [0.7, 1.01],\n",
    "        \"ChIP-Atlas\": [0.1, 1.01],\n",
    "        \"ENCODE_core\": [0.45, 1.01],\n",
    "        \"ENCODE_non-core\": [0.30, 1.01],\n",
    "        \"recount3\": [0, 1.01],\n",
    "    }\n",
    "\n",
    "    for i, DB in enumerate(DBs_order):\n",
    "        data = results_dict[DB]\n",
    "        for j, category in enumerate(category_order):\n",
    "            show_legend = bool(j == 0 and i == 0)\n",
    "\n",
    "            try:\n",
    "                threshold_df = data[\"results\"][category]\n",
    "            except KeyError as e:\n",
    "                print(f\"Could not find results for {DB} {category}: {e}\")\n",
    "                continue\n",
    "\n",
    "            add_acc_f1(fig, threshold_df, i + 1, j + 1, colors, show_legend)\n",
    "            add_subset_size(fig, threshold_df, i + 1, j + 1, colors, show_legend)\n",
    "\n",
    "            # Nb files + classes\n",
    "            try:\n",
    "                other_info = data[\"other_info\"][category]\n",
    "            except KeyError as e:\n",
    "                print(f\"Could not find other info for {DB} {category}: {e}\")\n",
    "                continue\n",
    "\n",
    "            N = other_info[\"nb_samples\"]\n",
    "            c_true = other_info[\"nb_classes\"]\n",
    "            c_all = other_info[\"total_possible_classes\"]\n",
    "            annotation_text = f\"N = {N}<br>C = {c_true}/{c_all}\"\n",
    "\n",
    "            # print(DB, category, annotation_text)\n",
    "\n",
    "            fig.add_annotation(\n",
    "                text=annotation_text,\n",
    "                showarrow=False,\n",
    "                font=dict(size=10, color=\"black\"),\n",
    "                row=i + 1,\n",
    "                col=j + 1,\n",
    "                xref=f\"x{i+1} domain\" if i >= 1 else \"x domain\",\n",
    "                yref=f\"y{j+1} domain\" if j >= 1 else \"y domain\",\n",
    "                x=0,\n",
    "                y=0.03,\n",
    "                align=\"left\",\n",
    "            )\n",
    "    # Set y-axis ranges\n",
    "    for i, DB in enumerate(DBs_order):\n",
    "        y_range = y_ranges[DB]\n",
    "        for j in range(1, 6):\n",
    "            dtick = 0.2\n",
    "            if DB in [\"ENCODE_core\", \"ENCODE_non-core\", \"EpiATLAS\"]:\n",
    "                dtick = 0.1\n",
    "\n",
    "            fig.update_yaxes(range=y_range, row=i + 1, col=j, dtick=dtick)\n",
    "\n",
    "    fig.update_xaxes(range=[0.1, 1.01], dtick=0.2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1000,\n",
    "        height=900,\n",
    "        # title=\"All Databases - 5 classifiers - Metrics at Different Pred. Score Thresholds\",\n",
    "    )\n",
    "\n",
    "    # 25% transparency\n",
    "    fig.update_layout(\n",
    "        hovermode=\"x unified\",\n",
    "        hoverlabel=dict(bgcolor=\"rgba(255,255,255,0.75)\", font_size=10),\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    if output_dir:\n",
    "        if not filename:\n",
    "            filename = \"all_DBs_5_classifiers_thresholds\"\n",
    "        fig.write_image(output_dir / f\"{filename}.svg\")\n",
    "        fig.write_image(output_dir / f\"{filename}.png\")\n",
    "        fig.write_html(output_dir / f\"{filename}_cdn.html\", include_plotlyjs=\"cdn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = base_fig_dir / \"threshold_graphs\" / \"100kb_all_none\"\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "graph_all_DB_threshold_graphs(\n",
    "    all_threshold_results,\n",
    "    output_dir=output_dir,\n",
    "    filename=\"all_DBs_5_classifiers_thresholds_w_ENCODE_split_w_LS_biomat_predScore_filter\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass_py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
