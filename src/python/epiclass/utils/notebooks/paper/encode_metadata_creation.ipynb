{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get ENCODE EpiRRs, and determine which datasets are in EpiATLAS.\n",
    "\n",
    "Found incoherences, so rest of script is metadata re-creation.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=import-error, redefined-outer-name, too-many-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, Iterable, List, Set, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from epiclass.core.metadata import Metadata\n",
    "from epiclass.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIOMAT = \"harmonized_biomaterial_type\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download summary of all EpiRR epigenomes: https://www.ebi.ac.uk/epirr/docs  \n",
    "This was already done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "metadata_dir = base_dir / \"data/metadata\"\n",
    "if not metadata_dir.exists():\n",
    "    raise ValueError(f\"Path {metadata_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_dir = metadata_dir / \"encode\"\n",
    "if not encode_metadata_dir.exists():\n",
    "    raise ValueError(f\"Path {encode_metadata_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt to establish EpiRR overlap between EpiATLAS and ENCODE, using 'old' metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup old ChIP metadata\n",
    "\n",
    "Fix experiment accession values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata_path = encode_metadata_dir / \"old_meta\" / \"encode_metadata_2023-10-25.csv\"\n",
    "chip_metadata_df = pd.read_csv(full_metadata_path)\n",
    "print(chip_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chip_metadata_df[\"md5sum\"].nunique() != chip_metadata_df.shape[0]:\n",
    "    raise ValueError(\"Duplicate filenames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in chip_metadata_df.columns:\n",
    "    if chip_metadata_df[col].str.slice(0, 5).isin([\"ENCSR\"]).sum() > 0:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accession_cols = [\n",
    "    \"Accession\",\n",
    "    \"accession\",\n",
    "    \"Experiment accession\",\n",
    "    \"Experiment_accession\",\n",
    "    \"experiment_accession\",\n",
    "]  # ENCSR[VAL]\n",
    "for col in accession_cols:\n",
    "    if (\n",
    "        chip_metadata_df.loc[:, col].str.slice(0, 5).isin([\"ENCSR\", \"unkno\"]).sum()\n",
    "        != chip_metadata_df.shape[0]\n",
    "    ):\n",
    "        raise ValueError(f\"Column {col} is not in the correct format\")\n",
    "\n",
    "chip_metadata_df.drop(columns=accession_cols, inplace=True)\n",
    "chip_metadata_df.loc[:, \"experiment_accession\"] = (\n",
    "    chip_metadata_df[\"uuid\"].str.split(\"-\", n=1).str[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_path = encode_metadata_dir / \"old_meta\" / \"encode_metadata_2023-10-25_clean-v2.csv\"\n",
    "if not new_df_path.exists():\n",
    "    chip_metadata_df.to_csv(new_df_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse EpiRR general metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"epirr_epigenomes_2025-02\"\n",
    "epigenomes_summary_path = encode_metadata_dir / \"new_meta\" / f\"{filename}.json\"\n",
    "\n",
    "with open(epigenomes_summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    epigenomes_summary = json.load(f)\n",
    "\n",
    "epigenomes_summary_df = pd.DataFrame(epigenomes_summary)\n",
    "epigenomes_summary_df.to_csv(epigenomes_summary_path.with_suffix(\".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(epigenomes_summary_df[\"project\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_epirrs = epigenomes_summary_df[epigenomes_summary_df[\"project\"] == \"ENCODE\"][\n",
    "    \"accession\"\n",
    "].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ENCODE EpiRRs: {len(encode_epirrs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del epigenomes_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download specific experiments metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download metadata for all encode epigenomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_path = (\n",
    "    encode_metadata_dir / \"new_meta\" / \"encode_epigenomes_metadata_2025-02.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not encode_metadata_path.exists():\n",
    "    # Base URL\n",
    "    base_url = \"https://www.ebi.ac.uk/epirr/api/v1/epigenome?accession={}\"\n",
    "\n",
    "    # Collect metadata in a list\n",
    "    metadata_list = []\n",
    "\n",
    "    # Use tqdm for a progress bar\n",
    "    for epirr in tqdm(encode_epirrs, desc=\"Fetching Metadata\", unit=\"entry\"):\n",
    "        response = requests.get(\n",
    "            base_url.format(epirr), headers={\"accept\": \"application/json\"}\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            metadata_list.append(response.json())  # Append parsed JSON\n",
    "        else:\n",
    "            print(f\"Failed to fetch {epirr}: {response.status_code}\")\n",
    "\n",
    "    with open(encode_metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata_list, f, indent=2)\n",
    "\n",
    "    print(f\"Metadata saved to {encode_metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse specific metadata for accessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_path = (\n",
    "    encode_metadata_dir / \"new_meta\" / \"encode_epigenomes_metadata_2025-02.json\"\n",
    ")\n",
    "with open(encode_metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    encode_metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessions_and_epirr = []\n",
    "for dset in encode_metadata:\n",
    "    epirr = dset[\"accession\"]\n",
    "    primary_ids = [file[\"primary_id\"] for file in dset[\"raw_data\"]]\n",
    "    for primary_id in primary_ids:\n",
    "        accessions_and_epirr.append((primary_id, epirr))\n",
    "\n",
    "    # # it's an input file, multiple occurences is fine\n",
    "    # if \"ENCSR266XMB\" in primary_ids:\n",
    "    #     print(dset[\"raw_data\"])\n",
    "print(\"ENCODE total accessions:\", len(accessions_and_epirr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_ids_count = Counter([primary_id for primary_id, _ in accessions_and_epirr])\n",
    "print(\"ENCODE unique accessions:\", len(set(primary_ids_count.keys())))\n",
    "print(primary_ids_count.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(epirr for _, epirr in accessions_and_epirr) == set(encode_epirrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with EpiATLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epiatlas_metadata_path = (\n",
    "    metadata_dir / \"official\" / \"IHEC_sample_metadata_harmonization.v1.2.extended.csv\"\n",
    ")\n",
    "epiatlas_df = pd.read_csv(epiatlas_metadata_path, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epiatlas_epirrs = set(epiatlas_df[\"epirr_id_without_version\"].tolist())\n",
    "common_epirrs = set(encode_epirrs).intersection(epiatlas_epirrs)\n",
    "diff_epirr = set(encode_epirrs).difference(epiatlas_epirrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ENCODE EpiRRs: {len(encode_epirrs)}\")\n",
    "print(f\"EpiATLAS EpiRRs: {len(epiatlas_epirrs)}\")\n",
    "print(f\"ENCODE EpiRRs in EpiATLAS: {len(common_epirrs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_accessions_df = pd.DataFrame.from_records(\n",
    "    accessions_and_epirr, columns=[\"experiment_accession\", \"epirr_no_version\"]\n",
    ")\n",
    "print(encode_accessions_df.shape)\n",
    "\n",
    "encode_accessions_df[\"in_epiatlas\"] = encode_accessions_df[\"epirr_no_version\"].isin(\n",
    "    common_epirrs\n",
    ")\n",
    "display(encode_accessions_df[\"in_epiatlas\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(encode_accessions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_accessions_df.to_csv(\n",
    "    encode_metadata_dir / \"new_meta\" / \"encode_epirrs_2025-02.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EpiRR is less useful because ENCODE only submitted complete epigenomes. EpiATLAS also includes partial ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del epiatlas_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with previous ENCODE metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_meta_df = pd.read_csv(\n",
    "    encode_metadata_dir / \"old_meta\" / \"encode_metadata_2023-10-25_clean-v2.csv\"\n",
    ")\n",
    "encode_ihec_df = pd.read_csv(\n",
    "    encode_metadata_dir / \"old_meta\" / \"ENCODE_IHEC_keys.tsv\", sep=\"\\t\"\n",
    ")\n",
    "print(encode_meta_df.shape)\n",
    "print(encode_ihec_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(encode_meta_df.head())\n",
    "display(encode_ihec_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_accession_1 = encode_meta_df[\"experiment_accession\"].nunique()\n",
    "N_accession_2 = encode_ihec_df[\"accession\"].nunique()\n",
    "print(f\"ENCODE metadata 2023-10-25 accessions: {N_accession_1}\")\n",
    "print(f\"ENCODE-IHEC file accessions: {N_accession_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(encode_ihec_df[ASSAY].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(encode_meta_df[ASSAY].value_counts(dropna=False))\n",
    "display(\n",
    "    encode_meta_df[~encode_meta_df[\"md5sum\"].isin(encode_ihec_df[\"ENC_ID\"])][\n",
    "        \"Assay\"\n",
    "    ].value_counts(dropna=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: non-core files are not included in ENCODE_IHEC_keys.tsv. That's okay because these files were only used for training assay13, and were not included in any other classifier training. We now have enough information to create an almost complete \"in_epiatlas\" column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `in_epiatlas` creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_ihec_df[\"in_epiatlas\"] = encode_ihec_df[\"is_EpiAtlas_EpiRR\"].notnull()\n",
    "display(encode_ihec_df[\"in_epiatlas\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: accession, in_epiatlas pairs consistent (accessions are not unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_ihec_df_pairs = encode_ihec_df[[\"accession\", \"in_epiatlas\"]].values.tolist()\n",
    "encode_ihec_df_pairs = tuple(zip(*encode_ihec_df_pairs))\n",
    "if len(encode_ihec_df_pairs) != len(set(encode_ihec_df_pairs)):\n",
    "    raise ValueError(\"Inconsistent 'in_epiatlas' values:\", encode_ihec_df_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_epirr_in_epiatlas(\n",
    "    encode_accessions_df: pd.DataFrame, encode_ihec_df: pd.DataFrame\n",
    ") -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Determine which ENCODE datasets are included in EpiATLAS.\"\"\"\n",
    "    epirr_in_epiatlas = encode_accessions_df[[\"in_epiatlas\", \"experiment_accession\"]]\n",
    "    alt_in_epiatlas = encode_ihec_df[[\"in_epiatlas\", \"accession\"]]\n",
    "\n",
    "    common_accessions = set(epirr_in_epiatlas[\"experiment_accession\"]).intersection(\n",
    "        set(alt_in_epiatlas[\"accession\"])\n",
    "    )\n",
    "    alt_in_epiatlas_common = alt_in_epiatlas[\n",
    "        alt_in_epiatlas[\"accession\"].isin(common_accessions)\n",
    "    ]\n",
    "    epirr_in_epiatlas_common = epirr_in_epiatlas[\n",
    "        epirr_in_epiatlas[\"experiment_accession\"].isin(common_accessions)\n",
    "    ]\n",
    "\n",
    "    inconsistent_accession_tuples = []\n",
    "    for accession in common_accessions:\n",
    "        in_epitlas_1 = epirr_in_epiatlas_common[\n",
    "            epirr_in_epiatlas_common[\"experiment_accession\"] == accession\n",
    "        ][\"in_epiatlas\"].values\n",
    "        in_epitlas_2 = alt_in_epiatlas_common[\n",
    "            alt_in_epiatlas_common[\"accession\"] == accession\n",
    "        ][\"in_epiatlas\"].values\n",
    "\n",
    "        if len(in_epitlas_1) != 1:\n",
    "            # print(accession, in_epitlas_1)\n",
    "            in_epitlas_1 = any(in_epitlas_1)\n",
    "\n",
    "        else:\n",
    "            in_epitlas_1 = in_epitlas_1[0]\n",
    "\n",
    "        if len(in_epitlas_2) != 1:\n",
    "            # print(accession, in_epitlas_2)\n",
    "            in_epitlas_2 = any(in_epitlas_2)\n",
    "        else:\n",
    "            in_epitlas_2 = in_epitlas_2[0]\n",
    "\n",
    "        if in_epitlas_1 != in_epitlas_2:\n",
    "            inconsistent_accession_tuples.append((accession, in_epitlas_1, in_epitlas_2))\n",
    "            # raise ValueError(\"Inconsistent 'in_epiatlas' values:\", accession, in_epitlas_1, in_epitlas_2)\n",
    "\n",
    "    return inconsistent_accession_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inconsistent_accession_tuples = check_epirr_in_epiatlas(\n",
    "    encode_accessions_df, encode_ihec_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inconsistent_accession_values = [dset[0] for dset in inconsistent_accession_tuples]\n",
    "suspect_df = encode_ihec_df[\n",
    "    encode_ihec_df[\"accession\"].isin(inconsistent_accession_values)\n",
    "]\n",
    "display(suspect_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_biosample_accs = suspect_df[suspect_df[\"biosample_accession\"].str.endswith(\"DMP\")][\n",
    "    \"accession\"\n",
    "].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_epirr_inputs_acc = [\"ENCSR000AHE\", \"ENCSR000DMW\", \"ENCSR000EWW\", \"ENCSR768LHG\"]\n",
    "for acc in one_epirr_inputs_acc:\n",
    "    print(acc, acc in inconsistent_accession_values)\n",
    "\n",
    "display(encode_ihec_df[encode_ihec_df[\"accession\"].isin(one_epirr_inputs_acc)])\n",
    "display(encode_meta_df[encode_meta_df[\"experiment_accession\"].isin(one_epirr_inputs_acc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(encode_ihec_df[encode_ihec_df[\"accession\"].isin(one_biosample_accs)])\n",
    "display(encode_meta_df[encode_meta_df[\"experiment_accession\"].isin(one_biosample_accs)])\n",
    "display(\n",
    "    encode_accessions_df[\n",
    "        encode_accessions_df[\"experiment_accession\"].isin(one_biosample_accs)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_epirr_example = set(\n",
    "    encode_accessions_df[\n",
    "        encode_accessions_df[\"experiment_accession\"].isin(one_biosample_accs)\n",
    "    ][\"epirr_no_version\"].values.tolist()\n",
    ")\n",
    "if len(problematic_epirr_example) > 1:\n",
    "    raise ValueError(\"One biosample with multiple epirrs\", problematic_epirr_example)\n",
    "\n",
    "problematic_epirr_example = problematic_epirr_example.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EpiClass actual training metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epiclass_metadata_path = (\n",
    "    metadata_dir / \"hg38_2023-epiatlas-dfreeze-pospurge-nodup_filterCtl.json\"\n",
    ")\n",
    "epiclass_metadata = Metadata(epiclass_metadata_path)\n",
    "epiclass_df = pd.DataFrame.from_records(list(epiclass_metadata.datasets))\n",
    "print(epiclass_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epiclass_epirrs = set(epiclass_df[\"epirr_id_without_version\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(problematic_epirr_example in epiclass_epirrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Some errors have been made during the creation of \"ENCODE_IHEC_keys.tsv\". As demonstrated by having a set of files from a biosamples being marked as not having an epirr, when we found the corresponding epirr in the training metadata. We need to recreate the metadata from zero to guarantee the right values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating metadata from file accessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect/Combine accessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_file_accessions = set(encode_meta_df[\"md5sum\"].unique().tolist())\n",
    "print(\"CHIP file accessions:\", len(chip_file_accessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del encode_meta_df\n",
    "del epiclass_df\n",
    "del epiclass_metadata\n",
    "del encode_accessions_df\n",
    "del encode_ihec_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_rna_file_meta_path = (\n",
    "    encode_metadata_dir / \"old_meta\" / \"metadata--ENCODE_RNA_2023mar_hg38_BW_default.tsv\"\n",
    ")\n",
    "encode_rna_meta_df = pd.read_csv(encode_rna_file_meta_path, sep=\"\\t\")\n",
    "print(encode_rna_meta_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_file_accessions = set(encode_rna_meta_df[\"File accession\"].unique().tolist())\n",
    "print(\"RNA file accessions:\", len(rna_file_accessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del encode_rna_meta_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WGBS accessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgbs_accessions_path = (\n",
    "    encode_metadata_dir / \"old_meta\" / \"ENCODE_WGBS_2023mar_hg38_BW_default.list\"\n",
    ")\n",
    "with open(wgbs_accessions_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    wgbs_file_accessions = f.read().splitlines()\n",
    "\n",
    "print(\"WGBS accessions:\", len(wgbs_file_accessions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and create new ENCODE metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_accessions = (\n",
    "    chip_file_accessions | rna_file_accessions | set(wgbs_file_accessions)\n",
    ")\n",
    "try:\n",
    "    all_file_accessions.remove(\"unknown\")\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "print(\"All file accessions:\", len(all_file_accessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_exp_template_object = (\n",
    "    \"https://www.encodeproject.org/experiments/{}/?frame=object&format=json\"\n",
    ")\n",
    "url_exp_template_embedded = (\n",
    "    \"https://www.encodeproject.org/experiments/{}/?frame=embedded&format=json\"\n",
    ")\n",
    "url_biosample_template_object = (\n",
    "    \"https://www.encodeproject.org/biosamples/{}/?frame=object&format=json\"\n",
    ")\n",
    "url_file_template_object = (\n",
    "    \"https://www.encodeproject.org/files/{}/?frame=object&format=json\"\n",
    ")\n",
    "url_biosample_type_template_object = (\n",
    "    \"https://www.encodeproject.org/biosample-types/{}/?frame=object&format=json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_json(url: str, headers: Dict[str, str]) -> Dict[str, Any] | None:\n",
    "    \"\"\"Helper function to fetch JSON data from a URL.\"\"\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "\n",
    "    print(f\"Failed to fetch {url}: {response.status_code}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_experiment_metadata(\n",
    "    experiment_acc: str, headers: Dict[str, str]\n",
    ") -> Dict[str, Any] | None:\n",
    "    \"\"\"Fetch experiment metadata from ENCODE API.\"\"\"\n",
    "    return fetch_json(url_exp_template_object.format(experiment_acc), headers)\n",
    "\n",
    "\n",
    "def fetch_biosample_type_metadata(\n",
    "    term_id: str, headers: Dict[str, str]\n",
    ") -> Dict[str, Any] | None:\n",
    "    \"\"\"Fetch biosample type metadata from ENCODE API.\n",
    "\n",
    "    Args:\n",
    "        term_id (str): Biosample type term ID, of the form \"[classification]_[ontology_id]\", e.g. \"cell_line_EFO_0001203\".\n",
    "        headers (Dict[str, str]): HTTP headers, such as authorization, to be passed.\n",
    "    \"\"\"\n",
    "    return fetch_json(url_biosample_type_template_object.format(term_id), headers)\n",
    "\n",
    "\n",
    "def fetch_file_metadata(file_acc: str, headers: Dict[str, str]) -> Dict[str, Any] | None:\n",
    "    \"\"\"Fetch experiment metadata from ENCODE API.\"\"\"\n",
    "    return fetch_json(url_file_template_object.format(file_acc), headers)\n",
    "\n",
    "\n",
    "def fetch_replicate_biosample(\n",
    "    experiment_acc: str, headers: Dict[str, str]\n",
    ") -> Dict[str, List[str]] | None:\n",
    "    \"\"\"Fetch biosample accessions for an experiment.\"\"\"\n",
    "    data = fetch_json(url_exp_template_embedded.format(experiment_acc), headers)\n",
    "    if data:\n",
    "        accession_list = [\n",
    "            replicate[\"library\"][\"biosample\"][\"accession\"]\n",
    "            for replicate in data.get(\"replicates\", [])\n",
    "        ]\n",
    "        return {experiment_acc: accession_list}\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_biosample_metadata(\n",
    "    biosample_acc: str, headers: Dict[str, str]\n",
    ") -> Dict[str, Any] | None:\n",
    "    \"\"\"Fetch biosample metadata from ENCODE API.\"\"\"\n",
    "    return fetch_json(url_biosample_template_object.format(biosample_acc), headers)\n",
    "\n",
    "\n",
    "def parallel_fetch(\n",
    "    func: Callable[[str, Dict[str, str]], Any],\n",
    "    identifiers: Iterable[str],\n",
    "    headers: Dict[str, str],\n",
    "    max_workers: int = 10,\n",
    "    task_name: str = \"Fetching Data\",\n",
    ") -> List[Any]:\n",
    "    \"\"\"\n",
    "    Generic function to fetch data in parallel.\n",
    "\n",
    "    Args:\n",
    "        func (Callable): The function to execute in parallel.\n",
    "        identifiers (Iterable[str]): List of input identifiers for the function.\n",
    "        headers (Dict[str, str]): HTTP headers, such as authorization, to be passed.\n",
    "        max_workers (int): Number of parallel threads.\n",
    "        task_name (str): Name for progress bar.\n",
    "\n",
    "    Returns:\n",
    "        List of results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_id = {executor.submit(func, id_, headers): id_ for id_ in identifiers}\n",
    "\n",
    "        for future in tqdm(\n",
    "            as_completed(future_to_id),\n",
    "            total=len(future_to_id),\n",
    "            desc=task_name,\n",
    "            unit=\"entry\",\n",
    "        ):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_update_metadata(\n",
    "    func: Callable[[str, Dict[str, str]], Dict[str, Any] | None],\n",
    "    all_accession_set: Set[str],\n",
    "    output_filepath: Path,\n",
    "    max_workers: int = 5,\n",
    "    task_name: str = \"Fetching metadata\",\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    General function to fetch new metadata and update an existing JSON/TSV file.\n",
    "\n",
    "    Args:\n",
    "        func (Callable): Function to fetch metadata for a single accession.\n",
    "        all_accession_set (Set[str]): Set of all accessions to process.\n",
    "        output_filepath (Path): Path to the JSON file storing metadata.\n",
    "        max_workers (int): Number of parallel workers (default: 5).\n",
    "        task_name (str): Progress bar name.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: The combined metadata after fetching new entries.\n",
    "    \"\"\"\n",
    "    headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "    # Step 1: Load existing metadata if the file exists\n",
    "    if output_filepath.exists():\n",
    "        with open(output_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing_metadata = json.load(f)\n",
    "    else:\n",
    "        existing_metadata = []\n",
    "\n",
    "    # Step 2: Identify already fetched accessions\n",
    "    attempt_keys = [\"accession\", \"obo_id\", \"name\"]\n",
    "    for key in attempt_keys:\n",
    "        try:\n",
    "            existing_accessions = {entry[key] for entry in existing_metadata}\n",
    "            break\n",
    "        except KeyError:\n",
    "            continue\n",
    "    else:\n",
    "        print(\n",
    "            f\"No {attempt_keys} keys in existing metadata. Using first key of each entry instead.\"\n",
    "        )\n",
    "        existing_accessions = {list(entry.keys())[0] for entry in existing_metadata}\n",
    "\n",
    "    # Step 3: Find missing accessions\n",
    "    missing_accessions = all_accession_set - existing_accessions\n",
    "\n",
    "    # Step 4: Fetch new metadata if needed\n",
    "    if missing_accessions:\n",
    "        print(f\"Fetching {len(missing_accessions)} new records...\")\n",
    "        new_metadata = parallel_fetch(\n",
    "            func=func,\n",
    "            identifiers=missing_accessions,\n",
    "            headers=headers,\n",
    "            max_workers=max_workers,\n",
    "            task_name=task_name,\n",
    "        )\n",
    "\n",
    "        # Merge old and new metadata\n",
    "        combined_metadata = existing_metadata + new_metadata\n",
    "\n",
    "        # Save updated JSON file\n",
    "        with open(output_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(combined_metadata, f, indent=2)\n",
    "\n",
    "        # Save updated TSV file\n",
    "        file_df = pd.DataFrame.from_records(combined_metadata)\n",
    "        file_df.to_csv(output_filepath.with_suffix(\".tsv\"), sep=\"\\t\", index=False)\n",
    "\n",
    "        return combined_metadata\n",
    "\n",
    "    print(\"No new metadata to fetch. Data is already up-to-date.\")\n",
    "    return existing_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = encode_metadata_dir / \"new_meta\" / \"encode_file_metadata_2025-02.json\"\n",
    "encode_file_metadata_list = fetch_and_update_metadata(\n",
    "    func=fetch_file_metadata,\n",
    "    all_accession_set=all_file_accessions,\n",
    "    output_filepath=output_file,\n",
    "    max_workers=5,\n",
    "    task_name=\"Fetching File metadata\",\n",
    ")\n",
    "encode_file_metadata: Dict[str, Dict[str, Any]] = {\n",
    "    dset[\"accession\"]: dset for dset in encode_file_metadata_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_title_counter = Counter()\n",
    "for dset in encode_file_metadata.values():\n",
    "    assay_title = dset[\"assay_title\"]\n",
    "    assay_title_counter[assay_title] += 1\n",
    "\n",
    "print(assay_title_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find experiment accessions from file metadata. Make sure exp accessions are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiment_accessions = set()\n",
    "for file_acc, dataset in encode_file_metadata.items():\n",
    "    acc_str = dataset[\"dataset\"]\n",
    "    if acc_str.count(\"ENCSR\") > 1:\n",
    "        raise ValueError(\"Multiple experiments per file:\", dataset)\n",
    "\n",
    "    exp_acc = acc_str.split(\"/\")[-2]\n",
    "    all_experiment_accessions.add(exp_acc)\n",
    "    encode_file_metadata[file_acc][\"experiment_accession\"] = exp_acc\n",
    "\n",
    "for val in all_experiment_accessions:\n",
    "    if not val.startswith(\"ENCSR\"):\n",
    "        raise ValueError(\"Experiment accessions do not start with ENCSR:\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = encode_metadata_dir / \"new_meta\" / \"encode_experiment_metadata_2025-02.json\"\n",
    "encode_exp_metadata_list = fetch_and_update_metadata(\n",
    "    func=fetch_experiment_metadata,\n",
    "    all_accession_set=all_experiment_accessions,\n",
    "    output_filepath=output_file,\n",
    "    max_workers=5,\n",
    "    task_name=\"Fetching Experiment metadata\",\n",
    ")\n",
    "encode_exp_metadata: Dict[str, Dict[str, Any]] = {\n",
    "    dset[\"accession\"]: dset for dset in encode_exp_metadata_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = (\n",
    "    encode_metadata_dir / \"new_meta\" / \"encode_biosample_accessions_2025-02.json\"\n",
    ")\n",
    "encode_biosample_accessions_list = fetch_and_update_metadata(\n",
    "    func=fetch_replicate_biosample,\n",
    "    all_accession_set=all_experiment_accessions,\n",
    "    output_filepath=output_file,\n",
    "    max_workers=5,\n",
    "    task_name=\"Fetching Experiment Biosample Accessions\",\n",
    ")\n",
    "encode_biosample_accessions_dict = {\n",
    "    list(dset.keys())[0]: list(dset.values())[0]\n",
    "    for dset in encode_biosample_accessions_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_biosamples_accessions = set()\n",
    "for biosample_accs in encode_biosample_accessions_dict.values():\n",
    "    all_biosamples_accessions.update(biosample_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = encode_metadata_dir / \"new_meta\" / \"encode_biosample_metadata_2025-02.json\"\n",
    "encode_biosample_metadata_list = fetch_and_update_metadata(\n",
    "    func=fetch_biosample_metadata,\n",
    "    all_accession_set=all_biosamples_accessions,\n",
    "    output_filepath=output_file,\n",
    "    max_workers=5,\n",
    "    task_name=\"Fetching Biosample Metadata\",\n",
    ")\n",
    "encode_biosample_metadata = {\n",
    "    dset[\"accession\"]: dset for dset in encode_biosample_metadata_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biosample_type(biosample_ontology: str) -> str:\n",
    "    \"\"\"Extract the term ID from a biosample ontology string.\n",
    "\n",
    "    Takes string of format '/biosample-types/cell_line_EFO_0001203/'\n",
    "    and returns 'cell_line_EFO_0001203'.\n",
    "    \"\"\"\n",
    "    biosample_ontology_no_prefix = biosample_ontology.replace(\"/\", \"\").replace(\n",
    "        \"biosample-types\", \"\"\n",
    "    )\n",
    "    return biosample_ontology_no_prefix\n",
    "\n",
    "\n",
    "def get_biosample_term_id(biosample_ontology: str) -> str:\n",
    "    \"\"\"Extract the term ID from a biosample ontology string.\n",
    "\n",
    "    Takes string of format '/biosample-types/cell_line_EFO_0001203/'\n",
    "    and returns 'EFO:0001203'.\n",
    "    \"\"\"\n",
    "    biosample_ontology_no_prefix = get_biosample_type(biosample_ontology)\n",
    "    biosample_term_id = \":\".join(biosample_ontology_no_prefix.split(\"_\")[-2:])\n",
    "    return biosample_term_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biosample_types = [\n",
    "    get_biosample_type(dset[\"biosample_ontology\"])\n",
    "    for dset in encode_biosample_metadata.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = encode_metadata_dir / \"new_meta\" / \"biosample_types_metadata.json\"\n",
    "\n",
    "biosample_type_metadata_list = fetch_and_update_metadata(\n",
    "    func=fetch_biosample_type_metadata,\n",
    "    all_accession_set=set(biosample_types),\n",
    "    output_filepath=new_file,\n",
    "    max_workers=5,\n",
    "    task_name=\"Fetching missing biosample type info\",\n",
    ")\n",
    "encode_biosample_type_metadata = {\n",
    "    dset[\"term_id\"]: dset for dset in biosample_type_metadata_list\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine experiment and biosample metadata\n",
    "\n",
    "Exclude datasets with incoherent biosamples, can't do a case by case basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "N_counter = Counter()\n",
    "missing_biosample_metadata = set()\n",
    "problematic_experiments = set()\n",
    "for exp_acc, biosample_accs in encode_biosample_accessions_dict.items():\n",
    "    N = len(biosample_accs)\n",
    "    N_counter[N] += 1\n",
    "    biosample_meta = []\n",
    "    if N > 1:\n",
    "        for biosample_acc in biosample_accs:\n",
    "            try:\n",
    "                meta = encode_biosample_metadata[biosample_acc]\n",
    "            except KeyError:\n",
    "                missing_biosample_metadata.add(biosample_acc)\n",
    "                biosample_meta.append((\"unknown\", \"unknown\", \"unknown\"))\n",
    "                if verbose:\n",
    "                    print(\"Missing biosample metadata:\", biosample_acc)\n",
    "                continue\n",
    "            sex = meta.get(\"sex\", \"unknown\")\n",
    "            life_stage = meta.get(\"life_stage\", \"unknown\")\n",
    "            health_status = meta.get(\"health_status\", \"unknown\")\n",
    "            biosample_meta.append((sex, life_stage, health_status))\n",
    "\n",
    "        # biosample info needs to be the same\n",
    "        if len(set(biosample_meta)) > 1:\n",
    "            problematic_experiments.add(exp_acc)\n",
    "            if verbose:\n",
    "                print(\"Inconsistent biosample metadata:\", exp_acc, biosample_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_biosample_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove experiments with mixed biosample metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nb exp to remove:\", len(problematic_experiments))\n",
    "print(\"Nb exp before:\", len(encode_exp_metadata))\n",
    "experiment_metadata = {\n",
    "    k: v for k, v in encode_exp_metadata.items() if k not in problematic_experiments\n",
    "}\n",
    "print(\"Nb exp after:\", len(experiment_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove experiments with no known biosamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = []\n",
    "for exp_acc in list(experiment_metadata.keys()):\n",
    "    biosample_accs = encode_biosample_accessions_dict[exp_acc]\n",
    "    # print(experiment_accession, biosample_accs)\n",
    "\n",
    "    known_biosamples_accs = set(biosample_accs) - set(missing_biosample_metadata)\n",
    "    if not known_biosamples_accs:\n",
    "        to_remove.append(exp_acc)\n",
    "        print(f\"Experiment {exp_acc} has no known biosamples: {biosample_accs}\")\n",
    "\n",
    "print(\"Number of experiments to remove:\", len(to_remove))\n",
    "print(\"to_remove:\", to_remove)\n",
    "print(\"Nb exp before:\", len(experiment_metadata))\n",
    "for acc in to_remove:\n",
    "    del experiment_metadata[acc]\n",
    "print(\"Nb exp after:\", len(experiment_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepend name to columns from file, experiment vs biosample metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_unique_keys = set()\n",
    "for dset in encode_file_metadata.values():\n",
    "    file_unique_keys.update(dset.keys())\n",
    "\n",
    "exp_unique_keys = set()\n",
    "for dset in encode_exp_metadata.values():\n",
    "    exp_unique_keys.update(dset.keys())\n",
    "\n",
    "biosample_unique_keys = set()\n",
    "for dset in encode_biosample_metadata.values():\n",
    "    biosample_unique_keys.update(dset.keys())\n",
    "\n",
    "biosample_type_unique_keys = set()\n",
    "for dset in encode_biosample_type_metadata.values():\n",
    "    biosample_type_unique_keys.update(dset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata = copy.deepcopy(encode_file_metadata)\n",
    "invalid_files = set()\n",
    "for file_acc, dset_metadata in list(full_metadata.items()):\n",
    "    exp_acc = dset_metadata[\"experiment_accession\"]\n",
    "\n",
    "    exp_metadata = copy.deepcopy(encode_exp_metadata[exp_acc])\n",
    "\n",
    "    biosample_accs = encode_biosample_accessions_dict[exp_acc]\n",
    "    known_biosamples_accs = set(biosample_accs) - set(missing_biosample_metadata)\n",
    "    if not known_biosamples_accs:\n",
    "        print(\n",
    "            f\"File {file_acc} with Experiment {exp_acc} has no known biosamples: {biosample_accs}\"\n",
    "        )\n",
    "        invalid_files.add(file_acc)\n",
    "        continue\n",
    "\n",
    "    biosample_acc = (\n",
    "        known_biosamples_accs.pop()\n",
    "    )  # choose random one, coherence check previously\n",
    "    try:\n",
    "        biosample_metadata = copy.deepcopy(encode_biosample_metadata[biosample_acc])\n",
    "    except KeyError:\n",
    "        print(f\"ERROR:{exp_acc}:{biosample_acc}\")\n",
    "        continue\n",
    "\n",
    "    biosample_term_id = get_biosample_term_id(biosample_metadata[\"biosample_ontology\"])\n",
    "    try:\n",
    "        biosample_type_metadata = copy.deepcopy(\n",
    "            encode_biosample_type_metadata[biosample_term_id]\n",
    "        )\n",
    "    except KeyError:\n",
    "        print(\n",
    "            f\"Missing biosample type metadata: {file_acc}-{exp_acc}-{biosample_acc}-{biosample_term_id}\"\n",
    "        )\n",
    "        biosample_type_metadata = {}\n",
    "\n",
    "    # create unique names\n",
    "    for unique_meta_labels, str_prepend, dset in zip(\n",
    "        [\n",
    "            file_unique_keys,\n",
    "            exp_unique_keys,\n",
    "            biosample_unique_keys,\n",
    "            biosample_type_unique_keys,\n",
    "        ],\n",
    "        [\"FILE\", \"EXPERIMENT\", \"BIOSAMPLE\", \"BIOSAMPLE_TYPE\"],\n",
    "        [dset_metadata, exp_metadata, biosample_metadata, biosample_type_metadata],\n",
    "    ):\n",
    "        for key in unique_meta_labels:\n",
    "            try:\n",
    "                dset[f\"{str_prepend}_{key}\"] = dset[key]\n",
    "                del dset[key]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    dset_metadata.update(exp_metadata)\n",
    "    dset_metadata.update(biosample_metadata)\n",
    "    dset_metadata.update(biosample_type_metadata)\n",
    "\n",
    "    dset_metadata[\"biosamples\"] = biosample_accs\n",
    "\n",
    "print(\"Nb of invalid files:\", len(invalid_files))\n",
    "for file_acc in invalid_files:\n",
    "    del full_metadata[file_acc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove line breaks from values, it made reading file difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata_df = pd.DataFrame(list(full_metadata.values()))\n",
    "cols_with_line_breaks = set()\n",
    "for col in full_metadata_df.columns:\n",
    "    for specific_value in full_metadata_df[col].astype(str).unique():\n",
    "        if \"\\n\" in specific_value:\n",
    "            cols_with_line_breaks.add(col)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_with_line_breaks:\n",
    "    for idx, value in full_metadata_df[col].items():\n",
    "        if \"\\n\" in str(value):\n",
    "            full_metadata_df.at[idx, col] = value.replace(\"\\n\", \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save the combined metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_metadata = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata_path = (\n",
    "    encode_metadata_dir / \"new_meta\" / \"encode_full_metadata_2025-02.json\"\n",
    ")\n",
    "if rewrite_metadata:\n",
    "    full_metadata_path.unlink(missing_ok=True)\n",
    "\n",
    "if not full_metadata_path.exists():\n",
    "    full_metadata_df.to_json(full_metadata_path, orient=\"records\", indent=2)\n",
    "    full_metadata_df.to_csv(full_metadata_path.with_suffix(\".csv\"), sep=\",\", index=False)\n",
    "else:\n",
    "    print(f\"Loading existing full metadata: {full_metadata_path}\")\n",
    "    with open(full_metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_metadata = json.load(f)\n",
    "        full_metadata_df = pd.DataFrame(full_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_metadata_df.shape[0] != full_metadata_df[\"FILE_accession\"].nunique():\n",
    "    print(\"MAJOR ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(full_metadata_df[\"FILE_assay_title\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check relevant metadata categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_cols = []\n",
    "for col in full_metadata_df.columns:\n",
    "    if any(\n",
    "        label in col.lower()\n",
    "        for label in [\n",
    "            \"cancer\",\n",
    "            \"health\",\n",
    "            \"status\",\n",
    "            \"life\",\n",
    "            \"sex\",\n",
    "            \"biosample_ontology\",\n",
    "            \"assay\",\n",
    "        ]\n",
    "    ):\n",
    "        print(col)\n",
    "        potential_cols.append(col)\n",
    "\n",
    "for col in potential_cols:\n",
    "    display(full_metadata_df[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sex, life stage, and biomaterial type metadata categories already available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata_df[LIFE_STAGE] = full_metadata_df[\"BIOSAMPLE_life_stage\"]\n",
    "full_metadata_df[SEX] = full_metadata_df[\"BIOSAMPLE_sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata_df[BIOMAT] = full_metadata_df[\"BIOSAMPLE_TYPE_classification\"]\n",
    "full_metadata_df[BIOMAT].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replace = {\n",
    "    \"tissue\": \"primary tissue\",\n",
    "    \"in vitro differentiated cells\": \"other\",\n",
    "    \"organoid\": \"other\",\n",
    "}\n",
    "full_metadata_df[BIOMAT].replace(to_replace, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: must remove revoked files/experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output_name = \"encode_full_metadata_2025-02_no_revoked.csv\"\n",
    "new_output_path = full_metadata_path.with_name(new_output_name)\n",
    "\n",
    "if rewrite_metadata:\n",
    "    new_output_path.unlink(missing_ok=True)\n",
    "\n",
    "if not new_output_path.exists():\n",
    "    N_before = full_metadata_df.shape[0]\n",
    "    for cat_type in [\"FILE\", \"EXPERIMENT\", \"BIOSAMPLE\"]:\n",
    "        cat = f\"{cat_type}_status\"\n",
    "        full_metadata_df = full_metadata_df[full_metadata_df[cat] != \"revoked\"]\n",
    "    N_after = full_metadata_df.shape[0]\n",
    "\n",
    "    print(f\"Removed {N_before - N_after} revoked entries\")\n",
    "\n",
    "    full_metadata_df.to_csv(new_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_cols = []\n",
    "for col in full_metadata_df.columns:\n",
    "    if any(label in col.lower() for label in [\"assay\", \"target\", \"antibody\"]):\n",
    "        potential_cols.append(col)\n",
    "\n",
    "for col in potential_cols:\n",
    "    display(full_metadata_df[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_target_df = full_metadata_df[full_metadata_df[\"FILE_target\"].isnull()].copy()\n",
    "for col in potential_cols:\n",
    "    display(unknown_target_df[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: No EXPERIMENT_target means RNA-seq or input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 'assay' and 'assay_epiclass' categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_core_assays = set(ASSAY_ORDER) | {\"mrna_seq\", \"wgbs_standard\", \"wgbs_pbat\"}\n",
    "print(all_core_assays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_target_mapping = {\n",
    "    \"Control ChIP-seq\": \"input\",\n",
    "    \"total RNA-seq\": \"rna_seq\",\n",
    "    \"polyA plus RNA-seq\": \"mrna_seq\",\n",
    "    \"WGBS\": \"wgbs\",\n",
    "}\n",
    "unknown_target_df[\"assay\"] = unknown_target_df[\"EXPERIMENT_assay_title\"].map(\n",
    "    no_target_mapping\n",
    ")\n",
    "display(unknown_target_df[\"assay\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_target_df = full_metadata_df[~full_metadata_df[\"FILE_target\"].isnull()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: are all targets marked as human?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_human_target = []\n",
    "for val in known_target_df[\"FILE_target\"].value_counts(dropna=False).keys():\n",
    "    if \"human\" not in val:\n",
    "        non_human_target.append(val)\n",
    "\n",
    "for val in non_human_target:\n",
    "    sub_df = known_target_df[known_target_df[\"FILE_target\"] == val]\n",
    "    print(f\"{val}: {sub_df.shape[0]} files\")\n",
    "    # display(\n",
    "    #     known_target_df[known_target_df[\"FILE_target\"] == val].head()\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vals in known_target_df[\n",
    "    known_target_df[\"FILE_target\"].str.lower().str.contains(\"cebpa\")\n",
    "][\n",
    "    [\"FILE_accession\", \"EXPERIMENT_accession\", \"BIOSAMPLE_accession\", \"FILE_target\"]\n",
    "].values:\n",
    "    print(\"\\t\".join(vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c'est en effet inhabituel, c'est possiblement une erreur d'annotation ou encore que l'anticorps utilisé pour faire le ChIP reconnaît à la fois la prt humaine et celle du rat car les séquences de protéines sont ~95% identique; le détail important est que l'expérience a été faite dans cellules humaines, ce qui est le cas ici alors je propose de simplement ignorer la présence de rat et combiner les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_target_df[\"assay\"] = (\n",
    "    known_target_df[\"FILE_target\"]\n",
    "    .str.split(r\"/targets/\", expand=True)[1]\n",
    "    .str.split(\"-\", expand=True)[0]\n",
    "    .str.lower()\n",
    ")\n",
    "display(known_target_df[\"assay\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata_df = pd.concat([known_target_df, unknown_target_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = list(all_core_assays) + [\"ctcf\"]\n",
    "full_metadata_df[ASSAY] = [\n",
    "    label if label in unique_names else \"non-core\" for label in full_metadata_df[\"assay\"]\n",
    "]\n",
    "display(full_metadata_df[ASSAY].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add 'sample_ontology' category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in full_metadata_df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curie_def_df = pd.read_csv(\n",
    "    encode_metadata_dir / \"EpiAtlas_list-curie_term_HSOI.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"biosample_term_id\", \"biosample_term_name\", \"epiclass_sample_ontology\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = full_metadata_df.merge(\n",
    "    right=curie_def_df[[\"biosample_term_id\", \"epiclass_sample_ontology\"]],\n",
    "    left_on=\"BIOSAMPLE_TYPE_term_id\",\n",
    "    right_on=\"biosample_term_id\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop(columns=[\"biosample_term_id\"])\n",
    "new_df.rename(columns={\"epiclass_sample_ontology\": CELL_TYPE}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add 'in_epiatlas\" category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"in_epiatlas\"] = (\n",
    "    new_df[\"EXPERIMENT_related_series\"].astype(str).str.contains(\"reference-epigenomes\")\n",
    ") & (new_df[ASSAY].isin(ASSAY_ORDER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check, reference epigenomes always mean IHEC?\n",
    "\n",
    "yes: https://www.encodeproject.org/profiles/reference_epigenome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"in_epiatlas\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = \"encode_full_metadata_2025-02_no_revoked.csv\"\n",
    "output_path = full_metadata_path.with_name(output_name)\n",
    "new_df.to_csv(output_path, sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_obj = {\"datasets\": list(new_df.to_dict(\"records\"))}\n",
    "with open(output_path.with_suffix(\".json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_obj, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
