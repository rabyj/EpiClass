{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initial analysis of shap values behavior.\"\"\"\n",
    "# pylint: disable=redefined-outer-name, expression-not-assigned, import-error, not-callable, pointless-statement, no-value-for-parameter, undefined-variable, unused-argument, line-too-long\n",
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from IPython.display import display\n",
    "from scipy.special import softmax  # type: ignore\n",
    "\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "from epi_ml.core import metadata\n",
    "from epi_ml.core.analysis import bins_to_bed_ranges, write_to_bed\n",
    "\n",
    "# from epi_ml.core.data import UnknownData\n",
    "# from epi_ml.core.hdf5_loader import Hdf5Loader\n",
    "# from epi_ml.core.model_pytorch import LightningDenseClassifier\n",
    "# from epi_ml.core.shap_values import SHAP_Analyzer, SHAP_Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chroms(chrom_file):\n",
    "    \"\"\"Return sorted chromosome names list.\"\"\"\n",
    "    with open(chrom_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        chroms = []\n",
    "        for line in file:\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                name, size = line.split()\n",
    "                chroms.append(tuple([name, int(size)]))\n",
    "    chroms.sort()\n",
    "    return chroms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroms = load_chroms(\n",
    "    \"/home/local/USHERBROOKE/rabj2301/Projects/epilap/input/chromsizes/hg38.noy.chrom.sizes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "home = Path(\"/home/local/USHERBROOKE/rabj2301/Projects\")\n",
    "input_dir = home / \"epilap/input\"\n",
    "metadata_path = (\n",
    "    input_dir\n",
    "    / \"metadata/hg38_2023_epiatlas_dfreeze_plus_encode_noncore_formatted_JR.json\"\n",
    ")\n",
    "\n",
    "output = home / \"epilap/output\"\n",
    "logdir = (\n",
    "    output / \"models/SHAP\" / \"harmonized_donor_sex_1l_3000n/100kb_all_none_blklst/split0/\"\n",
    ")\n",
    "\n",
    "my_meta = metadata.Metadata(metadata_path)\n",
    "# meta_copy = copy.deepcopy(my_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_shap_samples(shap_dict, n: int) -> Dict[str, List[np.ndarray]]:\n",
    "    \"\"\"Return a subset of shap values and their ids.\"\"\"\n",
    "    selected_shap_samples = {\"shap\": [], \"ids\": []}\n",
    "    total_samples = len(shap_dict[\"ids\"])\n",
    "    selected_indices = np.random.choice(total_samples, n, replace=False)\n",
    "\n",
    "    for class_shap_values in shap_dict[\"shap\"]:\n",
    "        selected_shap_samples[\"shap\"].append(class_shap_values[selected_indices, :])\n",
    "\n",
    "    selected_shap_samples[\"ids\"] = [shap_dict[\"ids\"][idx] for idx in selected_indices]\n",
    "\n",
    "    return selected_shap_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_archives(shap_values_dir: Path):\n",
    "    \"\"\"Return shap values and explainer background archives. from npz files.\"\"\"\n",
    "    try:\n",
    "        shap_values_path = next(shap_values_dir.glob(\"*evaluation*.npz\"))\n",
    "        background_info_path = next(shap_values_dir.glob(\"*explainer_background*.npz\"))\n",
    "    except StopIteration as err:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find shap values or explainer background archives in {shap_values_dir}\"\n",
    "        ) from err\n",
    "\n",
    "    with open(shap_values_path, \"rb\") as f:\n",
    "        shap_values_archive = np.load(f)\n",
    "        shap_values_archive = dict(shap_values_archive.items())\n",
    "\n",
    "    with open(background_info_path, \"rb\") as f:\n",
    "        explainer_background = np.load(f)\n",
    "        explainer_background = dict(explainer_background.items())\n",
    "\n",
    "    return shap_values_archive, explainer_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_impact(shap_values_matrices):\n",
    "    \"\"\"Return average absolute shap values.\"\"\"\n",
    "    shap_abs = np.zeros(shap_values_matrices[0].shape)\n",
    "    for matrix in shap_values_matrices:\n",
    "        shap_abs += np.absolute(matrix)\n",
    "    shap_abs /= len(shap_values_matrices)\n",
    "    return shap_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_most_important_features(sample_shaps, n):\n",
    "    \"\"\"Return features with highest absolute shap values.\"\"\"\n",
    "    return np.flip(np.argsort(np.absolute(sample_shaps)))[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_md5s(\n",
    "    md5s: List[str], metadata: metadata.Metadata, category_label: str, labels: List[str]\n",
    ") -> List[int]:\n",
    "    \"\"\"Subsample md5s based on metadata filtering provided, for a given category and filtering labels.\n",
    "\n",
    "    Args:\n",
    "            md5s (list): A list of MD5 hashes.\n",
    "            metadata (Metadata): A metadata object containing the data to be filtered.\n",
    "            category_label (str): The category label to be used for filtering the metadata.\n",
    "            labels (list): A list of labels to be used for selecting category subsets in the metadata.\n",
    "\n",
    "    Returns:\n",
    "            list: A list of indices corresponding to the selected md5s.\n",
    "    \"\"\"\n",
    "    meta = copy.deepcopy(metadata)\n",
    "    meta.select_category_subsets(category_label, labels)\n",
    "    chosen_idxs = []\n",
    "    for i, md5 in enumerate(md5s):\n",
    "        if md5 in meta:\n",
    "            chosen_idxs.append(i)\n",
    "    return chosen_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_feature(\n",
    "    pairwise_intersections: List[Set[int]], quantile_list: List[int]\n",
    ") -> Dict[int, List[int]]:\n",
    "    \"\"\"\n",
    "    Get a list of the most frequent features from multiple feature lists, according to some quantiles..\n",
    "\n",
    "    This function takes a list of feature lists and a quantile list. It calculates the occurrence frequency\n",
    "    of each feature and returns the list of features at least as frequent as the specified quantiles.\n",
    "\n",
    "    Args:\n",
    "        feature_lists (List[List[int]]): A list of feature lists, where each inner list contains feature indices.\n",
    "        quantile_list (List[int]: The quantile values for which the most frequent features will be returned.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int:List[int]]: A dict containing the list of features in each specified quantile.\n",
    "    \"\"\"\n",
    "    for quantile in quantile_list:\n",
    "        if quantile < 0 or quantile > 100:\n",
    "            raise ValueError(\"Quantile values must be between 0 and 100.\")\n",
    "\n",
    "    # Compute the features in the specified quantiles\n",
    "    intersection_counter = Counter()\n",
    "    for feature_set in pairwise_intersections:\n",
    "        intersection_counter.update(feature_set)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data=intersection_counter, orient=\"index\").reset_index()\n",
    "    df.columns = [\"Feature\", \"Count\"]\n",
    "\n",
    "    quantile_features_dict = {}\n",
    "    for quantile in quantile_list:\n",
    "        curr_q = df[\"Count\"].quantile(\n",
    "            quantile / 100\n",
    "        )  # this calculates the quantile value\n",
    "        curr_choice = df[\n",
    "            df[\"Count\"] >= curr_q\n",
    "        ]  # this selects all features within current quantile\n",
    "        quantile_features_dict[quantile] = curr_choice[\"Feature\"].tolist()\n",
    "\n",
    "    return quantile_features_dict\n",
    "\n",
    "\n",
    "def feature_overlap_stats(\n",
    "    feature_lists: List[List[int]], quantile_list: list[int]\n",
    ") -> Tuple[Set[int], Set[int], Dict[int, List]]:\n",
    "    \"\"\"\n",
    "    Calculate the statistics of feature overlap between multiple feature lists.\n",
    "\n",
    "    This function takes a list of feature lists and calculates the median and average\n",
    "    pairwise overlaps between them. It also computes the union and intersection of all features\n",
    "    in the given feature lists.\n",
    "\n",
    "    Args:\n",
    "        feature_lists (List[List[int]]): A list of feature lists, where each inner list contains feature indices.\n",
    "        quantile_list (List[int]: The quantile values for which the most frequent features will be returned.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Set[int], Set[int], Dict[int, List]]: A tuple containing 1) intersection of all features\n",
    "        2) union of all features 3) a dict containing the list of features in each specified quantile.\n",
    "    \"\"\"\n",
    "    # Compute the overlap between two feature lists\n",
    "    all_pairwise_overlaps = [\n",
    "        set(sample1) & set(sample2)\n",
    "        for sample1, sample2 in itertools.combinations(feature_lists, 2)\n",
    "    ]\n",
    "    all_pairwise_overlaps_len = [len(x) for x in all_pairwise_overlaps]\n",
    "    print(\"Pairwise feature overlap statistics:\")\n",
    "    display(pd.DataFrame(all_pairwise_overlaps_len).describe())\n",
    "\n",
    "    # Most frequent features (per quantile)\n",
    "    frequent_features = get_most_frequent_feature(all_pairwise_overlaps, quantile_list)\n",
    "\n",
    "    # Union and intersection of all features\n",
    "    all_features_union: Set[int] = set()\n",
    "    all_features_intersection: Set[int] = set(feature_lists[0])\n",
    "    for feature_set in feature_lists:\n",
    "        all_features_union.update(feature_set)\n",
    "        all_features_intersection &= set(feature_set)\n",
    "\n",
    "    return all_features_intersection, all_features_union, frequent_features  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shap_matrix(\n",
    "    meta: metadata.Metadata,\n",
    "    shap_matrices: List[np.ndarray],\n",
    "    eval_md5s: List[str],\n",
    "    assay_subsample: List[str] | None,\n",
    "    class_idx: int = 0,\n",
    ") -> Tuple[np.ndarray, List[int]]:\n",
    "    \"\"\"Generates a SHAP matrix corresponding to a selected subset of samples.\n",
    "\n",
    "    This function selects a subset of samples based on specified criteria\n",
    "    and then generates a SHAP matrix for these selected samples. It filters\n",
    "    the metadata if a specific assay subsample is provided, and selects a\n",
    "    subset of samples that are identified by their md5 hash. It then selects\n",
    "    the SHAP values of these samples for the class number.\n",
    "\n",
    "    Args:\n",
    "        meta (metadata.Metadata): Metadata object containing information about the samples.\n",
    "        shap_matrices (List[np.ndarray]): List of SHAP matrices for each class.\n",
    "        eval_md5s (List[str]): List of md5 hashes identifying the evaluation samples.\n",
    "        assay_subsample (List[str] | None): List of assay subsamples to consider.\n",
    "                                             If None, all samples are considered.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The selected SHAP matrix for the first class and for the\n",
    "                    chosen samples based on the provided criteria.\n",
    "        List[int]: The indices of the chosen samples in the original SHAP matrix.\n",
    "    \"\"\"\n",
    "    my_meta = copy.deepcopy(meta)\n",
    "\n",
    "    # Filter metadata\n",
    "    if assay_subsample:\n",
    "        my_meta.select_category_subsets(\"assay_epiclass\", assay_subsample)\n",
    "        print(f\"Subsampled metadata with {assay_subsample}\")\n",
    "        my_meta.display_labels(\"harmonized_donor_sex\")\n",
    "\n",
    "    selected_class = \"female\"\n",
    "    chosen_idxs = subsample_md5s(\n",
    "        eval_md5s, my_meta, \"harmonized_donor_sex\", [selected_class]\n",
    "    )\n",
    "    try:\n",
    "        class_shap = shap_matrices[class_idx]\n",
    "    except IndexError as err:\n",
    "        raise IndexError(f\"Class index {class_idx} is out of bounds.\") from err\n",
    "\n",
    "    selected_class_shap = np.array(class_shap[chosen_idxs, :])\n",
    "    print(\n",
    "        f\"Shape of selected class ({selected_class}) shap values: {selected_class_shap.shape}\"\n",
    "    )\n",
    "    print(f\"Chose {len(chosen_idxs)} samples from {class_shap.shape[0]} samples\")\n",
    "    return selected_class_shap, chosen_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_overlap_stats(feature_stats: Sequence):\n",
    "    \"\"\"Prints the statistics of feature overlap.\n",
    "\n",
    "    This function receives the feature statistics which include the intersection,\n",
    "    union and frequent features in each quantile of features. It then prints\n",
    "    these statistics for easy inspection.\n",
    "\n",
    "    Args:\n",
    "        feature_stats (Sequence): Tuple containing the intersection, union and\n",
    "                                  frequent features in each quantile of features.\n",
    "    \"\"\"\n",
    "    features_intersection, features_union, frequent_features = feature_stats\n",
    "    print(f\"Intersection of all features: {len(features_intersection)} features\")\n",
    "    print(f\"Fully intersecting features: {list(features_intersection)}\")\n",
    "    print(f\"Union of all features: {len(features_union)} features\\n\")\n",
    "    for k, v in frequent_features.items():\n",
    "        print(f\"Most frequent features in {k}th quantile: {len(v)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_importance_info(feature_selection: List[int], shap_matrix: np.ndarray):\n",
    "    \"\"\"Prints the feature importance information.\n",
    "\n",
    "    This function prints the feature importance information, which includes the\n",
    "    average expected contribution of the selected features and one feature (if\n",
    "    the importance was uniform), and statistical descriptions of the contributions\n",
    "    of the selected features.\n",
    "\n",
    "    Args:\n",
    "        feature_selection (List[int]): The indices of the selected features.\n",
    "        shap_matrix (np.ndarray): The SHAP values matrix.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(feature_selection)\n",
    "    nb_files = shap_matrix.shape[0]\n",
    "    print(\n",
    "        f\"Average expected contribution of {N} feature if uniform importance:{N/30321*100:.5f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Average expected contribution of 1 feature if uniform importance:{1/30321*100:.5f}%\"\n",
    "    )\n",
    "    print(f\"Average contribution of selected features for {nb_files} files:\")\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            softmax(shap_matrix, axis=1)[:, list(feature_selection)].sum(axis=1) * 100\n",
    "        ).describe()\n",
    "    )\n",
    "    print(f\"Individual contribution of selected features for {nb_files} files:\")\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            softmax(shap_matrix, axis=1)[:, list(feature_selection)] * 100\n",
    "        ).describe()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_whole_thing(metadata: metadata.Metadata, shap_logdir: Path, top_n: int = 100):\n",
    "    \"\"\"Execute the complete process of SHAP value analysis.\n",
    "\n",
    "    This function performs the complete SHAP value analysis given the metadata and the directory\n",
    "    of the SHAP value files. It carries out the following steps:\n",
    "    1. Load the SHAP value archives and print basic statistics.\n",
    "    2. Filter the metadata to match the samples in SHAP value archives.\n",
    "    3. Select a specific assay and compute SHAP values.\n",
    "    4. Determine the top N features for each sample.\n",
    "    5. Compute feature overlap statistics and print them.\n",
    "    6. Analyze feature importance.\n",
    "    7. Convert bin indices to genomic ranges and write to a BED file.\n",
    "    8. Display and save a plot of importance distribution for one sample.\n",
    "\n",
    "    Args:\n",
    "        metadata (metadata.Metadata): The metadata for the samples.\n",
    "        shap_logdir (Path): The directory path where SHAP value files are stored.\n",
    "    \"\"\"\n",
    "    metadata = copy.deepcopy(metadata)\n",
    "    shap_values_archive, _ = get_archives(shap_logdir)\n",
    "\n",
    "    eval_md5s: List[str] = shap_values_archive[\"evaluation_md5s\"]\n",
    "    shap_matrices = shap_values_archive[\"shap_values\"]\n",
    "\n",
    "    print(f\"nb classes: {len(shap_matrices)}\")\n",
    "    print(f\"nb samples: {len(eval_md5s)}\")\n",
    "    print(f\"dim shap value matrix: {shap_matrices[0].shape}\")\n",
    "    print(f\"Output classes of classifier:\\n {shap_values_archive['classes']}\")\n",
    "\n",
    "    for md5 in list(metadata.md5s):\n",
    "        if md5 not in set(eval_md5s):\n",
    "            del metadata[md5]\n",
    "\n",
    "    metadata.display_labels(\"assay_epiclass\")\n",
    "    metadata.display_labels(\"harmonized_donor_sex\")\n",
    "\n",
    "    chosen_assay = \"h3k27ac\"\n",
    "    shap_matrix, chosen_idxs = get_shap_matrix(\n",
    "        metadata, shap_matrices, eval_md5s, [chosen_assay]\n",
    "    )\n",
    "\n",
    "    print(f\"Selecting features with top {top_n} SHAP values for each sample.\")\n",
    "    top_n_features = []\n",
    "    for sample in shap_matrix:\n",
    "        top_n_features.append(list(n_most_important_features(sample, top_n)))\n",
    "\n",
    "    some_stats = feature_overlap_stats(top_n_features, [0, 90, 95, 99])\n",
    "    frequent_features = some_stats[2]\n",
    "\n",
    "    print_feature_overlap_stats(some_stats)\n",
    "\n",
    "    feature_selection = frequent_features[99]\n",
    "\n",
    "    print_importance_info(feature_selection, shap_matrix)\n",
    "\n",
    "    bed_vals = bins_to_bed_ranges(\n",
    "        sorted(feature_selection), chroms, resolution=100 * 1000\n",
    "    )\n",
    "    write_to_bed(\n",
    "        bed_vals,\n",
    "        shap_logdir / f\"frequent_features_99_female_{chosen_assay}.bed\",\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    print(\"One sample\")\n",
    "    probs_1sample = pd.DataFrame(softmax(shap_matrix, axis=1)[0, :] * 100)\n",
    "    display(probs_1sample.describe())\n",
    "    fig_title = f\"Importance distribution - One sample - {eval_md5s[chosen_idxs[0]]}\"\n",
    "    fig = px.violin(probs_1sample, box=True, points=\"all\", title=fig_title)\n",
    "    fig.write_image(shap_logdir / \"importance_dist_1sample.png\")\n",
    "\n",
    "    return frequent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_shap_logdir = output / \"models\" / \"SHAP\" / \"harmonized_donor_sex_1l_3000n\"\n",
    "names = [\n",
    "    \"100kb_all_none_0blklst/10fold-l1-50_dropout-0.25\",\n",
    "    \"100kb_all_none_0blklst_winsorized/10fold-l1-50_dropout-0.25\",\n",
    "]\n",
    "for name in names:\n",
    "    print(name)\n",
    "    path = gen_shap_logdir / name / \"split0\"\n",
    "    frequent_features = run_the_whole_thing(my_meta, path)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
