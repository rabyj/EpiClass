{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create plots of some results.\"\"\"\n",
    "# pylint: disable=import-error,redefined-outer-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All classifiers performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path.home() / \"downloads\" / \"temp\" / \"all_metrics - Pivot Table 1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_df(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Create a dataframe from the csv file.\"\"\"\n",
    "    metrics_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Fill missing values in the 'classifier' column using forward fill method\n",
    "    metrics_df[\"classifier\"] = metrics_df[\"classifier\"].fillna(method=\"ffill\")\n",
    "\n",
    "    # Rename columns to match the desired format\n",
    "    metrics_df.rename(\n",
    "        columns={\n",
    "            \"classifier\": \"Classifier\",\n",
    "            \"metric\": \"Metric\",\n",
    "            \"AVERAGE of value\": \"Average\",\n",
    "            \"STDEV of value\": \"Std\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifiers_performance(metrics_df: pd.DataFrame) -> None:\n",
    "    \"\"\"Plot the performance of multiple classifiers.\"\"\"\n",
    "    # Set the figure size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Create a bar plot without error bars\n",
    "    barplot = sns.barplot(\n",
    "        data=metrics_df, x=\"Classifier\", y=\"Average\", hue=\"Metric\", errorbar=None\n",
    "    )\n",
    "\n",
    "    # Get the x and y coordinates of the bars\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    for rect in barplot.patches:\n",
    "        x_coords.append(rect.get_x() + rect.get_width() / 2)\n",
    "        y_coords.append(rect.get_height())\n",
    "\n",
    "    # Calculate the number of metrics and classifiers to determine the positions of the error bars\n",
    "    num_metrics = metrics_df[\"Metric\"].nunique()\n",
    "    num_classifiers = metrics_df[\"Classifier\"].nunique()\n",
    "\n",
    "    # Add the error bars\n",
    "    for i in range(num_classifiers):\n",
    "        for j in range(num_metrics):\n",
    "            barplot.errorbar(\n",
    "                x_coords[i * num_metrics + j],\n",
    "                y_coords[i * num_metrics + j],\n",
    "                yerr=metrics_df[\"Std\"][i * num_metrics + j],\n",
    "                color=\"black\",\n",
    "                capsize=3,\n",
    "                fmt=\"none\",\n",
    "            )\n",
    "\n",
    "    # Set the y-axis limits center the value distribution\n",
    "    plt.ylim(min(y_coords) - 0.025, min(max(y_coords) + 0.025, 1))  # type: ignore\n",
    "\n",
    "    # Scale the y-axis to percentage\n",
    "    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "\n",
    "    plt.title(\"Classifier Performance\")\n",
    "    plt.ylabel(\"Performance\")\n",
    "\n",
    "    # Move the legend outside the plot\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence threshold impact on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_thresholds(df: pd.DataFrame, thresholds: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy and subset size for different probability thresholds with improved automatic column detection.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe containing true labels and predicted probabilities.\n",
    "    thresholds (list): List of probability thresholds to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe containing the accuracy and subset size for each threshold.\n",
    "    \"\"\"\n",
    "    # Improved automatic column detection\n",
    "    likely_true_class_cols = [col for col in df.columns if \"true\" in col.lower()]\n",
    "    likely_pred_class_cols = [col for col in df.columns if \"pred\" in col.lower()]\n",
    "\n",
    "    if not likely_true_class_cols or not likely_pred_class_cols:\n",
    "        raise ValueError(\n",
    "            \"Could not automatically detect 'True class' or 'Predicted class' columns.\"\n",
    "        )\n",
    "\n",
    "    true_col = likely_true_class_cols[0]\n",
    "    pred_col = likely_pred_class_cols[0]\n",
    "\n",
    "    # Assume remaining numeric columns contain predicted probabilities\n",
    "    pred_prob_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    results = []\n",
    "    total_size = len(df)\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        # Filter rows where the max predicted probability is above the threshold\n",
    "        subset_df = df[df[pred_prob_cols].max(axis=1) >= thresh]\n",
    "\n",
    "        if len(subset_df) == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate the accuracy for this subset\n",
    "        correct_preds = np.sum(subset_df[true_col] == subset_df[pred_col])\n",
    "        accuracy = (correct_preds / len(subset_df)) * 100\n",
    "\n",
    "        # Calculate the size of this subset as a percentage of the total dataset\n",
    "        subset_size_percent = (len(subset_df) / total_size) * 100\n",
    "\n",
    "        results.append([thresh, accuracy, subset_size_percent])\n",
    "\n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    results_df = pd.DataFrame(\n",
    "        results, columns=[\"Threshold\", \"Accuracy (%)\", \"Subset Size (%)\"]\n",
    "    )\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def create_thresholds_graph(threshold_df: pd.DataFrame, name: str):\n",
    "    \"\"\"Return graph of the accuracy and subset size at different probability thresholds.\"\"\"\n",
    "    # Plotting the final graph with dual y-axes and 11 ticks on the x-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Make the first plot for \"Accuracy (%)\"\n",
    "    ax1.plot(\n",
    "        threshold_df[\"Threshold\"],\n",
    "        threshold_df[\"Accuracy (%)\"],\n",
    "        label=\"Accuracy (%)\",\n",
    "        marker=\"o\",\n",
    "        color=\"b\",\n",
    "    )\n",
    "    ax1.set_xlabel(\"Probability Threshold\")\n",
    "    ax1.set_ylabel(\"Accuracy (%)\", color=\"b\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"b\")\n",
    "\n",
    "    # Make the second plot for \"Subset Size (%)\"\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(\n",
    "        threshold_df[\"Threshold\"],\n",
    "        threshold_df[\"Subset Size (%)\"],\n",
    "        label=\"Subset Size (%)\",\n",
    "        marker=\"x\",\n",
    "        color=\"r\",\n",
    "    )\n",
    "    ax2.set_ylabel(\"Subset Size (%)\", color=\"r\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"r\")\n",
    "\n",
    "    # Set 11 ticks on the x-axis\n",
    "    ax1.set_xticks(np.linspace(0, 1, 11))\n",
    "\n",
    "    # Add grid and title\n",
    "    ax1.grid(True)\n",
    "    plt.title(\"Accuracy and Subset Size at Different Probability Thresholds\\n\" + name)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_base = (\n",
    "    Path.home()\n",
    "    / \"mounts/narval-mount/project-rabyj/epilap/output/logs/epiatlas-dfreeze-v2.1/hg38_100kb_all_none\"\n",
    ")\n",
    "files = [\n",
    "    file\n",
    "    for file in list(source_base.glob(\"*/*/full-10fold-validation_prediction.csv\"))\n",
    "    if \"oversampling\" not in str(file)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in files:\n",
    "#     print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = list(np.arange(0, 1, 1 / 20)) + [0.99]\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=0)\n",
    "    nb_samples = df.shape[0]\n",
    "    nb_classes = len(df.select_dtypes(include=[np.number]).columns.tolist())\n",
    "    threshold_df = evaluate_thresholds(df, thresholds)\n",
    "    name = f\"{file.parents[1].name} - {file.parents[0].name} - {nb_classes} classes\"\n",
    "    graph = create_thresholds_graph(threshold_df, f\"{name} - n={nb_samples}\")\n",
    "    plt.savefig(file.parent / \"threshold_impact_graph.png\")\n",
    "    new_filename = (\n",
    "        name.replace(\" \", \"_\").replace(\"\\n\", \"_\").replace(\"-\", \"\").replace(\"__\", \"_\")\n",
    "    )\n",
    "    graph.savefig(f\"threshold_impact_graph_{new_filename}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
