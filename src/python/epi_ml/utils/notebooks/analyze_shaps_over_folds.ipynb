{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze shaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initial analysis of shap values behavior.\"\"\"\n",
    "# pylint: disable=redefined-outer-name, expression-not-assigned, import-error, not-callable, pointless-statement, no-value-for-parameter, undefined-variable, unused-argument, line-too-long, use-dict-literal, too-many-lines, unused-import, unused-variable, too-many-branches\n",
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Sequence, Set, Tuple\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patheffects as path_effects\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px  # type: ignore\n",
    "import plotly.graph_objects as go  # type: ignore\n",
    "import plotly.io as pio  # type: ignore\n",
    "import upsetplot  # type: ignore\n",
    "from IPython.display import display\n",
    "from matplotlib_venn import venn3  # type: ignore\n",
    "\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "from epi_ml.core import metadata\n",
    "from epi_ml.core.data_source import EpiDataSource\n",
    "from epi_ml.utils.bed_utils import bins_to_bed_ranges, write_to_bed\n",
    "from epi_ml.utils.general_utility import get_valid_filename\n",
    "from epi_ml.utils.metadata_utils import count_combinations, count_pairs\n",
    "from epi_ml.utils.shap.subset_features_handling import (\n",
    "    collect_all_features_from_feature_count_file,\n",
    "    collect_features_from_feature_count_file,\n",
    "    process_all_subsamplings,\n",
    ")\n",
    "\n",
    "BIOMATERIAL_TYPE = \"harmonized_biomaterial_type\"\n",
    "CELL_TYPE = \"harmonized_sample_ontology_intermediate\"\n",
    "ASSAY = \"assay_epiclass\"\n",
    "SEX = \"harmonized_donor_sex\"\n",
    "CANCER = \"harmonized_sample_cancer_high\"\n",
    "DISEASE = \"harmonized_sample_disease_high\"\n",
    "LIFE_STAGE = \"harmonized_donor_life_stage\"\n",
    "TRACK = \"track_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "home = Path().home() / \"Projects\"\n",
    "input_dir = home / \"epiclass/input\"\n",
    "metadata_path = (\n",
    "    input_dir\n",
    "    / \"metadata/dfreeze-v2/hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2.json\"\n",
    ")\n",
    "my_meta = metadata.Metadata(metadata_path)\n",
    "chroms = EpiDataSource.load_external_chrom_file(\n",
    "    input_dir / \"chromsizes/hg38.noy.chrom.sizes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 100 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = CELL_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_meta.remove_missing_labels(category)\n",
    "my_meta.display_labels(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze important features over all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logs_dir = (\n",
    "#     Path.home()\n",
    "#     / \"mounts/narval-mount/project-rabyj/epilap/output/logs/epiatlas-dfreeze-v2.1/hg38_100kb_all_none\"\n",
    "# )\n",
    "logs_dir = Path.home() / \"scratch/epiclass/join_important_features/\"\n",
    "\n",
    "if not logs_dir.exists():\n",
    "    raise ValueError(f\"Logs dir {logs_dir} does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect information from all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_hint_repr(obj: Any) -> str:\n",
    "    \"\"\"\n",
    "    Generates a string representation of the type hint for the given object.\n",
    "\n",
    "    Args:\n",
    "    obj (Any): The object to generate type hint for.\n",
    "\n",
    "    Returns:\n",
    "    str: A string representation of the type hint.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (dict, defaultdict)):\n",
    "        key_types = set(type_hint_repr(key) for key in obj.keys())\n",
    "        value_types = set(type_hint_repr(value) for value in obj.values())\n",
    "\n",
    "        key_type = next(iter(key_types)) if len(key_types) == 1 else \"Any\"\n",
    "        value_type = next(iter(value_types)) if len(value_types) == 1 else \"Any\"\n",
    "\n",
    "        dict_type = \"Dict\" if isinstance(obj, dict) else \"defaultdict\"\n",
    "        return f\"{dict_type}[{key_type}, {value_type}]\"\n",
    "    if isinstance(obj, list):\n",
    "        element_types = set(type_hint_repr(element) for element in obj)\n",
    "        element_type = next(iter(element_types)) if len(element_types) == 1 else \"Any\"\n",
    "        return f\"List[{element_type}]\"\n",
    "    if isinstance(obj, set):\n",
    "        element_types = set(type_hint_repr(element) for element in obj)\n",
    "        element_type = next(iter(element_types)) if len(element_types) == 1 else \"Any\"\n",
    "        return f\"Set[{element_type}]\"\n",
    "    if isinstance(obj, tuple):\n",
    "        element_types = tuple(type_hint_repr(element) for element in obj)\n",
    "        return f\"Tuple[{', '.join(element_types)}]\"\n",
    "\n",
    "    return obj.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_important_features(\n",
    "    parent_folder: Path, top_n: int, frequency_threshold: str\n",
    ") -> Dict[str, Dict[str, Dict[str, Dict[str, List[int]]]]]:\n",
    "    \"\"\"Join important features from all folds\n",
    "\n",
    "    Args:\n",
    "        parent_folder (Path): Parent folder of split folders.\n",
    "        top_n (int): Number of top features to consider for each sample,\n",
    "                    used to select the analysis folder.\n",
    "        frequency_threshold (str): Frequency threshold used to select the analysis folder.\n",
    "\n",
    "    Returns:\n",
    "        Dict: all_important_features\n",
    "        - level 1: folder_name (str): split_dict\n",
    "        - level 2: split_name (str) : all_class_features (dict)\n",
    "        - level 3: class_label (classifier outputs, str): important features for each frequency threshold (dict)\n",
    "        - level 4: frequency_threshold (str, 0 to 100): list of features (List[int])\n",
    "    \"\"\"\n",
    "    print(\"WARNING: Consider using a local copy of the data to speed up the process.\")\n",
    "    all_important_features = defaultdict(dict)\n",
    "    all_folders = parent_folder.glob(\n",
    "        f\"split*/shap/analysis_n{top_n}_f{frequency_threshold}/*\"\n",
    "    )\n",
    "    for important_features_path in all_folders:\n",
    "        split = important_features_path.parents[2].name\n",
    "        if \"split\" not in split:\n",
    "            raise ValueError(f\"Split not found in {split}\")\n",
    "        folder = important_features_path.name\n",
    "\n",
    "        json_path = important_features_path / \"important_features.json\"\n",
    "        try:\n",
    "            with open(json_path, \"r\", encoding=\"utf8\") as f:\n",
    "                features = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {json_path} not found\")\n",
    "            important_features_path.rmdir()\n",
    "            continue\n",
    "\n",
    "        if not features:\n",
    "            print(f\"File {json_path} is empty. Removing folder.\")\n",
    "            json_path.unlink()\n",
    "            important_features_path.rmdir()\n",
    "            continue\n",
    "\n",
    "        all_important_features[folder][split] = features\n",
    "    # print(type_hint_repr(all_important_features))\n",
    "    return all_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_kfold_shap_analysis(\n",
    "    important_features_all_splits: Dict[str, Dict[str, Dict[str, List[int]]]],\n",
    "    resolution: int,\n",
    "    chromsizes: List[Tuple[str, int]],\n",
    "    output_folder: Path,\n",
    "    name: str,\n",
    "    chosen_percentile: float | int = 80,\n",
    "    minimum_count: int = 8,\n",
    "    verbose: bool = False,\n",
    ") -> Dict[str, List[Tuple[str, List[int]]]]:\n",
    "    \"\"\"Compare the SHAP analysis results from multiple splits and writes the results based on frequency.\n",
    "\n",
    "    Args:\n",
    "        important_features_all_splits: Dictionary containing important features for each split.\n",
    "        resolution (int): Resolution for binning.\n",
    "        chromsizes (List[Tuple[str, int]]): List with chromosome names and sizes.\n",
    "        output_folder (Path): Output directory for writing BED files.\n",
    "        name (str): Name of the analysis.\n",
    "        chosen_percentile (float | int, optional): The chosen percentile for selecting important features.\n",
    "        minimum_count (int, optional): The minimum count of splits that a feature must be present in.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[Tuple[str, List[int]]]]: A dict containing the frequency of features for each class over all splits, for all given percentiles.\n",
    "    \"\"\"\n",
    "    class_features_frequency: Dict[str, List] = {}\n",
    "    class_labels = set()\n",
    "    # print(type_hint_repr(important_features_all_splits))\n",
    "    for split_dict in important_features_all_splits.values():\n",
    "        class_labels.update(list(split_dict.keys()))\n",
    "\n",
    "    percentile_labels = set()\n",
    "    for split_dict in important_features_all_splits.values():\n",
    "        percentile_lists = itertools.chain(\n",
    "            list(class_dict.keys()) for class_dict in list(split_dict.values())\n",
    "        )\n",
    "        for percentile_list in percentile_lists:\n",
    "            percentile_labels.update(percentile_list)\n",
    "\n",
    "    if str(chosen_percentile) not in percentile_labels:\n",
    "        raise ValueError(\n",
    "            f\"Chosen percentile {chosen_percentile} not found in {percentile_labels}.\"\n",
    "        )\n",
    "\n",
    "    for class_label in class_labels:\n",
    "        feature_counter = Counter()\n",
    "\n",
    "        # Count the occurrence of each feature across all splits\n",
    "        for features_dict in important_features_all_splits.values():\n",
    "            current_features = features_dict.get(class_label, {}).get(\n",
    "                str(chosen_percentile), []\n",
    "            )\n",
    "            feature_counter.update(current_features)\n",
    "\n",
    "        class_features_frequency[class_label] = feature_counter.most_common()\n",
    "\n",
    "    # Select features present in at least a certain count of splits (e.g., 8 out of 10)\n",
    "    for class_label, feature_count_list in class_features_frequency.items():\n",
    "        selected_features = {\n",
    "            feature for feature, count in feature_count_list if count >= minimum_count\n",
    "        }\n",
    "\n",
    "        if not selected_features:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"No features meeting the required count for class {class_label}\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        bed_vals = bins_to_bed_ranges(\n",
    "            sorted(selected_features), chromsizes, resolution=resolution\n",
    "        )\n",
    "\n",
    "        bed_filename = get_valid_filename(\n",
    "            f\"selected_features_{name}_f{chosen_percentile:.2f}_count{minimum_count}_{class_label}.bed\"\n",
    "        )\n",
    "\n",
    "        output_folder.mkdir(exist_ok=True, parents=True)\n",
    "        write_to_bed(\n",
    "            bed_vals,\n",
    "            output_folder / bed_filename,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "    return class_features_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_global_analysis(\n",
    "    all_important_features: Dict[str, Dict[str, Dict[str, Dict[str, List[int]]]]],\n",
    "    resolution: int,\n",
    "    chromsizes: List[Tuple[str, int]],\n",
    "    output_folder: Path,\n",
    "    chosen_percentile: float = 80.0,\n",
    "    minimum_count: int = 8,\n",
    "    top_n: int = 303,\n",
    "    verbose: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Perform global analysis of shap values. For each\n",
    "    subsampling, compute the frequency of features over all splits\n",
    "    and save it to a feature_count.json file.\n",
    "\n",
    "    Also write to bed the features that respect the chosen_percentile and\n",
    "    minimum_count thresholds.\n",
    "    \"\"\"\n",
    "    for folder_name, splits_dict in all_important_features.items():\n",
    "        parent = output_folder / \"global_shap_analysis\"\n",
    "        if not parent.exists():\n",
    "            raise ValueError(f\"Parent folder {parent} does not exist\")\n",
    "        subsampling_output = parent / f\"top{top_n}\" / folder_name\n",
    "        subsampling_output.mkdir(parents=True, exist_ok=True)\n",
    "        if verbose:\n",
    "            print(f\"Performing global analysis for {folder_name}\")\n",
    "        feature_count = compare_kfold_shap_analysis(\n",
    "            important_features_all_splits=splits_dict,\n",
    "            resolution=resolution,\n",
    "            chromsizes=chromsizes,\n",
    "            output_folder=subsampling_output,\n",
    "            name=folder_name,\n",
    "            chosen_percentile=chosen_percentile,\n",
    "            minimum_count=minimum_count,\n",
    "            verbose=False,\n",
    "        )\n",
    "        with open(subsampling_output / \"feature_count.json\", \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(feature_count, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dir = (\n",
    "#     logs_dir\n",
    "#     / \"harmonized_sample_ontology_intermediate_1l_3000n/10fold-oversampling/global_shap_analysis/top303\"\n",
    "# )\n",
    "# input_dir = logs_dir / \"harmonized_sample_cancer_high_1l_3000n/10fold-oversampling/global_shap_analysis/top303/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = (\n",
    "    Path.home()\n",
    "    / \"scratch/epiclass/join_important_features/hg38_regulatory_regions_n30321_100kb_coord\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = [ASSAY, CELL_TYPE, LIFE_STAGE, SEX, CANCER]\n",
    "# rest_of_paths = [\n",
    "#     \"11c/10fold-oversampling\",\n",
    "#     \"10fold-oversampling\",\n",
    "#     \"no-unknown/10fold-oversampling\",\n",
    "#     \"w-mixed/10fold-oversample\",\n",
    "#     \"10fold-oversampling\",\n",
    "# ]\n",
    "categories = [CELL_TYPE]\n",
    "rest_of_paths = [\"10fold-oversampling\"]\n",
    "\n",
    "# Variables used to find the right analysis folder and write new files\n",
    "top_n = 303\n",
    "chosen_percentile = 80.0\n",
    "minimum_count = 8\n",
    "frequency_threshold = f\"{chosen_percentile:.2f}\"\n",
    "\n",
    "pairs = list(zip(categories, rest_of_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, rest_of_path in pairs:\n",
    "    # if category != ASSAY:\n",
    "    #     continue\n",
    "\n",
    "    print(f\"Performing global analysis for classifiers of {category}\")\n",
    "    results_dir = logs_dir / f\"{category}_1l_3000n/{rest_of_path}\"\n",
    "    if not results_dir.is_dir():\n",
    "        raise ValueError(f\"Directory {results_dir} does not exist.\")\n",
    "\n",
    "    all_important_features = join_important_features(\n",
    "        results_dir, top_n=top_n, frequency_threshold=frequency_threshold\n",
    "    )\n",
    "\n",
    "    (results_dir / \"global_shap_analysis\").mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "    perform_global_analysis(\n",
    "        all_important_features=all_important_features,\n",
    "        resolution=RESOLUTION,\n",
    "        chromsizes=chroms,\n",
    "        output_folder=results_dir,\n",
    "        chosen_percentile=chosen_percentile,\n",
    "        minimum_count=minimum_count,\n",
    "        top_n=top_n,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subsamplings_intersection_matrix(\n",
    "    flat_features_dict: Dict[str, Set[int]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create an matrix of the intersection of each feature sets of flat_features_dict.\n",
    "\n",
    "    Args:\n",
    "        flat_features_dict (Dict[str, Set[int]]): A dictionary containing features sets.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Intersection matrix (dtype=int) of the feature sets.\n",
    "        The diagonal is the number of features in each set.\n",
    "        Labels are the keys of flat_features_dict.\n",
    "    \"\"\"\n",
    "    # Make pandas intersection matrix, diagonal is the number of features in each dict item\n",
    "    intersection_matrix = np.zeros(\n",
    "        (len(flat_features_dict), len(flat_features_dict)), dtype=int\n",
    "    )\n",
    "    for i, (_, features1) in enumerate(flat_features_dict.items()):\n",
    "        for j, (_, features2) in enumerate(flat_features_dict.items()):\n",
    "            intersection_matrix[i, j] = len(features1.intersection(features2))\n",
    "\n",
    "    labels = [label.lower() for label in flat_features_dict]\n",
    "    matrix_df = pd.DataFrame(data=intersection_matrix, index=labels, columns=labels)\n",
    "\n",
    "    # Remove completely empty row and columns\n",
    "    rem = np.where(matrix_df.sum(axis=1) == 0)\n",
    "    matrix_df = matrix_df.drop(matrix_df.index[rem])\n",
    "    matrix_df = matrix_df.drop(matrix_df.columns[rem], axis=1)\n",
    "\n",
    "    return matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_key_subsampling(value: str, invert: bool = False) -> Tuple[str, str]:\n",
    "    \"\"\"Sort in a smarter way the subsampling names.\"\"\"\n",
    "    try:\n",
    "        prefix, suffix = value.split(\" & \", 1)\n",
    "    except ValueError:\n",
    "        prefix = value\n",
    "        suffix = \"\"\n",
    "\n",
    "    if invert:\n",
    "        return (suffix.lower(), prefix.lower())\n",
    "    return (prefix.lower(), suffix.lower())\n",
    "\n",
    "\n",
    "def sorting_key_subsampling_invert(value: str) -> Tuple[str, str]:\n",
    "    \"\"\"Sort in a smarter way the subsampling names.\"\"\"\n",
    "    return sorting_key_subsampling(value, invert=True)\n",
    "\n",
    "\n",
    "def add_count_to_labels(\n",
    "    matrix_df: pd.DataFrame, my_meta: metadata.Metadata\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add sample count to the labels, only works for simple subsamplings\"\"\"\n",
    "    label_pairs = count_pairs(my_meta, ASSAY, CELL_TYPE)\n",
    "    label_pairs = {\n",
    "        f\"{names[0]} & {names[1].lower()}\": count for names, count in label_pairs.items()\n",
    "    }\n",
    "    new_index = [label for label in matrix_df.index if label in label_pairs]\n",
    "    new_index_mapping = {label: f\"{label} ({label_pairs[label]})\" for label in new_index}\n",
    "    matrix_df.rename(index=new_index_mapping, inplace=True)\n",
    "    return matrix_df\n",
    "\n",
    "\n",
    "def sort_matrix_labels(matrix_df: pd.DataFrame, output_class_first: bool) -> pd.DataFrame:\n",
    "    \"\"\"Sort the labels in the matrix\"\"\"\n",
    "    sorting_key = (\n",
    "        sorting_key_subsampling_invert if output_class_first else sorting_key_subsampling\n",
    "    )\n",
    "\n",
    "    sorted_index = sorted(matrix_df.index, key=sorting_key)\n",
    "    sorted_columns = sorted(matrix_df.columns, key=sorting_key)\n",
    "    sorted_matrix_df = matrix_df.reindex(index=sorted_index, columns=sorted_columns)\n",
    "    return sorted_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_intersection_matrix_heatmap(\n",
    "    matrix_df: pd.DataFrame, category: str, output_folder: Path\n",
    ") -> Path:\n",
    "    \"\"\"Create a heatmap of the intersection matrix.\n",
    "\n",
    "    Args:\n",
    "        matrix_df (pd.DataFrame): Intersection matrix. Index and columns labels used as is.\n",
    "        category (str): Category name. Used in the plot title.\n",
    "    \"\"\"\n",
    "    # Create the figure and axis objects\n",
    "    fig, ax = plt.subplots(figsize=(45, 40))  # Adjust the size as needed\n",
    "\n",
    "    # Create a colormap that sets the 'bad' values (masked) to white\n",
    "\n",
    "    masked_array = np.ma.array(matrix_df, mask=(matrix_df == 0))  # pylint: disable=C0325\n",
    "    cmap = mpl.cm.get_cmap(\"viridis\").copy()  # type: ignore\n",
    "    cmap.set_bad(color=\"white\")\n",
    "\n",
    "    # Display the data\n",
    "    cax = ax.imshow(masked_array, cmap=cmap, interpolation=\"none\")\n",
    "\n",
    "    # Add colorbar\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Annotate each cell with the numeric value\n",
    "    # We iterate over the indices and the data in the DataFrame\n",
    "    for (i, j), val in np.ndenumerate(matrix_df):\n",
    "        if val != 0:  # Skip zero values to keep the white background clean\n",
    "            ax.text(j, i, int(val), ha=\"center\", va=\"center\", color=\"white\", path_effects=[path_effects.withStroke(linewidth=1.5, foreground=\"black\")])  # type: ignore\n",
    "\n",
    "    # Adjust the grid lines and labels\n",
    "    ax.grid(which=\"major\", color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "    # Shift the ticks and labels to be at the center of each cell\n",
    "    ax.set_xticks(np.arange(len(matrix_df.columns)) - 0.5)\n",
    "    ax.set_yticks(np.arange(len(matrix_df.index)) - 0.5)\n",
    "\n",
    "    # Set the labels to be at the tick locations\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", length=0)  # Hide the tick marks\n",
    "\n",
    "    # Add new axis labels with the correct offset\n",
    "    ax2 = ax.secondary_xaxis(\"bottom\")\n",
    "    ax2.set_xticks(np.arange(len(matrix_df.columns)))\n",
    "    ax2.set_xticklabels(matrix_df.columns, rotation=45, ha=\"right\")\n",
    "\n",
    "    ax3 = ax.secondary_yaxis(\"left\")\n",
    "    ax3.set_yticks(np.arange(len(matrix_df.index)))\n",
    "    ax3.set_yticklabels(matrix_df.index)\n",
    "\n",
    "    # Set the title and show the plot\n",
    "    title = f\"{category} important features groups intersection heatmap (top100, f80, split_count=8)\"\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    img_path = output_folder / (get_valid_filename(title) + \".png\")\n",
    "    fig.savefig(str(img_path), dpi=150)\n",
    "    plt.close(fig)\n",
    "    return img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = Path.home() / \"scratch/epiclass/join_important_features/hg38_100kb_all_none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_task_features = {}\n",
    "global_union_features = {}\n",
    "for category, rest_of_path in pairs:\n",
    "    if category != CELL_TYPE:\n",
    "        continue\n",
    "\n",
    "    results_dir = logs_dir / f\"{category}_1l_3000n/{rest_of_path}\"\n",
    "\n",
    "    global_analysis_folder = results_dir / \"global_shap_analysis\" / \"top303\"\n",
    "    global_analysis_folder.mkdir(parents=False, exist_ok=True)\n",
    "    print(f\"Global analysis folder: {global_analysis_folder}\")\n",
    "\n",
    "    feature_sets = process_all_subsamplings(\n",
    "        global_analysis_folder, aggregate=True, minimum_count=8, verbose=False\n",
    "    )\n",
    "    global_union_features[category] = feature_sets[\"global_union\"]\n",
    "    every_task_features[category] = feature_sets\n",
    "\n",
    "    # # Create intersection matrix\n",
    "    # matrix_df = create_subsamplings_intersection_matrix(feature_sets)\n",
    "    # matrix_df = sort_matrix_labels(matrix_df, output_class_first=False)\n",
    "\n",
    "    # matrix_df.to_csv(\n",
    "    #     global_analysis_folder\n",
    "    #     / f\"intersection_matrix_{category}_top{top_n}_f{frequency_threshold}.csv\"\n",
    "    # )\n",
    "\n",
    "    # counter = count_combinations(my_meta, set([ASSAY, CELL_TYPE, category]))\n",
    "    # with open(\n",
    "    #     global_analysis_folder / f\"combination_count_{category}.csv\", \"w\", encoding=\"utf8\"\n",
    "    # ) as f:\n",
    "    #     for combination, count in sorted(counter.items()):\n",
    "    #         f.write(f\"{','.join(combination)},{count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_enrichment = defaultdict(set)\n",
    "for key, value in every_task_features[CELL_TYPE].items():\n",
    "    if \"merge_samplings\" in key:\n",
    "        for feature in value:\n",
    "            feature_enrichment[feature].add(key.replace(\"merge_samplings_\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_ranges = bins_to_bed_ranges(\n",
    "    bin_indexes=sorted(feature_enrichment.keys()),\n",
    "    chroms=chroms,\n",
    "    resolution=RESOLUTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    global_analysis_folder / \"cell_type_feature_enrichment_bed-details.tsv\",\n",
    "    \"w\",\n",
    "    encoding=\"utf8\",\n",
    ") as f:\n",
    "    for feature_idx, bed_range in zip(sorted(feature_enrichment.keys()), bed_ranges):\n",
    "        chrom, start, end = bed_range\n",
    "        f.write(\n",
    "            f\"{chrom}\\t{start}\\t{end}\\t{feature_idx}\\t{list(feature_enrichment[feature_idx])}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_cancer_folder = (\n",
    "    logs_dir / f\"{CANCER}_1l_3000n/10fold-oversampling/global_shap_analysis/top303\"\n",
    ")\n",
    "if not global_cancer_folder.exists():\n",
    "    raise ValueError(f\"Folder {global_cancer_folder} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_special_bed_detail(every_task_features):\n",
    "    \"\"\"Create a BED file with the details of the feature sets.\"\"\"\n",
    "\n",
    "    # Create initial reference features\n",
    "    cancer_feature_sets = every_task_features[CANCER]\n",
    "    reference_features = (\n",
    "        cancer_feature_sets[\"merge_samplings_cancer\"]\n",
    "        & cancer_feature_sets[\"merge_samplings_non-cancer\"]\n",
    "    )\n",
    "    relevant_names = [name for name in cancer_feature_sets if \"& cancer\" in name]\n",
    "\n",
    "    # Create a dict with the origin of each feature\n",
    "    reference_features_origin = defaultdict(set)\n",
    "    for feature in reference_features:\n",
    "        for set_name in relevant_names:\n",
    "            if feature in cancer_feature_sets[set_name]:\n",
    "                set_name = set_name.replace(\" & cancer\", \"\")\n",
    "                reference_features_origin[feature].add(set_name)\n",
    "\n",
    "    assert len(reference_features_origin) == len(reference_features)\n",
    "\n",
    "    for feature, origin in reference_features_origin.items():\n",
    "        assert len(origin) != 0\n",
    "        # print(feature, origin)\n",
    "\n",
    "    # Create a dict with the bed values of each feature\n",
    "    feature_bed_vals = {}\n",
    "    for feature in reference_features_origin:\n",
    "        bed_vals = bins_to_bed_ranges(\n",
    "            bin_indexes=[feature],\n",
    "            chroms=chroms,\n",
    "            resolution=RESOLUTION,\n",
    "        )\n",
    "        assert len(bed_vals) == 1\n",
    "        feature_bed_vals[feature] = bed_vals[0]\n",
    "\n",
    "    # Write to bed the feature positions with origin detail\n",
    "    with open(\n",
    "        logs_dir / \"cancer_intersection_merge_samplings_bed-details.tsv\",\n",
    "        \"w\",\n",
    "        encoding=\"utf8\",\n",
    "    ) as f:\n",
    "        for feature, bed_val in sorted(feature_bed_vals.items(), key=lambda x: x[0]):\n",
    "            print(feature)\n",
    "            f.write(\n",
    "                f\"{bed_val[0]}\\t{bed_val[1]}\\t{bed_val[2]}\\t{feature}\\t{sorted(reference_features_origin[feature])}\\n\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_special_bed_detail(every_task_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsetplot_global_union(global_union_features):\n",
    "    \"\"\"Create an upsetplot of the global union of features.\"\"\"\n",
    "    # Plot global union of features upsetplot\n",
    "    correct_dict = {\n",
    "        name: features\n",
    "        for name, features in global_union_features.items()\n",
    "        if name in categories\n",
    "    }\n",
    "    upset_features = upsetplot.from_contents(correct_dict)\n",
    "    fig = upsetplot.UpSet(\n",
    "        upset_features,\n",
    "        subset_size=\"count\",\n",
    "        show_counts=True,  # type: ignore\n",
    "        show_percentages=True,\n",
    "        sort_by=\"cardinality\",\n",
    "        sort_categories_by=\"cardinality\",\n",
    "    )\n",
    "    fig.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsetplot_global_union(global_union_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_global_task_features(\n",
    "    global_union_features: Dict[str, Set[int]]\n",
    ") -> Dict[str, List[int]]:\n",
    "    \"\"\"Write the global union of features (union of all subsampling sets) for each task to json.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Augmented global_union_features with total union and intersections over all tasks.\n",
    "    \"\"\"\n",
    "    feature_sets = list(global_union_features.values())\n",
    "\n",
    "    global_task_union = set().union(*feature_sets)\n",
    "\n",
    "    global_task_intersection = set(feature_sets[0])\n",
    "    for feature_set in global_union_features.values():\n",
    "        global_task_intersection.intersection_update(feature_set)\n",
    "\n",
    "    # print(len(global_task_union), len(global_task_intersection))\n",
    "\n",
    "    with open(logs_dir / \"global_task_features.json\", \"w\", encoding=\"utf8\") as f:\n",
    "        content = {\n",
    "            category: list(features)\n",
    "            for category, features in global_union_features.items()\n",
    "        }\n",
    "        content[\"global_tasks_union\"] = list(global_task_union)\n",
    "        content[\"global_tasks_intersection\"] = list(global_task_intersection)\n",
    "        # json.dump(content, f, indent=4)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_global_beds(global_union_features):\n",
    "    \"\"\"Create new BED files for the global union and intersection of features for all tasks\n",
    "    Also create correspond beds of random features of same size.\n",
    "    \"\"\"\n",
    "    N_BINS_HDF5_100KB = 30321\n",
    "    json_content = write_global_task_features(global_union_features)\n",
    "    for category in [\"global_tasks_union\", \"global_tasks_intersection\"]:\n",
    "        # Actual features\n",
    "        features = json_content[category]\n",
    "        nb_features = len(features)\n",
    "        bed_vals = bins_to_bed_ranges(\n",
    "            bin_indexes=features,\n",
    "            chroms=chroms,\n",
    "            resolution=RESOLUTION,\n",
    "        )\n",
    "\n",
    "        bed_filename = get_valid_filename(f\"{category}.bed\")\n",
    "        write_to_bed(bed_vals, logs_dir / bed_filename, verbose=False)\n",
    "\n",
    "        # Random features of the same size\n",
    "        random_idxs = np.random.default_rng(42).choice(\n",
    "            a=np.arange(0, N_BINS_HDF5_100KB), size=nb_features, replace=False\n",
    "        )\n",
    "        assert len(random_idxs) == len(set(random_idxs))  # sanity check: no replace\n",
    "        bed_vals_random = bins_to_bed_ranges(\n",
    "            bin_indexes=list(random_idxs),\n",
    "            chroms=chroms,\n",
    "            resolution=RESOLUTION,\n",
    "        )\n",
    "\n",
    "        bed_filename_random = get_valid_filename(f\"random_n{nb_features}.bed\")\n",
    "        write_to_bed(bed_vals_random, logs_dir / bed_filename_random, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"Stop here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venn Diagramm over multiple classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pylint: disable=unreachable\n",
    "# union_important_features = defaultdict(set)\n",
    "# for category in categories:\n",
    "#     print(f\"Category: {category}\")\n",
    "#     path = list(\n",
    "#         (logs_dir / f\"{category}_1l_3000n\").glob(\n",
    "#             \"10fold-oversampl*/global_shap_analysis/top100*\"\n",
    "#         )\n",
    "#     )\n",
    "#     if not path:\n",
    "#         path = list(\n",
    "#             (logs_dir / f\"{category}_1l_3000n\").glob(\n",
    "#                 \"*/10fold-oversampl*/global_shap_analysis/top100*\"\n",
    "#             )\n",
    "#         )\n",
    "#         if not path:\n",
    "#             raise ValueError(f\"Path {path} does not exist.\")\n",
    "#     path = path[0]\n",
    "#     print(f\"Path: {path}\")\n",
    "\n",
    "#     for folder in path.iterdir():\n",
    "#         if not folder.is_dir():\n",
    "#             continue\n",
    "\n",
    "#         json_path = folder / \"feature_count.json\"\n",
    "\n",
    "#         with open(json_path, \"r\", encoding=\"utf8\") as json_file:\n",
    "#             feature_count = json.load(json_file)\n",
    "\n",
    "#         for class_label, class_features in feature_count.items():\n",
    "#             features = [feature for feature, count in class_features if count >= 8]\n",
    "#             union_important_features[category].update(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = venn3(\n",
    "#     list(union_important_features.values()), set_labels=union_important_features.keys()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logdir1 = logs_dir / f\"{category}_1l_3000n/10fold-oversampling/split1/shap\"\n",
    "# if not logdir1.exists():\n",
    "#     raise ValueError(f\"{logdir1} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_meta.display_labels(ASSAY)\n",
    "# my_meta.display_labels(CELL_TYPE)\n",
    "# if category not in [ASSAY, CELL_TYPE]:\n",
    "#     my_meta.display_labels(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is intended as a stop to not run code further needlessly, but still be able to run the beginning in one shot\n",
    "raise ValueError(\"Stop here\")  # pylint: disable=unreachable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection/filtering from full jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.home() / \"scratch/epiclass/join_important_features\"\n",
    "feature_count_path = (\n",
    "    base_path\n",
    "    / \"hg38_regulatory_regions_n30321_100kb_coord/harmonized_sample_ontology_intermediate_1l_3000n/10fold-oversampling/global_shap_analysis/top303/h3k27ac/feature_count.json\"\n",
    ")\n",
    "\n",
    "feature_count_general_dir = feature_count_path.parent.parent\n",
    "\n",
    "for folder in feature_count_general_dir.iterdir():\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    feature_count_path = folder / \"feature_count.json\"\n",
    "    if not feature_count_path.exists():\n",
    "        continue\n",
    "\n",
    "    features = collect_features_from_feature_count_file(feature_count_path, n=8)\n",
    "    with open(folder / \"features_n8.json\", \"w\", encoding=\"utf8\") as f:\n",
    "        json.dump(features, f)\n",
    "\n",
    "    features_all = collect_all_features_from_feature_count_file(feature_count_path, n=8)\n",
    "    with open(folder / \"features_n8_all.json\", \"w\", encoding=\"utf8\") as f:\n",
    "        json.dump(features_all, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union of frequent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_important_features_dir = (\n",
    "    Path.home() / \"Projects/epiclass/output/models/SHAP/global_task_features/global_info\"\n",
    ")\n",
    "global_important_features_path = (\n",
    "    global_important_features_dir / \"global_task_features.json\"\n",
    ")\n",
    "with open(global_important_features_path, \"r\", encoding=\"utf8\") as f:\n",
    "    global_important_features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_special_merge_path = (\n",
    "    global_important_features_dir\n",
    "    / \"cancer/cancer_intersection_merge_samplings_bed-details_2.tsv\"\n",
    ")\n",
    "df = pd.read_csv(cancer_special_merge_path, sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_special_merge_features = df[3].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_important_features[\n",
    "    \"cancer_intersection_merge_samplings\"\n",
    "] = cancer_special_merge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(global_important_features_path, \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(global_important_features, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_dir = global_important_features_dir / \"global_task_features_beds\"\n",
    "for name, features in global_important_features.items():\n",
    "    bed_ranges = bins_to_bed_ranges(\n",
    "        bin_indexes=features, chroms=chroms, resolution=RESOLUTION\n",
    "    )\n",
    "    bed_path = bed_dir / f\"{name}_features.bed\"\n",
    "    write_to_bed(bed_ranges, bed_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epi_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c736fa4331366c124c8f78bd4f37e7b252afab1d364df23bf88752fcd9abf849"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
