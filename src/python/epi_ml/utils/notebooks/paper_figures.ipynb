{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create figures destined for the paper.\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix as sk_cm,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.core.metadata import Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "ASSAY = \"assay_epiclass\"\n",
    "CELL_TYPE = \"harmonized_sample_ontology_intermediate\"\n",
    "ASSAY_MERGE_DICT = {\"mrna_seq\": \"rna_seq\", \"wgbs-pbat\": \"wgbs\", \"wgbs-standard\": \"wgbs\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure colors management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map_path = base_fig_dir / \"IHEC_IA_colors_jan22_2024.json\"\n",
    "with open(color_map_path, \"r\", encoding=\"utf8\") as color_map_file:\n",
    "    ihec_color_map = json.load(color_map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_assay_color_map(ihec_color_map: List[Dict]) -> Dict[str, str]:\n",
    "    \"\"\"Create a rbg color map for ihec core assays.\"\"\"\n",
    "    colors = dict(ihec_color_map[0][\"histone\"][0].items())\n",
    "    for name, color in list(colors.items()):\n",
    "        rbg = color.split(\",\")\n",
    "        colors[name.lower()] = f\"rgb({rbg[0]},{rbg[1]},{rbg[2]})\"\n",
    "\n",
    "    colors.update({\"rna_seq\": \"rgb(0,204,150)\", \"wgbs\": \"rgb(171,99,250)\"})\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cell_type_color_map(ihec_color_map: List[Dict]) -> Dict[str, str]:\n",
    "    \"\"\"Read the rbg color map for ihec cell types.\"\"\"\n",
    "    colors = dict(ihec_color_map[3][\"harmonized_sample_ontology_intermediate\"][0].items())\n",
    "    for name, color in list(colors.items()):\n",
    "        rbg = color.split(\",\")\n",
    "        colors[name] = f\"rgb({rbg[0]},{rbg[1]},{rbg[2]})\"\n",
    "\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_colors = create_assay_color_map(ihec_color_map)\n",
    "cell_type_colors = create_cell_type_color_map(ihec_color_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_similar_assays(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Attempt to merge rna-seq/wgbs categories, included prediction score.\"\"\"\n",
    "    df = df.copy(deep=True)\n",
    "    try:\n",
    "        df[\"rna_seq\"] = df[\"rna_seq\"] + df[\"mrna_seq\"]\n",
    "        df[\"wgbs\"] = df[\"wgbs-standard\"] + df[\"wgbs-pbat\"]\n",
    "    except KeyError as exc:\n",
    "        raise ValueError(\n",
    "            \"Wrong results dataframe, label category is not assay specific.\"\n",
    "        ) from exc\n",
    "    df.drop(columns=[\"mrna_seq\", \"wgbs-standard\", \"wgbs-pbat\"], inplace=True)\n",
    "    df[\"True class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "    df[\"Predicted class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    try:\n",
    "        df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Recompute Max pred if it exists\n",
    "    classes = df[\"True class\"].unique()\n",
    "    if \"Max pred\" in df.columns:\n",
    "        df[\"Max pred\"] = df[classes].max(axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1.A\n",
    "\n",
    "Average distribution of prediction scores per assay \n",
    "violin plot. One point per UUID, track types averaged (combine 2xRNA and 2xWGBS)\n",
    "points with 3 colors: \n",
    "- black for pred same class\n",
    "- red for pred different class/mislabel\n",
    "- orange bad qual (IHEC flag, was removed in later stages)\n",
    "\n",
    "Graph version with color saturation gradient using max_pred/input score ratio\n",
    "\n",
    "Using [EpiClass_EA-21606_Assay11_100kb](https://drive.google.com/drive/folders/1SzyTFCVk2Cyw7NXW08sSYB_k49y-1KoJ) : EA_NN--full-10fold-validation_prediction_augmented-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in NN_results.columns:\n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig1_a(\n",
    "    NN_results: pd.DataFrame, logdir: Path, name: str, merge_assay_pairs: bool\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with violin plots and associated scatter plots for each class.\n",
    "    Red scatter points, indicating a mismatch, appear on top and have a larger size.\n",
    "\n",
    "    Args:\n",
    "        NN_results (pd.DataFrame): The DataFrame containing the neural network results.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "        name (str): The name of the figure.\n",
    "        merge_assay_pairs (bool): Whether to merge similar assays (mrna/rna, wgbs-pbat/wgbs-standard)\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Combine similar assays\n",
    "    if merge_assay_pairs:\n",
    "        NN_results = merge_similar_assays(NN_results)\n",
    "\n",
    "    # Adjustments for replacement and class ordering\n",
    "    class_labels = NN_results[\"True class\"].unique()\n",
    "    class_labels_sorted = sorted(class_labels)\n",
    "    class_index = {label: i for i, label in enumerate(class_labels_sorted)}\n",
    "\n",
    "    scatter_offset = 0.05  # Scatter plot jittering\n",
    "\n",
    "    for label in class_labels_sorted:\n",
    "        df = NN_results[NN_results[ASSAY] == label]\n",
    "\n",
    "        # Majority vote, mean prediction score\n",
    "        groupby_epirr = df.groupby([\"EpiRR\", \"Predicted class\"])[\"Max pred\"].aggregate(\n",
    "            [\"size\", \"mean\"]\n",
    "        )\n",
    "\n",
    "        groupby_epirr = groupby_epirr.reset_index().sort_values(\n",
    "            [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "        )\n",
    "        groupby_epirr = groupby_epirr.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "        assert groupby_epirr[\"EpiRR\"].is_unique\n",
    "\n",
    "        mean_pred = groupby_epirr[\"mean\"]\n",
    "\n",
    "        # Add violin plot with integer x positions\n",
    "        line_color = \"white\"\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[class_index[label]] * len(mean_pred),\n",
    "                y=mean_pred,\n",
    "                name=label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=False,\n",
    "                fillcolor=assay_colors[label],\n",
    "                line_color=line_color,\n",
    "                line=dict(width=0.8),\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Prepare data for scatter plots\n",
    "        jittered_x_positions = np.random.uniform(-scatter_offset, scatter_offset, size=len(mean_pred)) + class_index[label] - 0.25  # type: ignore\n",
    "\n",
    "        match_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] == label\n",
    "        ]\n",
    "        mismatch_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] != label\n",
    "        ]\n",
    "\n",
    "        match_x_positions = [\n",
    "            jittered_x_positions[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] == label\n",
    "        ]\n",
    "        mismatch_x_positions = [\n",
    "            jittered_x_positions[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] != label\n",
    "        ]\n",
    "\n",
    "        # Add scatter plots for matches in black\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=match_x_positions,\n",
    "                y=match_pred,\n",
    "                mode=\"markers\",\n",
    "                name=f\"Match {label}\",\n",
    "                marker=dict(\n",
    "                    color=\"black\",\n",
    "                    size=1,  # Standard size for matches\n",
    "                ),\n",
    "                hoverinfo=\"text\",\n",
    "                hovertext=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.2f}\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] == label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add scatter plots for mismatches in red, with larger size\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=mismatch_x_positions,\n",
    "                y=mismatch_pred,\n",
    "                mode=\"markers\",\n",
    "                name=f\"Mismatch {label}\",\n",
    "                marker=dict(\n",
    "                    color=\"red\",\n",
    "                    size=3,  # Larger size for mismatches\n",
    "                ),\n",
    "                hoverinfo=\"text\",\n",
    "                hovertext=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.3f}\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] != label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout to improve visualization\n",
    "    fig.update_layout(\n",
    "        title_text=\"Prediction score distribution per assay class\",\n",
    "        yaxis_title=\"Average prediction score (majority class)\",\n",
    "        xaxis_title=\"Expected class label\",\n",
    "    )\n",
    "    fig.update_yaxes(range=[0.25, 1.01])\n",
    "    fig.update_xaxes(tickvals=list(class_index.values()), ticktext=class_labels_sorted)\n",
    "\n",
    "    # Add a dummy scatter plot for legend - black points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Match\",\n",
    "            marker=dict(color=\"black\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"match\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - red points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Mismatch\",\n",
    "            marker=dict(color=\"red\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"mismatch\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update the layout to adjust the legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            title_text=\"Legend\",\n",
    "            itemsizing=\"constant\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_results_path = (\n",
    "    base_data_dir\n",
    "    / \"EpiClass_EA-21606_Assay11_100kb\"\n",
    "    / \"NN\"\n",
    "    / \"full-10fold-validation_prediction_augmented-all.csv\"\n",
    ")\n",
    "NN_results = pd.read_csv(NN_results_path, header=0, index_col=\"md5sum\", low_memory=False)\n",
    "\n",
    "# fig1_a(NN_results, logdir=base_fig_dir, name=\"fig1_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1.supp.A\n",
    "\n",
    "Violin plot (10 folds) of overall accuracy for each model (NN, LR, RF, LGBM, SV).  \n",
    "For each split, 4 box plot per model:\n",
    "  - Acc\n",
    "  - F1\n",
    "  - AUROC (OvR, both micro/macro)\n",
    "\n",
    "\n",
    "Source files:\n",
    "~~~bash\n",
    "cd ~/mounts/narval-mount/projects/rrg-jacquesp-ab/rabyj/epiclass-project/output/epiclass-logs/2023-01-epiatlas-freeze/  \n",
    "find assay_epiclass* -type f -name *validation_prediction.csv -print0 | rsync -av --files-from=- --from0 ./ ~/Projects/epiclass/output/paper/data/EpiClass_EA-21606_Assay11_100kb/all_splits/\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results_path = (\n",
    "    base_data_dir / \"EpiClass_EA-21606_Assay11_100kb\" / \"all-predictions-merged.csv\"\n",
    ")\n",
    "merged_results = pd.read_csv(\n",
    "    NN_results_path, header=0, index_col=\"md5sum\", low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying that results are from metadata v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that merged results and nn results have the same EpiRRs\n",
    "assert (\n",
    "    len(set(merged_results[\"epirr_id\"]) & set(NN_results[\"epirr_id\"]))\n",
    "    == NN_results[\"epirr_id\"].nunique()\n",
    ")\n",
    "assert len(set(merged_results.index) & set(NN_results.index)) == len(\n",
    "    set(NN_results.index)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(merged_results[\"epirr_id\"] == \"IHECRE00003355.2\") == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del merged_results  # using separate split results for this figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_split_results(\n",
    "    results_dir: Path, label_category: str, only_NN: bool = False\n",
    ") -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Gather split results for each classifier.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, pd.DataFrame]]: {split_name:{classifier_name: results_df}}\n",
    "    \"\"\"\n",
    "    all_split_dfs = {}\n",
    "    for split in [f\"split{i}\" for i in range(10)]:\n",
    "        # Get the csv paths\n",
    "        if label_category == ASSAY:\n",
    "            second_dir_end = \"\"\n",
    "        elif label_category == CELL_TYPE:\n",
    "            second_dir_end = \"-dfreeze-v2\"\n",
    "\n",
    "        NN_csv_path = (\n",
    "            results_dir\n",
    "            / f\"{label_category}_1l_3000n\"\n",
    "            / f\"10fold{second_dir_end}\"\n",
    "            / split\n",
    "            / \"validation_prediction.csv\"\n",
    "        )\n",
    "        other_csv_root = (\n",
    "            results_dir / f\"{label_category}\" / f\"predict-10fold{second_dir_end}\"\n",
    "        )\n",
    "\n",
    "        if not only_NN:\n",
    "            if not other_csv_root.exists():\n",
    "                raise FileNotFoundError(f\"Could not find {other_csv_root}\")\n",
    "            other_csv_paths = other_csv_root.glob(f\"*/*_{split}_validation_prediction.csv\")\n",
    "\n",
    "            other_csv_paths = list(other_csv_paths)\n",
    "            if len(other_csv_paths) != 4:\n",
    "                raise AssertionError(\n",
    "                    f\"Expected 4 other_csv_paths, got {len(other_csv_paths)}\"\n",
    "                )\n",
    "\n",
    "        # Load the dataframes\n",
    "        dfs = {}\n",
    "        dfs[\"NN\"] = pd.read_csv(NN_csv_path, header=0, index_col=0, low_memory=False)\n",
    "\n",
    "        if not only_NN:\n",
    "            for path in other_csv_paths:\n",
    "                name = path.name.split(\"_\", maxsplit=1)[0]\n",
    "                dfs[name] = pd.read_csv(path, header=0, index_col=0, low_memory=False)\n",
    "\n",
    "        # Verify that all dataframes have the same md5sums\n",
    "        md5s = {}\n",
    "        for key, df in dfs.items():\n",
    "            md5s[key] = set(df.index)\n",
    "\n",
    "        base_md5s = md5s[\"NN\"]\n",
    "        if not base_md5s.intersection(*list(md5s.values())) == base_md5s:\n",
    "            raise AssertionError(\"Not all dataframes have the same md5sums\")\n",
    "\n",
    "        all_split_dfs[split] = dfs\n",
    "\n",
    "    return all_split_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_split_metrics(\n",
    "    all_split_dfs: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> Dict[str, Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"Compute desired metrics for each split and classifier.\"\"\"\n",
    "    split_metrics = {}\n",
    "    for split in [f\"split{i}\" for i in range(10)]:\n",
    "        dfs = all_split_dfs[split]\n",
    "\n",
    "        # Compute metrics for the split\n",
    "        metrics = {}\n",
    "        for key, df in dfs.items():\n",
    "            # One-hot encode true and predicted classes\n",
    "            classes_order = df.columns[2:]\n",
    "            onehot_true = (\n",
    "                pd.get_dummies(df[\"True class\"], dtype=int)\n",
    "                .reindex(columns=classes_order, fill_value=0)\n",
    "                .values\n",
    "            )\n",
    "            pred_probs = df[\n",
    "                classes_order\n",
    "            ].values  # Ensure this aligns with your model's output format\n",
    "\n",
    "            ravel_true = np.argmax(onehot_true, axis=1)\n",
    "            ravel_pred = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "            metrics[key] = {\n",
    "                \"Accuracy\": accuracy_score(ravel_true, ravel_pred),\n",
    "                \"F1_macro\": f1_score(ravel_true, ravel_pred, average=\"macro\"),\n",
    "                \"AUC_micro\": roc_auc_score(\n",
    "                    onehot_true, pred_probs, multi_class=\"ovr\", average=\"micro\"\n",
    "                ),\n",
    "                \"AUC_macro\": roc_auc_score(\n",
    "                    onehot_true, pred_probs, multi_class=\"ovr\", average=\"macro\"\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            split_metrics[split] = metrics\n",
    "\n",
    "    return split_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_split_metrics(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    label_category: str,\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    ") -> None:\n",
    "    \"\"\"Render to box plots the metrics per classifier and split, each in its own subplot.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A dictionary containing metric scores for each classifier and split.\n",
    "    \"\"\"\n",
    "    metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "    classifiers = list(next(iter(split_metrics.values())).keys())\n",
    "\n",
    "    # Create subplots, one row for each metric\n",
    "    fig = make_subplots(rows=1, cols=len(metrics), subplot_titles=metrics)\n",
    "\n",
    "    colors = {\n",
    "        classifier: px.colors.qualitative.Plotly[i]\n",
    "        for i, classifier in enumerate(classifiers)\n",
    "    }\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        for classifier in classifiers:\n",
    "            values = [split_metrics[split][classifier][metric] for split in split_metrics]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=values,\n",
    "                    name=classifier,\n",
    "                    marker_color=colors[classifier],\n",
    "                    line=dict(color=\"black\", width=1),\n",
    "                    marker=dict(size=2),\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",  # or \"outliers\" to show only outliers\n",
    "                    pointpos=-1.4,\n",
    "                    showlegend=False,\n",
    "                    width=0.5,\n",
    "                    hoverinfo=\"text\",\n",
    "                    hovertext=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in zip(split_metrics, values)\n",
    "                    ],\n",
    "                ),\n",
    "                row=1,\n",
    "                col=i + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{label_category} classification - Metric distribution for 10fold cross-validation\",\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode=\"group\",\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_results_assay = base_data_dir / \"EpiClass_EA-21606_Assay11_100kb\" / \"all_splits\"\n",
    "path_results_cell_type = base_data_dir / CELL_TYPE / \"all_splits\"\n",
    "\n",
    "for label_category, path in zip(\n",
    "    [ASSAY, CELL_TYPE], [path_results_assay, path_results_cell_type]\n",
    "):\n",
    "    all_split_dfs = gather_split_results(results_dir=path, label_category=label_category)\n",
    "    split_metrics = compute_split_metrics(all_split_dfs)\n",
    "    dfreeze_version = \"dfreeze-v2\" if label_category == CELL_TYPE else \"dfreeze-v1\"\n",
    "    plot_split_metrics(split_metrics, label_category=label_category, logdir=base_fig_dir, name=f\"{label_category}_10fold_metrics_all_classifiers_{dfreeze_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1.supp.B\n",
    "\n",
    "Per model, compute score distribution per assay (1 violin per assay). No SVM. Agree black, red disagree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig1_supp_B(df_dict: Dict[str, pd.DataFrame], logdir: Path, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with subplots for each assay, each containing violin plots for different classifiers\n",
    "    and associated scatter plots for matches (in black) and mismatches (in red).\n",
    "\n",
    "    Args:\n",
    "        df_dict (Dict[str, pd.DataFrame]): Dictionary with the DataFrame containing the results for each classifier.\n",
    "        logdir (Path): The directory path for saving the figures.\n",
    "        name (str): The name for the saved figures.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    # Ignore LinearSVC and RandomForest for this figure\n",
    "    if \"LinearSVC\" in df_dict:\n",
    "        del df_dict[\"LinearSVC\"]\n",
    "    if \"RF\" in df_dict:\n",
    "        del df_dict[\"RF\"]\n",
    "\n",
    "    # Assuming all classifiers have the same assays for simplicity\n",
    "    first_key = next(iter(df_dict))\n",
    "    class_labels = df_dict[first_key][\"True class\"].unique()\n",
    "    class_labels_sorted = sorted(class_labels)\n",
    "    num_assays = len(class_labels_sorted)\n",
    "\n",
    "    classifiers = list(df_dict.keys())\n",
    "    classifier_index = {name: i for i, name in enumerate(classifiers)}\n",
    "    num_classifiers = len(classifiers)\n",
    "\n",
    "    scatter_offset = 0.05  # Scatter plot jittering\n",
    "\n",
    "    # Calculate the size of the grid\n",
    "    grid_size = int(np.ceil(np.sqrt(num_assays)))\n",
    "    rows, cols = grid_size, grid_size\n",
    "\n",
    "    # Create subplots with a square grid\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=class_labels_sorted,\n",
    "        shared_yaxes=\"all\",  # type: ignore\n",
    "        horizontal_spacing=0.05,\n",
    "        vertical_spacing=0.05,\n",
    "        y_title=\"Average prediction score\",\n",
    "    )\n",
    "    for idx, label in enumerate(class_labels_sorted):\n",
    "        row, col = divmod(idx, grid_size)\n",
    "        for classifier_name, classifier_df in df_dict.items():\n",
    "            df = classifier_df[classifier_df[\"True class\"] == label]\n",
    "\n",
    "            # Majority vote, mean prediction score\n",
    "            groupby_epirr = df.groupby([\"EpiRR\", \"Predicted class\"])[\n",
    "                \"Max pred\"\n",
    "            ].aggregate([\"size\", \"mean\"])\n",
    "            groupby_epirr = groupby_epirr.reset_index().sort_values(\n",
    "                [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "            )\n",
    "            groupby_epirr = groupby_epirr.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "            assert groupby_epirr[\"EpiRR\"].is_unique\n",
    "\n",
    "            mean_pred = groupby_epirr[\"mean\"]\n",
    "            classifier_pos = classifier_index[classifier_name]\n",
    "\n",
    "            # Add violin plot with integer x positions\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    x=classifier_pos * np.ones(len(mean_pred)),\n",
    "                    y=mean_pred,\n",
    "                    name=label,\n",
    "                    spanmode=\"hard\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=False,\n",
    "                    fillcolor=\"grey\",\n",
    "                    line_color=\"black\",\n",
    "                    line=dict(width=0.8),\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=row + 1,  # Plotly rows are 1-indexed\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "            # Prepare data for scatter plots\n",
    "            jittered_x_positions = np.random.uniform(-scatter_offset, scatter_offset, size=len(mean_pred)) + classifier_pos - 0.3  # type: ignore\n",
    "\n",
    "            match_pred = [\n",
    "                mean_pred.iloc[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] == label\n",
    "            ]\n",
    "            mismatch_pred = [\n",
    "                mean_pred.iloc[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] != label\n",
    "            ]\n",
    "\n",
    "            match_x_positions = [\n",
    "                jittered_x_positions[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] == label\n",
    "            ]\n",
    "            mismatch_x_positions = [\n",
    "                jittered_x_positions[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] != label\n",
    "            ]\n",
    "\n",
    "            # Add scatter plots for matches in black\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=match_x_positions,\n",
    "                    y=match_pred,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(color=\"black\", size=1),\n",
    "                    showlegend=False,\n",
    "                    name=f\"Match {classifier_name}\",\n",
    "                ),\n",
    "                row=row + 1,  # Plotly rows are 1-indexed\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "            # Add scatter plots for mismatches in red\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=mismatch_x_positions,\n",
    "                    y=mismatch_pred,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(color=\"red\", size=3),\n",
    "                    showlegend=False,\n",
    "                    name=f\"Mismatch {classifier_name}\",\n",
    "                ),\n",
    "                row=row + 1,  # Plotly rows are 1-indexed\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - black points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Match\",\n",
    "            marker=dict(color=\"black\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"match\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - red points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Mismatch\",\n",
    "            marker=dict(color=\"red\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"mismatch\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update the layout to adjust the legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            title_text=\"Legend\",\n",
    "            itemsizing=\"constant\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.025,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout to improve visualization, adjust if needed for better appearance with multiple classifiers\n",
    "    fig.update_layout(\n",
    "        title_text=\"Prediction score distribution per assay across classifiers\",\n",
    "        height=1500,  # Adjust the height as necessary\n",
    "        width=1500,  # Adjust the width based on the number of assays\n",
    "    )\n",
    "\n",
    "    fig.update_layout(yaxis2=dict(range=[0.9, 1.01]))\n",
    "\n",
    "    # Adjust tick names\n",
    "    # Assuming equal spacing between each classifier on the x-axis\n",
    "    tickvals = list(\n",
    "        range(0, num_classifiers + 1)\n",
    "    )  # Generate tick values (1-indexed for Plotly)\n",
    "    ticktext = classifiers  # Use classifier names as tick labels\n",
    "    for i, j in itertools.product(range(rows), range(cols)):\n",
    "        fig.update_xaxes(tickvals=tickvals, ticktext=ticktext, row=i + 1, col=j + 1)\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}_min0.9.svg\")\n",
    "    fig.write_image(logdir / f\"{name}_min0.9.png\")\n",
    "    fig.write_html(logdir / f\"{name}_min0.9.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_split_results(\n",
    "    split_dfs: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Concatenate split results for each different classifier.\n",
    "\n",
    "    Args:\n",
    "        split_dfs (Dict[str, Dict[str, pd.DataFrame]]): {split_name:{classifier_name: results_df}}\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: {classifier_name: concatenated_df}\n",
    "    \"\"\"\n",
    "    to_concat_dfs = defaultdict(list)\n",
    "    for dfs in split_dfs.values():\n",
    "        for classifier, df in dfs.items():\n",
    "            to_concat_dfs[classifier].append(df)\n",
    "\n",
    "    concatenated_dfs = {\n",
    "        classifier: pd.concat(dfs, axis=0) for classifier, dfs in to_concat_dfs.items()\n",
    "    }\n",
    "\n",
    "    # Verify index is still md5sum\n",
    "    for df in concatenated_dfs.values():\n",
    "        if not isinstance(df.index[0], str):\n",
    "            raise AssertionError(\"Index is not md5sum\")\n",
    "\n",
    "    return concatenated_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_metadata(df: pd.DataFrame, metadata: Metadata) -> pd.DataFrame:\n",
    "    \"\"\"Join the metadata to the results dataframe.\"\"\"\n",
    "    metadata_df = pd.DataFrame(metadata.datasets)\n",
    "    metadata_df.set_index(\"md5sum\", inplace=True)\n",
    "\n",
    "    diff_set = set(df.index) - set(metadata_df.index)\n",
    "    if diff_set:\n",
    "        err_df = pd.DataFrame(diff_set, columns=[\"md5sum\"])\n",
    "        err_df.to_csv(base_data_dir / \"join_missing_md5sums.csv\", index=False)\n",
    "        raise AssertionError(f\"{len(diff_set)} md5sums in the results dataframe are not present in the metadata dataframe. Saved error md5sums to join_missing_md5sums.csv.\")\n",
    "\n",
    "    merged_df = df.merge(metadata_df, how=\"left\", left_index=True, right_index=True)\n",
    "    if len(merged_df) != len(df):\n",
    "        raise AssertionError(\"Merged dataframe has different length than original dataframe\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_split_dfs = gather_split_results(path_results_assay, ASSAY)\n",
    "# full_dfs = concatenate_split_results(all_split_dfs)\n",
    "# merged_dfs = {classifier: merge_similar_assays(df) for classifier, df in full_dfs.items()}\n",
    "# assays = merged_dfs[next(iter(merged_dfs))][\"True class\"].unique()\n",
    "\n",
    "# # Add Max pred\n",
    "# for classifier, df in merged_dfs.items():\n",
    "#     df[\"Max pred\"] = df[assays].max(axis=1)\n",
    "\n",
    "# # Join metadata\n",
    "# metadata_path = (\n",
    "#     base_data_dir / \"metadata\" / \"hg38_2023-epiatlas_dfreeze_formatted_JR.json\"\n",
    "# )\n",
    "# metadata_dfreeze1 = Metadata(metadata_path)\n",
    "# metadata_dfreeze1_df = pd.DataFrame(metadata_dfreeze1.datasets)\n",
    "\n",
    "# for classifier, df in merged_dfs.items():\n",
    "#     merged_dfs[classifier] = df.merge(\n",
    "#         metadata_dfreeze1_df, how=\"left\", left_index=True, right_on=\"md5sum\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig1_supp_B(merged_dfs, logdir=base_fig_dir, name=\"fig1_supp_B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1.supp.F\n",
    "\n",
    "For each classifier type\n",
    "\n",
    "Confusion matrix (1point=1 uuid) for observed datasets with average scores>0.9\n",
    "- Goal: Represent global predictions/mislabels. 11c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    min_pred_score: float,\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    majority: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Create a confusion matrix for the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the neural network results.\n",
    "        min_pred_score (float): The minimum prediction score to consider.\n",
    "        logdir (Path): The directory path for saving the figures.\n",
    "        name (str): The name for the saved figures.\n",
    "        majority (bool): Whether to use majority vote (uuid-wise) for the predicted class.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    classes = sorted(df[\"True class\"].unique())\n",
    "    if \"Max pred\" not in df.columns:\n",
    "        df[\"Max pred\"] = df[classes].max(axis=1)\n",
    "    filtered_df = df[df[\"Max pred\"] > min_pred_score]\n",
    "\n",
    "    if majority:\n",
    "        # Majority vote for predicted class\n",
    "        groupby_uuid = df.groupby([\"uuid\", \"True class\", \"Predicted class\"])[\n",
    "            \"Max pred\"\n",
    "        ].aggregate([\"size\", \"mean\"])\n",
    "        groupby_uuid = groupby_uuid.reset_index().sort_values(\n",
    "            [\"uuid\", \"True class\", \"size\"], ascending=[True, True, False]\n",
    "        )\n",
    "        groupby_uuid = groupby_uuid.drop_duplicates(\n",
    "            subset=[\"uuid\", \"True class\"], keep=\"first\"\n",
    "        )\n",
    "        filtered_df = groupby_uuid\n",
    "\n",
    "    confusion_mat = sk_cm(\n",
    "        filtered_df[\"True class\"], filtered_df[\"Predicted class\"], labels=classes\n",
    "    )\n",
    "\n",
    "    mat_writer = ConfusionMatrixWriter(labels=classes, confusion_matrix=confusion_mat)\n",
    "    mat_writer.to_all_formats(logdir, name=f\"{name}_n{len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_pred_score = 0.9\n",
    "# majority = True\n",
    "\n",
    "# for classifier_name, df in full_dfs.items():\n",
    "#     df_with_meta = df.merge(\n",
    "#         metadata_dfreeze1_df, how=\"left\", left_index=True, right_on=\"md5sum\"\n",
    "#     )\n",
    "#     assert \"Predicted class\" in df_with_meta.columns\n",
    "\n",
    "#     name = f\"{classifier_name}_pred>{min_pred_score}\"\n",
    "#     if classifier_name == \"LinearSVC\":\n",
    "#         name = f\"{classifier_name}\"\n",
    "\n",
    "#     logdir = base_fig_dir / \"fig1_supp_F-assay_c11_confusion_matrices\"\n",
    "#     if majority:\n",
    "#         logdir = logdir / \"per_uuid\"\n",
    "#     else:\n",
    "#         logdir = logdir / \"per_file\"\n",
    "\n",
    "    # create_confusion_matrix(\n",
    "    #     df=df_with_meta,\n",
    "    #     min_pred_score=min_pred_score,\n",
    "    #     logdir=logdir,\n",
    "    #     name=name,\n",
    "    #     majority=majority\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1.supp.D\n",
    "\n",
    "Inference on imputed data: Violin plot with pred score per assay (like Fig1A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"fig1_supp_D\"\n",
    "this_data_dir = base_data_dir / \"imputation\"\n",
    "\n",
    "# Load data\n",
    "normal_inf_imputed_path = next(\n",
    "    (this_data_dir / \"hg38_100kb_all_none/assay_epiclass_1l_3000n\").glob(\"**/*.csv\")\n",
    ")\n",
    "normal_inf_imputed_df = pd.read_csv(\n",
    "    normal_inf_imputed_path, header=0, index_col=0, low_memory=False\n",
    ")\n",
    "\n",
    "imputed_inf_normal_path = next(\n",
    "    (this_data_dir / \"hg38_100kb_all_none_imputed/assay_epiclass_1l_3000n\").rglob(\n",
    "        \"**/*.csv\"\n",
    "    )\n",
    ")\n",
    "imputed_inf_normal_df = pd.read_csv(\n",
    "    imputed_inf_normal_path, header=0, index_col=0, low_memory=False\n",
    ")\n",
    "\n",
    "assay_labels = normal_inf_imputed_df[\"True class\"].unique()\n",
    "for name, df in zip(\n",
    "    [\"train_normal_inf_imputed\", \"train_imputed_inf_normal\"],\n",
    "    [normal_inf_imputed_df, imputed_inf_normal_df],\n",
    "):\n",
    "    df[\"EpiRR\"] = list(df.index)\n",
    "    df[ASSAY] = df[\"True class\"]\n",
    "    df[\"Max pred\"] = df[assay_labels].max(axis=1)\n",
    "    # fig1_a(\n",
    "    #     df, logdir=fig_dir, name=f\"fig1_supp_D-{name}_n{len(df)}\", merge_assay_pairs=False\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flagship paper figure\n",
    "\n",
    "cell type classifier:  \n",
    "\n",
    "  for each assay, have a violin plot for accuracy per cell type (16 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_flagship_ct(cell_type_df: pd.DataFrame, logdir: Path, name: str) -> None:\n",
    "    \"\"\"\n",
    "    [FILL HERE]\n",
    "\n",
    "    Args:\n",
    "        cell_type_df (pd.DataFrame): DataFrame containing the cell type prediction results.\n",
    "        logdir (Path): The directory path for saving the figure.\n",
    "        name (str): The name for the saved figure.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Assuming all classifiers have the same assays for simplicity\n",
    "    assay_labels = sorted(cell_type_df[ASSAY].unique())\n",
    "    num_assays = len(assay_labels)\n",
    "\n",
    "    ct_labels = sorted(cell_type_df[\"True class\"].unique())\n",
    "    if len(ct_labels) != 16:\n",
    "        raise AssertionError(f\"Expected 16 cell type labels, got {len(ct_labels)}\")\n",
    "    ct_colors = [cell_type_colors[ct_label] for ct_label in ct_labels]\n",
    "\n",
    "    scatter_offset = 0.1  # Scatter plot jittering\n",
    "\n",
    "    # Calculate the size of the grid\n",
    "    grid_size = int(np.ceil(np.sqrt(num_assays)))\n",
    "    rows, cols = grid_size, grid_size\n",
    "\n",
    "    # Create subplots with a square grid\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=assay_labels,\n",
    "        shared_yaxes=\"all\",  # type: ignore\n",
    "        horizontal_spacing=0,\n",
    "        vertical_spacing=0.02,\n",
    "        y_title=\"Cell type subclass accuracy\",\n",
    "    )\n",
    "    for idx, assay_label in enumerate(assay_labels):\n",
    "        row, col = divmod(idx, grid_size)\n",
    "        assay_df = cell_type_df[cell_type_df[ASSAY] == assay_label]\n",
    "\n",
    "        # cell type subclass accuracy\n",
    "        subclass_size = assay_df.groupby([\"True class\"]).agg(\"size\")\n",
    "        subclass_groupby_acc = assay_df.groupby([\"True class\", \"Predicted class\"]).agg(\"size\")\n",
    "        accuracies = {}\n",
    "        for ct_label in sorted(ct_labels):\n",
    "            acc_label = subclass_groupby_acc[ct_label][ct_label] / subclass_size[ct_label]\n",
    "            accuracies[ct_label] = acc_label\n",
    "\n",
    "        acc_values = list(accuracies.values())\n",
    "\n",
    "        # Add violin plot with integer x positions\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[idx] * len(accuracies),\n",
    "                y=acc_values,\n",
    "                name=assay_label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=False,\n",
    "                fillcolor=assay_colors[assay_label],\n",
    "                line_color=\"white\",\n",
    "                line=dict(width=0.8),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=row + 1,  # Plotly rows are 1-indexed\n",
    "            col=col + 1,\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(showticklabels=False)\n",
    "\n",
    "        # Prepare data for scatter plots\n",
    "        jittered_x_positions = np.random.uniform(-scatter_offset, scatter_offset, size=len(accuracies)) + idx - 0.4  # type: ignore\n",
    "\n",
    "\n",
    "        # Add scatter plots for matches in black\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=jittered_x_positions,\n",
    "                y=acc_values,\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    size=3,  # Standard size for matches\n",
    "                    color=ct_colors\n",
    "                ),\n",
    "                hoverinfo=\"text\",\n",
    "                hovertext=[f\"{ct_label} ({accuracies[ct_label]:.3f}, n={subclass_size[ct_label]})\" for ct_label in accuracies],\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=row + 1,  # Plotly rows are 1-indexed\n",
    "            col=col + 1,\n",
    "        )\n",
    "\n",
    "    # Add a dummy scatter plot for legend\n",
    "    for ct_label in ct_labels:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[None],\n",
    "                y=[None],\n",
    "                mode=\"markers\",\n",
    "                name=ct_label,\n",
    "                marker=dict(color=cell_type_colors[ct_label], size=3),\n",
    "                showlegend=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"cell type classifier: accuracy per output class for each assay\",\n",
    "        height=1500,\n",
    "        width=1500,\n",
    "    )\n",
    "\n",
    "    # # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_split_dfs = gather_split_results(path_results_cell_type, CELL_TYPE)\n",
    "ct_full_df = concatenate_split_results(ct_split_dfs)[\"NN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata_path = (\n",
    "#     base_data_dir / \"metadata\" / \"hg38_2023-epiatlas_dfreeze_formatted_JR.json\"\n",
    "# )\n",
    "# metadata_1 = Metadata(metadata_path)\n",
    "# metadata_df_1 = pd.DataFrame(metadata_1.datasets)\n",
    "# metadata_df_1.set_index(\"md5sum\", inplace=True)\n",
    "\n",
    "metadata_path = (\n",
    "    base_data_dir / \"metadata\" / \"hg38_2023-epiatlas-dfreeze-pospurge-nodup_filterCtl.json\"\n",
    ")\n",
    "metadata_2 = Metadata(metadata_path)\n",
    "# metadata_df_2 = pd.DataFrame(metadata_2.datasets)\n",
    "# metadata_df_2.set_index(\"md5sum\", inplace=True)\n",
    "# metadata_df_2.to_csv(base_data_dir / \"metadata\" / \"hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_full_df = join_metadata(ct_full_df, metadata_2)\n",
    "ct_full_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"flagship\"\n",
    "fig_flagship_ct(ct_full_df, logdir=fig_dir, name=\"ct_assay_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "10) cell type classification: check (input, ct) pairs for enrichment in any metadata category\n",
    "    - e.g. (input, myloid) all cancer, or all from a certain data_generating_center\n",
    "    - use biomaterial_type, sex, cancer, standedness-smth, data_generating_center, and other categories we have tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metadata_distribution(df: pd.DataFrame, columns: List[str]) -> Dict[str, pd.Series]:\n",
    "    \"\"\"\n",
    "    Calculates the percentage of metadata labels within specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: A pandas DataFrame containing the data.\n",
    "        columns: A list of column names to analyze.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are column names and values are Series objects containing\n",
    "        the percentage of each unique label in the respective column.\n",
    "    \"\"\"\n",
    "    distribution = {}\n",
    "    nb_samples = len(df)\n",
    "    for column in columns:\n",
    "        # Count the occurrences of each unique value in the column\n",
    "        value_counts = df[column].value_counts(dropna=False)\n",
    "        # Calculate the percentages\n",
    "        percentages = (value_counts / nb_samples) * 100\n",
    "        # Store the results in the dictionary\n",
    "        distribution[column] = percentages\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_label_ratios(target_distribution: Dict[str, pd.Series],\n",
    "                         comparison_distributions: List[Dict[str, pd.Series]],\n",
    "                         labels: List[str]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compares label ratios of a target distribution against multiple comparison distributions,\n",
    "    calculating the difference in percentage points for each label within each metadata category.\n",
    "\n",
    "    Args:\n",
    "        target_distribution: A dictionary of Series representing the target distribution for comparison.\n",
    "        comparison_distributions: A list of dictionaries of Series, where each dictionary\n",
    "                                  represents a distribution (e.g., assay, cell type, global) for comparison.\n",
    "        labels: A list of labels corresponding to each distribution in `comparison_distributions`,\n",
    "                used for labeling the columns in the result.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of DataFrames, where each DataFrame shows the difference in percentage points\n",
    "        for each label in a metadata category between the target distribution and each of the\n",
    "        comparison distributions.\n",
    "    \"\"\"\n",
    "    comparison_results = {}\n",
    "    for category, target_series in target_distribution.items():\n",
    "        # Initialize a DataFrame to store comparison results for this category\n",
    "        comparison_df = pd.DataFrame()\n",
    "\n",
    "        for label, comparison_distribution in zip(labels, comparison_distributions):\n",
    "            # Ensure the comparison distribution series for this category exists and align target with comparison\n",
    "            comparison_series = comparison_distribution.get(category, pd.Series(dtype='float64'))\n",
    "            aligned_target, aligned_comparison = target_series.align(comparison_series, fill_value=0)\n",
    "\n",
    "            # Calculate difference in percentage points\n",
    "            difference = aligned_target - aligned_comparison\n",
    "\n",
    "            # Store the results in the comparison DataFrame\n",
    "            comparison_df[f\"Difference_vs_{label}\"] = difference\n",
    "\n",
    "        comparison_results[category] = comparison_df\n",
    "\n",
    "    return comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_categories = [\"harmonized_donor_life_stage\", \"harmonized_donor_sex\", \"harmonized_sample_disease_high\", \"harmonized_biomaterial_type\", \"paired_end\", \"project\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dist = calculate_metadata_distribution(ct_full_df, metadata_categories)\n",
    "subclass_distributions = {}\n",
    "comparison_results = {}  # Initialize a dict to hold comparison results for each subgroup\n",
    "\n",
    "for group in ct_full_df.groupby(ASSAY):\n",
    "    label = group[0]\n",
    "    sub_df = group[1]\n",
    "    subclass_distributions[label] = calculate_metadata_distribution(sub_df, metadata_categories)\n",
    "\n",
    "for group in ct_full_df.groupby(CELL_TYPE):\n",
    "    label = group[0]\n",
    "    sub_df = group[1]\n",
    "    subclass_distributions[label] = calculate_metadata_distribution(sub_df, metadata_categories)\n",
    "\n",
    "# Loop through each group and compare to global\n",
    "for group in ct_full_df.groupby([ASSAY, CELL_TYPE]):\n",
    "    assay, cell_type = group[0]\n",
    "    sub_df = group[1]\n",
    "    pair_subclass_dist = calculate_metadata_distribution(sub_df, metadata_categories)\n",
    "    subclass_distributions[(assay, cell_type)] = pair_subclass_dist\n",
    "\n",
    "    assay_dist = subclass_distributions[assay]\n",
    "    cell_type_dist = subclass_distributions[cell_type]\n",
    "\n",
    "    comparisons_dists = [assay_dist, cell_type_dist, global_dist]\n",
    "    comparison_labels = [assay, cell_type, \"global\"]\n",
    "\n",
    "    comparison_results[(assay, cell_type)] = compare_label_ratios(\n",
    "        target_distribution=pair_subclass_dist,\n",
    "        comparison_distributions=comparisons_dists,\n",
    "        labels=comparison_labels,\n",
    "        )\n",
    "\n",
    "\n",
    "pair_dfs = []\n",
    "for (assay, cell_type), comparisons in comparison_results.items():\n",
    "    # Initialize an empty list to collect DataFrames for concatenation\n",
    "    dfs_to_concat = []\n",
    "\n",
    "    for category, df_comparison in comparisons.items():\n",
    "        df_comparison.columns = [\"Difference vs Assay\", \"Difference vs Cell Type\", \"Difference vs Global\"]\n",
    "        # Add identifiers for the assay, cell type, and category\n",
    "        df_comparison['Assay'] = assay\n",
    "        df_comparison['Cell Type'] = cell_type\n",
    "        df_comparison['Category'] = category\n",
    "\n",
    "        subclass_dist = subclass_distributions[(assay, cell_type)][category]\n",
    "        df_comparison[\"(assay, ct) subclass %\"] = subclass_dist\n",
    "\n",
    "        # Collect the DataFrame\n",
    "        dfs_to_concat.append(df_comparison.reset_index())\n",
    "\n",
    "\n",
    "    # Concatenate all DataFrames along rows\n",
    "    final_df = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "    final_df.fillna(0, inplace=True)\n",
    "\n",
    "    new_columns = final_df.columns.tolist()\n",
    "    new_first = [\"index\", \"Category\", \"(assay, ct) subclass %\"]\n",
    "    for label in new_first:\n",
    "        new_columns.remove(label)\n",
    "    new_columns = new_first + new_columns\n",
    "    final_df = final_df[new_columns]\n",
    "\n",
    "    pair_dfs.append(final_df)\n",
    "\n",
    "    # # Define the file path\n",
    "    # file_path = base_fig_dir / f\"metadata_comparison_{assay}_{cell_type}.csv\"\n",
    "\n",
    "    # # Save the DataFrame to CSV\n",
    "    # final_df.to_csv(file_path, index=False)\n",
    "\n",
    "# Concatenate all DataFrames along rows\n",
    "all_pairs_df = pd.concat(pair_dfs, axis=0, ignore_index=True)\n",
    "all_pairs_df.columns = [\"Label\" if x=='index' else x for x in all_pairs_df.columns]\n",
    "file_path = base_fig_dir / f\"metadata_comparison_all.csv\"\n",
    "all_pairs_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect all general run parameters: fix oversampling when missing\n",
    "i.e. create a new all_results_cometml_filtered_oversampling-fixed.csv\n",
    "- get difference of content between different metadata groups (diff md5, create new meta obj with just diff, display labels the usual way)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
