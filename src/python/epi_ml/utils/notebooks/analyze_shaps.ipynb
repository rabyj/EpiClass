{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze shaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initial analysis of shap values behavior.\"\"\"\n",
    "# pylint: disable=redefined-outer-name, expression-not-assigned, import-error, not-callable, pointless-statement, no-value-for-parameter, undefined-variable, unused-argument, line-too-long, use-dict-literal, too-many-lines, unused-import, unused-variable\n",
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence, Set, Tuple\n",
    "\n",
    "import matplotlib.patheffects as path_effects\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px  # type: ignore\n",
    "import plotly.graph_objects as go  # type: ignore\n",
    "import plotly.io as pio  # type: ignore\n",
    "import upsetplot  # type: ignore\n",
    "from IPython.display import display\n",
    "from scipy.special import softmax  # type: ignore\n",
    "\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "from epi_ml.core import metadata\n",
    "from epi_ml.core.data_source import EpiDataSource\n",
    "from epi_ml.utils.bed_utils import bins_to_bed_ranges, write_to_bed\n",
    "from epi_ml.utils.general_utility import get_valid_filename\n",
    "from epi_ml.utils.metadata_utils import count_combinations, count_pairs\n",
    "from epi_ml.utils.shap.analyze_shaps_kfold import compare_kfold_shap_analysis\n",
    "from epi_ml.utils.shap.shap_analysis import (\n",
    "    DECILES,\n",
    "    feature_overlap_stats,\n",
    "    print_feature_overlap_stats,\n",
    "    print_importance_info,\n",
    ")\n",
    "from epi_ml.utils.shap.shap_utils import (\n",
    "    extract_shap_values_and_info,\n",
    "    get_archives,\n",
    "    get_shap_matrix,\n",
    "    n_most_important_features,\n",
    ")\n",
    "from epi_ml.utils.time import time_now_str\n",
    "\n",
    "BIOMATERIAL_TYPE = \"harmonized_biomaterial_type\"\n",
    "CELL_TYPE = \"harmonized_sample_ontology_intermediate\"\n",
    "ASSAY = \"assay_epiclass\"\n",
    "SEX = \"harmonized_donor_sex\"\n",
    "CANCER = \"harmonized_sample_cancer_high\"\n",
    "DISEASE = \"harmonized_sample_disease_high\"\n",
    "LIFE_STAGE = \"harmonized_donor_life_stage\"\n",
    "TRACK = \"track_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "home = Path().home() / \"Projects\"\n",
    "input_dir = home / \"epilap/input\"\n",
    "metadata_path = (\n",
    "    input_dir\n",
    "    / \"metadata/dfreeze-v2/hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2.json\"\n",
    ")\n",
    "my_meta = metadata.Metadata(metadata_path)\n",
    "chroms = EpiDataSource.load_external_chrom_file(\n",
    "    input_dir / \"chromsizes/hg38.noy.chrom.sizes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 100 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = CELL_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_meta.remove_missing_labels(category)\n",
    "my_meta.display_labels(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_combinations(my_meta, [ASSAY, CELL_TYPE, TRACK])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze important features over all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dir = (\n",
    "    Path.home()\n",
    "    / \"mounts/narval-mount/project-rabyj/epilap/output/logs/epiatlas-dfreeze-v2.1/hg38_100kb_all_none\"\n",
    ")\n",
    "if not logs_dir.exists():\n",
    "    raise ValueError(f\"Logs dir {logs_dir} does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect information from all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_important_features(\n",
    "    parent_folder: Path, top_n: int, frequency_threshold: str\n",
    ") -> Dict[str, Dict[str, Dict[str, List[int]]]]:\n",
    "    \"\"\"Join important features from all folds\n",
    "\n",
    "    Args:\n",
    "        parent_folder (Path): Parent folder of split folders.\n",
    "        top_n (int): Number of top features to consider for each sample,\n",
    "                    used to select the analysis folder.\n",
    "        frequency_threshold (str): Frequency threshold used to select the analysis folder.\n",
    "\n",
    "    Returns:\n",
    "        Dict: all_important_features\n",
    "        - level 1: folder_name (str): split_dict\n",
    "        - level 2: split_name (str) : all_class_features (dict)\n",
    "        - level 3: class_label (classifier outputs, str): important features for each frequency threshold (dict)\n",
    "        - level 4: frequency_threshold (str, 0 to 100): list of features (List[int])\n",
    "    \"\"\"\n",
    "    print(\"WARNING: Consider using a local copy of the data to speed up the process.\")\n",
    "    all_important_features = defaultdict(dict)\n",
    "    all_folders = parent_folder.glob(\n",
    "        f\"split*/shap/analysis_n{top_n}_f{frequency_threshold}/*\"\n",
    "    )\n",
    "    for important_features_path in all_folders:\n",
    "        split = important_features_path.parents[2].name\n",
    "        if \"split\" not in split:\n",
    "            raise ValueError(f\"Split not found in {split}\")\n",
    "        folder = important_features_path.name\n",
    "\n",
    "        # # Do not consider assay+celltype subsamplings\n",
    "        # if folder not in assay_labels and folder != \"mixed_samples\":\n",
    "        #     continue\n",
    "\n",
    "        json_path = important_features_path / \"important_features.json\"\n",
    "        try:\n",
    "            with open(json_path, \"r\", encoding=\"utf8\") as f:\n",
    "                features = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            shutil.rmtree(important_features_path)\n",
    "            continue\n",
    "\n",
    "        all_important_features[folder][split] = features\n",
    "\n",
    "    return all_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_global_analysis(\n",
    "    all_important_features: Dict[str, Dict[str, Dict[str, List[int]]]],\n",
    "    resolution: int,\n",
    "    chromsizes: List[Tuple[str, int]],\n",
    "    output_folder: Path,\n",
    "    chosen_percentile: float = 80.0,\n",
    "    minimum_count: int = 8,\n",
    "    top_n: int = 303,\n",
    ") -> None:\n",
    "    \"\"\"Perform global analysis of shap values. For each\n",
    "    subsampling, compute the frequency of features over all splits\n",
    "    and save it to a feature_count.json file.\n",
    "\n",
    "    Also write to bed the features that respect the chosen_percentile and\n",
    "    minimum_count thresholds.\n",
    "    \"\"\"\n",
    "    for folder_name, splits_dict in all_important_features.items():\n",
    "        parent = output_folder / \"global_shap_analysis\"\n",
    "        if not parent.exists():\n",
    "            raise ValueError(f\"Parent folder {parent} does not exist\")\n",
    "        subsampling_output = parent / f\"top{top_n}\" / folder_name\n",
    "        subsampling_output.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Performing global analysis for {folder_name}\")\n",
    "        feature_count = compare_kfold_shap_analysis(\n",
    "            important_features_all_splits=splits_dict,\n",
    "            resolution=resolution,\n",
    "            chromsizes=chromsizes,\n",
    "            output_folder=subsampling_output,\n",
    "            name=folder_name,\n",
    "            chosen_percentile=chosen_percentile,\n",
    "            minimum_count=minimum_count,\n",
    "        )\n",
    "        with open(subsampling_output / \"feature_count.json\", \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(feature_count, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [CELL_TYPE, LIFE_STAGE, SEX, CANCER]\n",
    "rest_of_paths = [\n",
    "    \"10fold-oversampling\",\n",
    "    \"no-unknown/10fold-oversampling\",\n",
    "    \"w-mixed/10fold-oversample\",\n",
    "    \"10fold-oversampling\",\n",
    "]\n",
    "\n",
    "# Variables used to find the right analysis folder and write new files\n",
    "top_n = 303\n",
    "chosen_percentile = 80.0\n",
    "minimum_count = 8\n",
    "frequency_threshold = f\"{chosen_percentile:.2f}\"\n",
    "\n",
    "pairs = list(zip(categories, rest_of_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, rest_of_path in pairs:\n",
    "    if category != CANCER:\n",
    "        continue\n",
    "\n",
    "    results_dir = logs_dir / f\"{category}_1l_3000n/{rest_of_path}\"\n",
    "    if not results_dir.is_dir():\n",
    "        raise ValueError(f\"Directory {results_dir} does not exist.\")\n",
    "\n",
    "    all_important_features = join_important_features(\n",
    "        results_dir, top_n=top_n, frequency_threshold=frequency_threshold\n",
    "    )\n",
    "\n",
    "    perform_global_analysis(\n",
    "        all_important_features=all_important_features,\n",
    "        resolution=RESOLUTION,\n",
    "        chromsizes=chroms,\n",
    "        output_folder=results_dir,\n",
    "        chosen_percentile=chosen_percentile,\n",
    "        minimum_count=minimum_count,\n",
    "        top_n=top_n,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results_dir)\n",
    "join_important_features(results_dir, top_n=top_n, frequency_threshold=frequency_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_global_analysis(\n",
    "    all_important_features=all_important_features,\n",
    "    resolution=RESOLUTION,\n",
    "    chromsizes=chroms,\n",
    "    output_folder=results_dir,\n",
    "    chosen_percentile=chosen_percentile,\n",
    "    minimum_count=minimum_count,\n",
    "    top_n=top_n,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_samplings_shap_analysis(\n",
    "    jsons_parent_folder: Path, minimum_count: int = 8\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compare the SHAP analysis results from multiple subsamplings and make a matrix out of them.\n",
    "\n",
    "    Args:\n",
    "        jsons_parent_folder (Path): Parent folder containing subfolders with the feature count jsons.\n",
    "        minimum_count (int, optional): For a given feature, presence in how many training folds is required\n",
    "                                       to be selected. Defaults to 8.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Intersection matrix (dtype=int) of the feature sets.\n",
    "    \"\"\"\n",
    "    all_features_counts = {}\n",
    "    output_classes = []\n",
    "    for folder in jsons_parent_folder.iterdir():\n",
    "        if not folder.is_dir():\n",
    "            continue\n",
    "\n",
    "        json_path = folder / \"feature_count.json\"\n",
    "\n",
    "        with open(json_path, \"r\", encoding=\"utf8\") as json_file:\n",
    "            feature_count = json.load(json_file)\n",
    "\n",
    "        all_features_counts[folder.name] = feature_count\n",
    "\n",
    "        output_classes.extend(feature_count.keys())\n",
    "\n",
    "    output_classes = list(set(output_classes))\n",
    "\n",
    "    print(f\"Output classes: {output_classes}\")\n",
    "    print(f\"Number of output classes: {len(output_classes)}\")\n",
    "\n",
    "    # Filter features using minimum_count\n",
    "    filtered_class_features = {}\n",
    "    for subsampling_name, subsampling_feature_count in all_features_counts.items():\n",
    "        for class_label, class_features in subsampling_feature_count.items():\n",
    "            filtered_class_features[f\"{subsampling_name} & {class_label}\"] = [\n",
    "                feature for feature, count in class_features if count >= minimum_count\n",
    "            ]\n",
    "\n",
    "    # Make pandas intersection matrix, diagonal is the number of features in each dict item\n",
    "    intersection_matrix = np.zeros(\n",
    "        (len(filtered_class_features), len(filtered_class_features)), dtype=int\n",
    "    )\n",
    "    for i, (_, features1) in enumerate(filtered_class_features.items()):\n",
    "        for j, (_, features2) in enumerate(filtered_class_features.items()):\n",
    "            intersection_matrix[i, j] = len(set(features1).intersection(features2))\n",
    "\n",
    "    labels = [label.lower() for label in filtered_class_features]\n",
    "    matrix_df = pd.DataFrame(data=intersection_matrix, index=labels, columns=labels)\n",
    "\n",
    "    # Remove completely empty row and columns\n",
    "    rem = np.where(matrix_df.sum(axis=1) == 0)\n",
    "    matrix_df = matrix_df.drop(matrix_df.index[rem])\n",
    "    matrix_df = matrix_df.drop(matrix_df.columns[rem], axis=1)\n",
    "\n",
    "    return matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_analysis_folder = base_shap_results_dir / \"global_shap_analysis\" / \"top100\"\n",
    "global_analysis_folder.mkdir(parents=False, exist_ok=True)\n",
    "matrix_df = compare_samplings_shap_analysis(global_analysis_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sample count to the labels\n",
    "label_pairs = count_pairs(my_meta, ASSAY, CELL_TYPE)\n",
    "label_pairs = {\n",
    "    f\"{names[0]} & {names[1].lower()}\": count for names, count in label_pairs.items()\n",
    "}\n",
    "new_index = [label for label in matrix_df.index if label in label_pairs]\n",
    "new_index_mapping = {label: f\"{label} ({label_pairs[label]})\" for label in new_index}\n",
    "matrix_df.rename(index=new_index_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_key(value: str):\n",
    "    \"\"\"Return \"something_else\" from \"something & something_else (count)\"\"\"\n",
    "    main_part = value.split(\" & \")[-1]  # Part after \"&\"\n",
    "    sortable_part = main_part.split(\"(\")[0].strip()  # Part before \"(\"\n",
    "    return sortable_part.lower()\n",
    "\n",
    "\n",
    "# Sort the DataFrame\n",
    "sorted_index = sorted(matrix_df.index, key=sorting_key)\n",
    "sorted_columns = sorted(matrix_df.columns, key=sorting_key)\n",
    "matrix_df = matrix_df.reindex(sorted_index)\n",
    "matrix_df = matrix_df[sorted_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_intersection_matrix_heatmap(matrix_df: pd.DataFrame, category: str) -> None:\n",
    "    \"\"\"Create a heatmap of the intersection matrix.\n",
    "\n",
    "    Args:\n",
    "        matrix_df (pd.DataFrame): Intersection matrix. Index and columns labels used as is.\n",
    "        category (str): Category name. Used in the plot title.\n",
    "    \"\"\"\n",
    "    # Create the figure and axis objects\n",
    "    fig, ax = plt.subplots(figsize=(25, 20))  # Adjust the size as needed\n",
    "\n",
    "    # Create a colormap that sets the 'bad' values (masked) to white\n",
    "\n",
    "    masked_array = np.ma.array(matrix_df, mask=(matrix_df == 0))  # pylint: disable=C0325\n",
    "    cmap = mpl.cm.get_cmap(\"viridis\").copy()  # type: ignore\n",
    "    cmap.set_bad(color=\"white\")\n",
    "\n",
    "    # Display the data\n",
    "    cax = ax.imshow(masked_array, cmap=cmap, interpolation=\"none\")\n",
    "\n",
    "    # Add colorbar\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Annotate each cell with the numeric value\n",
    "    # We iterate over the indices and the data in the DataFrame\n",
    "    for (i, j), val in np.ndenumerate(matrix_df):\n",
    "        if val != 0:  # Skip zero values to keep the white background clean\n",
    "            ax.text(j, i, int(val), ha=\"center\", va=\"center\", color=\"white\", path_effects=[path_effects.withStroke(linewidth=1.5, foreground=\"black\")])  # type: ignore\n",
    "\n",
    "    # Adjust the grid lines and labels\n",
    "    ax.grid(which=\"major\", color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "    # Shift the ticks and labels to be at the center of each cell\n",
    "    ax.set_xticks(np.arange(len(matrix_df.columns)) - 0.5)\n",
    "    ax.set_yticks(np.arange(len(matrix_df.index)) - 0.5)\n",
    "\n",
    "    # Set the labels to be at the tick locations\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", length=0)  # Hide the tick marks\n",
    "\n",
    "    # Add new axis labels with the correct offset\n",
    "    ax2 = ax.secondary_xaxis(\"bottom\")\n",
    "    ax2.set_xticks(np.arange(len(matrix_df.columns)))\n",
    "    ax2.set_xticklabels(matrix_df.columns, rotation=45, ha=\"right\")\n",
    "\n",
    "    ax3 = ax.secondary_yaxis(\"left\")\n",
    "    ax3.set_yticks(np.arange(len(matrix_df.index)))\n",
    "    ax3.set_yticklabels(matrix_df.index)\n",
    "\n",
    "    # Set the title and show the plot\n",
    "    title = f\"{category} important features groups intersection heatmap (top100, f80, split_count=8)\"\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    img_path = global_analysis_folder / (get_valid_filename(title) + \".png\")\n",
    "    plt.savefig(img_path, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_intersection_matrix_heatmap(matrix_df, category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venn Diagramm over multiple classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [CANCER, LIFE_STAGE, CELL_TYPE]\n",
    "union_important_features = defaultdict(set)\n",
    "for category in categories:\n",
    "    print(f\"Category: {category}\")\n",
    "    path = list(\n",
    "        (logs_dir / f\"{category}_1l_3000n\").glob(\n",
    "            \"10fold-oversampl*/global_shap_analysis/top100*\"\n",
    "        )\n",
    "    )\n",
    "    if not path:\n",
    "        path = list(\n",
    "            (logs_dir / f\"{category}_1l_3000n\").glob(\n",
    "                \"*/10fold-oversampl*/global_shap_analysis/top100*\"\n",
    "            )\n",
    "        )\n",
    "        if not path:\n",
    "            raise ValueError(f\"Path {path} does not exist.\")\n",
    "    path = path[0]\n",
    "    print(f\"Path: {path}\")\n",
    "\n",
    "    for folder in path.iterdir():\n",
    "        if not folder.is_dir():\n",
    "            continue\n",
    "\n",
    "        json_path = folder / \"feature_count.json\"\n",
    "\n",
    "        with open(json_path, \"r\", encoding=\"utf8\") as json_file:\n",
    "            feature_count = json.load(json_file)\n",
    "\n",
    "        for class_label, class_features in feature_count.items():\n",
    "            features = [feature for feature, count in class_features if count >= 8]\n",
    "            union_important_features[category].update(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3  # type: ignore\n",
    "\n",
    "v = venn3(\n",
    "    list(union_important_features.values()), set_labels=union_important_features.keys()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(\n",
    "    sample_shap_values: np.ndarray,\n",
    "    important_features: list,\n",
    "    title: str,\n",
    "    plot_type: str,\n",
    "    logdir: str | Path,\n",
    ") -> None:\n",
    "    \"\"\"Plot feature importance in a sample, highlighting important features using Plotly.\n",
    "\n",
    "    Args:\n",
    "        sample_shap_values (np.ndarray): The SHAP values for a single sample.\n",
    "        important_features (list): List of indices corresponding to important features.\n",
    "        title (str): The title for the plot.\n",
    "        plot_type (str): Type of plot (\"raw\", \"softmax\", or \"rank\").\n",
    "    \"\"\"\n",
    "\n",
    "    if plot_type == \"raw\":\n",
    "        plot_values = sample_shap_values\n",
    "    elif plot_type == \"softmax\":\n",
    "        plot_values = softmax(sample_shap_values)\n",
    "    elif plot_type == \"rank\":\n",
    "        plot_values = np.argsort(\n",
    "            np.argsort(-np.abs(sample_shap_values))\n",
    "        )  # Rank based on absolute values\n",
    "    else:\n",
    "        raise ValueError(\"Invalid plot_type.\")\n",
    "\n",
    "    title = f\"{title} ({plot_type})\"\n",
    "    # General points\n",
    "    trace1 = go.Scatter(\n",
    "        x=list(range(len(plot_values))),\n",
    "        y=plot_values,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"blue\"),\n",
    "        name=\"All Features\",\n",
    "    )\n",
    "\n",
    "    # Important points\n",
    "    trace2 = go.Scatter(\n",
    "        x=important_features,\n",
    "        y=[plot_values[i] for i in important_features],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"red\"),\n",
    "        name=\"Important Features\",\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=title, xaxis=dict(title=\"Feature index\"), yaxis=dict(title=plot_type)\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "\n",
    "    logdir = Path(logdir)\n",
    "    fig.write_image(logdir / f\"{title}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_whole_thing(\n",
    "    metadata: metadata.Metadata,\n",
    "    shap_dir: Path,\n",
    "    output_dir: Path,\n",
    "    label_category: str,\n",
    "    top_n: int = 100,\n",
    ") -> Dict[str, List[int]]:\n",
    "    \"\"\"Execute the complete process of SHAP value analysis.\n",
    "\n",
    "    This function performs the complete SHAP value analysis given the metadata and the directory\n",
    "    of the SHAP value files. It carries out the following steps:\n",
    "    1. Load the SHAP value archives and print basic statistics.\n",
    "    2. Filter the metadata to match the samples in SHAP value archives.\n",
    "    For each output class class:\n",
    "        3. Extract SHAP values.\n",
    "        4. Determine the top N features for each sample.\n",
    "        5. Compute and print feature overlap statistics.\n",
    "        6. Analyze feature importance.\n",
    "        7. Convert bin indices to genomic ranges and write to a BED file.\n",
    "        8. Display and save a plot of importance distribution for one sample.\n",
    "\n",
    "    Args:\n",
    "        metadata (metadata.Metadata): The metadata for the samples.\n",
    "        shap_logdir (Path): The directory path where SHAP value files are stored.\n",
    "        label_category (str): The name of the classifier output category that computed the shaps.\n",
    "        top_n (int): The number of top features to be selected for each sample. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[int, List[int]]: Dictionary where keys are class labels and values are lists\n",
    "        of the most frequently occurring important (high shap) features for that class, for each computed quantile.\n",
    "        (see feature_overlap_stats function for more details).\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If \"evaluation_md5s\" or \"evaluation_ids\" are not found in the loaded SHAP value archives.\n",
    "    \"\"\"\n",
    "    metadata = copy.deepcopy(metadata)\n",
    "\n",
    "    # Extract shap values and md5s from archive\n",
    "    shap_matrices, eval_md5s, classes = extract_shap_values_and_info(shap_dir)\n",
    "\n",
    "    # Filter metadata to include only the samples that exist in the SHAP value archives\n",
    "    for md5 in list(metadata.md5s):\n",
    "        if md5 not in set(eval_md5s):\n",
    "            del metadata[md5]\n",
    "\n",
    "    metadata.display_labels(\"assay_epiclass\")\n",
    "    metadata.display_labels(\"harmonized_donor_sex\")\n",
    "\n",
    "    # Loop over each class to perform SHAP value analysis\n",
    "    important_features = {}\n",
    "    for class_int, class_label in classes:\n",
    "        class_int = int(class_int)\n",
    "        print(f\"\\n\\nClass: {class_label} ({class_int})\")\n",
    "\n",
    "        # Get the SHAP matrix for the current class,\n",
    "        # and only select samples that also correspond to that class\n",
    "        shap_matrix, chosen_idxs = get_shap_matrix(\n",
    "            meta=metadata,\n",
    "            shap_matrices=shap_matrices,\n",
    "            eval_md5s=eval_md5s,\n",
    "            label_category=label_category,\n",
    "            selected_labels=[class_label],\n",
    "            class_idx=class_int,\n",
    "        )\n",
    "\n",
    "        if len(chosen_idxs) < 5:\n",
    "            print(f\"Not enough samples (5) to perform analysis on {class_label}.\")\n",
    "            continue\n",
    "\n",
    "        chosen_percentile = 90\n",
    "        result_bed_filename = get_valid_filename(\n",
    "            f\"frequent_features_{chosen_percentile}_{class_label}.bed\"\n",
    "        )\n",
    "        if (output_dir / result_bed_filename).is_file():\n",
    "            print(f\"Skipping {class_label} because {result_bed_filename} already exists.\")\n",
    "            continue\n",
    "\n",
    "        # Computing statistics of feature overlap\n",
    "        print(\n",
    "            f\"Selecting features with top {top_n} SHAP values for each sample of {class_label}.\"\n",
    "        )\n",
    "        top_n_features = []\n",
    "        for sample in shap_matrix:\n",
    "            top_n_features.append(list(n_most_important_features(sample, top_n)))\n",
    "\n",
    "        feat_intersect, feat_union, frequent_features, hist_fig = feature_overlap_stats(\n",
    "            top_n_features, [90, 95, 99]\n",
    "        )\n",
    "        important_features[class_label] = frequent_features\n",
    "\n",
    "        hist_fig.write_image(\n",
    "            file=output_dir / f\"top{top_n}_feature_frequency_{class_label}.png\",\n",
    "            format=\"png\",\n",
    "        )\n",
    "\n",
    "        # print_feature_overlap_stats(some_stats)\n",
    "\n",
    "        feature_selection = frequent_features[chosen_percentile]\n",
    "\n",
    "        # print_importance_info(feature_selection, shap_matrix)\n",
    "\n",
    "        # Convert bin indices to genomic ranges and write to a BED file\n",
    "        bed_vals = bins_to_bed_ranges(\n",
    "            sorted(feature_selection), chroms, resolution=RESOLUTION\n",
    "        )\n",
    "\n",
    "        write_to_bed(\n",
    "            bed_vals,\n",
    "            output_dir / result_bed_filename,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        # # Display and save a plot of importance distribution for one sample\n",
    "        # print(\"One sample\")\n",
    "        # probs_1sample = pd.DataFrame(softmax(shap_matrix, axis=1)[0, :] * 100)\n",
    "        # display(probs_1sample.describe(percentiles=DECILES))\n",
    "        # fig_title = f\"Importance distribution - One sample - {eval_md5s[chosen_idxs[0]]}\"\n",
    "        # fig = px.violin(probs_1sample, box=True, points=\"all\", title=fig_title)\n",
    "        # fig.write_image(shap_logdir / \"importance_dist_1sample.png\")\n",
    "\n",
    "    return important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_shap_analysis(\n",
    "    shap_values_and_info: Tuple[np.ndarray, List[str], List[Tuple[str, str]]],\n",
    "    metadata: metadata.Metadata,\n",
    "    output_dir: Path,\n",
    "    label_category: str,\n",
    "    top_n: int = 100,\n",
    "    filter_metdata: bool = True,\n",
    ") -> Dict[str, List[int]]:\n",
    "    \"\"\"Execute the complete process of SHAP value analysis, without the value extraction part.\n",
    "\n",
    "    Extraction part metadata filtering takes a long time, this is intended to not repeat that part if possible.\n",
    "\n",
    "    This function performs the complete SHAP value analysis given\n",
    "    - shap values archive information (extract_shap_values_and_info output)\n",
    "    - metadata\n",
    "    - output directory\n",
    "    It carries out the following steps:\n",
    "    1. Filter the metadata to match the samples in SHAP value archives.\n",
    "    For each output class class:\n",
    "        2. Extract SHAP values.\n",
    "        3. Determine the top N features for each sample.\n",
    "        4. Compute and print feature overlap statistics.\n",
    "        5. Analyze feature importance.\n",
    "        6. Convert bin indices to genomic ranges and write to a BED file.\n",
    "        7. Display and save a plot of importance distribution for one sample.\n",
    "\n",
    "    Args:\n",
    "        shap_values_and_info : Tuple containing the shap values archive information.\n",
    "        metadata (metadata.Metadata): The metadata for the samples.\n",
    "        output_dir (Path): The directory path where the results will be stored.\n",
    "        label_category (str): The name of the classifier output category that computed the shaps.\n",
    "        top_n (int): The number of top features to be selected for each sample. Defaults to 100.\n",
    "        filter_metdata (bool): Whether to filter the metadata to match the samples in SHAP value archives.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[int, List[int]]: Dictionary where keys are class labels and values are lists\n",
    "        of the most frequently occurring important (high shap) features for that class, for each computed quantile.\n",
    "        (see feature_overlap_stats function for more details).\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If \"evaluation_md5s\" or \"evaluation_ids\" are not found in the loaded SHAP value archives.\n",
    "    \"\"\"\n",
    "    shap_matrices, eval_md5s, classes = shap_values_and_info\n",
    "\n",
    "    # Filter metadata to include only the samples that exist in the SHAP value archives\n",
    "    if filter_metdata:\n",
    "        metadata = copy.deepcopy(metadata)\n",
    "        for md5 in list(metadata.md5s):\n",
    "            if md5 not in set(eval_md5s):\n",
    "                del metadata[md5]\n",
    "    else:\n",
    "        print(\"Warning: Skipping metadata filtering.\")\n",
    "\n",
    "    metadata.display_labels(\"assay_epiclass\")\n",
    "    metadata.display_labels(\"harmonized_donor_sex\")\n",
    "\n",
    "    # Loop over each class to perform SHAP value analysis\n",
    "    important_features = {}\n",
    "    for class_int, class_label in classes:\n",
    "        class_int = int(class_int)\n",
    "        print(f\"\\n\\nClass: {class_label} ({class_int})\")\n",
    "\n",
    "        # Get the SHAP matrix for the current class,\n",
    "        # and only select samples that also correspond to that class\n",
    "        shap_matrix, chosen_idxs = get_shap_matrix(\n",
    "            meta=metadata,\n",
    "            shap_matrices=shap_matrices,\n",
    "            eval_md5s=eval_md5s,\n",
    "            label_category=label_category,\n",
    "            selected_labels=[class_label],\n",
    "            class_idx=class_int,\n",
    "        )\n",
    "\n",
    "        if len(chosen_idxs) < 5:\n",
    "            print(f\"Not enough samples (5) to perform analysis on {class_label}.\")\n",
    "            continue\n",
    "\n",
    "        chosen_percentile = 90\n",
    "        result_bed_filename = get_valid_filename(\n",
    "            f\"frequent_features_{chosen_percentile}_{class_label}.bed\"\n",
    "        )\n",
    "\n",
    "        # Don't redo work\n",
    "        if (output_dir / result_bed_filename).is_file():\n",
    "            print(f\"Skipping {class_label} because {result_bed_filename} already exists.\")\n",
    "            continue\n",
    "\n",
    "        # Computing statistics of feature overlap\n",
    "        print(\n",
    "            f\"Selecting features with top {top_n} SHAP values for each sample of {class_label}.\"\n",
    "        )\n",
    "        top_n_features = []\n",
    "        for sample in shap_matrix:\n",
    "            top_n_features.append(list(n_most_important_features(sample, top_n)))\n",
    "\n",
    "        feat_intersect, feat_union, frequent_features, hist_fig = feature_overlap_stats(\n",
    "            top_n_features, [90, 95, 99]\n",
    "        )\n",
    "        important_features[class_label] = frequent_features\n",
    "\n",
    "        hist_fig.write_image(\n",
    "            file=output_dir / f\"top{top_n}_feature_frequency_{class_label}.png\",\n",
    "            format=\"png\",\n",
    "        )\n",
    "\n",
    "        # print_feature_overlap_stats(some_stats)\n",
    "\n",
    "        feature_selection = frequent_features[chosen_percentile]\n",
    "\n",
    "        # print_importance_info(feature_selection, shap_matrix)\n",
    "\n",
    "        # Convert bin indices to genomic ranges and write to a BED file\n",
    "        bed_vals = bins_to_bed_ranges(\n",
    "            sorted(feature_selection), chroms, resolution=RESOLUTION\n",
    "        )\n",
    "\n",
    "        write_to_bed(\n",
    "            bed_vals,\n",
    "            output_dir / result_bed_filename,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        # # Display and save a plot of importance distribution for one sample\n",
    "        # print(\"One sample\")\n",
    "        # probs_1sample = pd.DataFrame(softmax(shap_matrix, axis=1)[0, :] * 100)\n",
    "        # display(probs_1sample.describe(percentiles=DECILES))\n",
    "        # fig_title = f\"Importance distribution - One sample - {eval_md5s[chosen_idxs[0]]}\"\n",
    "        # fig = px.violin(probs_1sample, box=True, points=\"all\", title=fig_title)\n",
    "        # fig.write_image(shap_logdir / \"importance_dist_1sample.png\")\n",
    "\n",
    "    return important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_alternative_analysis(\n",
    "    metadata: metadata.Metadata,\n",
    "    shap_logdir: Path,\n",
    "    label_category: str,\n",
    "    selected_classes: list[str],\n",
    "    top_n: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"Run an alternative analysis that involves plotting feature importance for selected classes.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Extracts SHAP values and associated metadata.\n",
    "    2. Filters out samples from the metadata that are not present in the SHAP value archives.\n",
    "    3. Collects the most important features for selected classes.\n",
    "    4. Plots the feature importance for samples of these selected classes using different metrics (\"raw\", \"softmax\", \"rank\").\n",
    "\n",
    "    Args:\n",
    "        metadata (\"metadata.Metadata\"): The metadata object containing sample information.\n",
    "        shap_logdir (Path): The directory where the SHAP value archives are stored.\n",
    "        label_category (str): The category of the label to be used for class selection.\n",
    "        selected_classes (List[str]): A list of classes for which the analysis should be run.\n",
    "        top_n (int, optional): The top N most important features to consider. Default is 100.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If sample indices are not unique across classes.\n",
    "    \"\"\"\n",
    "    metadata = copy.deepcopy(metadata)\n",
    "\n",
    "    # Extract shap values and md5s from archive\n",
    "    shap_matrices, eval_md5s, classes = extract_shap_values_and_info(shap_logdir)\n",
    "\n",
    "    # Filter metadata to include only the samples that exist in the SHAP value archives\n",
    "    for md5 in list(metadata.md5s):\n",
    "        if md5 not in set(eval_md5s):\n",
    "            del metadata[md5]\n",
    "\n",
    "    # collect important features for selected classes\n",
    "    selected_percentile = 90\n",
    "    classes_dict = {\n",
    "        class_label: int(class_int)\n",
    "        for class_int, class_label in classes\n",
    "        if class_label in selected_classes\n",
    "    }\n",
    "    important_features = {}\n",
    "    sample_idxs = {}\n",
    "    for class_label, class_int in classes_dict.items():\n",
    "        print(f\"\\n\\nClass: {class_label} ({class_int})\")\n",
    "\n",
    "        # Get the SHAP matrix for the current class,\n",
    "        # and only select samples that also correspond to that class\n",
    "        shap_matrix, chosen_idxs = get_shap_matrix(\n",
    "            meta=metadata,\n",
    "            shap_matrices=shap_matrices,\n",
    "            eval_md5s=eval_md5s,\n",
    "            label_category=label_category,\n",
    "            selected_labels=[class_label],\n",
    "            class_idx=class_int,\n",
    "        )\n",
    "        sample_idxs[class_label] = chosen_idxs\n",
    "\n",
    "        # Computing statistics of feature overlap\n",
    "        top_n_features = []\n",
    "        for sample in shap_matrix:\n",
    "            top_n_features.append(list(n_most_important_features(sample, top_n)))\n",
    "\n",
    "        some_stats = feature_overlap_stats(top_n_features, [selected_percentile])\n",
    "        frequent_features = some_stats[2]\n",
    "        important_features[class_label] = frequent_features[selected_percentile]\n",
    "\n",
    "    all_chosen_idxs = set()\n",
    "    for idxs in sample_idxs.values():\n",
    "        all_chosen_idxs.update(idxs)\n",
    "\n",
    "    if len(all_chosen_idxs) != sum(len(idxs) for idxs in sample_idxs.values()):\n",
    "        raise ValueError(\"Sample indices are not unique across classes.\")\n",
    "\n",
    "    logdir = shap_logdir / \"feature_rank_analysis\"\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for class_of_interest in selected_classes:\n",
    "        important_feats = important_features[class_of_interest]\n",
    "        for comparison_class in selected_classes:\n",
    "            shap_matrix = shap_matrices[classes_dict[comparison_class]]\n",
    "\n",
    "            # Lists to hold md5s, metadata category, and feature ranks for each sample\n",
    "            md5_list = []\n",
    "            cell_type_list = []\n",
    "            ranks_list = []\n",
    "\n",
    "            for i, sample_shap_values in enumerate(shap_matrix):\n",
    "                if i not in sample_idxs[comparison_class]:\n",
    "                    continue\n",
    "                # if i not in all_chosen_idxs:\n",
    "                #     continue\n",
    "\n",
    "                md5_list.append(eval_md5s[i])\n",
    "                cell_type_list.append(metadata[eval_md5s[i]][CELL_TYPE])\n",
    "\n",
    "                ranks = np.argsort(\n",
    "                    np.argsort(-np.abs(sample_shap_values))\n",
    "                )  # Ranking in descending order of absolute SHAP value\n",
    "                ranks_of_important_feats = ranks[\n",
    "                    important_feats\n",
    "                ]  # Get the ranks of the important features\n",
    "                ranks_list.append(ranks_of_important_feats)\n",
    "\n",
    "            # Combine all the lists into a DataFrame\n",
    "            ranks_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"md5sum\": md5_list,\n",
    "                    CELL_TYPE: cell_type_list,\n",
    "                    **{\n",
    "                        f\"Feature_{feat}\": [ranks[i] for ranks in ranks_list]\n",
    "                        for i, feat in enumerate(important_feats)\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Save the DataFrame to CSV\n",
    "            title = f\"important_{class_of_interest}_features_in_{comparison_class}_shap_matrix.csv\".replace(\n",
    "                \" \", \"_\"\n",
    "            )\n",
    "            ranks_df.to_csv(logdir / title, index=True)\n",
    "            print(\n",
    "                f\"Feature ranks for '{class_of_interest}' features in '{comparison_class}' samples have been saved.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(logdir1, logdir2)\n",
    "# print(logdir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir1 = output / f\"{category}_1l_3000n/10fold/split0/shap/rna_only\"\n",
    "if not logdir1.exists():\n",
    "    raise ValueError(f\"{logdir1} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_matrices, eval_md5s, classes = extract_shap_values_and_info(logdir1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata filtering\n",
    "eval_md5s_set = set(eval_md5s)\n",
    "for md5 in list(my_meta.md5s):\n",
    "    if md5 not in eval_md5s_set:\n",
    "        del my_meta[md5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_meta.display_labels(ASSAY)\n",
    "my_meta.display_labels(CELL_TYPE)\n",
    "if category not in [ASSAY, CELL_TYPE]:\n",
    "    my_meta.display_labels(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is intended as a stop to not run code further needlessly, but still be able to run the beginning in one shot\n",
    "raise ValueError(\"Stop here\")  # pylint: disable=unreachable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donor sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=unreachable\n",
    "assay_labels = (\n",
    "    [[assay_label] for assay_label in my_meta.unique_classes(label_category=ASSAY)]\n",
    "    + [[\"rna_seq\", \"mrna_seq\"]]\n",
    "    + [[\"wbgs-pbat\", \"wgbs-standard\"]]\n",
    ")\n",
    "cell_type_labels = [\n",
    "    \"T cell\",\n",
    "    \"neutrophil\",\n",
    "    \"monocyte\",\n",
    "    \"lymphocyte of B lineage\",\n",
    "    \"brain\",\n",
    "    \"myeloid cell\",\n",
    "]\n",
    "\n",
    "for assay_label_list in assay_labels:\n",
    "    for ct_label in cell_type_labels:\n",
    "        print(assay_label_list, ct_label)\n",
    "        meta = copy.deepcopy(my_meta)\n",
    "        meta.select_category_subsets(ASSAY, assay_label_list)\n",
    "        meta.select_category_subsets(CELL_TYPE, [ct_label])\n",
    "\n",
    "        if len(meta) < 5:\n",
    "            print(\"Not enough samples for this combination. Continuing\")\n",
    "            continue\n",
    "\n",
    "        name = \"_\".join(assay_label_list) + \"_\" + ct_label\n",
    "        run_logdir = logdir1 / \"shap_analysis_and_go\" / get_valid_filename(name)\n",
    "        run_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "        # Already have female/male results bed\n",
    "        if len(list(run_logdir.glob(\"*.bed\"))) >= 2:\n",
    "            continue\n",
    "\n",
    "        print(run_logdir)\n",
    "        important_features = run_the_whole_thing(\n",
    "            metadata=meta,\n",
    "            shap_dir=logdir1,\n",
    "            output_dir=run_logdir,\n",
    "            label_category=category,\n",
    "            top_n=100,\n",
    "        )\n",
    "\n",
    "        feat_90 = {name: sets[90] for name, sets in important_features.items()}\n",
    "\n",
    "        if len(feat_90) <= 1:\n",
    "            continue\n",
    "\n",
    "        upset_features = upsetplot.from_contents(feat_90)\n",
    "\n",
    "        upsetplot.UpSet(\n",
    "            upset_features,\n",
    "            subset_size=\"count\",\n",
    "            show_counts=True,  # type: ignore\n",
    "            show_percentages=True,\n",
    "            sort_by=\"cardinality\",\n",
    "            sort_categories_by=\"cardinality\",\n",
    "        ).plot()\n",
    "\n",
    "        plt.savefig(run_logdir / f\"upset_{name}.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_intersection = set.intersection(\n",
    "#     *[set(feat_list[90]) for feat_list in important_features.values()]\n",
    "# )\n",
    "\n",
    "# male_feat = set(important_features[\"male\"][90])\n",
    "# female_feat = set(important_features[\"female\"][90])\n",
    "\n",
    "# only_male = male_feat - female_feat\n",
    "# only_female = female_feat - male_feat\n",
    "\n",
    "# for feat_set, name in zip([only_female, only_male], [\"only_female\", \"only_male\"]):\n",
    "#     bed_vals = bins_to_bed_ranges(sorted(feat_set), chroms, resolution=RESOLUTION)\n",
    "#     var_name = f\"{feat_set=}\".split(\"=\")[0]\n",
    "#     write_to_bed(\n",
    "#         bed_vals,\n",
    "#         logdir / f\"frequent_features_{90}_{name}.bed\",\n",
    "#         verbose=False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_alternative_analysis(\n",
    "#     metadata=my_meta,\n",
    "#     shap_logdir=logdir,\n",
    "#     label_category=SEX,\n",
    "#     selected_classes=[\"female\", \"male\"],\n",
    "#     top_n=100,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample ontology - 6hist_6ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feat_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_labels = [\n",
    "    [assay_label] for assay_label in my_meta.unique_classes(label_category=ASSAY)\n",
    "]\n",
    "cell_type_labels = [\n",
    "    [ct_label] for ct_label in my_meta.unique_classes(label_category=CELL_TYPE)\n",
    "]\n",
    "\n",
    "general_logdir = logdir1 / \"shap_analysis_and_go\" / \"feature_frequency_method\"\n",
    "general_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "for assay_label_list in assay_labels:\n",
    "    assay_label = \"_\".join(assay_label_list)\n",
    "    assay_logdir = general_logdir / assay_label\n",
    "    assay_logdir.mkdir(parents=False, exist_ok=True)\n",
    "    for ct_label_list in cell_type_labels:\n",
    "        print(assay_label_list, ct_label_list)\n",
    "        meta = copy.deepcopy(my_meta)\n",
    "        meta.select_category_subsets(ASSAY, assay_label_list)\n",
    "        meta.select_category_subsets(CELL_TYPE, ct_label_list)\n",
    "\n",
    "        meta.display_labels(ASSAY)\n",
    "        meta.display_labels(CELL_TYPE)\n",
    "\n",
    "        if len(meta) < 5:\n",
    "            print(\"Not enough samples for this combination. Continuing\")\n",
    "            continue\n",
    "\n",
    "        important_features = run_full_shap_analysis(\n",
    "            shap_values_and_info=(shap_matrices, eval_md5s, classes),\n",
    "            metadata=meta,\n",
    "            output_dir=assay_logdir,\n",
    "            label_category=category,\n",
    "            top_n=100,\n",
    "            filter_metdata=False,\n",
    "        )\n",
    "\n",
    "        cell_type_name = get_valid_filename(\"_\".join(ct_label_list))\n",
    "        important_feat_dict[(assay_label, cell_type_name)] = {\n",
    "            name: sets[90] for name, sets in important_features.items()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in important_feat_dict.items():\n",
    "    print(k, v.keys(), len(list(v.values())[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample ontology - (m)rna-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_labels = [\n",
    "    [assay_label] for assay_label in my_meta.unique_classes(label_category=ASSAY)\n",
    "] + [[\"rna_seq\", \"mrna_seq\"]]\n",
    "cell_type_labels = [\n",
    "    [ct_label] for ct_label in my_meta.unique_classes(label_category=CELL_TYPE)\n",
    "]\n",
    "\n",
    "general_logdir = logdir1 / \"frequent_features\" / \"feature_frequency_method\"\n",
    "general_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "for assay_label_list in assay_labels:\n",
    "    assay_label = \"_\".join(assay_label_list)\n",
    "    assay_logdir = general_logdir / assay_label\n",
    "    assay_logdir.mkdir(parents=False, exist_ok=True)\n",
    "    for ct_label_list in cell_type_labels:\n",
    "        print(assay_label_list, ct_label_list)\n",
    "\n",
    "        meta = copy.deepcopy(my_meta)\n",
    "        meta.select_category_subsets(ASSAY, assay_label_list)\n",
    "        meta.select_category_subsets(CELL_TYPE, ct_label_list)\n",
    "\n",
    "        meta.display_labels(ASSAY)\n",
    "        meta.display_labels(CELL_TYPE)\n",
    "\n",
    "        if len(meta) < 5:\n",
    "            print(\"Not enough samples for this combination. Continuing\")\n",
    "            continue\n",
    "\n",
    "        important_features = run_full_shap_analysis(\n",
    "            shap_values_and_info=(shap_matrices, eval_md5s, classes),\n",
    "            metadata=meta,\n",
    "            output_dir=assay_logdir,\n",
    "            label_category=category,\n",
    "            top_n=100,\n",
    "            filter_metdata=False,\n",
    "        )\n",
    "\n",
    "        cell_type_name = get_valid_filename(\"_\".join(ct_label_list))\n",
    "        important_feat_dict[(assay_label, cell_type_name)] = {\n",
    "            name: sets[90] for name, sets in important_features.items()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in important_feat_dict.items():\n",
    "    print(k, v.keys(), len(list(v.values())[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upset plots for sample_ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- assay upset plot --\n",
    "for assay_label_list in assay_labels:\n",
    "    assay_label = \"_\".join(assay_label_list)\n",
    "    assay_logdir = general_logdir / assay_label\n",
    "    if not assay_logdir.exists():\n",
    "        raise ValueError(f\"{assay_logdir} does not exist.\")\n",
    "\n",
    "    ct_labels = [pair[1] for pair in important_feat_dict if pair[0] == assay_label]\n",
    "    feat_90 = {\n",
    "        ct_label: important_feat_dict[(assay_label, ct_label)].values()\n",
    "        for ct_label in ct_labels\n",
    "    }\n",
    "    upset_features = upsetplot.from_contents(feat_90)\n",
    "\n",
    "    upsetplot.UpSet(\n",
    "        upset_features,\n",
    "        subset_size=\"count\",\n",
    "        show_counts=True,  # type: ignore\n",
    "        show_percentages=True,\n",
    "        sort_by=\"cardinality\",\n",
    "        sort_categories_by=\"cardinality\",\n",
    "    ).plot()\n",
    "\n",
    "    upset_filename = assay_logdir / f\"upset_{assay_label}.png\"\n",
    "    print(f\"Saving UpSet to {upset_filename}\")\n",
    "    plt.savefig(upset_filename, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "# -- cell type upset plot --\n",
    "\n",
    "# cell_types = set(pair[1] for pair in important_feat_dict if pair[0][0:2] == \"h3\") # mixing rna-only mixing with 6hist_6ct results\n",
    "\n",
    "for ct_label_list in cell_type_labels:\n",
    "    ct_label = \"_\".join(ct_label_list)\n",
    "    assay_labels = [pair[0] for pair in important_feat_dict if pair[1] == ct_label]\n",
    "    feat_90 = {\n",
    "        assay_label: important_feat_dict[(assay_label, ct_label)].values()\n",
    "        for assay_label in assay_labels\n",
    "    }\n",
    "\n",
    "    upset_features = upsetplot.from_contents(feat_90)\n",
    "    upsetplot.UpSet(\n",
    "        upset_features,\n",
    "        subset_size=\"count\",\n",
    "        show_counts=True,  # type: ignore\n",
    "        show_percentages=True,\n",
    "        sort_by=\"cardinality\",\n",
    "        sort_categories_by=\"cardinality\",\n",
    "    ).plot()\n",
    "\n",
    "    upset_filename = general_logdir / f\"upset_{ct_label}_w_rna.png\"\n",
    "    print(f\"Saving UpSet to {upset_filename}\")\n",
    "    plt.savefig(upset_filename, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_violin_plots(logdir: str | Path, selected_classes: list[str]) -> None:\n",
    "    \"\"\"Create violin plots of important feature ranks for selected classes.\"\"\"\n",
    "    logdir = Path(logdir)\n",
    "    # Iterate through each class of interest\n",
    "    for class_of_interest in selected_classes:\n",
    "        # Initialize an empty figure\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Iterate through each class to add its violin plot to the figure\n",
    "        for comparison_class in selected_classes:\n",
    "            df = pd.read_csv(\n",
    "                logdir\n",
    "                / f\"important_{class_of_interest}_features_in_{comparison_class}_shap_matrix.csv\"\n",
    "            )\n",
    "            df = df.filter(\n",
    "                like=\"Feature_\"\n",
    "            )  # Remove non-feature columns from DataFrame for plotting\n",
    "\n",
    "            # Create violin plot for the current comparison_class\n",
    "            violin = go.Violin(\n",
    "                y=df.values.flatten(),  # Flattened feature ranks\n",
    "                name=f\"{comparison_class} (n={df.shape[0]})\",  # Name of the violin plot\n",
    "                box_visible=True,  # Display box inside the violin\n",
    "                line_color=px.colors.qualitative.Plotly[\n",
    "                    len(fig.data)  # type: ignore\n",
    "                ],  # Different color for each violin\n",
    "                points=\"all\",  # Display all points\n",
    "            )\n",
    "            fig.add_trace(violin)\n",
    "\n",
    "            print(df.shape)\n",
    "        # Set title and axis labels\n",
    "        fig.update_layout(\n",
    "            title=f\"Violin plot of ranks for important features of '{class_of_interest}' ({df.shape[1]} features)\",  # type: ignore\n",
    "            xaxis_title=\"Source Matrix\",\n",
    "            yaxis_title=\"Feature Rank\",\n",
    "        )\n",
    "\n",
    "        # Show the figure\n",
    "        fig.write_image(\n",
    "            logdir / f\"important_{class_of_interest}_feature_ranks_violin_plot.png\"\n",
    "        )\n",
    "        fig.write_html(\n",
    "            logdir / f\"important_{class_of_interest}_feature_ranks_violin_plot.html\"\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_violin_plots(\n",
    "#     logdir=logdir / \"feature_rank_analysis\", selected_classes=[\"female\", \"male\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples ontology class pairs important features overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_union = set()\n",
    "# feature_intersection = set(list(important_features.values())[0][90])\n",
    "# for label, features in important_features.items():\n",
    "#     features_90 = features[90]\n",
    "#     print(f\"\\n\\nClass: {label}\")\n",
    "#     print(f\"Most frequent features in 90th quantile: {features_90}\")\n",
    "#     feature_union.update(features_90)\n",
    "#     feature_intersection &= set(features_90)\n",
    "\n",
    "# print(f\"\\n\\nUnion of all features: {len(feature_union)} features\")\n",
    "# print(\n",
    "#     f\"\\n\\nIntersection of all features: {len(feature_intersection)} features: {feature_intersection}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intersections(important_features: dict) -> pd.DataFrame:\n",
    "    \"\"\"Compute all possible intersections between pairs of sets and store them in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        important_features (dict): Dictionary where keys are class labels and values are sets of important features.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the intersections and their properties.\n",
    "    \"\"\"\n",
    "    sets = {\n",
    "        label: set(quantile_features[90])\n",
    "        for label, quantile_features in important_features.items()\n",
    "    }\n",
    "    records = []\n",
    "\n",
    "    for set1_info, set2_info in itertools.combinations(sets.items(), 2):\n",
    "        label1, set1 = set1_info\n",
    "        label2, set2 = set2_info\n",
    "        intersection = set1.intersection(set2)\n",
    "\n",
    "        record = {\n",
    "            \"Set1_Label\": label1,\n",
    "            \"Set2_Label\": label2,\n",
    "            \"Set1_Size\": len(set1),\n",
    "            \"Set2_Size\": len(set2),\n",
    "            \"Intersection\": intersection,\n",
    "            \"Intersection_Size\": len(intersection),\n",
    "        }\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = compute_intersections(important_features)\n",
    "# df.to_csv(logdir / \"feature_intersections_q90.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = Counter()\n",
    "# for feature_set in df[\"Intersection\"]:\n",
    "#     counter.update(feature_set)\n",
    "\n",
    "# for k, v in counter.most_common():\n",
    "#     print(f\"{k} {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection = []\n",
    "# sort_order = np.argsort(feature_selection)\n",
    "\n",
    "# bed_vals = bins_to_bed_ranges(sorted(feature_selection), chroms, resolution=RESOLUTION)\n",
    "\n",
    "# write_to_bed(bed_ranges=bed_vals, bed_path=logdir / \"frequent_features.bed\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val in np.array(bed_vals)[sort_order.argsort()]:\n",
    "#     print(val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epi_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c736fa4331366c124c8f78bd4f37e7b252afab1d364df23bf88752fcd9abf849"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
