{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initial analysis of shap values behavior.\"\"\"\n",
    "# pylint: disable=redefined-outer-name, expression-not-assigned, import-error, not-callable, pointless-statement, no-value-for-parameter, undefined-variable, unused-argument, line-too-long, use-dict-literal, too-many-lines, unused-import, unused-variable\n",
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import upsetplot\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot\n",
    "from scipy.special import softmax  # type: ignore\n",
    "\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "from epi_ml.core import metadata\n",
    "from epi_ml.utils.bed_utils import bins_to_bed_ranges, write_to_bed\n",
    "from epi_ml.utils.general_utility import get_valid_filename\n",
    "\n",
    "BIOMATERIAL_TYPE = \"harmonized_biomaterial_type\"\n",
    "CELL_TYPE = \"harmonized_sample_ontology_intermediate\"\n",
    "ASSAY = \"assay_epiclass\"\n",
    "SEX = \"harmonized_donor_sex\"\n",
    "CANCER = \"harmonized_sample_cancer_high\"\n",
    "DISEASE = \"harmonized_sample_disease_high\"\n",
    "LIFE_STAGE = \"harmonized_donor_life_stage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECILES = list(np.arange(10, 100, 10) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chroms(chrom_file):\n",
    "    \"\"\"Return sorted chromosome names list.\"\"\"\n",
    "    with open(chrom_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        chroms = []\n",
    "        for line in file:\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                name, size = line.split()\n",
    "                chroms.append(tuple([name, int(size)]))\n",
    "    chroms.sort()\n",
    "    return chroms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "home = Path().home() / \"Projects\"\n",
    "input_dir = home / \"epilap/input\"\n",
    "metadata_path = (\n",
    "    input_dir\n",
    "    / \"metadata/dfreeze-v2/hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2.json\"\n",
    ")\n",
    "my_meta = metadata.Metadata(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroms = load_chroms(input_dir / \"chromsizes/hg38.noy.chrom.sizes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = CELL_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = home / \"epilap/output/logs/epiatlas-dfreeze-v2.1/hg38_100kb_all_none/shap\"\n",
    "# logdir = (\n",
    "#     output / \"models/SHAP\" / \"harmonized_donor_sex_1l_3000n/100kb_all_none_blklst/split0/\"\n",
    "# )\n",
    "# logdir = output / \"2023-01-epiatlas-freeze/hg38_1kb_all_none/harmonized_donor_sex_1l_200n/10fold-l1-100_l2-0.01_dropout-0.50/split0/SHAP/\"\n",
    "# logdir = output / \"2023-01-epiatlas-freeze/hg38_100kb_all_none/harmonized_donor_sex/predict-10fold-binary/lgbm-dart/lgbm-l1-0.01-l2-0.01/SHAP/split0/\"\n",
    "logdir1 = output / f\"{category}_1l_3000n/10fold/split0/shap/rna_only/\"\n",
    "logdir2 = output / f\"{category}_1l_3000n/10fold/split0/shap/6hist_6ct/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 100 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_shap_samples(shap_dict, n: int) -> Dict[str, List[np.ndarray]]:\n",
    "    \"\"\"Return a subset of shap values and their ids.\"\"\"\n",
    "    selected_shap_samples = {\"shap\": [], \"ids\": []}\n",
    "    total_samples = len(shap_dict[\"ids\"])\n",
    "    selected_indices = np.random.choice(total_samples, n, replace=False)\n",
    "\n",
    "    for class_shap_values in shap_dict[\"shap\"]:\n",
    "        selected_shap_samples[\"shap\"].append(class_shap_values[selected_indices, :])\n",
    "\n",
    "    selected_shap_samples[\"ids\"] = [shap_dict[\"ids\"][idx] for idx in selected_indices]\n",
    "\n",
    "    return selected_shap_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_archives(shap_values_dir: str | Path):\n",
    "    \"\"\"Return shap values and explainer background archives. from npz files.\"\"\"\n",
    "    shap_values_dir = Path(shap_values_dir)\n",
    "    try:\n",
    "        shap_values_path = next(shap_values_dir.glob(\"*evaluation*.npz\"))\n",
    "        background_info_path = next(shap_values_dir.glob(\"*explainer_background*.npz\"))\n",
    "    except StopIteration as err:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find shap values or explainer background archives in {shap_values_dir}\"\n",
    "        ) from err\n",
    "\n",
    "    with open(shap_values_path, \"rb\") as f:\n",
    "        shap_values_archive = np.load(f)\n",
    "        shap_values_archive = dict(shap_values_archive.items())\n",
    "\n",
    "    with open(background_info_path, \"rb\") as f:\n",
    "        explainer_background = np.load(f)\n",
    "        explainer_background = dict(explainer_background.items())\n",
    "\n",
    "    return shap_values_archive, explainer_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_archives(logdir1)[0][\"classes\"])\n",
    "print(get_archives(logdir2)[0][\"classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_impact(shap_values_matrices):\n",
    "    \"\"\"Return average absolute shap values.\"\"\"\n",
    "    shap_abs = np.zeros(shap_values_matrices[0].shape)\n",
    "    for matrix in shap_values_matrices:\n",
    "        shap_abs += np.absolute(matrix)\n",
    "    shap_abs /= len(shap_values_matrices)\n",
    "    return shap_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_most_important_features(sample_shaps, n):\n",
    "    \"\"\"Return features with highest absolute shap values.\"\"\"\n",
    "    return np.flip(np.argsort(np.absolute(sample_shaps)))[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_md5s(\n",
    "    md5s: List[str], metadata: metadata.Metadata, category_label: str, labels: List[str]\n",
    ") -> List[int]:\n",
    "    \"\"\"Subsample md5s based on metadata filtering provided, for a given category and filtering labels.\n",
    "\n",
    "    Args:\n",
    "            md5s (list): A list of MD5 hashes.\n",
    "            metadata (Metadata): A metadata object containing the data to be filtered.\n",
    "            category_label (str): The category label to be used for filtering the metadata.\n",
    "            labels (list): A list of labels to be used for selecting category subsets in the metadata.\n",
    "\n",
    "    Returns:\n",
    "            list: A list of indices corresponding to the selected md5s.\n",
    "    \"\"\"\n",
    "    meta = copy.deepcopy(metadata)\n",
    "    meta.select_category_subsets(category_label, labels)\n",
    "    chosen_idxs = []\n",
    "    for i, md5 in enumerate(md5s):\n",
    "        if md5 in meta:\n",
    "            chosen_idxs.append(i)\n",
    "    return chosen_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_feature(\n",
    "    pairwise_intersections: List[Set[int]], quantile_list: List[int]\n",
    ") -> Dict[int, List[int]]:\n",
    "    \"\"\"\n",
    "    Get a list of the most frequent features from multiple feature lists, according to some quantiles..\n",
    "\n",
    "    This function takes a list of feature lists and a quantile list. It calculates the occurrence frequency\n",
    "    of each feature and returns the list of features at least as frequent as the specified quantiles.\n",
    "\n",
    "    Args:\n",
    "        feature_lists (List[List[int]]): A list of feature lists, where each inner list contains feature indices.\n",
    "        quantile_list (List[int]: The quantile values for which the most frequent features will be returned.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int:List[int]]: A dict containing the list of features in each specified quantile.\n",
    "    \"\"\"\n",
    "    for quantile in quantile_list:\n",
    "        if quantile < 0 or quantile > 100:\n",
    "            raise ValueError(\"Quantile values must be between 0 and 100.\")\n",
    "\n",
    "    # Compute the features in the specified quantiles\n",
    "    intersection_counter = Counter()\n",
    "    for feature_set in pairwise_intersections:\n",
    "        intersection_counter.update(feature_set)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data=intersection_counter, orient=\"index\").reset_index()\n",
    "    df.columns = [\"Feature\", \"Count\"]\n",
    "\n",
    "    quantile_features_dict = {}\n",
    "    for quantile in quantile_list:\n",
    "        curr_q = df[\"Count\"].quantile(\n",
    "            quantile / 100\n",
    "        )  # this calculates the quantile value\n",
    "        curr_choice = df[\n",
    "            df[\"Count\"] >= curr_q\n",
    "        ]  # this selects all features within current quantile\n",
    "        quantile_features_dict[quantile] = curr_choice[\"Feature\"].tolist()\n",
    "\n",
    "    return quantile_features_dict\n",
    "\n",
    "\n",
    "def feature_overlap_stats(\n",
    "    feature_lists: List[List[int]], quantile_list: list[int]\n",
    ") -> Tuple[Set[int], Set[int], Dict[int, List[int]]]:\n",
    "    \"\"\"\n",
    "    Calculate the statistics of feature overlap between multiple feature lists.\n",
    "\n",
    "    This function takes a list of feature lists and calculates the median and average\n",
    "    pairwise overlaps between them. It also computes the union and intersection of all features\n",
    "    in the given feature lists.\n",
    "\n",
    "    Args:\n",
    "        feature_lists (List[List[int]]): A list of feature lists, where each inner list contains feature indices.\n",
    "        quantile_list (List[int]: The quantile values for which the most frequent features will be returned.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Set[int], Set[int], Dict[int, List]]: A tuple containing 1) intersection of all features\n",
    "        2) union of all features 3) a dict containing the list of features in each specified quantile.\n",
    "    \"\"\"\n",
    "    # Compute the overlap between two feature lists\n",
    "    all_pairwise_overlaps = [\n",
    "        set(sample1) & set(sample2)\n",
    "        for sample1, sample2 in itertools.combinations(feature_lists, 2)\n",
    "    ]\n",
    "    all_pairwise_overlaps_len = [len(x) for x in all_pairwise_overlaps]\n",
    "    print(\"Pairwise feature overlap statistics:\")\n",
    "    display(pd.DataFrame(all_pairwise_overlaps_len).describe())\n",
    "\n",
    "    # Most frequent features (per quantile)\n",
    "    frequent_features = get_most_frequent_feature(all_pairwise_overlaps, quantile_list)\n",
    "\n",
    "    # Union and intersection of all features\n",
    "    all_features_union: Set[int] = set()\n",
    "    all_features_intersection: Set[int] = set(feature_lists[0])\n",
    "    for feature_set in feature_lists:\n",
    "        all_features_union.update(feature_set)\n",
    "        all_features_intersection &= set(feature_set)\n",
    "\n",
    "    return all_features_intersection, all_features_union, frequent_features  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shap_matrix(\n",
    "    meta: metadata.Metadata,\n",
    "    shap_matrices: np.ndarray,\n",
    "    eval_md5s: List[str],\n",
    "    label_category: str,\n",
    "    selected_labels: List[str],\n",
    "    class_idx: int,\n",
    ") -> Tuple[np.ndarray, List[int]]:\n",
    "    \"\"\"Generates a SHAP matrix corresponding to a selected subset of samples.\n",
    "\n",
    "    This function selects a subset of samples based on specified criteria\n",
    "    and then generates a SHAP matrix for these selected samples. It filters\n",
    "    the metadata if a specific target subsample is provided, and selects a\n",
    "    subset of samples that are identified by their md5 hash. It then selects\n",
    "    the SHAP values of these samples under the matrix of the given class number.\n",
    "\n",
    "    Args:\n",
    "        meta (metadata.Metadata): Metadata object containing information about the samples.\n",
    "        shap_matrices (np.ndarray): Array of SHAP matrices for each class.\n",
    "        eval_md5s (List[str]): List of md5 hashes identifying the evaluation samples.\n",
    "        label_category (str): Name of the category in the metadata that contains the desired labels.\n",
    "        selected_labels (List[str]): Name of the classes for which samples will be considered.\n",
    "        class_idx (int): Index of the class for which the shap values matrix will be used.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The selected SHAP matrix for the first class and for the\n",
    "                    chosen samples based on the provided criteria.\n",
    "        List[int]: The indices of the chosen samples in the original SHAP matrix.\n",
    "\n",
    "    Raises:\n",
    "            IndexError: If the `class_idx` is out of bounds for the `shap_matrices`.\n",
    "    \"\"\"\n",
    "    my_meta = copy.deepcopy(meta)\n",
    "\n",
    "    chosen_idxs = subsample_md5s(\n",
    "        md5s=eval_md5s,\n",
    "        metadata=my_meta,\n",
    "        category_label=label_category,\n",
    "        labels=selected_labels,\n",
    "    )\n",
    "    if len(shap_matrices.shape) == 3:  # deepSHAP\n",
    "        try:\n",
    "            class_shap = shap_matrices[class_idx]\n",
    "        except IndexError as err:\n",
    "            raise IndexError(f\"Class index {class_idx} is out of bounds.\") from err\n",
    "\n",
    "        selected_class_shap = np.array(class_shap[chosen_idxs, :])\n",
    "    else:  # TreeExplainer 2D\n",
    "        class_shap = shap_matrices\n",
    "        selected_class_shap = class_shap[chosen_idxs]\n",
    "    print(\n",
    "        f\"Shape of selected class ({selected_labels}) shap values: {selected_class_shap.shape}\"\n",
    "    )\n",
    "    print(f\"Chose {len(chosen_idxs)} samples from {class_shap.shape[0]} samples\")\n",
    "    return selected_class_shap, chosen_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_overlap_stats(feature_stats: Sequence):\n",
    "    \"\"\"Prints the statistics of feature overlap.\n",
    "\n",
    "    This function receives the feature statistics which include the intersection,\n",
    "    union and frequent features in each quantile of features. It then prints\n",
    "    these statistics for easy inspection.\n",
    "\n",
    "    Args:\n",
    "        feature_stats (Sequence): Tuple containing the intersection, union and\n",
    "                                  frequent features in each quantile of features.\n",
    "    \"\"\"\n",
    "    features_intersection, features_union, frequent_features = feature_stats\n",
    "    print(f\"Intersection of all features: {len(features_intersection)} features\")\n",
    "    print(f\"Fully intersecting features: {list(features_intersection)}\")\n",
    "    print(f\"Union of all features: {len(features_union)} features\\n\")\n",
    "    for k, v in frequent_features.items():\n",
    "        print(f\"Most frequent features in {k}th quantile: {len(v)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_importance_info(feature_selection: List[int], shap_matrix: np.ndarray):\n",
    "    \"\"\"Prints the feature importance information.\n",
    "\n",
    "    This function prints the feature importance information, which includes the\n",
    "    average expected contribution of the selected features and one feature (if\n",
    "    the importance was uniform), and statistical descriptions of the contributions\n",
    "    of the selected features.\n",
    "\n",
    "    Args:\n",
    "        feature_selection (List[int]): The indices of the selected features.\n",
    "        shap_matrix (np.ndarray): The SHAP values matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    N = len(feature_selection)\n",
    "    nb_files, nb_bins = shap_matrix.shape\n",
    "    print(\n",
    "        f\"Average expected contribution of {N} feature if uniform importance:{N/nb_bins*100:.5f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Average expected contribution of 1 feature if uniform importance:{1/nb_bins*100:.5f}%\"\n",
    "    )\n",
    "    print(f\"Average contribution of selected features for {nb_files} files:\")\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            softmax(shap_matrix, axis=1)[:, list(feature_selection)].sum(axis=1) * 100\n",
    "        ).describe(percentiles=DECILES)\n",
    "    )\n",
    "    print(f\"Individual contribution of selected features for {nb_files} files:\")\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            softmax(shap_matrix, axis=1)[:, list(feature_selection)] * 100\n",
    "        ).describe(percentiles=DECILES)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_subsample_coherence(\n",
    "    shap_matrices: np.ndarray, chosen_idxs: List[int], class_int: int\n",
    ") -> None:\n",
    "    \"\"\"Verify if the subsampling is coherent with the SHAP values.\n",
    "\n",
    "    This function calculates the mean absolute SHAP values for the samples\n",
    "    identified by chosen_idxs in each class' SHAP matrix. It then checks\n",
    "    if the class index for which the subsampling was done (class_int) has\n",
    "    the highest mean absolute SHAP value.\n",
    "\n",
    "    Args:\n",
    "        shap_matrices (np.ndarray): Array of SHAP matrices for each class.\n",
    "        chosen_idxs (List[int]): Indices of the samples chosen during subsampling.\n",
    "        class_int (int): The class index for which the subsampling was performed.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints out the results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the mean of absolute SHAP values for each class for selected samples.\n",
    "    avg_abs_shap_per_class = [\n",
    "        np.mean(np.abs(shap_matrices[i][chosen_idxs, :]))\n",
    "        for i in range(len(shap_matrices))\n",
    "    ]\n",
    "\n",
    "    # Find the index of the class with highest average absolute SHAP value\n",
    "    highest_shap_class_idx = np.argmax(avg_abs_shap_per_class)\n",
    "\n",
    "    print(f\"Average absolute SHAP values per class: {avg_abs_shap_per_class}\")\n",
    "    print(f\"Class with highest average absolute SHAP value: {highest_shap_class_idx}\")\n",
    "\n",
    "    # Compare the index with class_int to check if they are same.\n",
    "    if highest_shap_class_idx == class_int:\n",
    "        print(\n",
    "            f\"The subsampling for class index {class_int} is coherent with SHAP values.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Warning: The subsampling for class index {class_int} may not be coherent with SHAP values. Highest SHAP values belong to class index {highest_shap_class_idx}.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(\n",
    "    sample_shap_values: np.ndarray,\n",
    "    important_features: list,\n",
    "    title: str,\n",
    "    plot_type: str,\n",
    "    logdir: str | Path,\n",
    ") -> None:\n",
    "    \"\"\"Plot feature importance in a sample, highlighting important features using Plotly.\n",
    "\n",
    "    Args:\n",
    "        sample_shap_values (np.ndarray): The SHAP values for a single sample.\n",
    "        important_features (list): List of indices corresponding to important features.\n",
    "        title (str): The title for the plot.\n",
    "        plot_type (str): Type of plot (\"raw\", \"softmax\", or \"rank\").\n",
    "    \"\"\"\n",
    "\n",
    "    if plot_type == \"raw\":\n",
    "        plot_values = sample_shap_values\n",
    "    elif plot_type == \"softmax\":\n",
    "        plot_values = softmax(sample_shap_values)\n",
    "    elif plot_type == \"rank\":\n",
    "        plot_values = np.argsort(\n",
    "            np.argsort(-np.abs(sample_shap_values))\n",
    "        )  # Rank based on absolute values\n",
    "    else:\n",
    "        raise ValueError(\"Invalid plot_type.\")\n",
    "\n",
    "    title = f\"{title} ({plot_type})\"\n",
    "    # General points\n",
    "    trace1 = go.Scatter(\n",
    "        x=list(range(len(plot_values))),\n",
    "        y=plot_values,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"blue\"),\n",
    "        name=\"All Features\",\n",
    "    )\n",
    "\n",
    "    # Important points\n",
    "    trace2 = go.Scatter(\n",
    "        x=important_features,\n",
    "        y=[plot_values[i] for i in important_features],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"red\"),\n",
    "        name=\"Important Features\",\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=title, xaxis=dict(title=\"Feature index\"), yaxis=dict(title=plot_type)\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "\n",
    "    logdir = Path(logdir)\n",
    "    fig.write_image(logdir / f\"{title}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shap_values_and_info(\n",
    "    shap_logdir: str | Path, verbose: bool = True\n",
    ") -> Tuple[np.ndarray, List[str], List[Tuple[str, str]]]:\n",
    "    \"\"\"Extract and print basic statistics about SHAP values from an archive.\n",
    "\n",
    "    Args:\n",
    "        shap_logdir (str): The directory where the SHAP values archive is located.\n",
    "        verbose (bool): Whether to print basic statistics about the SHAP values.\n",
    "\n",
    "    Returns:\n",
    "        shap_matrices (np.ndarray): SHAP matrices.\n",
    "        eval_md5s (List[str]): List of evaluation MD5s.\n",
    "        classes (List[Tuple[str, str]]): List of classes. Each class is a tuple containing the class index and the class label.\n",
    "    \"\"\"\n",
    "    # Extract shap values and md5s from archive\n",
    "    shap_values_archive, _ = get_archives(shap_logdir)\n",
    "    try:\n",
    "        eval_md5s: List[str] = shap_values_archive[\"evaluation_md5s\"]\n",
    "    except KeyError:\n",
    "        eval_md5s: List[str] = shap_values_archive[\"evaluation_ids\"]\n",
    "    shap_matrices: np.ndarray = shap_values_archive[\"shap_values\"]\n",
    "\n",
    "    # Print basic statistics about the loaded SHAP values\n",
    "    if verbose:\n",
    "        print(f\"nb classes: {len(shap_matrices)}\")\n",
    "        print(f\"nb samples: {len(eval_md5s)}\")\n",
    "        print(f\"dim shap value matrix: {shap_matrices[0].shape}\")\n",
    "        print(f\"Output classes of classifier:\\n {shap_values_archive['classes']}\")\n",
    "\n",
    "    return shap_matrices, eval_md5s, shap_values_archive[\"classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_whole_thing(\n",
    "    metadata: metadata.Metadata,\n",
    "    shap_dir: Path,\n",
    "    output_dir: Path,\n",
    "    label_category: str,\n",
    "    top_n: int = 100,\n",
    ") -> Dict[str, List[int]]:\n",
    "    \"\"\"Execute the complete process of SHAP value analysis.\n",
    "\n",
    "    This function performs the complete SHAP value analysis given the metadata and the directory\n",
    "    of the SHAP value files. It carries out the following steps:\n",
    "    1. Load the SHAP value archives and print basic statistics.\n",
    "    2. Filter the metadata to match the samples in SHAP value archives.\n",
    "    For each output class class:\n",
    "        3. Extract SHAP values.\n",
    "        4. Determine the top N features for each sample.\n",
    "        5. Compute and print feature overlap statistics.\n",
    "        6. Analyze feature importance.\n",
    "        7. Convert bin indices to genomic ranges and write to a BED file.\n",
    "        8. Display and save a plot of importance distribution for one sample.\n",
    "\n",
    "    Args:\n",
    "        metadata (metadata.Metadata): The metadata for the samples.\n",
    "        shap_logdir (Path): The directory path where SHAP value files are stored.\n",
    "        label_category (str): The name of the classifier output category that computed the shaps.\n",
    "        top_n (int): The number of top features to be selected for each sample. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[int, List[int]]: Dictionary where keys are class labels and values are lists\n",
    "        of the most frequently occurring important (high shap) features for that class, for each computed quantile.\n",
    "        (see feature_overlap_stats function for more details).\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If \"evaluation_md5s\" or \"evaluation_ids\" are not found in the loaded SHAP value archives.\n",
    "    \"\"\"\n",
    "    metadata = copy.deepcopy(metadata)\n",
    "\n",
    "    # Extract shap values and md5s from archive\n",
    "    shap_matrices, eval_md5s, classes = extract_shap_values_and_info(shap_dir)\n",
    "\n",
    "    # Filter metadata to include only the samples that exist in the SHAP value archives\n",
    "    for md5 in list(metadata.md5s):\n",
    "        if md5 not in set(eval_md5s):\n",
    "            del metadata[md5]\n",
    "\n",
    "    metadata.display_labels(\"assay_epiclass\")\n",
    "    metadata.display_labels(\"harmonized_donor_sex\")\n",
    "\n",
    "    # Loop over each class to perform SHAP value analysis\n",
    "    important_features = {}\n",
    "    for class_int, class_label in classes:\n",
    "        class_int = int(class_int)\n",
    "        print(f\"\\n\\nClass: {class_label} ({class_int})\")\n",
    "\n",
    "        # Get the SHAP matrix for the current class,\n",
    "        # and only select samples that also correspond to that class\n",
    "        shap_matrix, chosen_idxs = get_shap_matrix(\n",
    "            meta=metadata,\n",
    "            shap_matrices=shap_matrices,\n",
    "            eval_md5s=eval_md5s,\n",
    "            label_category=label_category,\n",
    "            selected_labels=[class_label],\n",
    "            class_idx=class_int,\n",
    "        )\n",
    "\n",
    "        if len(chosen_idxs) < 3:\n",
    "            print(f\"Not enough samples to perform analysis on {class_label}.\")\n",
    "            continue\n",
    "\n",
    "        # Computing statistics of feature overlap\n",
    "        print(\n",
    "            f\"Selecting features with top {top_n} SHAP values for each sample of {class_label}.\"\n",
    "        )\n",
    "        top_n_features = []\n",
    "        for sample in shap_matrix:\n",
    "            top_n_features.append(list(n_most_important_features(sample, top_n)))\n",
    "\n",
    "        some_stats = feature_overlap_stats(top_n_features, [0, 90, 95, 99])\n",
    "        frequent_features = some_stats[2]\n",
    "        important_features[class_label] = frequent_features\n",
    "\n",
    "        # print_feature_overlap_stats(some_stats)\n",
    "\n",
    "        chosen_percentile = 90\n",
    "        feature_selection = frequent_features[chosen_percentile]\n",
    "\n",
    "        # print_importance_info(feature_selection, shap_matrix)\n",
    "\n",
    "        # Convert bin indices to genomic ranges and write to a BED file\n",
    "        # bed_vals = bins_to_bed_ranges(\n",
    "        #     sorted(feature_selection), chroms, resolution=RESOLUTION\n",
    "        # )\n",
    "        # bed_filename = get_valid_filename(\n",
    "        #     f\"frequent_features_{chosen_percentile}_{class_label}.bed\"\n",
    "        # )\n",
    "        # write_to_bed(\n",
    "        #     bed_vals,\n",
    "        #     output_dir / bed_filename,\n",
    "        #     verbose=True,\n",
    "        # )\n",
    "\n",
    "        # # Display and save a plot of importance distribution for one sample\n",
    "        # print(\"One sample\")\n",
    "        # probs_1sample = pd.DataFrame(softmax(shap_matrix, axis=1)[0, :] * 100)\n",
    "        # display(probs_1sample.describe(percentiles=DECILES))\n",
    "        # fig_title = f\"Importance distribution - One sample - {eval_md5s[chosen_idxs[0]]}\"\n",
    "        # fig = px.violin(probs_1sample, box=True, points=\"all\", title=fig_title)\n",
    "        # fig.write_image(shap_logdir / \"importance_dist_1sample.png\")\n",
    "\n",
    "    return important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_alternative_analysis(\n",
    "    metadata: metadata.Metadata,\n",
    "    shap_logdir: Path,\n",
    "    label_category: str,\n",
    "    selected_classes: list[str],\n",
    "    top_n: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"Run an alternative analysis that involves plotting feature importance for selected classes.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Extracts SHAP values and associated metadata.\n",
    "    2. Filters out samples from the metadata that are not present in the SHAP value archives.\n",
    "    3. Collects the most important features for selected classes.\n",
    "    4. Plots the feature importance for samples of these selected classes using different metrics (\"raw\", \"softmax\", \"rank\").\n",
    "\n",
    "    Args:\n",
    "        metadata (\"metadata.Metadata\"): The metadata object containing sample information.\n",
    "        shap_logdir (Path): The directory where the SHAP value archives are stored.\n",
    "        label_category (str): The category of the label to be used for class selection.\n",
    "        selected_classes (List[str]): A list of classes for which the analysis should be run.\n",
    "        top_n (int, optional): The top N most important features to consider. Default is 100.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If sample indices are not unique across classes.\n",
    "    \"\"\"\n",
    "    metadata = copy.deepcopy(metadata)\n",
    "\n",
    "    # Extract shap values and md5s from archive\n",
    "    shap_matrices, eval_md5s, classes = extract_shap_values_and_info(shap_logdir)\n",
    "\n",
    "    # Filter metadata to include only the samples that exist in the SHAP value archives\n",
    "    for md5 in list(metadata.md5s):\n",
    "        if md5 not in set(eval_md5s):\n",
    "            del metadata[md5]\n",
    "\n",
    "    # collect important features for selected classes\n",
    "    selected_percentile = 90\n",
    "    classes_dict = {\n",
    "        class_label: int(class_int)\n",
    "        for class_int, class_label in classes\n",
    "        if class_label in selected_classes\n",
    "    }\n",
    "    important_features = {}\n",
    "    sample_idxs = {}\n",
    "    for class_label, class_int in classes_dict.items():\n",
    "        print(f\"\\n\\nClass: {class_label} ({class_int})\")\n",
    "\n",
    "        # Get the SHAP matrix for the current class,\n",
    "        # and only select samples that also correspond to that class\n",
    "        shap_matrix, chosen_idxs = get_shap_matrix(\n",
    "            meta=metadata,\n",
    "            shap_matrices=shap_matrices,\n",
    "            eval_md5s=eval_md5s,\n",
    "            label_category=label_category,\n",
    "            selected_labels=[class_label],\n",
    "            class_idx=class_int,\n",
    "        )\n",
    "        sample_idxs[class_label] = chosen_idxs\n",
    "\n",
    "        # Computing statistics of feature overlap\n",
    "        top_n_features = []\n",
    "        for sample in shap_matrix:\n",
    "            top_n_features.append(list(n_most_important_features(sample, top_n)))\n",
    "\n",
    "        some_stats = feature_overlap_stats(top_n_features, [selected_percentile])\n",
    "        frequent_features = some_stats[2]\n",
    "        important_features[class_label] = frequent_features[selected_percentile]\n",
    "\n",
    "    all_chosen_idxs = set()\n",
    "    for idxs in sample_idxs.values():\n",
    "        all_chosen_idxs.update(idxs)\n",
    "\n",
    "    if len(all_chosen_idxs) != sum(len(idxs) for idxs in sample_idxs.values()):\n",
    "        raise ValueError(\"Sample indices are not unique across classes.\")\n",
    "\n",
    "    # # ----------------------\n",
    "    # # Here we check the value of the shap values for each feature important feature of the class, for each (m)rna sample\n",
    "    # print(\"plotting time!\")\n",
    "\n",
    "    # for class_label in selected_classes:\n",
    "    #     features = important_features[class_label]\n",
    "    #     class_idx = classes_dict[class_label]\n",
    "\n",
    "    #     shap_matrix = shap_matrices[class_idx]\n",
    "    #     logdir = shap_logdir / \"feature_importance_plots\" / class_label\n",
    "    #     logdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #     for i, sample_shap_values in enumerate(shap_matrix):\n",
    "    #         if i not in all_chosen_idxs:\n",
    "    #             continue\n",
    "\n",
    "    #         md5 = eval_md5s[i]\n",
    "    #         cell_type = metadata[md5][CELL_TYPE]\n",
    "    #         plot_title = f\"{cell_type} sample ({eval_md5s[i]})<br>{class_label} shap values\"\n",
    "    #         print(plot_title)\n",
    "    #         for plot_type in [\"raw\", \"rank\"]:\n",
    "    #             plot_feature_importance(\n",
    "    #                 sample_shap_values,\n",
    "    #                 features,\n",
    "    #                 title=plot_title,\n",
    "    #                 plot_type=plot_type,\n",
    "    #                 logdir=logdir\n",
    "    #             )\n",
    "    # # ----------------------\n",
    "\n",
    "    logdir = shap_logdir / \"feature_rank_analysis\"\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for class_of_interest in selected_classes:\n",
    "        important_feats = important_features[class_of_interest]\n",
    "        for comparison_class in selected_classes:\n",
    "            shap_matrix = shap_matrices[classes_dict[comparison_class]]\n",
    "\n",
    "            # Lists to hold md5s, metadata category, and feature ranks for each sample\n",
    "            md5_list = []\n",
    "            cell_type_list = []\n",
    "            ranks_list = []\n",
    "\n",
    "            for i, sample_shap_values in enumerate(shap_matrix):\n",
    "                if i not in sample_idxs[comparison_class]:\n",
    "                    continue\n",
    "                # if i not in all_chosen_idxs:\n",
    "                #     continue\n",
    "\n",
    "                md5_list.append(eval_md5s[i])\n",
    "                cell_type_list.append(metadata[eval_md5s[i]][CELL_TYPE])\n",
    "\n",
    "                ranks = np.argsort(\n",
    "                    np.argsort(-np.abs(sample_shap_values))\n",
    "                )  # Ranking in descending order of absolute SHAP value\n",
    "                ranks_of_important_feats = ranks[\n",
    "                    important_feats\n",
    "                ]  # Get the ranks of the important features\n",
    "                ranks_list.append(ranks_of_important_feats)\n",
    "\n",
    "            # Combine all the lists into a DataFrame\n",
    "            ranks_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"md5sum\": md5_list,\n",
    "                    CELL_TYPE: cell_type_list,\n",
    "                    **{\n",
    "                        f\"Feature_{feat}\": [ranks[i] for ranks in ranks_list]\n",
    "                        for i, feat in enumerate(important_feats)\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Save the DataFrame to CSV\n",
    "            title = f\"important_{class_of_interest}_features_in_{comparison_class}_shap_matrix.csv\".replace(\n",
    "                \" \", \"_\"\n",
    "            )\n",
    "            ranks_df.to_csv(logdir / title, index=True)\n",
    "            print(\n",
    "                f\"Feature ranks for '{class_of_interest}' features in '{comparison_class}' samples have been saved.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feat_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logdir1, logdir2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = copy.deepcopy(my_meta)\n",
    "meta.select_category_subsets(\"assay_epiclass\", [\"rna_seq\", \"mrna_seq\"])\n",
    "\n",
    "name = \"rna-seq\"\n",
    "run_logdir = logdir1 / name\n",
    "run_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "important_features = run_the_whole_thing(\n",
    "    metadata=meta,\n",
    "    shap_dir=logdir1,\n",
    "    output_dir=run_logdir,\n",
    "    label_category=category,\n",
    "    top_n=100,\n",
    ")\n",
    "feat_90 = {name: sets[90] for name, sets in important_features.items()}\n",
    "\n",
    "important_feat_dict[name] = feat_90\n",
    "\n",
    "upset_features = upsetplot.from_contents(feat_90)\n",
    "\n",
    "fig = upsetplot.UpSet(\n",
    "    upset_features,\n",
    "    subset_size=\"count\",\n",
    "    show_counts=True,\n",
    "    show_percentages=True,\n",
    "    sort_by=\"cardinality\",\n",
    "    sort_categories_by=\"cardinality\",\n",
    ").plot()\n",
    "pyplot.savefig(run_logdir / f\"upset_{name}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for histone in [\"h3k27ac\", \"h3k27me3\", \"h3k36me3\", \"h3k4me1\", \"h3k4me3\", \"h3k9me3\"]:\n",
    "    meta = copy.deepcopy(my_meta)\n",
    "    meta.select_category_subsets(\"assay_epiclass\", [histone])\n",
    "\n",
    "    run_logdir = logdir2 / histone\n",
    "    run_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "    important_features = run_the_whole_thing(\n",
    "        metadata=meta,\n",
    "        shap_dir=logdir2,\n",
    "        output_dir=run_logdir,\n",
    "        label_category=category,\n",
    "        top_n=100,\n",
    "    )\n",
    "    feat_90 = {name: sets[90] for name, sets in important_features.items()}\n",
    "\n",
    "    important_feat_dict[histone] = feat_90\n",
    "\n",
    "    # upset_features = upsetplot.from_contents(feat_90)\n",
    "\n",
    "    # fig = upsetplot.UpSet(\n",
    "    #     upset_features, subset_size=\"count\", show_counts=True, show_percentages=True, sort_by=\"cardinality\", sort_categories_by=\"cardinality\"\n",
    "    # ).plot()\n",
    "    # pyplot.savefig(run_logdir / f\"upset_{histone}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(important_feat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dict = {\n",
    "#     f\"{assay}_{output_class}\": class_set\n",
    "#     for assay, dicts in important_feat_dict.items()\n",
    "#     for output_class, class_set in dicts.items()\n",
    "# }\n",
    "\n",
    "# # add merge of output classes\n",
    "# assays = set(assay.split(\"_\")[0] for assay in new_dict.keys())\n",
    "# for assay in assays:\n",
    "#     merged_features = set()\n",
    "\n",
    "#     for key, features in new_dict.items():\n",
    "#         if key.startswith(assay):\n",
    "#             merged_features.update(features)\n",
    "\n",
    "#     new_dict[f\"{assay}_all_classes\"] = list(merged_features)\n",
    "#     # del new_dict[f\"{assay}_cancer\"]\n",
    "#     # del new_dict[f\"{assay}_non-cancer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upset_features = upsetplot.from_contents(new_dict)\n",
    "# fig = upsetplot.UpSet(\n",
    "#     upset_features, subset_size=\"count\", show_counts=True, show_percentages=True, sort_by=\"cardinality\", sort_categories_by=\"cardinality\"\n",
    "# ).plot()\n",
    "# pyplot.savefig(run_logdir / \"upset_assay.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cell_types = [\n",
    "    \"T cell\",\n",
    "    \"lymphocyte of B lineage\",\n",
    "    \"monocyte\",\n",
    "    \"muscle organ\",\n",
    "    \"myeloid cell\",\n",
    "    \"neutrophil\",\n",
    "]\n",
    "for cell_type in relevant_cell_types:\n",
    "    new_dict = {\n",
    "        hist: hist_dict.get(cell_type, [])\n",
    "        for hist, hist_dict in important_feat_dict.items()\n",
    "    }\n",
    "    upset_features = upsetplot.from_contents(new_dict)\n",
    "\n",
    "    plot_filename = f\"upset_{get_valid_filename(cell_type)}.png\"\n",
    "    fig = upsetplot.UpSet(\n",
    "        upset_features,\n",
    "        subset_size=\"count\",\n",
    "        show_counts=True,\n",
    "        show_percentages=True,\n",
    "        sort_by=\"cardinality\",\n",
    "        sort_categories_by=\"cardinality\",\n",
    "    ).plot()\n",
    "    pyplot.savefig(logdir1.parent / plot_filename, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample ontology specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_intersection = set.intersection(\n",
    "#     *[set(feat_list[90]) for feat_list in important_features.values()]\n",
    "# )\n",
    "\n",
    "# male_feat = set(important_features[\"male\"][90])\n",
    "# female_feat = set(important_features[\"female\"][90])\n",
    "\n",
    "# only_male = male_feat - female_feat\n",
    "# only_female = female_feat - male_feat\n",
    "\n",
    "# for feat_set, name in zip([only_female, only_male], [\"only_female\", \"only_male\"]):\n",
    "#     bed_vals = bins_to_bed_ranges(sorted(feat_set), chroms, resolution=RESOLUTION)\n",
    "#     var_name = f\"{feat_set=}\".split(\"=\")[0]\n",
    "#     write_to_bed(\n",
    "#         bed_vals,\n",
    "#         logdir / f\"frequent_features_{90}_{name}.bed\",\n",
    "#         verbose=False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_alternative_analysis(\n",
    "#     metadata=my_meta,\n",
    "#     shap_logdir=logdir,\n",
    "#     label_category=SEX,\n",
    "#     selected_classes=[\"female\", \"male\"],\n",
    "#     top_n=100,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_violin_plots(logdir: str | Path, selected_classes: list[str]) -> None:\n",
    "    \"\"\"Create violin plots of important feature ranks for selected classes.\"\"\"\n",
    "    logdir = Path(logdir)\n",
    "    # Iterate through each class of interest\n",
    "    for class_of_interest in selected_classes:\n",
    "        # Initialize an empty figure\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Iterate through each class to add its violin plot to the figure\n",
    "        for comparison_class in selected_classes:\n",
    "            df = pd.read_csv(\n",
    "                logdir\n",
    "                / f\"important_{class_of_interest}_features_in_{comparison_class}_shap_matrix.csv\"\n",
    "            )\n",
    "            df = df.filter(\n",
    "                like=\"Feature_\"\n",
    "            )  # Remove non-feature columns from DataFrame for plotting\n",
    "\n",
    "            # Create violin plot for the current comparison_class\n",
    "            violin = go.Violin(\n",
    "                y=df.values.flatten(),  # Flattened feature ranks\n",
    "                name=f\"{comparison_class} (n={df.shape[0]})\",  # Name of the violin plot\n",
    "                box_visible=True,  # Display box inside the violin\n",
    "                line_color=px.colors.qualitative.Plotly[\n",
    "                    len(fig.data)\n",
    "                ],  # Different color for each violin\n",
    "                points=\"all\",  # Display all points\n",
    "            )\n",
    "            fig.add_trace(violin)\n",
    "\n",
    "            print(df.shape)\n",
    "        # Set title and axis labels\n",
    "        fig.update_layout(\n",
    "            title=f\"Violin plot of ranks for important features of '{class_of_interest}' ({df.shape[1]} features)\",\n",
    "            xaxis_title=\"Source Matrix\",\n",
    "            yaxis_title=\"Feature Rank\",\n",
    "        )\n",
    "\n",
    "        # Show the figure\n",
    "        fig.write_image(\n",
    "            logdir / f\"important_{class_of_interest}_feature_ranks_violin_plot.png\"\n",
    "        )\n",
    "        fig.write_html(\n",
    "            logdir / f\"important_{class_of_interest}_feature_ranks_violin_plot.html\"\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_violin_plots(\n",
    "#     logdir=logdir / \"feature_rank_analysis\", selected_classes=[\"female\", \"male\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples ontology class pairs important features overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_union = set()\n",
    "# feature_intersection = set(list(important_features.values())[0][90])\n",
    "# for label, features in important_features.items():\n",
    "#     features_90 = features[90]\n",
    "#     print(f\"\\n\\nClass: {label}\")\n",
    "#     print(f\"Most frequent features in 90th quantile: {features_90}\")\n",
    "#     feature_union.update(features_90)\n",
    "#     feature_intersection &= set(features_90)\n",
    "\n",
    "# print(f\"\\n\\nUnion of all features: {len(feature_union)} features\")\n",
    "# print(\n",
    "#     f\"\\n\\nIntersection of all features: {len(feature_intersection)} features: {feature_intersection}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intersections(important_features: dict) -> pd.DataFrame:\n",
    "    \"\"\"Compute all possible intersections between pairs of sets and store them in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        important_features (dict): Dictionary where keys are class labels and values are sets of important features.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the intersections and their properties.\n",
    "    \"\"\"\n",
    "    sets = {\n",
    "        label: set(quantile_features[90])\n",
    "        for label, quantile_features in important_features.items()\n",
    "    }\n",
    "    records = []\n",
    "\n",
    "    for set1_info, set2_info in itertools.combinations(sets.items(), 2):\n",
    "        label1, set1 = set1_info\n",
    "        label2, set2 = set2_info\n",
    "        intersection = set1.intersection(set2)\n",
    "\n",
    "        record = {\n",
    "            \"Set1_Label\": label1,\n",
    "            \"Set2_Label\": label2,\n",
    "            \"Set1_Size\": len(set1),\n",
    "            \"Set2_Size\": len(set2),\n",
    "            \"Intersection\": intersection,\n",
    "            \"Intersection_Size\": len(intersection),\n",
    "        }\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = compute_intersections(important_features)\n",
    "# df.to_csv(logdir / \"feature_intersections_q90.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = Counter()\n",
    "# for feature_set in df[\"Intersection\"]:\n",
    "#     counter.update(feature_set)\n",
    "\n",
    "# for k, v in counter.most_common():\n",
    "#     print(f\"{k} {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection = []\n",
    "# sort_order = np.argsort(feature_selection)\n",
    "\n",
    "# bed_vals = bins_to_bed_ranges(sorted(feature_selection), chroms, resolution=RESOLUTION)\n",
    "\n",
    "# write_to_bed(bed_ranges=bed_vals, bed_path=logdir / \"frequent_features.bed\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val in np.array(bed_vals)[sort_order.argsort()]:\n",
    "#     print(val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epi_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c736fa4331366c124c8f78bd4f37e7b252afab1d364df23bf88752fcd9abf849"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
