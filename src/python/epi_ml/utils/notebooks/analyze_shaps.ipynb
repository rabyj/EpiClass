{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initial analysis of shap values behavior.\"\"\"\n",
    "# pylint: disable=redefined-outer-name, expression-not-assigned, import-error, not-callable, pointless-statement, no-value-for-parameter, undefined-variable, unused-argument, line-too-long, use-dict-literal, too-many-lines, unused-import, unused-variable\n",
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence, Set, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import upsetplot\n",
    "from IPython.display import display\n",
    "from scipy.special import softmax  # type: ignore\n",
    "\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "from epi_ml.core import metadata\n",
    "from epi_ml.utils.bed_utils import bins_to_bed_ranges, write_to_bed\n",
    "from epi_ml.utils.general_utility import get_valid_filename\n",
    "from epi_ml.utils.shap_utils import n_most_important_features\n",
    "\n",
    "BIOMATERIAL_TYPE = \"harmonized_biomaterial_type\"\n",
    "CELL_TYPE = \"harmonized_sample_ontology_intermediate\"\n",
    "ASSAY = \"assay_epiclass\"\n",
    "SEX = \"harmonized_donor_sex\"\n",
    "CANCER = \"harmonized_sample_cancer_high\"\n",
    "DISEASE = \"harmonized_sample_disease_high\"\n",
    "LIFE_STAGE = \"harmonized_donor_life_stage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECILES = list(np.arange(10, 100, 10) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chroms(chrom_file):\n",
    "    \"\"\"Return sorted chromosome names list.\"\"\"\n",
    "    with open(chrom_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        chroms = []\n",
    "        for line in file:\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                name, size = line.split()\n",
    "                chroms.append(tuple([name, int(size)]))\n",
    "    chroms.sort()\n",
    "    return chroms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "home = Path().home() / \"Projects\"\n",
    "input_dir = home / \"epilap/input\"\n",
    "metadata_path = (\n",
    "    input_dir\n",
    "    / \"metadata/dfreeze-v2/hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2.json\"\n",
    ")\n",
    "my_meta = metadata.Metadata(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroms = load_chroms(input_dir / \"chromsizes/hg38.noy.chrom.sizes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = SEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = home / \"epilap/output/logs/epiatlas-dfreeze-v2.1/hg38_100kb_all_none/shap\"\n",
    "# logdir = (\n",
    "#     output / \"models/SHAP\" / \"harmonized_donor_sex_1l_3000n/100kb_all_none_blklst/split0/\"\n",
    "# )\n",
    "# logdir = output / \"2023-01-epiatlas-freeze/hg38_1kb_all_none/harmonized_donor_sex_1l_200n/10fold-l1-100_l2-0.01_dropout-0.50/split0/SHAP/\"\n",
    "# logdir = output / \"2023-01-epiatlas-freeze/hg38_100kb_all_none/harmonized_donor_sex/predict-10fold-binary/lgbm-dart/lgbm-l1-0.01-l2-0.01/SHAP/split0/\"\n",
    "logdir1 = output / f\"{category}_1l_3000n/w-mixed/10fold-oversample/split0/shap/\"\n",
    "# logdir2 = output / f\"{category}_1l_3000n/10fold/split0/shap/6hist_6ct/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOLUTION = 100 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_archives(shap_values_dir: str | Path):\n",
    "    \"\"\"Return shap values and explainer background archives. from npz files.\"\"\"\n",
    "    shap_values_dir = Path(shap_values_dir)\n",
    "    try:\n",
    "        shap_values_path = next(shap_values_dir.glob(\"*evaluation*.npz\"))\n",
    "        background_info_path = next(shap_values_dir.glob(\"*explainer_background*.npz\"))\n",
    "    except StopIteration as err:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find shap values or explainer background archives in {shap_values_dir}\"\n",
    "        ) from err\n",
    "\n",
    "    with open(shap_values_path, \"rb\") as f:\n",
    "        shap_values_archive = np.load(f)\n",
    "        shap_values_archive = dict(shap_values_archive.items())\n",
    "\n",
    "    with open(background_info_path, \"rb\") as f:\n",
    "        explainer_background = np.load(f)\n",
    "        explainer_background = dict(explainer_background.items())\n",
    "\n",
    "    return shap_values_archive, explainer_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_archives(logdir1)[0][\"classes\"])\n",
    "# print(get_archives(logdir2)[0][\"classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_md5s(\n",
    "    md5s: List[str], metadata: metadata.Metadata, category_label: str, labels: List[str]\n",
    ") -> List[int]:\n",
    "    \"\"\"Subsample md5s based on metadata filtering provided, for a given category and filtering labels.\n",
    "\n",
    "    Args:\n",
    "            md5s (list): A list of MD5 hashes.\n",
    "            metadata (Metadata): A metadata object containing the data to be filtered.\n",
    "            category_label (str): The category label to be used for filtering the metadata.\n",
    "            labels (list): A list of labels to be used for selecting category subsets in the metadata.\n",
    "\n",
    "    Returns:\n",
    "            list: A list of indices corresponding to the selected md5s.\n",
    "    \"\"\"\n",
    "    meta = copy.deepcopy(metadata)\n",
    "    meta.select_category_subsets(category_label, labels)\n",
    "    chosen_idxs = []\n",
    "    for i, md5 in enumerate(md5s):\n",
    "        if md5 in meta:\n",
    "            chosen_idxs.append(i)\n",
    "    return chosen_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_overlap_stats(\n",
    "    feature_lists: List[List[int]], percentile_list: list[int]\n",
    ") -> Tuple[Set[int], Set[int], Dict[int, List[int]], go.Figure]:\n",
    "    \"\"\"\n",
    "    Calculate the statistics of feature overlap between multiple feature lists.\n",
    "\n",
    "    This function takes a list of feature lists and computes feature frequency percentiles.\n",
    "    It also computes the union and intersection of all features from the given feature lists.\n",
    "\n",
    "    Args:\n",
    "        feature_lists (List[List[int]]): A list of feature lists, where each inner list contains feature indices.\n",
    "        percentile_list (List[int]: The percentile values for which the most frequent features will be returned.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Set[int], Set[int], Dict[int, List]]: A tuple containing\n",
    "        1) intersection of all features\n",
    "        2) union of all features\n",
    "        3) a dict containing the list of features present present in each file number percentile.\n",
    "        4) a plotly figure showing the histogram of feature frequency\n",
    "    \"\"\"\n",
    "    nb_files = len(feature_lists)\n",
    "    if not feature_lists:\n",
    "        raise ValueError(\"Input list must not be empty.\")\n",
    "\n",
    "    for percentile in percentile_list:\n",
    "        if percentile < 0 or percentile > 100:\n",
    "            raise ValueError(\"Percentile values must be between 0 and 100.\")\n",
    "\n",
    "    # Most frequent features (per percentile)\n",
    "    feature_counter = Counter()\n",
    "    for feature_list in feature_lists:\n",
    "        feature_counter.update(feature_list)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data=feature_counter, orient=\"index\").reset_index()\n",
    "    df.columns = [\"Feature\", \"Count\"]\n",
    "\n",
    "    # Histogram of feature frequency\n",
    "    nb_features = len(feature_counter)\n",
    "    nbins = int(np.sqrt(nb_features))\n",
    "    hist = px.histogram(\n",
    "        df,\n",
    "        x=\"Count\",\n",
    "        title=f\"Top N features: frequency of {nb_features} features in {nb_files} files\",\n",
    "        nbins=nbins,\n",
    "        range_x=[0, nb_files],\n",
    "    )\n",
    "    hist.update_layout(xaxis_title=\"Nb files\", yaxis_title=\"Feature count\")\n",
    "\n",
    "    # Feature frequency stats\n",
    "    describe_percentiles = sorted([0.25, 0.5, 0.75] + [p / 100 for p in percentile_list])\n",
    "    count_stats = pd.DataFrame(df[\"Count\"].describe(percentiles=describe_percentiles))\n",
    "    count_stats[\"% of files\"] = count_stats[\"Count\"] / nb_files * 100\n",
    "    count_stats[\"% of files\"][\"count\"] = \"nan\"\n",
    "    display(count_stats)\n",
    "\n",
    "    percentile_features_dict = {}\n",
    "    for percentile in percentile_list:\n",
    "        # Calculate percentile count value, then select all features >= current percentile\n",
    "        curr_perc = nb_files * percentile / 100\n",
    "        features_above_perc = df[df[\"Count\"] >= curr_perc]\n",
    "        percentile_features_dict[percentile] = features_above_perc[\"Feature\"].tolist()\n",
    "\n",
    "    # Union and intersection of all features\n",
    "    all_features_union: Set[int] = set()\n",
    "    all_features_intersection: Set[int] = set(feature_lists[0])\n",
    "    for feature_set in feature_lists:\n",
    "        all_features_union.update(feature_set)\n",
    "        all_features_intersection &= set(feature_set)\n",
    "\n",
    "    return all_features_intersection, all_features_union, percentile_features_dict, hist  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shap_matrix(\n",
    "    meta: metadata.Metadata,\n",
    "    shap_matrices: np.ndarray,\n",
    "    eval_md5s: List[str],\n",
    "    label_category: str,\n",
    "    selected_labels: List[str],\n",
    "    class_idx: int,\n",
    ") -> Tuple[np.ndarray, List[int]]:\n",
    "    \"\"\"Generates a SHAP matrix corresponding to a selected subset of samples.\n",
    "\n",
    "    This function selects a subset of samples based on specified criteria\n",
    "    and then generates a SHAP matrix for these selected samples. It filters\n",
    "    the metadata if a specific target subsample is provided, and selects a\n",
    "    subset of samples that are identified by their md5 hash. It then selects\n",
    "    the SHAP values of these samples under the matrix of the given class number.\n",
    "\n",
    "    Args:\n",
    "        meta (metadata.Metadata): Metadata object containing information about the samples.\n",
    "        shap_matrices (np.ndarray): Array of SHAP matrices for each class.\n",
    "        eval_md5s (List[str]): List of md5 hashes identifying the evaluation samples.\n",
    "        label_category (str): Name of the category in the metadata that contains the desired labels.\n",
    "        selected_labels (List[str]): Name of the classes for which samples will be considered.\n",
    "        class_idx (int): Index of the class for which the shap values matrix will be used.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The selected SHAP matrix for the first class and for the\n",
    "                    chosen samples based on the provided criteria.\n",
    "        List[int]: The indices of the chosen samples in the original SHAP matrix.\n",
    "\n",
    "    Raises:\n",
    "            IndexError: If the `class_idx` is out of bounds for the `shap_matrices`.\n",
    "    \"\"\"\n",
    "    my_meta = copy.deepcopy(meta)\n",
    "\n",
    "    chosen_idxs = subsample_md5s(\n",
    "        md5s=eval_md5s,\n",
    "        metadata=my_meta,\n",
    "        category_label=label_category,\n",
    "        labels=selected_labels,\n",
    "    )\n",
    "    if len(shap_matrices.shape) == 3:  # deepSHAP\n",
    "        try:\n",
    "            class_shap = shap_matrices[class_idx]\n",
    "        except IndexError as err:\n",
    "            raise IndexError(f\"Class index {class_idx} is out of bounds.\") from err\n",
    "\n",
    "        selected_class_shap = np.array(class_shap[chosen_idxs, :])\n",
    "    else:  # TreeExplainer 2D\n",
    "        class_shap = shap_matrices\n",
    "        selected_class_shap = class_shap[chosen_idxs]\n",
    "    print(\n",
    "        f\"Shape of selected class ({selected_labels}) shap values: {selected_class_shap.shape}\"\n",
    "    )\n",
    "    print(f\"Chose {len(chosen_idxs)} samples from {class_shap.shape[0]} samples\")\n",
    "    return selected_class_shap, chosen_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_overlap_stats(feature_stats: Sequence):\n",
    "    \"\"\"Prints the statistics of feature overlap.\n",
    "\n",
    "    This function receives the feature statistics which include the intersection,\n",
    "    union and frequent features in each quantile of features. It then prints\n",
    "    these statistics for easy inspection.\n",
    "\n",
    "    Args:\n",
    "        feature_stats (Sequence): Tuple containing the intersection, union and\n",
    "                                  frequent features in each quantile of features.\n",
    "    \"\"\"\n",
    "    features_intersection, features_union, frequent_features = feature_stats\n",
    "    print(f\"Intersection of all features: {len(features_intersection)} features\")\n",
    "    print(f\"Fully intersecting features: {list(features_intersection)}\")\n",
    "    print(f\"Union of all features: {len(features_union)} features\\n\")\n",
    "    for k, v in frequent_features.items():\n",
    "        print(f\"Most frequent features in {k}th quantile: {len(v)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_importance_info(feature_selection: List[int], shap_matrix: np.ndarray):\n",
    "    \"\"\"Prints the feature importance information.\n",
    "\n",
    "    This function prints the feature importance information, which includes the\n",
    "    average expected contribution of the selected features and one feature (if\n",
    "    the importance was uniform), and statistical descriptions of the contributions\n",
    "    of the selected features.\n",
    "\n",
    "    Args:\n",
    "        feature_selection (List[int]): The indices of the selected features.\n",
    "        shap_matrix (np.ndarray): The SHAP values matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    N = len(feature_selection)\n",
    "    nb_files, nb_bins = shap_matrix.shape\n",
    "    print(\n",
    "        f\"Average expected contribution of {N} feature if uniform importance:{N/nb_bins*100:.5f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Average expected contribution of 1 feature if uniform importance:{1/nb_bins*100:.5f}%\"\n",
    "    )\n",
    "    print(f\"Average contribution of selected features for {nb_files} files:\")\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            softmax(shap_matrix, axis=1)[:, list(feature_selection)].sum(axis=1) * 100\n",
    "        ).describe(percentiles=DECILES)\n",
    "    )\n",
    "    print(f\"Individual contribution of selected features for {nb_files} files:\")\n",
    "    display(\n",
    "        pd.DataFrame(\n",
    "            softmax(shap_matrix, axis=1)[:, list(feature_selection)] * 100\n",
    "        ).describe(percentiles=DECILES)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shap_values_and_info(\n",
    "    shap_logdir: str | Path, verbose: bool = True\n",
    ") -> Tuple[np.ndarray, List[str], List[Tuple[str, str]]]:\n",
    "    \"\"\"Extract and print basic statistics about SHAP values from an archive.\n",
    "\n",
    "    Args:\n",
    "        shap_logdir (str): The directory where the SHAP values archive is located.\n",
    "        verbose (bool): Whether to print basic statistics about the SHAP values.\n",
    "\n",
    "    Returns:\n",
    "        shap_matrices (np.ndarray): SHAP matrices.\n",
    "        eval_md5s (List[str]): List of evaluation MD5s.\n",
    "        classes (List[Tuple[str, str]]): List of classes. Each class is a tuple containing the class index and the class label.\n",
    "    \"\"\"\n",
    "    # Extract shap values and md5s from archive\n",
    "    shap_values_archive, _ = get_archives(shap_logdir)\n",
    "    try:\n",
    "        eval_md5s: List[str] = shap_values_archive[\"evaluation_md5s\"]\n",
    "    except KeyError:\n",
    "        eval_md5s: List[str] = shap_values_archive[\"evaluation_ids\"]\n",
    "    shap_matrices: np.ndarray = shap_values_archive[\"shap_values\"]\n",
    "\n",
    "    # Print basic statistics about the loaded SHAP values\n",
    "    if verbose:\n",
    "        print(f\"nb classes: {len(shap_matrices)}\")\n",
    "        print(f\"nb samples: {len(eval_md5s)}\")\n",
    "        print(f\"dim shap value matrix: {shap_matrices[0].shape}\")\n",
    "        print(f\"Output classes of classifier:\\n {shap_values_archive['classes']}\")\n",
    "\n",
    "    return shap_matrices, eval_md5s, shap_values_archive[\"classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_the_whole_thing(\n",
    "    metadata: metadata.Metadata,\n",
    "    shap_dir: Path,\n",
    "    output_dir: Path,\n",
    "    label_category: str,\n",
    "    top_n: int = 100,\n",
    ") -> Dict[str, List[int]]:\n",
    "    \"\"\"Execute the complete process of SHAP value analysis.\n",
    "\n",
    "    This function performs the complete SHAP value analysis given the metadata and the directory\n",
    "    of the SHAP value files. It carries out the following steps:\n",
    "    1. Load the SHAP value archives and print basic statistics.\n",
    "    2. Filter the metadata to match the samples in SHAP value archives.\n",
    "    For each output class class:\n",
    "        3. Extract SHAP values.\n",
    "        4. Determine the top N features for each sample.\n",
    "        5. Compute and print feature overlap statistics.\n",
    "        6. Analyze feature importance.\n",
    "        7. Convert bin indices to genomic ranges and write to a BED file.\n",
    "        8. Display and save a plot of importance distribution for one sample.\n",
    "\n",
    "    Args:\n",
    "        metadata (metadata.Metadata): The metadata for the samples.\n",
    "        shap_logdir (Path): The directory path where SHAP value files are stored.\n",
    "        label_category (str): The name of the classifier output category that computed the shaps.\n",
    "        top_n (int): The number of top features to be selected for each sample. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[int, List[int]]: Dictionary where keys are class labels and values are lists\n",
    "        of the most frequently occurring important (high shap) features for that class, for each computed quantile.\n",
    "        (see feature_overlap_stats function for more details).\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If \"evaluation_md5s\" or \"evaluation_ids\" are not found in the loaded SHAP value archives.\n",
    "    \"\"\"\n",
    "    metadata = copy.deepcopy(metadata)\n",
    "\n",
    "    # Extract shap values and md5s from archive\n",
    "    shap_matrices, eval_md5s, classes = extract_shap_values_and_info(shap_dir)\n",
    "\n",
    "    # Filter metadata to include only the samples that exist in the SHAP value archives\n",
    "    for md5 in list(metadata.md5s):\n",
    "        if md5 not in set(eval_md5s):\n",
    "            del metadata[md5]\n",
    "\n",
    "    metadata.display_labels(\"assay_epiclass\")\n",
    "    metadata.display_labels(\"harmonized_donor_sex\")\n",
    "\n",
    "    # Loop over each class to perform SHAP value analysis\n",
    "    important_features = {}\n",
    "    for class_int, class_label in classes:\n",
    "        class_int = int(class_int)\n",
    "        print(f\"\\n\\nClass: {class_label} ({class_int})\")\n",
    "\n",
    "        # Get the SHAP matrix for the current class,\n",
    "        # and only select samples that also correspond to that class\n",
    "        shap_matrix, chosen_idxs = get_shap_matrix(\n",
    "            meta=metadata,\n",
    "            shap_matrices=shap_matrices,\n",
    "            eval_md5s=eval_md5s,\n",
    "            label_category=label_category,\n",
    "            selected_labels=[class_label],\n",
    "            class_idx=class_int,\n",
    "        )\n",
    "\n",
    "        if len(chosen_idxs) < 5:\n",
    "            print(f\"Not enough samples (5) to perform analysis on {class_label}.\")\n",
    "            continue\n",
    "\n",
    "        # Computing statistics of feature overlap\n",
    "        print(\n",
    "            f\"Selecting features with top {top_n} SHAP values for each sample of {class_label}.\"\n",
    "        )\n",
    "        top_n_features = []\n",
    "        for sample in shap_matrix:\n",
    "            top_n_features.append(list(n_most_important_features(sample, top_n)))\n",
    "\n",
    "        feat_intersect, feat_union, frequent_features, hist_fig = feature_overlap_stats(\n",
    "            top_n_features, [90, 95, 99]\n",
    "        )\n",
    "        important_features[class_label] = frequent_features\n",
    "\n",
    "        hist_fig.write_image(\n",
    "            file=output_dir / f\"top{top_n}_feature_frequency_{class_label}.png\",\n",
    "            format=\"png\",\n",
    "        )\n",
    "\n",
    "        # print_feature_overlap_stats(some_stats)\n",
    "\n",
    "        chosen_percentile = 90\n",
    "        feature_selection = frequent_features[chosen_percentile]\n",
    "\n",
    "        # print_importance_info(feature_selection, shap_matrix)\n",
    "\n",
    "        # Convert bin indices to genomic ranges and write to a BED file\n",
    "        bed_vals = bins_to_bed_ranges(\n",
    "            sorted(feature_selection), chroms, resolution=RESOLUTION\n",
    "        )\n",
    "        bed_filename = get_valid_filename(\n",
    "            f\"frequent_features_{chosen_percentile}_{class_label}.bed\"\n",
    "        )\n",
    "        write_to_bed(\n",
    "            bed_vals,\n",
    "            output_dir / bed_filename,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        # # Display and save a plot of importance distribution for one sample\n",
    "        # print(\"One sample\")\n",
    "        # probs_1sample = pd.DataFrame(softmax(shap_matrix, axis=1)[0, :] * 100)\n",
    "        # display(probs_1sample.describe(percentiles=DECILES))\n",
    "        # fig_title = f\"Importance distribution - One sample - {eval_md5s[chosen_idxs[0]]}\"\n",
    "        # fig = px.violin(probs_1sample, box=True, points=\"all\", title=fig_title)\n",
    "        # fig.write_image(shap_logdir / \"importance_dist_1sample.png\")\n",
    "\n",
    "    return important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_alternative_analysis(\n",
    "    metadata: metadata.Metadata,\n",
    "    shap_logdir: Path,\n",
    "    label_category: str,\n",
    "    selected_classes: list[str],\n",
    "    top_n: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"Run an alternative analysis that involves plotting feature importance for selected classes.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Extracts SHAP values and associated metadata.\n",
    "    2. Filters out samples from the metadata that are not present in the SHAP value archives.\n",
    "    3. Collects the most important features for selected classes.\n",
    "    4. Plots the feature importance for samples of these selected classes using different metrics (\"raw\", \"softmax\", \"rank\").\n",
    "\n",
    "    Args:\n",
    "        metadata (\"metadata.Metadata\"): The metadata object containing sample information.\n",
    "        shap_logdir (Path): The directory where the SHAP value archives are stored.\n",
    "        label_category (str): The category of the label to be used for class selection.\n",
    "        selected_classes (List[str]): A list of classes for which the analysis should be run.\n",
    "        top_n (int, optional): The top N most important features to consider. Default is 100.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If sample indices are not unique across classes.\n",
    "    \"\"\"\n",
    "    metadata = copy.deepcopy(metadata)\n",
    "\n",
    "    # Extract shap values and md5s from archive\n",
    "    shap_matrices, eval_md5s, classes = extract_shap_values_and_info(shap_logdir)\n",
    "\n",
    "    # Filter metadata to include only the samples that exist in the SHAP value archives\n",
    "    for md5 in list(metadata.md5s):\n",
    "        if md5 not in set(eval_md5s):\n",
    "            del metadata[md5]\n",
    "\n",
    "    # collect important features for selected classes\n",
    "    selected_percentile = 90\n",
    "    classes_dict = {\n",
    "        class_label: int(class_int)\n",
    "        for class_int, class_label in classes\n",
    "        if class_label in selected_classes\n",
    "    }\n",
    "    important_features = {}\n",
    "    sample_idxs = {}\n",
    "    for class_label, class_int in classes_dict.items():\n",
    "        print(f\"\\n\\nClass: {class_label} ({class_int})\")\n",
    "\n",
    "        # Get the SHAP matrix for the current class,\n",
    "        # and only select samples that also correspond to that class\n",
    "        shap_matrix, chosen_idxs = get_shap_matrix(\n",
    "            meta=metadata,\n",
    "            shap_matrices=shap_matrices,\n",
    "            eval_md5s=eval_md5s,\n",
    "            label_category=label_category,\n",
    "            selected_labels=[class_label],\n",
    "            class_idx=class_int,\n",
    "        )\n",
    "        sample_idxs[class_label] = chosen_idxs\n",
    "\n",
    "        # Computing statistics of feature overlap\n",
    "        top_n_features = []\n",
    "        for sample in shap_matrix:\n",
    "            top_n_features.append(list(n_most_important_features(sample, top_n)))\n",
    "\n",
    "        some_stats = feature_overlap_stats(top_n_features, [selected_percentile])\n",
    "        frequent_features = some_stats[2]\n",
    "        important_features[class_label] = frequent_features[selected_percentile]\n",
    "\n",
    "    all_chosen_idxs = set()\n",
    "    for idxs in sample_idxs.values():\n",
    "        all_chosen_idxs.update(idxs)\n",
    "\n",
    "    if len(all_chosen_idxs) != sum(len(idxs) for idxs in sample_idxs.values()):\n",
    "        raise ValueError(\"Sample indices are not unique across classes.\")\n",
    "\n",
    "    logdir = shap_logdir / \"feature_rank_analysis\"\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for class_of_interest in selected_classes:\n",
    "        important_feats = important_features[class_of_interest]\n",
    "        for comparison_class in selected_classes:\n",
    "            shap_matrix = shap_matrices[classes_dict[comparison_class]]\n",
    "\n",
    "            # Lists to hold md5s, metadata category, and feature ranks for each sample\n",
    "            md5_list = []\n",
    "            cell_type_list = []\n",
    "            ranks_list = []\n",
    "\n",
    "            for i, sample_shap_values in enumerate(shap_matrix):\n",
    "                if i not in sample_idxs[comparison_class]:\n",
    "                    continue\n",
    "                # if i not in all_chosen_idxs:\n",
    "                #     continue\n",
    "\n",
    "                md5_list.append(eval_md5s[i])\n",
    "                cell_type_list.append(metadata[eval_md5s[i]][CELL_TYPE])\n",
    "\n",
    "                ranks = np.argsort(\n",
    "                    np.argsort(-np.abs(sample_shap_values))\n",
    "                )  # Ranking in descending order of absolute SHAP value\n",
    "                ranks_of_important_feats = ranks[\n",
    "                    important_feats\n",
    "                ]  # Get the ranks of the important features\n",
    "                ranks_list.append(ranks_of_important_feats)\n",
    "\n",
    "            # Combine all the lists into a DataFrame\n",
    "            ranks_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"md5sum\": md5_list,\n",
    "                    CELL_TYPE: cell_type_list,\n",
    "                    **{\n",
    "                        f\"Feature_{feat}\": [ranks[i] for ranks in ranks_list]\n",
    "                        for i, feat in enumerate(important_feats)\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Save the DataFrame to CSV\n",
    "            title = f\"important_{class_of_interest}_features_in_{comparison_class}_shap_matrix.csv\".replace(\n",
    "                \" \", \"_\"\n",
    "            )\n",
    "            ranks_df.to_csv(logdir / title, index=True)\n",
    "            print(\n",
    "                f\"Feature ranks for '{class_of_interest}' features in '{comparison_class}' samples have been saved.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feat_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(logdir1, logdir2)\n",
    "print(logdir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, eval_md5s, _ = extract_shap_values_and_info(logdir1, verbose=False)\n",
    "eval_md5s = set(eval_md5s)\n",
    "for md5 in list(my_meta.md5s):\n",
    "    if md5 not in eval_md5s:\n",
    "        del my_meta[md5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_meta.display_labels(CELL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assay_labels = [[assay_label] for assay_label in my_meta.unique_classes(label_category=ASSAY)] + [[\"rna_seq\", \"mrna_seq\"]] + [[\"wbgs-pbat\", \"wgbs-standard\"]]\n",
    "cell_type_labels = [\n",
    "    \"T cell\",\n",
    "    \"neutrophil\",\n",
    "    \"monocyte\",\n",
    "    \"lymphocyte of B lineage\",\n",
    "    \"brain\",\n",
    "    \"myeloid cell\",\n",
    "]\n",
    "\n",
    "# for assay_label_list in assay_labels:\n",
    "for ct_label in cell_type_labels:\n",
    "    meta = copy.deepcopy(my_meta)\n",
    "    # meta.select_category_subsets(ASSAY, assay_label_list)\n",
    "    meta.select_category_subsets(CELL_TYPE, [ct_label])\n",
    "\n",
    "    if len(meta) < 5:\n",
    "        continue\n",
    "\n",
    "    # if len(assay_label_list) == 2:\n",
    "    #     continue\n",
    "\n",
    "    # name = \"_\".join(assay_label_list) + \"_\" + ct_label\n",
    "    name = ct_label\n",
    "    run_logdir = logdir1 / name\n",
    "    run_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "    important_features = run_the_whole_thing(\n",
    "        metadata=meta,\n",
    "        shap_dir=logdir1,\n",
    "        output_dir=run_logdir,\n",
    "        label_category=category,\n",
    "        top_n=100,\n",
    "    )\n",
    "\n",
    "    feat_90 = {name: sets[90] for name, sets in important_features.items()}\n",
    "\n",
    "    print(len(feat_90))\n",
    "    if len(feat_90) <= 1:\n",
    "        continue\n",
    "\n",
    "    upset_features = upsetplot.from_contents(feat_90)\n",
    "\n",
    "    upsetplot.UpSet(\n",
    "        upset_features,\n",
    "        subset_size=\"count\",\n",
    "        show_counts=True,\n",
    "        show_percentages=True,\n",
    "        sort_by=\"cardinality\",\n",
    "        sort_categories_by=\"cardinality\",\n",
    "    ).plot()\n",
    "\n",
    "    plt.savefig(run_logdir / f\"upset_{name}.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(important_feat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dict = {\n",
    "#     f\"{assay}_{output_class}\": class_set\n",
    "#     for assay, dicts in important_feat_dict.items()\n",
    "#     for output_class, class_set in dicts.items()\n",
    "# }\n",
    "\n",
    "# # add merge of output classes\n",
    "# assays = set(assay.split(\"_\")[0] for assay in new_dict.keys())\n",
    "# for assay in assays:\n",
    "#     merged_features = set()\n",
    "\n",
    "#     for key, features in new_dict.items():\n",
    "#         if key.startswith(assay):\n",
    "#             merged_features.update(features)\n",
    "\n",
    "#     new_dict[f\"{assay}_all_classes\"] = list(merged_features)\n",
    "#     # del new_dict[f\"{assay}_cancer\"]\n",
    "#     # del new_dict[f\"{assay}_non-cancer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upset_features = upsetplot.from_contents(new_dict)\n",
    "# upsetplot.UpSet(\n",
    "#     upset_features, subset_size=\"count\", show_counts=True, show_percentages=True, sort_by=\"cardinality\", sort_categories_by=\"cardinality\"\n",
    "# ).plot()\n",
    "# plt.savefig(run_logdir / \"upset_assay.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cell_types = [\n",
    "    \"T cell\",\n",
    "    \"lymphocyte of B lineage\",\n",
    "    \"monocyte\",\n",
    "    \"muscle organ\",\n",
    "    \"myeloid cell\",\n",
    "    \"neutrophil\",\n",
    "]\n",
    "for cell_type in relevant_cell_types:\n",
    "    new_dict = {\n",
    "        hist: hist_dict.get(cell_type, [])\n",
    "        for hist, hist_dict in important_feat_dict.items()\n",
    "    }\n",
    "    upset_features = upsetplot.from_contents(new_dict)\n",
    "\n",
    "    plot_filename = f\"upset_{get_valid_filename(cell_type)}.png\"\n",
    "    fig = upsetplot.UpSet(\n",
    "        upset_features,\n",
    "        subset_size=\"count\",\n",
    "        show_counts=True,\n",
    "        show_percentages=True,\n",
    "        sort_by=\"cardinality\",\n",
    "        sort_categories_by=\"cardinality\",\n",
    "    ).plot()\n",
    "    plt.savefig(logdir1.parent / plot_filename, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample ontology specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_intersection = set.intersection(\n",
    "#     *[set(feat_list[90]) for feat_list in important_features.values()]\n",
    "# )\n",
    "\n",
    "# male_feat = set(important_features[\"male\"][90])\n",
    "# female_feat = set(important_features[\"female\"][90])\n",
    "\n",
    "# only_male = male_feat - female_feat\n",
    "# only_female = female_feat - male_feat\n",
    "\n",
    "# for feat_set, name in zip([only_female, only_male], [\"only_female\", \"only_male\"]):\n",
    "#     bed_vals = bins_to_bed_ranges(sorted(feat_set), chroms, resolution=RESOLUTION)\n",
    "#     var_name = f\"{feat_set=}\".split(\"=\")[0]\n",
    "#     write_to_bed(\n",
    "#         bed_vals,\n",
    "#         logdir / f\"frequent_features_{90}_{name}.bed\",\n",
    "#         verbose=False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_alternative_analysis(\n",
    "#     metadata=my_meta,\n",
    "#     shap_logdir=logdir,\n",
    "#     label_category=SEX,\n",
    "#     selected_classes=[\"female\", \"male\"],\n",
    "#     top_n=100,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_violin_plots(logdir: str | Path, selected_classes: list[str]) -> None:\n",
    "    \"\"\"Create violin plots of important feature ranks for selected classes.\"\"\"\n",
    "    logdir = Path(logdir)\n",
    "    # Iterate through each class of interest\n",
    "    for class_of_interest in selected_classes:\n",
    "        # Initialize an empty figure\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Iterate through each class to add its violin plot to the figure\n",
    "        for comparison_class in selected_classes:\n",
    "            df = pd.read_csv(\n",
    "                logdir\n",
    "                / f\"important_{class_of_interest}_features_in_{comparison_class}_shap_matrix.csv\"\n",
    "            )\n",
    "            df = df.filter(\n",
    "                like=\"Feature_\"\n",
    "            )  # Remove non-feature columns from DataFrame for plotting\n",
    "\n",
    "            # Create violin plot for the current comparison_class\n",
    "            violin = go.Violin(\n",
    "                y=df.values.flatten(),  # Flattened feature ranks\n",
    "                name=f\"{comparison_class} (n={df.shape[0]})\",  # Name of the violin plot\n",
    "                box_visible=True,  # Display box inside the violin\n",
    "                line_color=px.colors.qualitative.Plotly[\n",
    "                    len(fig.data)\n",
    "                ],  # Different color for each violin\n",
    "                points=\"all\",  # Display all points\n",
    "            )\n",
    "            fig.add_trace(violin)\n",
    "\n",
    "            print(df.shape)\n",
    "        # Set title and axis labels\n",
    "        fig.update_layout(\n",
    "            title=f\"Violin plot of ranks for important features of '{class_of_interest}' ({df.shape[1]} features)\",\n",
    "            xaxis_title=\"Source Matrix\",\n",
    "            yaxis_title=\"Feature Rank\",\n",
    "        )\n",
    "\n",
    "        # Show the figure\n",
    "        fig.write_image(\n",
    "            logdir / f\"important_{class_of_interest}_feature_ranks_violin_plot.png\"\n",
    "        )\n",
    "        fig.write_html(\n",
    "            logdir / f\"important_{class_of_interest}_feature_ranks_violin_plot.html\"\n",
    "        )\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_violin_plots(\n",
    "#     logdir=logdir / \"feature_rank_analysis\", selected_classes=[\"female\", \"male\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples ontology class pairs important features overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_union = set()\n",
    "# feature_intersection = set(list(important_features.values())[0][90])\n",
    "# for label, features in important_features.items():\n",
    "#     features_90 = features[90]\n",
    "#     print(f\"\\n\\nClass: {label}\")\n",
    "#     print(f\"Most frequent features in 90th quantile: {features_90}\")\n",
    "#     feature_union.update(features_90)\n",
    "#     feature_intersection &= set(features_90)\n",
    "\n",
    "# print(f\"\\n\\nUnion of all features: {len(feature_union)} features\")\n",
    "# print(\n",
    "#     f\"\\n\\nIntersection of all features: {len(feature_intersection)} features: {feature_intersection}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intersections(important_features: dict) -> pd.DataFrame:\n",
    "    \"\"\"Compute all possible intersections between pairs of sets and store them in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        important_features (dict): Dictionary where keys are class labels and values are sets of important features.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the intersections and their properties.\n",
    "    \"\"\"\n",
    "    sets = {\n",
    "        label: set(quantile_features[90])\n",
    "        for label, quantile_features in important_features.items()\n",
    "    }\n",
    "    records = []\n",
    "\n",
    "    for set1_info, set2_info in itertools.combinations(sets.items(), 2):\n",
    "        label1, set1 = set1_info\n",
    "        label2, set2 = set2_info\n",
    "        intersection = set1.intersection(set2)\n",
    "\n",
    "        record = {\n",
    "            \"Set1_Label\": label1,\n",
    "            \"Set2_Label\": label2,\n",
    "            \"Set1_Size\": len(set1),\n",
    "            \"Set2_Size\": len(set2),\n",
    "            \"Intersection\": intersection,\n",
    "            \"Intersection_Size\": len(intersection),\n",
    "        }\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = compute_intersections(important_features)\n",
    "# df.to_csv(logdir / \"feature_intersections_q90.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = Counter()\n",
    "# for feature_set in df[\"Intersection\"]:\n",
    "#     counter.update(feature_set)\n",
    "\n",
    "# for k, v in counter.most_common():\n",
    "#     print(f\"{k} {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_selection = []\n",
    "# sort_order = np.argsort(feature_selection)\n",
    "\n",
    "# bed_vals = bins_to_bed_ranges(sorted(feature_selection), chroms, resolution=RESOLUTION)\n",
    "\n",
    "# write_to_bed(bed_ranges=bed_vals, bed_path=logdir / \"frequent_features.bed\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for val in np.array(bed_vals)[sort_order.argsort()]:\n",
    "#     print(val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epi_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c736fa4331366c124c8f78bd4f37e7b252afab1d364df23bf88752fcd9abf849"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
