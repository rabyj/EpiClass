{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Explore metadata distribution of correct/wrong predictions\"\"\"\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, pointless-statement, use-dict-literal, expression-not-assigned, unused-import, too-many-lines, unreachable\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSAY = \"assay_epiclass\"\n",
    "TRACK = \"track_type\"\n",
    "\n",
    "BIOMATERIAL_TYPE = \"harmonized_biomaterial_type\"\n",
    "CANCER = \"harmonized_sample_cancer_high\"\n",
    "CELL_TYPE = \"harmonized_sample_ontology_intermediate\"\n",
    "DISEASE = \"harmonized_sample_disease_high\"\n",
    "LIFE_STAGE = \"harmonized_donor_life_stage\"\n",
    "SEX = \"harmonized_donor_sex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = (\n",
    "    Path.home()\n",
    "    / \"projects/epilap/output/logs/epiatlas-dfreeze-v2.1/merged_results/epiatlas/with_split_nb/merged_pred_results_all_2.1_chrY_zscores.csv\"\n",
    ")\n",
    "results_df = pd.read_csv(results_path, index_col=\"md5sum\", header=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_target = SEX\n",
    "\n",
    "print([column for column in results_df.columns if SEX in column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"harmonized_donor_sex_1l_3000n_w-mixed_10fold-oversample\"\n",
    "\n",
    "classifier_preds_colname = f\"Predicted class {classifier_name}\"\n",
    "classifer_correct_colname = f\"True class {classifier_name}\"\n",
    "classifer_same_colname = f\"Same? {classifier_name}\"\n",
    "\n",
    "classifier_df = results_df[results_df[classifier_preds_colname].notnull()]\n",
    "print(results_df.shape, classifier_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(classifier_df[classifier_preds_colname].value_counts())\n",
    "display(classifier_df[classifer_correct_colname].value_counts())\n",
    "display(classifier_df[classifer_same_colname].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_error_count_df(\n",
    "    groupby_cols: List[str], classifier_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Count errors by groupby_cols\"\"\"\n",
    "    global_metadata_distribution = classifier_df.groupby(by=groupby_cols).size()\n",
    "\n",
    "    # pylint: disable=singleton-comparison\n",
    "    error_metadata_distribution = (\n",
    "        classifier_df[classifier_df[classifer_same_colname] == False]\n",
    "        .groupby(by=groupby_cols)\n",
    "        .size()\n",
    "    )\n",
    "\n",
    "    error_count_df = []\n",
    "\n",
    "    for labels, global_count in global_metadata_distribution.items():\n",
    "        error_count = error_metadata_distribution.get(labels, default=0)  # type: ignore\n",
    "        error_rate = error_count / global_count  # type: ignore\n",
    "        if isinstance(labels, str):\n",
    "            labels = [labels]\n",
    "        error_count_df.append(list(labels) + [error_rate, error_count, global_count])  # type: ignore\n",
    "\n",
    "    error_count_df = pd.DataFrame(\n",
    "        error_count_df, columns=groupby_cols + [\"error rate\", \"n error\", \"n total\"]\n",
    "    )\n",
    "    if error_count_df[\"n total\"].sum() != global_metadata_distribution.sum():\n",
    "        raise ValueError(\"Error: n total does not match global count\")\n",
    "\n",
    "    return error_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_cols = [\n",
    "    ASSAY,\n",
    "    TRACK,\n",
    "    CELL_TYPE,\n",
    "    BIOMATERIAL_TYPE,\n",
    "] + [analysis_target]\n",
    "\n",
    "groupby_selections = [groupby_cols]\n",
    "groupby_selections.extend([[col] for col in groupby_cols])\n",
    "\n",
    "for groupy_selection in groupby_selections:\n",
    "    error_df = create_error_count_df(groupy_selection, classifier_df)\n",
    "    display(error_df.sort_values(by=[\"n error\", \"error rate\"], ascending=False).head(10))\n",
    "    # error_count_df.to_csv(\n",
    "    #     results_path.parent / f\"{classifier_name}_error_rate.csv\", index=False\n",
    "    # )\n",
    "    fig = px.scatter(\n",
    "        title=f\"Error rate by {groupy_selection}\",\n",
    "        data_frame=error_df,\n",
    "        x=\"n total\",\n",
    "        y=\"error rate\",\n",
    "        hover_data=groupy_selection,\n",
    "        range_x=[0, max(error_df[\"n total\"]) * 1.01],\n",
    "    )\n",
    "    fig.add_vline(x=25)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cond = (\n",
    "    (classifier_df[ASSAY] == \"h3k27ac\")\n",
    "    & (classifier_df[TRACK] == \"pval\")\n",
    "    & (classifier_df[CELL_TYPE] == \"brain\")\n",
    ")\n",
    "filtered_df = classifier_df[filter_cond]\n",
    "\n",
    "fig = px.violin(\n",
    "    filtered_df,\n",
    "    x=\"Predicted class harmonized_donor_sex_1l_3000n_w-mixed_10fold-oversample\",\n",
    "    y=\"expected_assay_track_chrY_z-score\",\n",
    "    color=\"harmonized_donor_sex\",\n",
    "    points=\"all\",\n",
    "    box=True,\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()\n",
    "\n",
    "# fig = px.violin(\n",
    "#     filtered_df,\n",
    "#     x=\"Predicted class harmonized_donor_sex_1l_3000n_w-mixed_10fold-oversample\",\n",
    "#     y=\"\",\n",
    "#     color=\"harmonized_donor_sex\",\n",
    "#     points=\"all\",\n",
    "#     box=True,\n",
    "#     )\n",
    "# fig.update_traces(marker=dict(size=3))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suspicious epiRRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epirr_cat = \"epirr_no_version\"\n",
    "to_verify_epirr = [\n",
    "    \"IHECRE00004623\",\n",
    "    \"IHECRE00000171\",\n",
    "    \"IHECRE00001957\",\n",
    "    \"IHECRE00000152\",\n",
    "    \"IHECRE00001531\",\n",
    "    \"IHECRE00000951\",\n",
    "    \"IHECRE00001965\",\n",
    "    \"IHECRE00000099\",\n",
    "    \"IHECRE00000316\",\n",
    "    \"IHECRE00004877\",\n",
    "    \"IHECRE00003706\",\n",
    "    \"IHECRE00001370\",\n",
    "    \"IHECRE00001001\",\n",
    "    \"IHECRE00000954\",\n",
    "    \"IHECRE00004890\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(to_verify_epirr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[epirr_cat] = results_df[\"EpiRR\"].str.split(\".\").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_df = results_df[results_df[SEX].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_df[SEX].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sus_df = sex_df[sex_df[epirr_cat].isin(to_verify_epirr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sus_df[epirr_cat].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_10fold = \"harmonized_donor_sex_1l_3000n_w-mixed_10fold-oversample\"\n",
    "pred_val_label_10fold = f\"Max pred {name_10fold}\"\n",
    "pred_class_label_10fold = f\"Predicted class {name_10fold}\"\n",
    "split_nb_10fold = f\"split_nb {name_10fold}\"\n",
    "\n",
    "sus_df[split_nb_10fold] = sus_df[split_nb_10fold].fillna(-666).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epirr in to_verify_epirr:\n",
    "    epirr_df = sus_df[sus_df[epirr_cat] == epirr]\n",
    "    print(epirr)\n",
    "    print(\n",
    "        epirr_df[[ASSAY, pred_class_label_10fold, pred_val_label_10fold, split_nb_10fold]]\n",
    "        .sort_values(ASSAY)\n",
    "        .values,\n",
    "        \"\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in sus_df.columns if \"complete\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_new_preds_name = (\n",
    "    \"harmonized_donor_sex_1l_3000n_w-mixed_complete_no_valid_oversample_predictions\"\n",
    ")\n",
    "pred_val_label_new_preds = f\"Max pred {sex_new_preds_name}\"\n",
    "pred_class_label_new_preds = f\"Predicted class {sex_new_preds_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epirr in to_verify_epirr:\n",
    "    epirr_df = sus_df[sus_df[epirr_cat] == epirr]\n",
    "    if epirr_df[pred_val_label_new_preds].notnull().sum() == 0:\n",
    "        continue\n",
    "    print(epirr)\n",
    "    partial_df = epirr_df[\n",
    "        [ASSAY, pred_class_label_new_preds, pred_val_label_new_preds]\n",
    "    ].sort_values(ASSAY)\n",
    "    partial_df.columns = [ASSAY, \"Predicted class\", \"Max pred\"]\n",
    "    print(partial_df.to_markdown(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epirr in to_verify_epirr:\n",
    "    epirr_df = sus_df[sus_df[epirr_cat] == epirr]\n",
    "    total_n = epirr_df.shape[0]\n",
    "    try:\n",
    "        print(f\"{epirr}\")\n",
    "        for max_pred in [0, 0.7, 0.9]:\n",
    "            subset_df = epirr_df[epirr_df[pred_val_label_10fold] >= max_pred]\n",
    "            pivot = (\n",
    "                subset_df.pivot_table(\n",
    "                    values=pred_val_label_10fold,\n",
    "                    index=pred_class_label_10fold,\n",
    "                    columns=split_nb_10fold,\n",
    "                    aggfunc=\"count\",\n",
    "                    margins=True,\n",
    "                )\n",
    "                .fillna(0)\n",
    "                .astype(int)\n",
    "            )\n",
    "\n",
    "            count_pred = pivot[\"All\"]\n",
    "            f_count = count_pred.get(\"female\", default=0)\n",
    "            m_count = count_pred.get(\"male\", default=0)\n",
    "            mix_count = count_pred.get(\"mixed\", default=0)\n",
    "            count = count_pred[\"All\"]\n",
    "\n",
    "            if mix_count != 0:\n",
    "                print(\n",
    "                    f\"pred>{max_pred} (n={count}/{total_n}): (F={f_count}, M={m_count}, mix={mix_count})\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"pred>{max_pred} (n={count}/{total_n}): (F={f_count}, M={m_count})\"\n",
    "                )\n",
    "            print(f\"Splits: {pivot.shape[1] - 1}\")\n",
    "            print(pivot.to_string(), \"\\n\")\n",
    "        print(\"\\n\")\n",
    "    except ValueError:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
