{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Notebook to merge various augmented (with no metadata) prediction results into a single file. Mostly working with ENCODE metadata\"\"\"\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, unused-import, pointless-statement, unreachable, unnecessary-lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import collections\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from epi_ml.core.metadata import Metadata\n",
    "from epi_ml.utils.classification_merging_utils import (\n",
    "    merge_dataframes,\n",
    "    merge_two_columns,\n",
    "    remove_pred_vector,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_base_dir = (\n",
    "    Path.home() / \"projects/epilap/output/logs/epiatlas-dfreeze-v2.1/merged_results\"\n",
    ").resolve()\n",
    "if not gen_base_dir.exists():\n",
    "    raise ValueError(f\"Path {gen_base_dir} does not exist.\")\n",
    "\n",
    "prediction_results_dir = gen_base_dir / \"encode/input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = prediction_results_dir / \"hg38_ENCODE_total_final.json\"\n",
    "metadata = Metadata(metadata_path)\n",
    "meta_df = pd.DataFrame.from_records(list(metadata.datasets), index=\"md5sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_df.shape)\n",
    "for col in [\"assay_epiclass\", \"sex\", \"donor_sex\", \"life_stage\", \"biosample_type\"]:\n",
    "    print(col)\n",
    "    if col in meta_df.columns:\n",
    "        display(meta_df[col].value_counts(dropna=False))\n",
    "    else:\n",
    "        print(\"Not in metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in meta_df.columns.values:\n",
    "    if \"sex\" in category:\n",
    "        print(category)\n",
    "        display(meta_df[category].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in meta_df.columns.values:\n",
    "    if \"life\" in category or \"age\" in category:\n",
    "        print(category)\n",
    "        display(meta_df[category].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct some metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col1, col2 in zip(\n",
    "    [\"biosample_type\", \"project_x\", \"project_y\"],\n",
    "    [\"Biosample_type\", \"Project_x\", \"Project_y\"],\n",
    "):\n",
    "    if col1 in meta_df.columns and col2 in meta_df.columns:\n",
    "        meta_df = merge_two_columns(meta_df, col1, col2)\n",
    "    else:\n",
    "        print(f\"Column {col1} or {col2} not in metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(meta_df[\"biosample_type\"].value_counts(dropna=False))\n",
    "display(meta_df[\"project_x\"].value_counts(dropna=False))\n",
    "display(meta_df[\"project_y\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.drop(columns=[\"project_y\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_metadata_path = Path.home() / \"downloads\" / \"encodeproject.json\"\n",
    "with open(extra_metadata_path, \"r\", encoding=\"utf8\") as f:\n",
    "    extra_metadata_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore extra metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in extra_metadata_dict.items():\n",
    "    try:\n",
    "        print(f\"{k}: {len(v)}\")\n",
    "    except TypeError:\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_counter = collections.Counter()\n",
    "for val in extra_metadata_dict[\"@graph\"]:\n",
    "    key_counter.update(val.keys())\n",
    "\n",
    "for key in key_counter.most_common():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for graph in extra_metadata_dict[\"@graph\"]:\n",
    "    print(graph.keys())\n",
    "    for k in graph.keys():\n",
    "        print(f\"{k}: {graph[k]}\\n\")\n",
    "    if i == 0:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_all = collections.Counter()\n",
    "for graph in extra_metadata_dict[\"@graph\"]:\n",
    "    files_all.update([file_id.split(\"/\")[2] for file_id in graph[\"original_files\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert files_all.most_common()[0][1] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(meta_df.index.values) & set(files_all.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_json = [elem for idx, elem in enumerate(extra_metadata_dict[\"@graph\"]) if idx < 5000]\n",
    "# for graph in reduced_json:\n",
    "#     try:\n",
    "#         del graph[\"revoked_files\"]\n",
    "#     except KeyError:\n",
    "#         continue\n",
    "\n",
    "# with open(extra_metadata_path.parent, \"w\", encoding=\"utf8\") as f:\n",
    "#     json.dump(reduced_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_all = collections.Counter()\n",
    "ct_all = collections.Counter()\n",
    "for graph in extra_metadata_dict[\"@graph\"]:\n",
    "    biosample_info = graph[\"replicates\"][0][\"library\"][\"biosample\"]\n",
    "    sex_label = biosample_info[\"sex\"]\n",
    "\n",
    "    ct_info = graph[\"biosample_ontology\"][\"cell_slims\"]\n",
    "\n",
    "    sex_all.update([sex_label])\n",
    "    ct_all.update(ct_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sex_all.most_common())\n",
    "display(ct_all.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get new metadata values and integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate values with metadata\n",
    "# verifying sex and life_stage values\n",
    "new_extra_metadata = []\n",
    "for graph in extra_metadata_dict[\"@graph\"]:\n",
    "    biosample_info = graph[\"replicates\"][0][\"library\"][\"biosample\"]\n",
    "    sex_label = biosample_info[\"sex\"]\n",
    "\n",
    "    cancer_info = graph[\"biosample_ontology\"][\"cell_slims\"]\n",
    "    if \"cancer cell\" in cancer_info:\n",
    "        cancer_info = \"cancer\"\n",
    "    else:\n",
    "        cancer_info = \"non-cancer\"\n",
    "\n",
    "    files = [file_id.split(\"/\")[2] for file_id in graph[\"original_files\"]]\n",
    "\n",
    "    for file in files:\n",
    "        new_extra_metadata.append(\n",
    "            {\"md5sum\": file, \"donor_sex\": sex_label, \"cancer\": cancer_info}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_extra_metadata_df = pd.DataFrame.from_records(new_extra_metadata, index=\"md5sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_extra_metadata_df.shape)\n",
    "# display(new_extra_metadata_df.donor_life_stage.value_counts())\n",
    "# display(new_extra_metadata_df.donor_sex.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = merge_dataframes(meta_df, new_extra_metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df[meta_df[\"assay_epiclass\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.fillna(\"unknown\", inplace=True)\n",
    "meta_df.replace(to_replace=\"\", value=\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_df.shape)\n",
    "display(meta_df.donor_sex.value_counts(dropna=False))\n",
    "display(meta_df.assay_epiclass.value_counts(dropna=False))\n",
    "display(meta_df.cancer.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(meta_df[\"life_stage\"].value_counts())\n",
    "# sum(meta_df[\"life_stage\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(meta_df[\"donor_life_stage\"].value_counts())\n",
    "# sum(meta_df[\"donor_life_stage\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update prediction files as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for pred_file in prediction_results_dir.glob(\"*.csv\"):\n",
    "    df = pd.read_csv(pred_file, sep=\",\", index_col=\"md5sum\", dtype=str)\n",
    "    df_name = pred_file.stem.replace(\"_prediction_100kb_all_none_augmented\", \"\")\n",
    "\n",
    "    # # Add true class if inexistent\n",
    "    # if \"True class\" not in df.columns:\n",
    "    #     print(f\"Adding 'True class' to {df_name}\")\n",
    "    #     df.insert(0, \"True class\", \"unknown\")\n",
    "    #     df.to_csv(pred_file, sep=\",\", index=True)\n",
    "\n",
    "    # # Add true class if inexistent\n",
    "    # if \"Same?\" not in df.columns:\n",
    "    #     print(f\"Adding 'Same?' to {df_name}\")\n",
    "    #     df.insert(2, \"Same?\", \"False\")\n",
    "    #     df.to_csv(pred_file, sep=\",\", index=True)\n",
    "\n",
    "    # Augment if not already done\n",
    "    if \"Max pred\" not in df.columns:\n",
    "        print(f\"Augmenting {df_name}\")\n",
    "        current_dir = Path(os.path.abspath(\"\"))\n",
    "        output = subprocess.check_output(\n",
    "            args=[\n",
    "                \"python\",\n",
    "                str(current_dir.parent / \"augment_predict_file.py\"),\n",
    "                str(pred_file),\n",
    "                str(metadata_path),\n",
    "            ]\n",
    "        ).decode(\"utf-8\")\n",
    "        new_name = str(pred_file).replace(\".csv\", \"_augmented.csv\")\n",
    "        df = pd.read_csv(new_name, sep=\",\", index_col=0)\n",
    "\n",
    "    dfs[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add 'True class' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_class_dict = {\n",
    "    \"predict_assay7_oversample_test\": \"assay_epiclass\",\n",
    "    \"predict_assay7_test\": \"assay_epiclass\",\n",
    "    \"predict_assay11_test\": \"assay_epiclass\",\n",
    "    \"predict_assay13_test\": \"assay_epiclass\",\n",
    "    \"predict_biomat_test\": \"biosample_type\",\n",
    "    \"predict_donorlife_oversample_test\": \"life_stage\",\n",
    "    \"predict_project_oversample_test\": \"project_x\",\n",
    "    \"predict_sex2_test\": \"donor_sex\",\n",
    "    \"predict_sex3_oversample_test\": \"donor_sex\",\n",
    "    \"predict_cancer_oversample_test\": \"cancer\",\n",
    "    \"predict_disease_oversample_test\": \"cancer\",\n",
    "    \"predict_disease_test\": \"cancer\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(\n",
    "    set.union(*[set(dfs[df_name].index.values) for df_name in true_class_dict])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df.loc[samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, class_label in true_class_dict.items():\n",
    "    df = dfs[df_name]\n",
    "    df[\"True class\"] = meta_df[class_label]\n",
    "    try:\n",
    "        df[\"Same?\"] = df[\"True class\"].str.lower() == df[\"Predicted class\"].str.lower()\n",
    "    except KeyError as err:\n",
    "        print(err)\n",
    "        print(df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in list(dfs.items()):\n",
    "    try:\n",
    "        df = remove_pred_vector(df)\n",
    "    except KeyError:\n",
    "        print(f\"Could not remove pred vector from {df_name}\")\n",
    "\n",
    "    dfs[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns\n",
    "for name, df in dfs.items():\n",
    "    df.replace(to_replace=[\"--empty--\", \"\", \"NA\", None], value=np.nan, inplace=True)\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    dfs[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df_name, df in dfs.items():\n",
    "#     print(df.columns.values, df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make result column names unique (not metadata columns)\n",
    "old_column_names = list(dfs.values())[0].columns.values\n",
    "for df_name, df in dfs.items():\n",
    "    if df.shape[1] != 7:\n",
    "        raise ValueError(f\"Wrong number of columns in {df_name}. {df.columns.values}\")\n",
    "    new_column_names = [old_name + f\" {df_name}\" for old_name in old_column_names]\n",
    "    df.rename(columns=dict(zip(old_column_names, new_column_names)), inplace=True)\n",
    "    df.name = df_name\n",
    "    dfs[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [meta_df] + [df for _, df in sorted(dfs.items())]\n",
    "df_final = functools.reduce(merge_dataframes, df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate metadata columns (those that end by _delete)\n",
    "df_final = df_final.filter(regex=r\"^(?:(?!_delete).)+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-arrange columns\n",
    "all_columns = df_final.columns.tolist()\n",
    "\n",
    "# Separate metadata and result columns\n",
    "result_columns = [col for col in all_columns if col.rsplit(\" \", 1)[0] in old_column_names]\n",
    "meta_columns = [col for col in all_columns if col not in result_columns]\n",
    "\n",
    "new_order = meta_columns + result_columns\n",
    "df_final = df_final[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in list(df_final.columns):\n",
    "    if all(df_final[column] == \"unknown\"):\n",
    "        df_final.drop(columns=[column], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\n",
    "    prediction_results_dir.parent / \"encode_predictions_merged_results_V2.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
