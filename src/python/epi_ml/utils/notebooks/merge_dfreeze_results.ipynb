{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Notebook to work on proper way to merge a lot of already augmented output files.\"\"\"\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, unused-import, pointless-statement, unreachable,unnecessary-lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import collections\n",
    "import functools\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "from IPython.display import display\n",
    "\n",
    "from epi_ml.core.metadata import Metadata\n",
    "from epi_ml.utils.classification_merging_utils import merge_dataframes, remove_pred_vector\n",
    "from epi_ml.utils.ssh_utils import createSCPClient, createSSHClient, run_commands_via_ssh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIR = Path(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_base_dir = (Path.home() / \"mounts/narval-mount/logs-dfreeze-2.1\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narval_base_dir = \"~/logs-dfreeze-2.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge_dir = (\n",
    "    Path.home()\n",
    "    / \"projects/epilap/output/logs/epiatlas-dfreeze-v2.1/merged_results/epiatlas\"\n",
    ")\n",
    "# to_merge_files = to_merge_dir / \"valid_pred_files_all.list\"\n",
    "to_merge_files = to_merge_dir / \"valid_pred_files_non_augmented_all.list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not to_merge_files.exists():\n",
    "    cmd1 = f\"cd {narval_base_dir} && find . -mindepth 3 -maxdepth 5 -type f -name full-10fold-validation_prediction.csv\"\n",
    "    cmd2 = (\n",
    "        f\"cd {narval_base_dir} && find . -mindepth 5 -maxdepth 6 -type f -name *test*.csv\"\n",
    "    )\n",
    "    cmd_results = run_commands_via_ssh(\n",
    "        cmds=[cmd1, cmd2],\n",
    "        username=\"rabyj\",\n",
    "        hostname=\"narval.computecanada.ca\",\n",
    "        port=22,\n",
    "    )\n",
    "    # join results of the two cmds\n",
    "    valid_pred_files = [cmd_result.splitlines() for cmd_result in cmd_results]\n",
    "    valid_pred_files = valid_pred_files[0] + valid_pred_files[1]\n",
    "\n",
    "    with open(to_merge_files, \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(\"\\n\".join(valid_pred_files))\n",
    "else:\n",
    "    with open(to_merge_files, \"r\", encoding=\"utf8\") as f:\n",
    "        valid_pred_files = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in valid_pred_files:\n",
    "#     print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path.home() / \"downloads\" / \"merged_pred_results_blblbllblb.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_dirs = [\n",
    "    \"noFC\",\n",
    "    \"raw\",\n",
    "    \"pval\",\n",
    "    \"l1\",\n",
    "    \"harmonized_donor_sex_1l_3000n/no-mixed\",\n",
    "    \"groups_second_level_name_1l_3000n/w-mix\",\n",
    "    \"w-unknown\",\n",
    "    \"10fold-2\",\n",
    "    \"10fold-oversampling2\",\n",
    "    \"10fold-oversample2\",\n",
    "    \"random_1l_3000n/10fold-11c\",\n",
    "]\n",
    "valid_pred_files = [Path(file) for file in valid_pred_files]\n",
    "valid_pred_files = [\n",
    "    file\n",
    "    for file in valid_pred_files\n",
    "    if all(name not in str(file) for name in invalid_dirs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = collections.defaultdict(list)\n",
    "for file in valid_pred_files:\n",
    "    categories[file.parent.parent].append(file.parent.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampling_dirs = []\n",
    "for folder, result_list in categories.items():\n",
    "    if any(\n",
    "        result in [\"10fold-oversampling\", \"10fold-oversample\"] for result in result_list\n",
    "    ):\n",
    "        oversampling_dirs.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampling_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-oversampling results when w-oversampling also exits\n",
    "for file in list(valid_pred_files):\n",
    "    # sanity check\n",
    "    if (file.parent.name == \"10fold\") != file.parent.stem.endswith(\"10fold\"):\n",
    "        raise ValueError(f\"wat: {str(file)}\")\n",
    "\n",
    "    if file.parent.parent in oversampling_dirs and file.parent.name == \"10fold\":\n",
    "        print(f\"Removing {file}\")\n",
    "        valid_pred_files.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(valid_pred_files))\n",
    "# for file in valid_pred_files:\n",
    "#     print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base_dir = to_merge_dir / \"input_non_augmented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    results_base_dir.parent / \"valid_pred_files_non_augmented_filtered.list\",\n",
    "    \"w\",\n",
    "    encoding=\"utf8\",\n",
    ") as f:\n",
    "    f.write(\"\\n\".join([str(path) for path in valid_pred_files]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra reprocessing to add split_nb to predictions\n",
    "\n",
    "In-between, all splits were downloaded, via commands make in valid_pred_files_non_augmented_filtered_splits.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base_dir = to_merge_dir / \"input_non_augmented\"\n",
    "\n",
    "script = CURRENT_DIR.parent / \"merge_validation_predictions.py\"\n",
    "\n",
    "for file in results_base_dir.rglob(\"validation_prediction.csv\"):\n",
    "    folds_dir = file.parent.parent\n",
    "    if list(folds_dir.glob(\"*.csv\")):\n",
    "        continue\n",
    "\n",
    "    subprocess.check_output(args=[\"python\", str(script), str(folds_dir), \"-n\", \"10\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_files = list(results_base_dir.rglob(\"complete*unknown.csv\"))\n",
    "valid_pred_files += list(results_base_dir.rglob(\"full*prediction.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(valid_pred_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order paths in desired order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_instructions(instructions: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Parse the instructions from A and return a dictionary with keys and their orders.\n",
    "\n",
    "    Args:\n",
    "        instructions (str): The instructions from A.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: Dictionary containing the keys and their orders.\n",
    "    \"\"\"\n",
    "    order_dict = {}\n",
    "    for line in instructions.strip().split(\"\\n\"):\n",
    "        if line.startswith(\"#\"):\n",
    "            match = re.match(r\"#(\\d+)\", line)\n",
    "            if match:\n",
    "                order = int(match.group(1))\n",
    "                key = re.search(r\"[* ]([a-zA-Z_]+)\", line[match.end() :]).group(1)\n",
    "                order_dict[key] = order\n",
    "    return order_dict\n",
    "\n",
    "\n",
    "def fuzzy_sort_paths(paths: List[Path], order_dict: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Sort a list of paths based on the fuzzy matching with keys from an order dictionary.\n",
    "\n",
    "    Args:\n",
    "        paths (List[str]): The list of paths to sort.\n",
    "        order_dict (Dict[str, int]): The dictionary containing keys and their orders.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of paths sorted according to their best fuzzy-matched keys.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_order(path: Path) -> int:\n",
    "        parent_names = [parent.name for parent in path.parents]\n",
    "        key = \"/\".join(parent_names[0:3][::-1])\n",
    "        best_match, _ = process.extractOne(key, order_dict.keys())\n",
    "        return order_dict.get(best_match, 9999)\n",
    "\n",
    "    return sorted(paths, key=get_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "#1 assay_epiclass\n",
    "#2 assay_epiclass_encode\n",
    "#9 harmonized_biomaterial_type\n",
    "#3 harmonized_donor_sex (trinary)\n",
    "#6 harmonized_sample_disease_high\n",
    "#6 harmonized_sample_cancer_high\n",
    "#10 paired_end\n",
    "#5 groups_second_level_name, no “mixed.mixed”\n",
    "#4 harmonized_sample_ontology_intermediate\n",
    "#12 random_16c\n",
    "#8 project\n",
    "#11 track_type\n",
    "#7 harmonized_donor_life_stage\n",
    "#13 complete_no_valid_oversample/predictions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dict = parse_instructions(instructions)\n",
    "sorted_paths = fuzzy_sort_paths(valid_pred_files, order_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for elem in sorted(order_dict.items(), key=lambda x: x[1]):\n",
    "#     print(elem)\n",
    "\n",
    "# for i, path in enumerate(sorted_paths):\n",
    "#     print(i, str(path).split(\"/\")[-4:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename(path: Path) -> str:\n",
    "    \"\"\"Create filename from important path information.\"\"\"\n",
    "    if \"predictions\" in str(path):\n",
    "        important_names = [path.name for path in list(path.parents)[0:4][::-1]]\n",
    "    else:\n",
    "        important_names = [path.name for path in list(path.parents)[0:3][::-1]]\n",
    "\n",
    "    for important_name in important_names:\n",
    "        if \"encode\" in important_name:\n",
    "            important_names.remove(important_name)\n",
    "            important_names.insert(0, \"encode\")\n",
    "        elif \"hg38_100kb_all_none\" in important_name:\n",
    "            important_names.remove(important_name)\n",
    "\n",
    "    name = \"_\".join(important_names)\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in sorted_paths:\n",
    "    print(create_filename(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scp_client = None\n",
    "\n",
    "# new_sorted_paths = []\n",
    "# for input_file in sorted_paths:\n",
    "#     input_file = Path(input_file)\n",
    "# new_filename = f\"{create_filename(input_file)}.csv\"\n",
    "#     if not (results_base_dir / new_filename).is_file():\n",
    "#         files = [f\"{narval_base_dir}/{input_file}\", f\"{results_base_dir}/{new_filename}\"]\n",
    "#         try:\n",
    "#             scp_client.get(*files)\n",
    "#         except AttributeError:\n",
    "#             print(\"Creating new scp client\")\n",
    "#             scp_client = createSCPClient(\n",
    "#                 createSSHClient(\"narval.computecanada.ca\", 22, \"rabyj\")\n",
    "#             )\n",
    "#             scp_client.get(*files)\n",
    "#     new_path = results_base_dir / new_filename\n",
    "#     new_sorted_paths.append(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in sorted_paths:\n",
    "    print(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_script = CURRENT_DIR.parent / \"augment_predict_file.py\"\n",
    "metadata_file = (\n",
    "    Path.home()\n",
    "    / \"projects/epilap/input/metadata/dfreeze-v2/hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2.json\"\n",
    ")\n",
    "meta_df = pd.DataFrame.from_records(\n",
    "    list(Metadata(metadata_file).datasets), index=\"md5sum\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sorted_paths = []\n",
    "for input_file in list(sorted_paths):\n",
    "    new_file = input_file.parent / Path(input_file.stem + \"_augmented.csv\")  # type: ignore\n",
    "    if new_file.is_file():\n",
    "        new_sorted_paths.append(new_file)\n",
    "        continue\n",
    "\n",
    "    print(f\"Creating {new_file}\")\n",
    "    args = [\n",
    "        \"python\",\n",
    "        str(python_script),\n",
    "        str(input_file),\n",
    "        str(metadata_file),\n",
    "        \"--compute-coherence\",\n",
    "    ]\n",
    "    subprocess.check_output(args=args)\n",
    "\n",
    "    if not new_file.is_file():\n",
    "        raise FileNotFoundError(f\"Did not create {new_file}.\")\n",
    "    new_sorted_paths.append(new_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for input_file in new_sorted_paths:\n",
    "    df_name = create_filename(input_file)\n",
    "    try:\n",
    "        df = pd.read_csv(input_file, index_col=\"md5sum\", low_memory=False)\n",
    "    except ValueError as err:\n",
    "        print(f\"Error reading {input_file}: {err}\")\n",
    "        continue\n",
    "\n",
    "    df.dropna(axis=1, how=\"all\")\n",
    "    if df_name in dfs:\n",
    "        raise ValueError(\n",
    "            f\"Conflicting names from {input_file}: {df_name} file already exists.\"\n",
    "        )\n",
    "\n",
    "    dfs[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in list(dfs.items()):\n",
    "    df = remove_pred_vector(df)\n",
    "    dfs[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns\n",
    "for name, df in dfs.items():\n",
    "    df.replace(to_replace=[\"--empty--\", \"\", \"NA\", None], value=np.nan, inplace=True)\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    dfs[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all different columns have unique relevant names, hardcoded 13 work only on output of augmented files with added coherence columns\n",
    "# https://stackoverflow.com/questions/38101009/changing-multiple-column-names-but-not-all-of-them-pandas-python\n",
    "# 13 without split_nb col, 14 with.\n",
    "nb_diff_columns = 14\n",
    "old_names = list(dfs.values())[0].columns[-nb_diff_columns:]\n",
    "for cat, df in dfs.items():\n",
    "    new_names = [old_name + f\" {cat}\" for old_name in old_names if name[-1] != \"n\"]\n",
    "    df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "    dfs[cat] = df\n",
    "    # print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge encode and epiatlas df, encode metadata is not redundant with epiatlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key1 = \"assay_epiclass_1l_3000n_11c_10fold-oversampling\"\n",
    "df_key2 = \"encode_assay_epiclass_1l_3000n_10fold-oversampling\"\n",
    "df_key3 = \"partial_merge\"\n",
    "\n",
    "partial_merge = merge_dataframes(dfs[df_key1], dfs[df_key2])\n",
    "\n",
    "dfs[df_key3] = partial_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in [df_key1, df_key2, df_key3]:\n",
    "#     df = dfs[name]\n",
    "#     print(name, df.shape)\n",
    "#     # print(df.index.name)\n",
    "#     display(df[\"assay_epiclass\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name in [df_key1, df_key2]:\n",
    "    try:\n",
    "        del dfs[df_name]\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in dfs.items():\n",
    "    if any(df[\"assay_epiclass\"].isnull()):\n",
    "        print(f\"assay_epiclass is null in {df_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all the rest of dataframes\n",
    "\n",
    "starting with biggest dataframes first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = sorted(list(dfs.values()), key=lambda x: len(x), reverse=True)\n",
    "df_final = functools.reduce(merge_dataframes, df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in df_final.columns:\n",
    "#     print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.merge(\n",
    "    meta_df, left_index=True, right_index=True, how=\"inner\", suffixes=(\"\", \"_delete\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate metadata columns (those that end by _delete)\n",
    "df_final = df_final.filter(regex=r\"^(?:(?!_delete).)+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-arrange columns\n",
    "all_columns = df_final.columns.tolist()\n",
    "\n",
    "# Separate metadata and result columns\n",
    "result_columns = [col for col in all_columns if col.rsplit(\" \", 1)[0] in old_names]\n",
    "meta_columns = [col for col in all_columns if col not in result_columns]\n",
    "\n",
    "new_order = meta_columns + result_columns\n",
    "df_final = df_final[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"STOP HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add ChrY/X coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(OUTPUT_PATH, index_col=\"md5sum\", low_memory=False)\n",
    "\n",
    "chrY_path = (\n",
    "    Path.home()\n",
    "    / \"Projects/epilap/output/logs/epiatlas-dfreeze-v2.1/chrY_coverage_results/chrY_coverage_zscores.csv\"\n",
    ")\n",
    "df_chrY = pd.read_csv(chrY_path, index_col=\"filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.shape, df_chrY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_final = df_final.join(df_chrY, how=\"left\")\n",
    "assert new_final.shape == (\n",
    "    df_final.shape[0],\n",
    "    df_final.shape[1] + df_chrY.shape[1],\n",
    ")  # same number as og samples, but more columns\n",
    "new_final.to_csv(OUTPUT_PATH.parent / \"merged_pred_results_all_2.1_chrY_zscores.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-epilap-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
