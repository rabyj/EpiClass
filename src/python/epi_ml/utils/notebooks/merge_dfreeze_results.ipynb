{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Notebook to work on proper way to merge a lot of already augmented output files.\"\"\"\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, unused-import, pointless-statement, unreachable,unnecessary-lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import collections\n",
    "import functools\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "from IPython.display import display\n",
    "\n",
    "from epi_ml.utils.ssh_utils import createSCPClient, createSSHClient, run_commands_via_ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_base_dir = (Path.home() / \"mounts/narval-mount/logs-dfreeze-2.1\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "narval_base_dir = \"~/logs-dfreeze-2.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd1 = f\"cd {narval_base_dir} && find . -mindepth 3 -maxdepth 5 -type f -name full-10fold-validation_prediction_augmented-all.csv\"\n",
    "cmd2 = f\"cd {narval_base_dir} && find . -mindepth 5 -maxdepth 6 -type f -name *test*augmented*.csv\"\n",
    "cmd_results = run_commands_via_ssh(\n",
    "    cmds=[cmd1, cmd2],\n",
    "    username=\"rabyj\",\n",
    "    hostname=\"narval.computecanada.ca\",\n",
    "    port=22,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_files = [cmd_result.splitlines() for cmd_result in cmd_results]\n",
    "valid_pred_files = valid_pred_files[0] + valid_pred_files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in valid_pred_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path.home() / \"downloads\" / \"merged_pred_results_blblbllblb.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_dirs = [\n",
    "    \"noFC\",\n",
    "    \"raw\",\n",
    "    \"pval\",\n",
    "    \"l1\",\n",
    "    \"harmonized_donor_sex_1l_3000n/no-mixed\",\n",
    "    \"groups_second_level_name_1l_3000n/w-mix\",\n",
    "    \"w-unknown\",\n",
    "    \"10fold-2\",\n",
    "    \"10fold-oversampling2\",\n",
    "    \"10fold-oversample2\",\n",
    "    \"random_1l_3000n/10fold-11c\",\n",
    "]\n",
    "valid_pred_files = [Path(file) for file in valid_pred_files]\n",
    "valid_pred_files = [\n",
    "    file\n",
    "    for file in valid_pred_files\n",
    "    if all(name not in str(file) for name in invalid_dirs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = collections.defaultdict(list)\n",
    "for file in valid_pred_files:\n",
    "    categories[file.parent.parent].append(file.parent.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampling_dirs = []\n",
    "for folder, result_list in categories.items():\n",
    "    if any(\n",
    "        result in [\"10fold-oversampling\", \"10fold-oversample\"] for result in result_list\n",
    "    ):\n",
    "        oversampling_dirs.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampling_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-oversampling results when w-oversampling also exits\n",
    "for file in list(valid_pred_files):\n",
    "    # sanity check\n",
    "    if (file.parent.name == \"10fold\") != file.parent.stem.endswith(\"10fold\"):\n",
    "        raise ValueError(f\"wat: {str(file)}\")\n",
    "\n",
    "    if file.parent.parent in oversampling_dirs and file.parent.name == \"10fold\":\n",
    "        print(f\"Removing {file}\")\n",
    "        valid_pred_files.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(valid_pred_files))\n",
    "for file in valid_pred_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order paths in desired order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_instructions(instructions: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Parse the instructions from A and return a dictionary with keys and their orders.\n",
    "\n",
    "    Args:\n",
    "        instructions (str): The instructions from A.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: Dictionary containing the keys and their orders.\n",
    "    \"\"\"\n",
    "    order_dict = {}\n",
    "    for line in instructions.strip().split(\"\\n\"):\n",
    "        if line.startswith(\"#\"):\n",
    "            match = re.match(r\"#(\\d+)\", line)\n",
    "            if match:\n",
    "                order = int(match.group(1))\n",
    "                key = re.search(r\"[* ]([a-zA-Z_]+)\", line[match.end() :]).group(1)\n",
    "                order_dict[key] = order\n",
    "    return order_dict\n",
    "\n",
    "\n",
    "def fuzzy_sort_paths(paths: List[Path], order_dict: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Sort a list of paths based on the fuzzy matching with keys from an order dictionary.\n",
    "\n",
    "    Args:\n",
    "        paths (List[str]): The list of paths to sort.\n",
    "        order_dict (Dict[str, int]): The dictionary containing keys and their orders.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of paths sorted according to their best fuzzy-matched keys.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_order(path: Path) -> int:\n",
    "        parent_names = [parent.name for parent in path.parents]\n",
    "        key = \"/\".join(parent_names[0:3][::-1])\n",
    "        best_match, _ = process.extractOne(key, order_dict.keys())\n",
    "        return order_dict.get(best_match, 9999)\n",
    "\n",
    "    return sorted(paths, key=get_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "#1 assay_epiclass\n",
    "#2 assay_epiclass_encode\n",
    "#9 harmonized_biomaterial_type\n",
    "#3 harmonized_donor_sex (trinary)\n",
    "#6 harmonized_sample_disease_high\n",
    "#6 harmonized_sample_cancer_high\n",
    "#10 paired_end\n",
    "#5 groups_second_level_name, no “mixed.mixed”\n",
    "#4 harmonized_sample_ontology_intermediate\n",
    "#12 random_16c\n",
    "#8 project\n",
    "#11 track_type\n",
    "#7 harmonized_donor_life_stage\n",
    "#13 complete_no_valid_oversample/predictions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dict = parse_instructions(instructions)\n",
    "sorted_paths = fuzzy_sort_paths(valid_pred_files, order_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for elem in sorted(order_dict.items(), key=lambda x: x[1]):\n",
    "#     print(elem)\n",
    "\n",
    "# for i, path in enumerate(sorted_paths):\n",
    "#     print(i, str(path).split(\"/\")[-4:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename(path: Path) -> str:\n",
    "    \"\"\"Create filename from important path information.\"\"\"\n",
    "    if \"predictions\" in str(path):\n",
    "        important_names = [path.name for path in list(path.parents)[0:4][::-1]]\n",
    "    else:\n",
    "        important_names = [path.name for path in list(path.parents)[0:3][::-1]]\n",
    "\n",
    "    for important_name in important_names:\n",
    "        if \"encode\" in important_name:\n",
    "            important_names.remove(important_name)\n",
    "            important_names.insert(0, \"encode\")\n",
    "        elif \"hg38_100kb_all_none\" in important_name:\n",
    "            important_names.remove(important_name)\n",
    "\n",
    "    name = \"_\".join(important_names)\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in sorted_paths:\n",
    "#     print(create_filename(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scp_client = None\n",
    "\n",
    "results_base_dir = (\n",
    "    Path.home()\n",
    "    / \"Projects/epilap/output/logs/epiatlas-dfreeze-v2.1/merged_results/input/\"\n",
    ")\n",
    "new_sorted_paths = []\n",
    "for input_file in sorted_paths:\n",
    "    input_file = Path(input_file)\n",
    "    new_filename = f\"{create_filename(input_file)}.csv\"\n",
    "    if not (results_base_dir / new_filename).is_file():\n",
    "        try:\n",
    "            scp_client.get(\n",
    "                f\"{narval_base_dir}/{input_file}\", f\"{results_base_dir}/{new_filename}\"\n",
    "            )\n",
    "        except AttributeError:\n",
    "            scp_client = createSCPClient(\n",
    "                createSSHClient(\"narval.computecanada.ca\", 22, \"rabyj\")\n",
    "            )\n",
    "    new_path = results_base_dir / new_filename\n",
    "    new_sorted_paths.append(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_file in new_sorted_paths:\n",
    "    print(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for input_file in new_sorted_paths:\n",
    "    df_name = input_file.name.split(\".\")[0]\n",
    "    df = pd.read_csv(input_file, index_col=\"md5sum\", low_memory=False)\n",
    "    df.name = df_name\n",
    "\n",
    "    df.dropna(axis=1, how=\"all\")\n",
    "    if df_name in dfs:\n",
    "        raise ValueError(\n",
    "            f\"Conflicting names from {input_file}: {df_name} file already exists.\"\n",
    "        )\n",
    "\n",
    "    dfs[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove detail of prediction probabilities\n",
    "col1 = \"1rst/2nd prob ratio\"\n",
    "col2 = \"files/epiRR\"\n",
    "for cat, df in dfs.items():\n",
    "    column_names = df.columns\n",
    "    try:\n",
    "        cut_pos_1 = column_names.get_loc(col1)\n",
    "        cut_pos_2 = column_names.get_loc(col2)\n",
    "        df = df.drop(df.columns[cut_pos_1 + 1 : cut_pos_2], axis=1)\n",
    "        df = df.drop(columns=[\"EpiRR\", \"md5sum.1\"])\n",
    "    except KeyError:\n",
    "        print(\"df seems already reduced\")\n",
    "\n",
    "    dfs[cat] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns\n",
    "for cat, df in dfs.items():\n",
    "    df.replace(to_replace=[\"--empty--\", \"\", \"NA\", None], value=np.nan, inplace=True)\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    dfs[cat] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all different columns have unique relevant names\n",
    "# https://stackoverflow.com/questions/38101009/changing-multiple-column-names-but-not-all-of-them-pandas-python\n",
    "nb_diff_columns = 13\n",
    "old_names = list(dfs.values())[0].columns[-nb_diff_columns:]\n",
    "for cat, df in dfs.items():\n",
    "    new_names = [name + f\" {cat}\" for name in old_names if name[-1] != \"n\"]\n",
    "    df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "    df.name = cat\n",
    "    dfs[cat] = df\n",
    "    # print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge two DataFrames by concatenating along the index, aligning common columns\n",
    "    and appending non-common columns.\n",
    "\n",
    "    Parameters:\n",
    "    df1 (pd.DataFrame): The first DataFrame\n",
    "    df2 (pd.DataFrame): The second DataFrame\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Merged DataFrame with the index name preserved.\n",
    "    \"\"\"\n",
    "    print(f\"Entering shapes: {df1.shape}, {df2.shape}\")\n",
    "    if df1.index.name != df2.index.name:\n",
    "        raise ValueError(\n",
    "            f\"Index names are different: {df1.index.name} != {df2.index.name}\"\n",
    "        )\n",
    "\n",
    "    result = pd.merge(df1, df2, left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "    result.index.name = df1.index.name\n",
    "    print(f\"Output shape: {result.shape}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge encode and epiatlas df, encode metadata is not redundant with epiatlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key1 = \"assay_epiclass_1l_3000n_11c_10fold-oversampling\"\n",
    "df_key2 = \"encode_assay_epiclass_1l_3000n_10fold-oversampling\"\n",
    "df_key3 = \"partial_merge\"\n",
    "\n",
    "partial_merge = merge_dataframes(dfs[df_key1], dfs[df_key2])\n",
    "\n",
    "dfs[df_key3] = partial_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in partial_merge.columns:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [df_key1, df_key2, df_key3]:\n",
    "    df = dfs[name]\n",
    "    display(name, df.shape)\n",
    "    print(df.index.name)\n",
    "    display(df[\"assay_epiclass\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name in [df_key1, df_key2]:\n",
    "    try:\n",
    "        del dfs[df_name]\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in dfs.items():\n",
    "    if any(df[\"assay_epiclass\"].isnull()):\n",
    "        print(f\"assay_epiclass is null in {df_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge all dataframes\n",
    "\n",
    "starting with biggest dataframes first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = sorted(list(dfs.values()), key=lambda x: len(x), reverse=True)\n",
    "df_final = functools.reduce(\n",
    "    lambda left, right: pd.merge(\n",
    "        left, right, on=\"md5sum\", how=\"outer\", suffixes=(\"\", \"_delete\")\n",
    "    ),\n",
    "    df_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in df_final.columns:\n",
    "#     print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate metadata columns (those that end by _delete)\n",
    "df_final = df_final.filter(regex=r\"^(?:(?!_delete).)+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-arrange columns\n",
    "all_columns = df_final.columns.tolist()\n",
    "\n",
    "# Separate metadata and result columns\n",
    "result_columns = [col for col in all_columns if col.rsplit(\" \", 1)[0] in old_names]\n",
    "meta_columns = [col for col in all_columns if col not in result_columns]\n",
    "\n",
    "new_order = meta_columns + result_columns\n",
    "df_final = df_final[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"STOP HERE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add ChrY/X coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(OUTPUT_PATH, index_col=\"md5sum\", low_memory=False)\n",
    "df_chrY = pd.read_csv(\n",
    "    Path.home() / \"downloads\" / \"temp\" / \"coverage_combined.csv\", index_col=\"filename\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_final.shape, df_chrY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_final = df_final.join(df_chrY, how=\"left\")\n",
    "assert new_final.shape == (\n",
    "    df_final.shape[0],\n",
    "    df_final.shape[1] + df_chrY.shape[1],\n",
    ")  # same number as og samples, but more columns\n",
    "new_final.to_csv(OUTPUT_PATH.parent / \"merged_pred_results_chrY.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-epilap-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
