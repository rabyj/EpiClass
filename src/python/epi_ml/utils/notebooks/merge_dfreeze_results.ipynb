{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Notebook to work on proper way to merge a lot of already augmented output files.\"\"\"\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_base_dir = (\n",
    "    Path.home() / \"projects/epilap/output/logs/epiatlas-dfreeze-v2.1\"\n",
    ").resolve()\n",
    "base_dir_1 = gen_base_dir / \"hg38_100kb_all_none/general_results\"\n",
    "base_dir_2 = gen_base_dir / \"hg38_100kb_all_none_w_encode_noncore\"\n",
    "valid_pred_files = list(\n",
    "    base_dir_1.rglob(\"full-10fold-validation_prediction_augmented-all.csv\")\n",
    ")\n",
    "valid_pred_files += list(\n",
    "    base_dir_2.rglob(\"full-10fold-validation_prediction_augmented-all.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_dirs = [\"oversampling\", \"noFC\", \"raw\", \"pval\", \"l1\", \"11c\", \"w-mix\"]\n",
    "valid_pred_files = [\n",
    "    file\n",
    "    for file in valid_pred_files\n",
    "    if all(name not in str(file) for name in invalid_dirs)\n",
    "]\n",
    "print(len(valid_pred_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in valid_pred_files:\n",
    "    print(path.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(gen_base_dir) / \"merged_pred_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_files = sorted_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename(path: Path) -> str:\n",
    "    \"\"\"Create filename from path.\"\"\"\n",
    "    category = path.parents[1].name\n",
    "\n",
    "    pattern = r\"10fold-?(.+)?/\"\n",
    "    extra_info = re.search(pattern, str(path)).group(1)\n",
    "    if extra_info is None:\n",
    "        extra_info = \"\"\n",
    "\n",
    "    if \"encode\" in str(path):\n",
    "        extra_info += \"encode\"\n",
    "\n",
    "    if extra_info:\n",
    "        name = f\"{category}_{extra_info}\"\n",
    "    else:\n",
    "        name = category\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for input_file in valid_pred_files:\n",
    "    input_file = Path(input_file)\n",
    "\n",
    "    df_name = create_filename(input_file)\n",
    "\n",
    "    df = pd.read_csv(input_file, index_col=\"md5sum\", low_memory=False)\n",
    "    df.name = df_name\n",
    "\n",
    "    df.dropna(axis=1, how=\"all\")\n",
    "    if df_name in dfs:\n",
    "        raise ValueError(\n",
    "            f\"Conflicting names from {input_file}: {df_name} file already exists.\"\n",
    "        )\n",
    "\n",
    "    dfs[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cols(dfs: Dict):\n",
    "    \"\"\"Print all columns of first df in dict.\"\"\"\n",
    "    a_df = list(dfs.values())[0]\n",
    "    print(a_df.shape)\n",
    "    for column in a_df.columns:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove detail of prediction probabilities\n",
    "col1 = \"1rst/2nd prob ratio\"\n",
    "col2 = \"files/epiRR\"\n",
    "for cat, df in dfs.items():\n",
    "    column_names = df.columns\n",
    "    try:\n",
    "        cut_pos_1 = column_names.get_loc(col1)\n",
    "        cut_pos_2 = column_names.get_loc(col2)\n",
    "        df = df.drop(df.columns[cut_pos_1 + 1 : cut_pos_2], axis=1)\n",
    "        df = df.drop(columns=[\"EpiRR\", \"md5sum.1\"])\n",
    "    except KeyError:\n",
    "        print(\"df seems already reduced\")\n",
    "\n",
    "    dfs[cat] = df\n",
    "    print(df.shape)\n",
    "    # if df.shape[1] != 72:\n",
    "    #     print(df.name)\n",
    "    #     raise ValueError(\"wtf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns\n",
    "for cat, df in dfs.items():\n",
    "    df.replace(to_replace=[\"--empty--\", \"\", \"NA\", None], value=np.nan, inplace=True)\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    dfs[cat] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all different columns have unique relevant names\n",
    "# https://stackoverflow.com/questions/38101009/changing-multiple-column-names-but-not-all-of-them-pandas-python\n",
    "nb_diff_columns = 13\n",
    "for cat, df in dfs.items():\n",
    "    old_names = df.columns[-nb_diff_columns:]\n",
    "    new_names = [name + f\" {cat}\" for name in old_names if name[-1] != \"n\"]\n",
    "    df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "    df.name = cat\n",
    "    dfs[cat] = df\n",
    "    # print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes\n",
    "df_list = list(dfs.values())\n",
    "df_final = functools.reduce(\n",
    "    lambda left, right: pd.merge(\n",
    "        left, right, on=\"md5sum\", how=\"outer\", suffixes=(None, \"_x\")\n",
    "    ),\n",
    "    df_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate metadata columns (those that end by _x)\n",
    "filtered_final = df_final.filter(regex=r\"^(?:(?!_x).)+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = filtered_final.columns.values\n",
    "display(len(cols), cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_final.to_csv(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra path ordering stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "def parse_instructions(instructions: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Parse the instructions from A and return a dictionary with keys and their orders.\n",
    "\n",
    "    Args:\n",
    "        instructions (str): The instructions from A.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: Dictionary containing the keys and their orders.\n",
    "    \"\"\"\n",
    "    order_dict = {}\n",
    "    for line in instructions.strip().split(\"\\n\"):\n",
    "        if line.startswith(\"#\"):\n",
    "            match = re.match(r\"#(\\d+)\", line)\n",
    "            if match:\n",
    "                order = int(match.group(1))\n",
    "                key = re.search(r\"[* ]([a-zA-Z_]+)\", line[match.end() :]).group(1)\n",
    "                order_dict[key] = order\n",
    "    return order_dict\n",
    "\n",
    "\n",
    "def fuzzy_sort_paths(paths: List[Path], order_dict: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Sort a list of paths based on the fuzzy matching with keys from an order dictionary.\n",
    "\n",
    "    Args:\n",
    "        paths (List[str]): The list of paths to sort.\n",
    "        order_dict (Dict[str, int]): The dictionary containing keys and their orders.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of paths sorted according to their best fuzzy-matched keys.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_order(path: Path) -> int:\n",
    "        key = path.parents[1].name\n",
    "        best_match, _ = process.extractOne(key, order_dict.keys())\n",
    "        return order_dict.get(best_match, 9999)\n",
    "\n",
    "    return sorted(paths, key=get_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "#1 assay_epiclass\n",
    "#2 assay_epiclass_encode\n",
    "#9 harmonized_biomaterial_type\n",
    "#3 harmonized_donor_sex (binary)\n",
    "#6 harmonized_sample_disease_high\n",
    "#10 paired_end\n",
    "#5 groups_second_level_name, no “mixed.mixed”\n",
    "#4 harmonized_sample_ontology_intermediate\n",
    "#12 random_16c\n",
    "#8 project\n",
    "#11 track_type\n",
    "#7 harmonized_donor_life_stage\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dict = parse_instructions(instructions)\n",
    "sorted_paths = fuzzy_sort_paths(valid_pred_files, order_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in sorted(order_dict.items(), key=lambda x: x[1]):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, path in enumerate(sorted_paths):\n",
    "    print(i, path.parents[1].name, path.parent.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-epilap-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
