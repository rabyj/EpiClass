{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Notebook to work on proper way to merge a lot of already augmented output files.\"\"\"\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, unused-import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import re\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_base_dir = (Path.home() / \"mounts/narval-mount/logs-dfreeze-2.1\").resolve()\n",
    "\n",
    "valid_pred_files = (\n",
    "    subprocess.check_output(\n",
    "        (\n",
    "            \"find\",\n",
    "            f\"{gen_base_dir}\",\n",
    "            \"-mindepth\",\n",
    "            \"3\",\n",
    "            \"-maxdepth\",\n",
    "            \"5\",\n",
    "            \"-type\",\n",
    "            \"f\",\n",
    "            \"-name\",\n",
    "            \"full-10fold-validation_prediction_augmented-all.csv\",\n",
    "        )\n",
    "    )\n",
    "    .decode(\"utf-8\")\n",
    "    .splitlines()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path.home() / \"downloads\" / \"merged_pred_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_dirs = [\n",
    "    \"noFC\",\n",
    "    \"raw\",\n",
    "    \"pval\",\n",
    "    \"l1\",\n",
    "    \"harmonized_donor_sex_1l_3000n/no-mixed\",\n",
    "    \"groups_second_level_name_1l_3000n/w-mix\",\n",
    "    \"w-unknown\",\n",
    "    \"10fold-2\",\n",
    "    \"10fold-oversampling2\",\n",
    "    \"10fold-oversample2\",\n",
    "    \"random_1l_3000n/10fold-11c\",\n",
    "]\n",
    "valid_pred_files = [Path(file) for file in valid_pred_files]\n",
    "valid_pred_files = [\n",
    "    file\n",
    "    for file in valid_pred_files\n",
    "    if all(name not in str(file) for name in invalid_dirs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = collections.defaultdict(list)\n",
    "for file in valid_pred_files:\n",
    "    categories[file.parent.parent].append(file.parent.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampling_dirs = []\n",
    "for folder, result_list in categories.items():\n",
    "    if any(\n",
    "        result in [\"10fold-oversampling\", \"10fold-oversample\"] for result in result_list\n",
    "    ):\n",
    "        oversampling_dirs.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in valid_pred_files:\n",
    "    if file.parent.parent in oversampling_dirs and file.parent.stem.endswith(\"10fold\"):\n",
    "        valid_pred_files.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in valid_pred_files:\n",
    "#     print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order paths in desired order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "def parse_instructions(instructions: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Parse the instructions from A and return a dictionary with keys and their orders.\n",
    "\n",
    "    Args:\n",
    "        instructions (str): The instructions from A.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: Dictionary containing the keys and their orders.\n",
    "    \"\"\"\n",
    "    order_dict = {}\n",
    "    for line in instructions.strip().split(\"\\n\"):\n",
    "        if line.startswith(\"#\"):\n",
    "            match = re.match(r\"#(\\d+)\", line)\n",
    "            if match:\n",
    "                order = int(match.group(1))\n",
    "                key = re.search(r\"[* ]([a-zA-Z_]+)\", line[match.end() :]).group(1)\n",
    "                order_dict[key] = order\n",
    "    return order_dict\n",
    "\n",
    "\n",
    "def fuzzy_sort_paths(paths: List[Path], order_dict: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Sort a list of paths based on the fuzzy matching with keys from an order dictionary.\n",
    "\n",
    "    Args:\n",
    "        paths (List[str]): The list of paths to sort.\n",
    "        order_dict (Dict[str, int]): The dictionary containing keys and their orders.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of paths sorted according to their best fuzzy-matched keys.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_order(path: Path) -> int:\n",
    "        parent_names = [parent.name for parent in path.parents]\n",
    "        key = \"/\".join(parent_names[0:3][::-1])\n",
    "        print(key)\n",
    "        best_match, _ = process.extractOne(key, order_dict.keys())\n",
    "        return order_dict.get(best_match, 9999)\n",
    "\n",
    "    return sorted(paths, key=get_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "#1 assay_epiclass\n",
    "#2 assay_epiclass_encode\n",
    "#9 harmonized_biomaterial_type\n",
    "#3 harmonized_donor_sex (trinary)\n",
    "#6 harmonized_sample_disease_high\n",
    "#6 harmonized_sample_cancer_high\n",
    "#10 paired_end\n",
    "#5 groups_second_level_name, no “mixed.mixed”\n",
    "#4 harmonized_sample_ontology_intermediate\n",
    "#12 random_16c\n",
    "#8 project\n",
    "#11 track_type\n",
    "#7 harmonized_donor_life_stage\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dict = parse_instructions(instructions)\n",
    "sorted_paths = fuzzy_sort_paths(valid_pred_files, order_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for elem in sorted(order_dict.items(), key=lambda x: x[1]):\n",
    "#     print(elem)\n",
    "\n",
    "# for i, path in enumerate(sorted_paths):\n",
    "#     print(i, str(path).split(\"/\")[-4:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cols(dfs: Dict):\n",
    "    \"\"\"Print all columns of first df in dict.\"\"\"\n",
    "    a_df = list(dfs.values())[0]\n",
    "    print(a_df.shape)\n",
    "    for column in a_df.columns:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename(path: Path) -> str:\n",
    "    \"\"\"Create filename from important path information.\"\"\"\n",
    "    important_names = [path.name for path in list(path.parents)[0:3][::-1]]\n",
    "    for important_name in important_names:\n",
    "        if \"hg38_100kb_all_none\" in important_name:\n",
    "            important_names.remove(important_name)\n",
    "\n",
    "    name = \"_\".join(important_names)\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for input_file in sorted_paths:\n",
    "    input_file = Path(input_file)\n",
    "\n",
    "    df_name = create_filename(input_file)\n",
    "\n",
    "    df = pd.read_csv(input_file, index_col=\"md5sum\", low_memory=False)\n",
    "    df.name = df_name\n",
    "\n",
    "    df.dropna(axis=1, how=\"all\")\n",
    "    if df_name in dfs:\n",
    "        raise ValueError(\n",
    "            f\"Conflicting names from {input_file}: {df_name} file already exists.\"\n",
    "        )\n",
    "\n",
    "    dfs[df_name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove detail of prediction probabilities\n",
    "col1 = \"1rst/2nd prob ratio\"\n",
    "col2 = \"files/epiRR\"\n",
    "for cat, df in dfs.items():\n",
    "    column_names = df.columns\n",
    "    try:\n",
    "        cut_pos_1 = column_names.get_loc(col1)\n",
    "        cut_pos_2 = column_names.get_loc(col2)\n",
    "        df = df.drop(df.columns[cut_pos_1 + 1 : cut_pos_2], axis=1)\n",
    "        df = df.drop(columns=[\"EpiRR\", \"md5sum.1\"])\n",
    "    except KeyError:\n",
    "        print(\"df seems already reduced\")\n",
    "\n",
    "    dfs[cat] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns\n",
    "for cat, df in dfs.items():\n",
    "    df.replace(to_replace=[\"--empty--\", \"\", \"NA\", None], value=np.nan, inplace=True)\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    dfs[cat] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all different columns have unique relevant names\n",
    "# https://stackoverflow.com/questions/38101009/changing-multiple-column-names-but-not-all-of-them-pandas-python\n",
    "nb_diff_columns = 13\n",
    "old_names = list(dfs.values())[0].columns[-nb_diff_columns:]\n",
    "for cat, df in dfs.items():\n",
    "    new_names = [name + f\" {cat}\" for name in old_names if name[-1] != \"n\"]\n",
    "    df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "    df.name = cat\n",
    "    dfs[cat] = df\n",
    "    # print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes\n",
    "df_list = list(dfs.values())\n",
    "df_final = functools.reduce(\n",
    "    lambda left, right: pd.merge(\n",
    "        left, right, on=\"md5sum\", how=\"outer\", suffixes=(\"\", \"_delete\")\n",
    "    ),\n",
    "    df_list,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate metadata columns (those that end by _delete)\n",
    "df_final = df_final.filter(regex=r\"^(?:(?!_delete).)+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-arrange columns\n",
    "all_columns = df_final.columns.tolist()\n",
    "\n",
    "# Separate metadata and result columns\n",
    "result_columns = [col for col in all_columns if col.rsplit(\" \", 1)[0] in old_names]\n",
    "meta_columns = [col for col in all_columns if col not in result_columns]\n",
    "\n",
    "new_order = meta_columns + result_columns\n",
    "df_final = df_final[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-epilap-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
