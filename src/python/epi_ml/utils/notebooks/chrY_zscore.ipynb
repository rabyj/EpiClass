{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute chrY z-scores with regards to different distributions, and plot the results.\"\"\"\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, pointless-statement, use-dict-literal, expression-not-assigned, unused-import, too-many-lines, unreachable\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from epi_ml.core.metadata import Metadata\n",
    "from epi_ml.utils.general_utility import get_valid_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSAY = \"assay_epiclass\"\n",
    "SEX = \"harmonized_donor_sex\"\n",
    "TRACK = \"track_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epilap_project = Path.home() / \"Projects/epilap\"\n",
    "\n",
    "metadata = (\n",
    "    epilap_project\n",
    "    / \"input\"\n",
    "    / \"metadata\"\n",
    "    / \"dfreeze-v2\"\n",
    "    / \"hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2.json\"\n",
    ")\n",
    "\n",
    "logs = epilap_project / \"output/logs/epiatlas-dfreeze-v2.1\"\n",
    "chrY_coverage_file = logs / \"chrY_coverage_results\" / \"chrXY_coverage_all.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = Metadata(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chrY = pd.read_csv(chrY_coverage_file, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_chrY.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge metadata and chrY coverage info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.DataFrame.from_records(list(meta.datasets)).set_index(\"md5sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[ASSAY].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-core tracks\n",
    "meta_df = meta_df[~meta_df[ASSAY].str.contains(\"non-core|CTCF\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df_chrY.merge(meta_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_df.shape, df_chrY.shape, merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the merge\n",
    "assert (merged_df.shape[0] == meta_df.shape[0]) and (\n",
    "    df_chrY.shape[1] + meta_df.shape[1] == merged_df.shape[1]\n",
    "), \"not right shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in set(meta_df.index.values) - set(df_chrY.index.values):\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute chrY zscore coverage by assay and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby_columns = [ASSAY, SEX, TRACK]\n",
    "# groupby_columns = [ASSAY, SEX]\n",
    "groupby_columns = [ASSAY, TRACK]\n",
    "\n",
    "chrY_dists = merged_df.groupby(groupby_columns).agg({\"chrY\": [\"mean\", \"std\", \"count\"]})\n",
    "display(chrY_dists)\n",
    "# display(chrY_dists.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_name = \"assay_track\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrY_dists.to_csv(\n",
    "    logs / \"chrY_coverage_results\" / f\"chrY_coverage_distributions_{partial_name}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_name = f\"expected_{partial_name}_chrY_z-score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for row in chrY_dists.iterrows():\n",
    "    index, vals = row\n",
    "    # assay, sex, track = index # type: ignore\n",
    "    # assay, sex = index # type: ignore\n",
    "    # assay = index  # type: ignore\n",
    "    assay, track = index  # type: ignore\n",
    "\n",
    "    mean = vals[\"chrY\"][\"mean\"]\n",
    "    std = vals[\"chrY\"][\"std\"]\n",
    "    count = vals[\"chrY\"][\"count\"]\n",
    "\n",
    "    # partial_df = merged_df[(merged_df[ASSAY] == assay) & (merged_df[SEX] == sex) & (merged_df[TRACK] == track)]\n",
    "    # partial_df = merged_df[(merged_df[ASSAY] == assay) & (merged_df[SEX] == sex)]\n",
    "    # partial_df = merged_df[(merged_df[ASSAY] == assay)]\n",
    "    partial_df = merged_df[(merged_df[ASSAY] == assay) & (merged_df[TRACK] == track)]\n",
    "    partial_df = partial_df[[\"chrY\"]].copy()\n",
    "\n",
    "    partial_df.loc[:, score_name] = (partial_df[\"chrY\"] - mean) / std\n",
    "    partial_df.loc[:, f\"count_{score_name}\"] = count\n",
    "\n",
    "    new_data.append(partial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_zscores_df = pd.concat(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df_chrY.merge(\n",
    "    all_zscores_df, how=\"left\", left_index=True, right_index=True, suffixes=(\"\", \"_DROP\")\n",
    ").sort_index()\n",
    "final_df = final_df.drop(columns=[\"chrY_DROP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_path = (\n",
    "    logs / \"chrY_coverage_results\" / f\"chrY_coverage_zscores_{partial_name}.csv\"\n",
    ")\n",
    "final_df.to_csv(final_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map predictions to chrY z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_pred_dir = Path.home() / \"downloads\" / \"temp\"\n",
    "sex_pred_file = (\n",
    "    sex_pred_dir / \"sex3_oversample_full-10fold-validation_prediction_augmented-all.csv\"\n",
    ")\n",
    "\n",
    "full_chrY_df = pd.read_csv(final_df_path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.read_csv(sex_pred_file, index_col=0, header=0)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pred_df = pred_df.merge(\n",
    "    full_chrY_df, how=\"inner\", left_index=True, right_index=True, suffixes=(\"\", \"_DROP\")\n",
    ").sort_index()\n",
    "merged_pred_df = merged_pred_df.drop(\n",
    "    columns=[column for column in pred_df.columns if column.endswith(\"_DROP\")]\n",
    ")\n",
    "print(merged_pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise UserWarning(\"stop here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chrY z-scores confusion matrix for sex3 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS_DICT = {\"female\": \"red\", \"male\": \"blue\", \"mixed\": \"purple\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_pred_df.to_csv(sex_pred_dir / \"sex3_oversample_full-10fold-validation_prediction_augmented-all_chrY_zscores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([col for col in merged_pred_df.columns if \"z-score\" in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_pred_df = merged_pred_df[\n",
    "#     ~merged_pred_df[ASSAY].str.contains(pat=\"wgb\", case=False)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### graphs per assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_label = \"expected_assay_chrY_z-score\"\n",
    "count_label = f\"count_{coverage_label}\"\n",
    "\n",
    "classes = merged_pred_df[\"True class\"].unique()\n",
    "assays = merged_pred_df[ASSAY].unique()\n",
    "\n",
    "matrix_logdir = chrY_coverage_file = (\n",
    "    logs\n",
    "    / \"chrY_coverage_results\"\n",
    "    / \"10fold_valid\"\n",
    "    / \"z-score\"\n",
    "    / \"per_assay\"\n",
    "    / \"w-unknown\"\n",
    "    / \"per_assay_graph\"\n",
    ")\n",
    "\n",
    "for assay_label in assays:\n",
    "    assay_df = merged_pred_df[merged_pred_df[ASSAY] == assay_label]\n",
    "\n",
    "    # confusion matrix for chrY z-score\n",
    "    for threshold in [0, 0.7, 0.9]:\n",
    "        row = 1\n",
    "        col = 1\n",
    "        fig = make_subplots(\n",
    "            rows=3,\n",
    "            cols=3,\n",
    "            shared_yaxes=True,\n",
    "            x_title=\"Predicted class (nb of predictions)\",\n",
    "            y_title=\"z-score vs expected assay\",\n",
    "            row_titles=list(classes),\n",
    "            column_titles=list(classes),\n",
    "            vertical_spacing=0.08,\n",
    "            horizontal_spacing=0.01,\n",
    "        )\n",
    "        threshold_df = assay_df[assay_df[\"Max pred\"] >= threshold]\n",
    "\n",
    "        for label in classes:\n",
    "            df_label = threshold_df[threshold_df[\"True class\"] == label]\n",
    "\n",
    "            # Iterate over each target and add a violin plot for it\n",
    "            for target in classes:\n",
    "                sub_df = df_label[df_label[\"Predicted class\"] == target]\n",
    "\n",
    "                if sub_df.shape[0] == 0:\n",
    "                    y_values = [\n",
    "                        threshold_df[coverage_label].mean()\n",
    "                    ]  # Minimal synthetic data\n",
    "                    sample_count = 0\n",
    "                    hovertext = [\"PLACEHOLDER - NO DATA\"]\n",
    "                else:\n",
    "                    y_values = sub_df[coverage_label]\n",
    "                    hovertext = [\n",
    "                        f\"{md5sum, assay}:(z-score={z_score:.3f} (n={int(count)}), pred={pred:.3f})\"\n",
    "                        for md5sum, pred, z_score, count, assay in zip(\n",
    "                            sub_df.index,\n",
    "                            sub_df[\"Max pred\"],\n",
    "                            sub_df[coverage_label],\n",
    "                            sub_df[count_label],\n",
    "                            sub_df[ASSAY],\n",
    "                        )\n",
    "                    ]\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Violin(\n",
    "                        y=y_values,\n",
    "                        name=f\"{target} ({sub_df.shape[0]})\",\n",
    "                        box_visible=True,\n",
    "                        meanline_visible=True,\n",
    "                        points=\"all\",\n",
    "                        text=hovertext,\n",
    "                        line_color=COLORS_DICT[target],\n",
    "                        hovertemplate=\"%{text}\",\n",
    "                    ),\n",
    "                    row=row,\n",
    "                    col=col,\n",
    "                )\n",
    "\n",
    "                # Move to the next subplot position\n",
    "                col += 1\n",
    "                if col > 3:\n",
    "                    col = 1\n",
    "                    row += 1\n",
    "\n",
    "        # Update global layout and traces\n",
    "        fig.update_traces(marker=dict(size=1))\n",
    "        fig.update_yaxes(\n",
    "            range=[\n",
    "                min(assay_df[coverage_label]) - 0.01,\n",
    "                max(assay_df[coverage_label]) + 0.01,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Directly using annotations param does not work with make_subplots\n",
    "        existing_annotations = fig.layout.annotations\n",
    "        new_annotation = dict(\n",
    "            x=1.01,  # Position on the x-axis\n",
    "            y=0.5,  # Position on the y-axis\n",
    "            showarrow=False,  # Do not show arrow\n",
    "            text=\"Reference class\",  # The text you want to display\n",
    "            xref=\"paper\",  # 'x' coordinate is set in relative coordinates\n",
    "            yref=\"paper\",  # 'y' coordinate is set in relative coordinates\n",
    "            xanchor=\"left\",  # Text starts from the left of the x-coordinate\n",
    "            yanchor=\"middle\",  # Middle aligned vertically\n",
    "            font=dict(size=16),\n",
    "            textangle=90,\n",
    "        )\n",
    "        updated_annotations = list(existing_annotations) + [new_annotation]\n",
    "\n",
    "        title = f\"z-score(mean chrY coverage per file):{assay_label} (pred>{threshold})\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"{title} (n={threshold_df.shape[0]})\",\n",
    "            showlegend=False,\n",
    "            annotations=updated_annotations,\n",
    "        )\n",
    "\n",
    "        # fig.show()\n",
    "\n",
    "        title = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "        html_file = matrix_logdir / f\"{title}.html\"\n",
    "        png_file = matrix_logdir / f\"{title}.png\"\n",
    "        if not png_file.exists():\n",
    "            fig.write_image(png_file, scale=2)\n",
    "        if not html_file.exists():\n",
    "            fig.write_html(html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs per assay and track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_label = f\"expected_{partial_name}_chrY_z-score\"\n",
    "count_label = f\"count_{coverage_label}\"\n",
    "\n",
    "classes = merged_pred_df[\"True class\"].unique()\n",
    "assays = merged_pred_df[ASSAY].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_logdir = chrY_coverage_file = (\n",
    "    logs\n",
    "    / \"chrY_coverage_results\"\n",
    "    / \"10fold_valid\"\n",
    "    / \"z-score\"\n",
    "    / \"per_assay_track\"\n",
    "    / \"per_assay_track_confusion_graphs\"\n",
    ")\n",
    "matrix_logdir.mkdir(parents=False, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrices style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for assay_label in assays:\n",
    "    assay_df = merged_pred_df[merged_pred_df[ASSAY] == assay_label]\n",
    "\n",
    "    for track_type in assay_df[TRACK].unique():\n",
    "        assay_track_df = assay_df[assay_df[TRACK] == track_type]\n",
    "\n",
    "        # confusion matrix for chrY z-score\n",
    "        for threshold in [0, 0.7, 0.9]:\n",
    "            row = 1\n",
    "            col = 1\n",
    "            fig = make_subplots(\n",
    "                rows=3,\n",
    "                cols=3,\n",
    "                shared_yaxes=True,\n",
    "                x_title=\"Predicted class (nb of predictions)\",\n",
    "                y_title=\"z-score vs expected assay\",\n",
    "                row_titles=list(classes),\n",
    "                column_titles=list(classes),\n",
    "                vertical_spacing=0.08,\n",
    "                horizontal_spacing=0.01,\n",
    "            )\n",
    "            threshold_df = assay_track_df[assay_track_df[\"Max pred\"] >= threshold]\n",
    "\n",
    "            title = f\"z-score(mean chrY coverage per file):{assay_label},{track_type} (pred>{threshold})\"\n",
    "            filename = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "\n",
    "            html_file = matrix_logdir / f\"{filename}.html\"\n",
    "            png_file = matrix_logdir / f\"{filename}.png\"\n",
    "            if png_file.exists() or html_file.exists():\n",
    "                continue\n",
    "\n",
    "            for label in classes:\n",
    "                df_label = threshold_df[threshold_df[\"True class\"] == label]\n",
    "\n",
    "                # Iterate over each target and add a violin plot for it\n",
    "                for target in classes:\n",
    "                    sub_df = df_label[df_label[\"Predicted class\"] == target]\n",
    "\n",
    "                    if sub_df.shape[0] == 0:\n",
    "                        y_values = [\n",
    "                            threshold_df[coverage_label].mean()\n",
    "                        ]  # Minimal synthetic data\n",
    "                        sample_count = 0\n",
    "                        hovertext = [\"PLACEHOLDER - NO DATA\"]\n",
    "                    else:\n",
    "                        y_values = sub_df[coverage_label]\n",
    "                        hovertext = [\n",
    "                            f\"{md5sum}:(z-score={z_score:.3f} (n={int(count)}), pred={pred:.3f})\"\n",
    "                            for md5sum, pred, z_score, count, assay in zip(\n",
    "                                sub_df.index,\n",
    "                                sub_df[\"Max pred\"],\n",
    "                                sub_df[coverage_label],\n",
    "                                sub_df[count_label],\n",
    "                                sub_df[ASSAY],\n",
    "                            )\n",
    "                        ]\n",
    "\n",
    "                    fig.add_trace(\n",
    "                        go.Violin(\n",
    "                            y=y_values,\n",
    "                            name=f\"{target} ({sub_df.shape[0]})\",\n",
    "                            box_visible=True,\n",
    "                            meanline_visible=True,\n",
    "                            points=\"all\",\n",
    "                            text=hovertext,\n",
    "                            line_color=COLORS_DICT[target],\n",
    "                            hovertemplate=\"%{text}\",\n",
    "                        ),\n",
    "                        row=row,\n",
    "                        col=col,\n",
    "                    )\n",
    "\n",
    "                    # Move to the next subplot position\n",
    "                    col += 1\n",
    "                    if col > 3:\n",
    "                        col = 1\n",
    "                        row += 1\n",
    "\n",
    "            # Update global layout and traces\n",
    "            fig.update_traces(marker=dict(size=1))\n",
    "            fig.update_yaxes(\n",
    "                range=[\n",
    "                    min(assay_track_df[coverage_label]) - 0.01,\n",
    "                    max(assay_track_df[coverage_label]) + 0.01,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Directly using annotations param does not work with make_subplots\n",
    "            existing_annotations = fig.layout.annotations\n",
    "            new_annotation = dict(\n",
    "                x=1.01,  # Position on the x-axis\n",
    "                y=0.5,  # Position on the y-axis\n",
    "                showarrow=False,  # Do not show arrow\n",
    "                text=\"Reference class\",  # The text you want to display\n",
    "                xref=\"paper\",  # 'x' coordinate is set in relative coordinates\n",
    "                yref=\"paper\",  # 'y' coordinate is set in relative coordinates\n",
    "                xanchor=\"left\",  # Text starts from the left of the x-coordinate\n",
    "                yanchor=\"middle\",  # Middle aligned vertically\n",
    "                font=dict(size=16),\n",
    "                textangle=90,\n",
    "            )\n",
    "            updated_annotations = list(existing_annotations) + [new_annotation]\n",
    "\n",
    "            fig.update_layout(\n",
    "                title_text=f\"{title} (n={threshold_df.shape[0]})\",\n",
    "                showlegend=False,\n",
    "                annotations=updated_annotations,\n",
    "            )\n",
    "\n",
    "            # fig.show()\n",
    "            # break\n",
    "            fig.write_image(png_file, scale=2)\n",
    "            fig.write_html(html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global assay/track distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-core tracks\n",
    "merged_pred_df = merged_pred_df[~merged_pred_df[ASSAY].str.contains(\"wgb\")]\n",
    "assays = merged_pred_df[ASSAY].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pred_df[ASSAY].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_logdir = chrY_coverage_file = (\n",
    "    logs / \"chrY_coverage_results\" / \"10fold_valid\" / \"z-score\" / \"per_assay_track\"\n",
    ")\n",
    "\n",
    "# Prepare a subplot grid; one for each assay + track type combination\n",
    "num_assay_track_combinations = sum(\n",
    "    len(merged_pred_df[merged_pred_df[ASSAY] == assay_label][TRACK].unique())\n",
    "    for assay_label in assays\n",
    ")\n",
    "total_cols = num_assay_track_combinations + len(merged_pred_df[TRACK].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_colors_dict = dict(zip(assays, px.colors.qualitative.Dark24))\n",
    "track_colors_dict = dict(\n",
    "    zip(merged_pred_df[TRACK].unique(), px.colors.qualitative.Dark24_r)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 1\n",
    "row = 1\n",
    "x_title = \"Assay+Track distribution\"\n",
    "fig = make_subplots(\n",
    "    rows=row,\n",
    "    cols=total_cols,\n",
    "    shared_yaxes=True,\n",
    "    x_title=x_title,\n",
    "    y_title=\"z-score\",\n",
    "    horizontal_spacing=0.001,\n",
    ")\n",
    "\n",
    "for assay_label in sorted(assays):\n",
    "    assay_df = merged_pred_df[merged_pred_df[ASSAY] == assay_label]\n",
    "\n",
    "    for track_type in sorted(assay_df[TRACK].unique()):\n",
    "        sub_df = assay_df[assay_df[TRACK] == track_type]\n",
    "\n",
    "        y_values = sub_df[coverage_label]\n",
    "        hovertext = [\n",
    "            f\"{md5sum}:z-score={z_score:.3f} (n={int(count)}), pred={pred:.3f}\"\n",
    "            for md5sum, pred, z_score, count in zip(\n",
    "                sub_df.index,\n",
    "                sub_df[\"Max pred\"],\n",
    "                sub_df[coverage_label],\n",
    "                sub_df[count_label],\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                y=y_values,\n",
    "                name=f\"{assay_label},{track_type} ({sub_df.shape[0]})\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=\"all\",\n",
    "                text=hovertext,\n",
    "                hovertemplate=\"%{text}\",\n",
    "                line_color=assay_colors_dict[assay_label],\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "        col += 1\n",
    "\n",
    "\n",
    "# global track type distribution (all assays)\n",
    "for track_type in sorted(merged_pred_df[TRACK].unique()):\n",
    "    sub_df = merged_pred_df[merged_pred_df[TRACK] == track_type]\n",
    "\n",
    "    y_values = sub_df[coverage_label]\n",
    "    hovertext = [\n",
    "        f\"{md5sum,assay}:z-score={z_score:.3f} (n={int(count)}), pred={pred:.3f}\"\n",
    "        for md5sum, pred, z_score, count, assay in zip(\n",
    "            sub_df.index,\n",
    "            sub_df[\"Max pred\"],\n",
    "            sub_df[coverage_label],\n",
    "            sub_df[count_label],\n",
    "            sub_df[ASSAY],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Violin(\n",
    "            y=y_values,\n",
    "            name=f\"{track_type} ({sub_df.shape[0]})\",\n",
    "            box_visible=True,\n",
    "            meanline_visible=True,\n",
    "            points=\"all\",\n",
    "            text=hovertext,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            line_color=track_colors_dict[track_type],\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    col += 1\n",
    "\n",
    "# Final graphing\n",
    "\n",
    "# Update global layout and traces\n",
    "fig.update_traces(marker=dict(size=1))\n",
    "fig.update_yaxes(\n",
    "    range=[\n",
    "        min(merged_pred_df[coverage_label]) - 0.01,\n",
    "        max(merged_pred_df[coverage_label]) + 0.01,\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickangle=50)\n",
    "\n",
    "fig.update_annotations(y=1.5, selector={\"text\": x_title})\n",
    "\n",
    "title = \"z-score(mean chrY coverage per file) distribution per assay+track type\"\n",
    "fig.update_layout(\n",
    "    title_text=f\"{title}\",\n",
    "    showlegend=False,\n",
    "    autosize=True,\n",
    "    width=3000,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "filename = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "html_file = matrix_logdir / f\"{filename}.html\"\n",
    "png_file = matrix_logdir / f\"{filename}.png\"\n",
    "\n",
    "fig.write_image(png_file, scale=3)\n",
    "fig.write_html(html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs for all assays mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_label = \"expected_assay_chrY_z-score\"\n",
    "count_label = f\"count_{coverage_label}\"\n",
    "\n",
    "classes = merged_pred_df[\"True class\"].unique()\n",
    "\n",
    "matrix_logdir = chrY_coverage_file = (\n",
    "    logs\n",
    "    / \"chrY_coverage_results\"\n",
    "    / \"10fold_valid\"\n",
    "    / \"z-score\"\n",
    "    / \"per_assay\"\n",
    "    / \"w-unknown\"\n",
    ")\n",
    "\n",
    "\n",
    "# confusion matrix for chrY z-score\n",
    "for threshold in [0, 0.7, 0.9]:\n",
    "    row = 1\n",
    "    col = 1\n",
    "    fig = make_subplots(\n",
    "        rows=3,\n",
    "        cols=3,\n",
    "        shared_yaxes=True,\n",
    "        x_title=\"Predicted class (nb of predictions)\",\n",
    "        y_title=\"z-score for expected assay\",\n",
    "        row_titles=list(classes),\n",
    "        column_titles=list(classes),\n",
    "        vertical_spacing=0.08,\n",
    "        horizontal_spacing=0.01,\n",
    "    )\n",
    "    threshold_df = merged_pred_df[merged_pred_df[\"Max pred\"] >= threshold]\n",
    "\n",
    "    title = (\n",
    "        f\"z-score(mean chrY coverage per file) - (pred>{threshold})<br>w fc/pval, no wgbs\"\n",
    "    )\n",
    "\n",
    "    filename = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "    html_file = matrix_logdir / f\"{filename}.html\"\n",
    "    png_file = matrix_logdir / f\"{filename}.png\"\n",
    "    if png_file.exists() or html_file.exists():\n",
    "        continue\n",
    "\n",
    "    for label in classes:\n",
    "        df_label = threshold_df[threshold_df[\"True class\"] == label]\n",
    "\n",
    "        # Iterate over each target and add a violin plot for it\n",
    "        for target in classes:\n",
    "            sub_df = df_label[df_label[\"Predicted class\"] == target]\n",
    "\n",
    "            if sub_df.shape[0] == 0:\n",
    "                y_values = [threshold_df[coverage_label].mean()]  # Minimal synthetic data\n",
    "                sample_count = 0\n",
    "                hovertext = [\"PLACEHOLDER - NO DATA\"]\n",
    "            else:\n",
    "                y_values = sub_df[coverage_label]\n",
    "                hovertext = [\n",
    "                    f\"{md5sum, assay}:(z-score={z_score:.3f} (n={int(count)}), pred={pred:.3f})\"\n",
    "                    for md5sum, pred, z_score, count, assay in zip(\n",
    "                        sub_df.index,\n",
    "                        sub_df[\"Max pred\"],\n",
    "                        sub_df[coverage_label],\n",
    "                        sub_df[count_label],\n",
    "                        sub_df[ASSAY],\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    y=y_values,\n",
    "                    name=f\"{target} ({sub_df.shape[0]})\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=\"all\",\n",
    "                    text=hovertext,\n",
    "                    line_color=COLORS_DICT[target],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "            # Move to the next subplot position\n",
    "            col += 1\n",
    "            if col > 3:\n",
    "                col = 1\n",
    "                row += 1\n",
    "\n",
    "    # Update global layout and traces\n",
    "    fig.update_traces(marker=dict(size=1))\n",
    "    fig.update_yaxes(\n",
    "        range=[\n",
    "            min(merged_pred_df[coverage_label]) - 0.01,\n",
    "            max(merged_pred_df[coverage_label]) + 0.01,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Directly using annotations param does not work with make_subplots\n",
    "    existing_annotations = fig.layout.annotations  # type: ignore\n",
    "    new_annotation = dict(\n",
    "        x=1.01,  # Position on the x-axis\n",
    "        y=0.5,  # Position on the y-axis\n",
    "        showarrow=False,  # Do not show arrow\n",
    "        text=\"Reference class\",  # The text you want to display\n",
    "        xref=\"paper\",  # 'x' coordinate is set in relative coordinates\n",
    "        yref=\"paper\",  # 'y' coordinate is set in relative coordinates\n",
    "        xanchor=\"left\",  # Text starts from the left of the x-coordinate\n",
    "        yanchor=\"middle\",  # Middle aligned vertically\n",
    "        font=dict(size=16),\n",
    "        textangle=90,\n",
    "    )\n",
    "    updated_annotations = list(existing_annotations) + [new_annotation]\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{title} (n={threshold_df.shape[0]})\",\n",
    "        showlegend=False,\n",
    "        annotations=updated_annotations,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    fig.write_image(png_file, scale=2)\n",
    "    fig.write_html(html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with global results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file_dir = logs / \"merged_results\"\n",
    "target_file = target_file_dir / \"merged_pred_results_all_2.1_chrY.csv\"\n",
    "\n",
    "target_df = pd.read_csv(target_file, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_target_df = target_df.merge(\n",
    "    full_chrY_df, how=\"left\", left_index=True, right_index=True, suffixes=(\"\", \"_DROP\")\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_cols = []\n",
    "for column in updated_target_df.columns:\n",
    "    if column.endswith(\"_DROP\"):\n",
    "        same_cols.append(column[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in same_cols:\n",
    "    assert np.isclose(\n",
    "        0, (updated_target_df[column] - updated_target_df[f\"{column}_DROP\"]).sum()\n",
    "    ), f\"{column} not equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_target_df = updated_target_df.drop(\n",
    "    columns=[column for column in updated_target_df.columns if column.endswith(\"_DROP\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_target_df.to_csv(target_file_dir / \"merged_pred_results_all_2.1_chrY_zscores.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
