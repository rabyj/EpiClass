{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot accuracy, precision, and subset size for different probability thresholds.\"\"\"\n",
    "\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, pointless-statement, use-dict-literal, expression-not-assigned, unused-import, too-many-lines, too-many-branches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from PIL import ImageColor\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from epi_ml.utils.general_utility import get_valid_filename\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    BIOMATERIAL_TYPE,\n",
    "    CANCER,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    SplitResultsHandler,\n",
    "    format_labels,\n",
    "    merge_life_stages,\n",
    "    rename_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_seq = np.typing.NDArray[np.floating] | Sequence[float | np.floating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "paper_dir = base_dir\n",
    "if not paper_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {paper_dir} does not exist.\")\n",
    "\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "table_dir = paper_dir / \"tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core7_assays = ASSAY_ORDER[:7]\n",
    "core9_assays = ASSAY_ORDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence threshold impact on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB: {\"results\": dict, \"other_info\": dict}\n",
    "all_threshold_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing and co. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    threshold: float,\n",
    "    true_col: str,\n",
    "    pred_col: str,\n",
    "    pred_prob_cols: List[str],\n",
    "    target_class: str | None,\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, and subset size for a given probability threshold and class.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the true labels, predicted labels, and predicted probabilities.\n",
    "    threshold (float): The probability threshold for filtering the DataFrame.\n",
    "    true_col (str): The column name containing the true labels.\n",
    "    pred_col (str): The column name containing the predicted labels.\n",
    "    pred_prob_cols (List[str]): List of column names containing the predicted probabilities.\n",
    "    target_class (str|None): The class for which precision is to be calculated. Return np.nan if None.\n",
    "\n",
    "    Considers target class for computations if given, otherwise considers all samples.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, float, float, float]: A tuple containing the threshold, the calculated accuracy (%), the calculated precision (%),\n",
    "                                       and the subset size (%) respectively.\n",
    "    \"\"\"\n",
    "    # Targeting a class or not\n",
    "    if target_class in [None, \"all\"]:\n",
    "        total_size = len(df)\n",
    "    else:\n",
    "        total_size = len(df[true_col] == target_class)\n",
    "\n",
    "    # Filter rows where the max predicted probability is above the threshold\n",
    "    try:\n",
    "        subset_df = df[df[pred_prob_cols].max(axis=1) >= threshold]\n",
    "    except TypeError as e:\n",
    "        print(\n",
    "            f\"Error: Could not filter rows.\\npred_cols: {pred_prob_cols}\\nthreshold: {threshold}\"\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "    if len(subset_df) == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    # Calculate the accuracy for this subset\n",
    "    if target_class in [None, \"all\"]:\n",
    "        correct_preds = np.sum(subset_df[true_col] == subset_df[pred_col])\n",
    "        subset_size = len(subset_df)\n",
    "    else:\n",
    "        correct_preds = np.sum(\n",
    "            (subset_df[true_col] == subset_df[pred_col])\n",
    "            & (subset_df[true_col] == target_class)\n",
    "        )\n",
    "        subset_size = np.sum(subset_df[true_col] == target_class)\n",
    "    accuracy = (correct_preds / subset_size) * 100\n",
    "    subset_size_percent = (subset_size / total_size) * 100\n",
    "\n",
    "    # Calculate precision for the target class\n",
    "    if target_class in [None, \"all\"]:\n",
    "        precision = np.nan\n",
    "        return threshold, accuracy, precision, subset_size_percent\n",
    "\n",
    "    true_positives = np.sum(\n",
    "        (subset_df[true_col] == target_class) & (subset_df[pred_col] == target_class)\n",
    "    )\n",
    "    false_positives = np.sum(\n",
    "        (subset_df[true_col] != target_class) & (subset_df[pred_col] == target_class)\n",
    "    )\n",
    "\n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = np.nan\n",
    "    else:\n",
    "        precision = (true_positives / (true_positives + false_positives)) * 100\n",
    "\n",
    "    return threshold, accuracy, precision, subset_size_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_global(\n",
    "    df: pd.DataFrame,\n",
    "    threshold: float | np.floating,\n",
    "    true_col: str,\n",
    "    pred_col: str,\n",
    "    pred_prob_cols: List[str],\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, and subset size for a given probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the true labels, predicted labels, and predicted probabilities.\n",
    "    threshold (float): The probability threshold for filtering the DataFrame.\n",
    "    true_col (str): The column name containing the true labels.\n",
    "    pred_col (str): The column name containing the predicted labels.\n",
    "    pred_prob_cols (List[str]|str): List of column names containing the predicted probabilities.\n",
    "                                OR a Max PredScore column\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, float, float, float, int]: A tuple containing the threshold, the accuracy (%), the macro f1-score (%) and the subset size (%) respectively.\n",
    "    \"\"\"\n",
    "    total_size = len(df)\n",
    "\n",
    "    # Filter rows where the max predicted probability is above the threshold\n",
    "    # Normally expecting a matrix of probabilities\n",
    "    # But can deal with a Max PredScore column\n",
    "    if isinstance(pred_prob_cols, str):\n",
    "        pred_prob_cols = [pred_prob_cols]\n",
    "    try:\n",
    "        subset_df = df[df[pred_prob_cols].max(axis=1) >= threshold]\n",
    "    except TypeError as e:\n",
    "        print(\n",
    "            f\"Error: Could not filter rows.\\npred_cols: {pred_prob_cols}\\nthreshold: {threshold}\"\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "    N = len(subset_df)\n",
    "    if N == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    # Metrics\n",
    "    existing_labels = sorted(df[true_col].unique())\n",
    "    acc: float = accuracy_score(subset_df[true_col], subset_df[pred_col])  # type: ignore\n",
    "    f1: float = f1_score(subset_df[true_col], subset_df[pred_col], average=\"macro\", labels=existing_labels)  # type: ignore\n",
    "    relative_size = N / total_size\n",
    "\n",
    "    return float(threshold), acc, f1, relative_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCURACY_NAME = \"rec\"\n",
    "PRECISION_NAME = \"prec\"\n",
    "SUBSET_SIZE_NAME = \"sz\"\n",
    "\n",
    "\n",
    "def find_columns(df: pd.DataFrame, verbose: bool = False) -> Dict[str, List[str] | str]:\n",
    "    \"\"\"\n",
    "    Find the columns containing true labels, predicted labels, and predicted probabilities in a DataFrame.\n",
    "    \"\"\"\n",
    "    df_cols = df.columns\n",
    "    df_cols = [col for col in df_cols if str(col) not in [\"TRUE\", \"FALSE\"]]\n",
    "\n",
    "    likely_true_class_cols = [\n",
    "        col for col in df_cols if \"true\" in col.lower() or \"expected\" in col.lower()\n",
    "    ]\n",
    "    likely_pred_class_cols = [col for col in df_cols if \"pred\" in col.lower()]\n",
    "\n",
    "    if not likely_true_class_cols or not likely_pred_class_cols:\n",
    "        raise ValueError(\n",
    "            \"Could not automatically detect 'True class' or 'Predicted class' columns.\"\n",
    "        )\n",
    "\n",
    "    true_col = likely_true_class_cols[0]\n",
    "    pred_col = likely_pred_class_cols[0]\n",
    "    if df[true_col].dtype != object or df[pred_col].dtype != object:\n",
    "        print(f\"{true_col} and {pred_col} are not string columns. Could cause issues.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"True class: {true_col}\")\n",
    "        print(f\"Predicted class: {pred_col}\")\n",
    "\n",
    "    classes = df[true_col].unique().tolist() + [\"all\"]\n",
    "    pred_prob_cols = classes[0:-1]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Classes: {classes}\")\n",
    "        print(f\"Predicted probability columns: {pred_prob_cols}\")\n",
    "\n",
    "    for col in pred_prob_cols:\n",
    "        if df[col].dtype != float:\n",
    "            print(f\"{col} is not a float column ({df[col].dtype}). Could cause issues.\")\n",
    "\n",
    "    return {\n",
    "        \"true_col\": true_col,\n",
    "        \"pred_col\": pred_col,\n",
    "        \"classes\": classes,\n",
    "        \"pred_prob_cols\": pred_prob_cols,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_thresholds(\n",
    "    df: pd.DataFrame, thresholds: List[float], verbose: bool = False\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy and subset size for different probability thresholds with improved automatic column detection.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe containing true labels and predicted probabilities.\n",
    "    thresholds (list): List of probability thresholds to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe containing the accuracy and subset size for each threshold.\n",
    "    \"\"\"\n",
    "    columns = find_columns(df, verbose=verbose)\n",
    "    true_col: str = columns[\"true_col\"]  # type: ignore\n",
    "    pred_col: str = columns[\"pred_col\"]  # type: ignore\n",
    "    classes: List[str] = columns[\"classes\"]  # type: ignore\n",
    "    pred_prob_cols: List[str] = columns[\"pred_prob_cols\"]  # type: ignore\n",
    "\n",
    "    # Evaluate each threshold over each class\n",
    "    results_dfs = {}\n",
    "    for class_label in classes:\n",
    "        results = []\n",
    "        filtered_df = (\n",
    "            df\n",
    "            if class_label == \"all\"\n",
    "            else df[(df[true_col] == class_label) | (df[pred_col] == class_label)]\n",
    "        )\n",
    "\n",
    "        for thresh in thresholds:\n",
    "            try:\n",
    "                result = compute_metrics(\n",
    "                    filtered_df,\n",
    "                    thresh,\n",
    "                    true_col,\n",
    "                    pred_col,\n",
    "                    pred_prob_cols,\n",
    "                    target_class=class_label,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Error. Could not compute metric with class {class_label}.\\ntrue_col: {true_col}\\npred_col: {pred_col}\\npred_prob_cols: {pred_prob_cols}\\n\"\n",
    "                )\n",
    "                raise e\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "        # Convert to DataFrame for easier manipulation\n",
    "        short_class_label = class_label[0:10]\n",
    "        results_df = pd.DataFrame(\n",
    "            results,\n",
    "            columns=[\n",
    "                \"Threshold\",\n",
    "                f\"{ACCURACY_NAME}_{short_class_label} (%)\",\n",
    "                f\"{PRECISION_NAME}_{short_class_label} (%)\",\n",
    "                f\"{SUBSET_SIZE_NAME}_{short_class_label} (%) ({filtered_df.shape[0]})\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        results_dfs[class_label] = results_df\n",
    "\n",
    "    return results_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_thresholds_global(\n",
    "    df: pd.DataFrame,\n",
    "    thresholds: float_seq,\n",
    "    verbose: bool = False,\n",
    "    columns: Dict[str, List[str] | str] | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy and subset size for different probability thresholds with improved automatic column detection.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe containing true labels and predicted probabilities.\n",
    "        thresholds (list): List of probability thresholds to evaluate.\n",
    "        verbose (bool): Whether to print verbose information.\n",
    "        columns (dict): A dictionary containing the column names for true labels, predicted labels, and predicted probabilities.\n",
    "                        Expecting entries: \"true_col\", \"pred_col\", \"pred_prob_cols\"|\"max_pred\".\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing the accuracy and subset size for each threshold.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = find_columns(df, verbose=verbose)\n",
    "        true_col: str = columns[\"true_col\"]  # type: ignore\n",
    "        pred_col: str = columns[\"pred_col\"]  # type: ignore\n",
    "        pred_prob_cols: List[str] = columns[\"pred_prob_cols\"]  # type: ignore\n",
    "    else:\n",
    "        true_col: str = columns[\"true_col\"]  # type: ignore\n",
    "        pred_col: str = columns[\"pred_col\"]  # type: ignore\n",
    "        try:\n",
    "            pred_prob_cols: List[str] = columns[\"pred_prob_cols\"]  # type: ignore\n",
    "        except KeyError:\n",
    "            pred_prob_cols: List[str] = [columns[\"max_pred\"]]  # type: ignore\n",
    "\n",
    "    # Evaluate each threshold over each class\n",
    "    results = []\n",
    "    for tresh in thresholds:\n",
    "        try:\n",
    "            result = compute_metrics_global(df, tresh, true_col, pred_col, pred_prob_cols)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error. Could not compute metrics.\\ntrue_col: {true_col}\\npred_col: {pred_col}\\npred_prob_cols: {pred_prob_cols}\\n\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    results_df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\n",
    "            \"Threshold\",\n",
    "            \"Accuracy (%)\",\n",
    "            \"F1-score\",\n",
    "            f\"Subset size (%) ({df.shape[0]})\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thresholds_graph_global_plotly(\n",
    "    metrics_df: pd.DataFrame, name: str, xrange: Tuple[float, float] | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Return graph of the accuracy and subset size at different probability thresholds for global results.\n",
    "\n",
    "    Parameters:\n",
    "    metrics_df (pd.DataFrame): DataFrame with metrics at different probability thresholds.\n",
    "    name (str): Graph title.\n",
    "\n",
    "    Returns:\n",
    "    go.Figure: Plotly figure object with the plotted graph.\n",
    "    \"\"\"\n",
    "    # color-blind friendly\n",
    "    # black, blue, red\n",
    "    colors = [\"#000000\", \"#005AB5\", \"#DC3220\"]\n",
    "    marker1 = \"square-open\"\n",
    "    marker2 = \"cross-open\"\n",
    "    marker3 = \"circle\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    acc_label = metrics_df.filter(like=\"Acc\").columns[0]\n",
    "    f1_score_label = metrics_df.filter(like=\"F1\").columns[0]\n",
    "    subset_size_label = metrics_df.filter(like=\"Subset\").columns[0]\n",
    "\n",
    "    # Plot accuracy\n",
    "    vals = metrics_df[acc_label]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=metrics_df[\"Threshold\"],\n",
    "            y=vals,\n",
    "            name=acc_label,\n",
    "            line=dict(color=colors[2]),\n",
    "            marker_symbol=marker1,\n",
    "            mode=\"lines+markers\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot f1_score\n",
    "    vals = metrics_df[f1_score_label]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=metrics_df[\"Threshold\"],\n",
    "            y=vals,\n",
    "            name=f1_score_label,\n",
    "            line=dict(color=colors[1], dash=\"dot\"),\n",
    "            marker_symbol=marker2,\n",
    "            mode=\"lines+markers\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot subset size on secondary Y-axis\n",
    "    vals = metrics_df[subset_size_label]\n",
    "    min_y2 = vals.min()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=metrics_df[\"Threshold\"],\n",
    "            y=vals,\n",
    "            name=subset_size_label.split(\"(\")[0].strip(),\n",
    "            line=dict(color=colors[0], dash=\"dash\"),\n",
    "            marker_symbol=marker3,\n",
    "            yaxis=\"y2\",\n",
    "            mode=\"lines+markers\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Adjusting the layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Metrics at Different Pred. Score Thresholds<br>{name}\",\n",
    "        xaxis_title=\"Prediction Score Threshold\",\n",
    "        xaxis=dict(\n",
    "            tickvals=np.linspace(0, 1, 11),\n",
    "            ticktext=[f\"{x:.1f}\" for x in np.linspace(0, 1, 11)],\n",
    "        ),\n",
    "        yaxis_title=\"Accuracy / F1-score (%)\",\n",
    "        yaxis2=dict(title=\"Subset Size (%)\", overlaying=\"y\", side=\"right\"),\n",
    "        legend=dict(orientation=\"v\", x=1.1, y=1),\n",
    "        height=500,\n",
    "        width=500,\n",
    "        yaxis2_range=[min_y2 - 0.001, 1.001],\n",
    "    )\n",
    "\n",
    "    if not xrange:\n",
    "        xrange = (-0.001, 1.001)\n",
    "    fig.update_xaxes(range=xrange)\n",
    "\n",
    "    fig.update_traces(line={\"width\": 1})\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thresholds_graph_plotly(threshold_dfs: Dict[str, pd.DataFrame], name: str):\n",
    "    \"\"\"\n",
    "    Return graph of the accuracy and subset size at different probability thresholds for all classes.\n",
    "\n",
    "    Parameters:\n",
    "    threshold_metrics_df (Dict[str, pd.DataFrame]): A dictionary containing dfs with metrics for each class label and the general case.\n",
    "    name (str): Graph title.\n",
    "\n",
    "    Returns:\n",
    "    go.Figure: Plotly figure object with the plotted graph.\n",
    "    \"\"\"\n",
    "    colors = px.colors.qualitative.Dark24\n",
    "    marker1 = \"circle\"\n",
    "    marker2 = \"cross-open\"\n",
    "    marker3 = \"circle-open\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for idx, (_, threshold_metrics) in enumerate(threshold_dfs.items()):\n",
    "        color = colors[idx % len(colors)]\n",
    "\n",
    "        acc_label = threshold_metrics.filter(like=f\"{ACCURACY_NAME}\").columns[0]\n",
    "        acc_subset = threshold_metrics.filter(like=f\"{SUBSET_SIZE_NAME}\").columns[0]\n",
    "        prec_label = threshold_metrics.filter(like=f\"{PRECISION_NAME}\").columns[0]\n",
    "\n",
    "        # Plot accuracy\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=threshold_metrics[\"Threshold\"],\n",
    "                y=threshold_metrics[acc_label],\n",
    "                name=acc_label,\n",
    "                line=dict(color=color),\n",
    "                marker_symbol=marker1,\n",
    "                mode=\"lines+markers\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Plot precision\n",
    "        prec_vals = threshold_metrics[prec_label]\n",
    "        if not prec_vals.isna().all():\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=threshold_metrics[\"Threshold\"],\n",
    "                    y=prec_vals,\n",
    "                    name=prec_label,\n",
    "                    line=dict(color=color, dash=\"dot\"),\n",
    "                    marker_symbol=marker2,\n",
    "                    mode=\"lines+markers\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Plot subset size on secondary Y-axis\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=threshold_metrics[\"Threshold\"],\n",
    "                y=threshold_metrics[acc_subset],\n",
    "                name=acc_subset,\n",
    "                line=dict(color=color, dash=\"dash\"),\n",
    "                marker_symbol=marker3,\n",
    "                yaxis=\"y2\",\n",
    "                mode=\"lines+markers\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Adjusting the layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Accuracy and Subset Size at Different Probability Thresholds<br>{name}\",\n",
    "        xaxis_title=\"Probability Threshold\",\n",
    "        xaxis=dict(\n",
    "            tickvals=np.linspace(0, 1, 11),\n",
    "            ticktext=[f\"{x:.1f}\" for x in np.linspace(0, 1, 11)],\n",
    "        ),\n",
    "        yaxis_title=\"Accuracy (%)\",\n",
    "        yaxis2=dict(title=\"Subset Size (%)\", overlaying=\"y\", side=\"right\"),\n",
    "        legend=dict(orientation=\"v\", x=1.05, y=1),\n",
    "        height=1000,\n",
    "        width=1600,\n",
    "    )\n",
    "    fig.update_xaxes(range=[-0.001, 1.001])\n",
    "    fig.update_traces(line={\"width\": 1})\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds: List[float] = [float(x) for x in np.arange(0, 1, 1 / 20)] + [0.99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP EpiAtlas cross-validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_remapper = {\n",
    "    \"assay\": ASSAY,\n",
    "    \"assay7\": ASSAY,\n",
    "    f\"{ASSAY}_11c\": ASSAY,\n",
    "    ASSAY: ASSAY,\n",
    "    \"sex\": SEX,\n",
    "    \"sex3\": SEX,\n",
    "    SEX: SEX,\n",
    "    \"harmonized_donor_sex_w-mixed\": SEX,\n",
    "    \"cancer\": CANCER,\n",
    "    CANCER: CANCER,\n",
    "    \"biomat\": BIOMATERIAL_TYPE,\n",
    "    BIOMATERIAL_TYPE: BIOMATERIAL_TYPE,\n",
    "}\n",
    "\n",
    "for l in [\"donorlife\", \"lifestage\", LIFE_STAGE]:\n",
    "    category_remapper[l] = LIFE_STAGE\n",
    "    category_remapper[f\"{l}_merged\"] = LIFE_STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    ASSAY,\n",
    "    CELL_TYPE,\n",
    "    SEX,\n",
    "    LIFE_STAGE,\n",
    "    BIOMATERIAL_TYPE,\n",
    "    CANCER,\n",
    "    \"paired_end\",\n",
    "    \"project\",\n",
    "]\n",
    "split_results_handler = SplitResultsHandler()\n",
    "\n",
    "data_dir_100kb = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select 10-fold oversampling runs\n",
    "# all_split_dfs = split_results_handler.general_split_metrics(\n",
    "#     results_dir=data_dir_100kb,\n",
    "#     merge_assays=False,\n",
    "#     include_categories=categories,\n",
    "#     exclude_names=[\"reg\", \"no-mixed\", \"chip\", \"16ct\", \"27ct\"],\n",
    "#     return_type=\"split_results\",\n",
    "#     oversampled_only=True,\n",
    "#     verbose=False,\n",
    "# )\n",
    "# all_split_dfs_concat: Dict = split_results_handler.concatenate_split_results(all_split_dfs, concat_first_level=True)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing special case \"paired_end\" which has bool values that aren't treated as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = [\"True class\", \"Predicted class\"]\n",
    "# df = all_split_dfs_concat[\"paired_end\"].copy()\n",
    "\n",
    "# # labels: bool -> str\n",
    "# df[cols] = df[cols].astype(str)\n",
    "# for col in cols:\n",
    "#     df[col] = df[col].str.lower()\n",
    "\n",
    "# # make sure column names = class names\n",
    "# df = df.rename(columns={\"TRUE\": \"true\", \"FALSE\": \"false\"})\n",
    "\n",
    "# all_split_dfs_concat[\"paired_end\"] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing all values separately from graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_dfs = {}\n",
    "# for task_name, df in all_split_dfs_concat.items():\n",
    "#     print(\"TASK:\",task_name)\n",
    "#     threshold_dfs[task_name] = evaluate_thresholds(df, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = base_fig_dir / \"threshold_graphs\" / \"100kb_all_none\"\n",
    "# if not output_dir.exists():\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for task_name, df in all_split_dfs_concat.items():\n",
    "#     print(\"TASK:\", task_name)\n",
    "#     nb_samples = len(df)\n",
    "#     nb_classes = df[\"True class\"].nunique()\n",
    "\n",
    "#     df = threshold_dfs[task_name]\n",
    "\n",
    "#     # create figure\n",
    "#     name = f\"{task_name} - {nb_classes} classes\"\n",
    "#     fig = create_thresholds_graph_plotly(df, f\"{name} - n={nb_samples}\")\n",
    "#     fig.show()\n",
    "\n",
    "#     # # save\n",
    "#     filename = f\"threshold_impact_graph_full_{get_valid_filename(name)}\".replace(\n",
    "#         \"_-_\", \"-\"\n",
    "#     )\n",
    "#     fig.write_image(output_dir / f\"{filename}.png\")\n",
    "#     fig.write_image(output_dir / f\"{filename}.svg\")\n",
    "#     fig.write_html(output_dir / f\"{filename}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_dfs = {}\n",
    "# other_info = {}\n",
    "# for task_name, df in all_split_dfs_concat.items():\n",
    "#     print(\"TASK:\", task_name)\n",
    "#     nb_samples = len(df)\n",
    "#     nb_classes = df[\"True class\"].nunique()\n",
    "\n",
    "#     other_info[task_name] = {\"nb_samples\": nb_samples, \"nb_classes\": nb_classes}\n",
    "\n",
    "#     threshold_dfs[task_name] = evaluate_thresholds_global(df, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = base_fig_dir / \"threshold_graphs\" / \"100kb_all_none\" / \"EpiATLAS\"\n",
    "# if not output_dir.exists():\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for task_name, df in all_split_dfs_concat.items():\n",
    "#     print(\"TASK:\", task_name)\n",
    "#     nb_samples = len(df)\n",
    "#     nb_classes = df[\"True class\"].nunique()\n",
    "\n",
    "#     df = threshold_dfs[task_name]\n",
    "\n",
    "#     # create figure\n",
    "#     name = f\"{task_name} - {nb_classes} classes\"\n",
    "#     fig = create_thresholds_graph_global_plotly(df, f\"{name} - n={nb_samples}\", xrange=(max(0, 1.0/nb_classes-0.05), 1.001))\n",
    "#     # fig.show()\n",
    "\n",
    "#     # # save\n",
    "#     filename = f\"threshold_impact_graph_global_{get_valid_filename(name)}\".replace(\n",
    "#         \"_-_\", \"-\"\n",
    "#     )\n",
    "#     fig.write_image(output_dir / f\"{filename}.png\")\n",
    "#     fig.write_image(output_dir / f\"{filename}.svg\")\n",
    "#     fig.write_html(output_dir / f\"{filename}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename / drop classifier metrics for future graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in [f\"{ASSAY}_7c\", \"project\", \"paired_end\"]:\n",
    "#     threshold_dfs.pop(label, None)\n",
    "\n",
    "# for name in list(threshold_dfs.keys()):\n",
    "#     try:\n",
    "#         new_name = category_remapper[name]\n",
    "#     except KeyError:\n",
    "#         # Undesired category for rest\n",
    "#         del threshold_dfs[name]\n",
    "#         continue\n",
    "\n",
    "#     threshold_dfs[new_name] = threshold_dfs.pop(name)\n",
    "\n",
    "# all_threshold_results[\"EpiATLAS\"] = {\"results\": threshold_dfs, \"other_info\": other_info}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE, ChIP-Atlas and recount3 inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'other'/'unknown' are too undefined, we exclude from life stage predictions\n",
    "cell_line_vals = [\"cell_line\", \"cell line\", \"unknown\", \"other\"]\n",
    "\n",
    "unknown_values = [\"unknown\", \"other\", \"indeterminate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dir = table_dir / \"dfreeze_v2\" / \"predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not apply `life stage classifier` on `cell line` samples because it was not part of the training data,\n",
    "and the notion of life stage for a cell line is dubious. \n",
    "\n",
    "Also, we merge `perinatal stages` public DB inference (embryonic, fetal, newborn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_category_labels(\n",
    "    df: pd.DataFrame, categories: List[str], verbose: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Uniformize class labels for each category labels.\"\"\"\n",
    "    # Uniformize class labels\n",
    "    to_format = []\n",
    "    for col in df.columns:\n",
    "        cond1 = any(category in col.lower() for category in categories)\n",
    "        cond2 = any(l in col.lower() for l in [\"true\", \"expected\", \"predicted\"])\n",
    "        if cond1 and (cond2 or col in categories):\n",
    "            if verbose:\n",
    "                print(f\"Formatting {col}\")\n",
    "            to_format.append(col)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Formatting {len(to_format)} columns: {to_format}\")\n",
    "\n",
    "    df = format_labels(\n",
    "        df=df,\n",
    "        columns=to_format,\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_target_recall(\n",
    "    df,\n",
    "    category_name,\n",
    "    col_templates,\n",
    "    class_of_interest,\n",
    "    target_recall=0.9,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Find the first threshold such that recall ≥ target_recall\n",
    "    among predictions with proba >= threshold.\n",
    "\n",
    "    Returns:\n",
    "        threshold, recall\n",
    "    \"\"\"\n",
    "    true_col = col_templates[\"true_col\"].format(category_name)\n",
    "    pred_col = col_templates[\"pred_col\"].format(category_name)\n",
    "    max_pred_col = col_templates[\"max_pred\"].format(category_name)\n",
    "\n",
    "    thresholds = np.linspace(0.6, 1.0, 250)\n",
    "\n",
    "    for t in thresholds:\n",
    "        tmp_df = df[df[max_pred_col] >= t]\n",
    "\n",
    "        # Number of true positives for class_of_interest in filtered set\n",
    "        tp = (\n",
    "            (tmp_df[true_col] == class_of_interest)\n",
    "            & (tmp_df[pred_col] == class_of_interest)\n",
    "        ).sum()\n",
    "\n",
    "        # Number of all true class_of_interest samples in filtered set\n",
    "        total = (tmp_df[true_col] == class_of_interest).sum()\n",
    "\n",
    "        recall = tp / total\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{t:.3f}:{recall:.3f}\")\n",
    "\n",
    "        if recall >= target_recall:\n",
    "            return t, recall\n",
    "\n",
    "    return None, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_biomat_LS(df: pd.DataFrame, biomaterial_cat_name:str, col_templates: Dict[str, str], predScore_threshold: float=0.8, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Filter biomaterials for to enable more life stage predictions.\n",
    "\n",
    "    Remove samples expected to be from cell lines.\n",
    "\n",
    "    For unknown biomat, retain samples where biomat predictions are not\n",
    "    cell line with predScore > predScore_threshold.\n",
    "    \"\"\"\n",
    "    print(\"Filtering biomaterial types for life stage predictions.\")\n",
    "    true_col = col_templates[\"true_col\"].format(biomaterial_cat_name)\n",
    "    pred_col = col_templates[\"pred_col\"].format(biomaterial_cat_name)\n",
    "    max_pred_col = col_templates[\"max_pred_col\"].format(biomaterial_cat_name)\n",
    "\n",
    "    print(f\"Before:\\n{df[true_col].value_counts(dropna=False)}\\n{df[pred_col].value_counts(dropna=False)}\")\n",
    "\n",
    "    # First filter out cell lines\n",
    "    df = df[~df[true_col].isin([\"cell_line\"])]\n",
    "\n",
    "    # Then retain unknown that are probably not cell lines\n",
    "    unknown_subset = df[df[true_col].isin(unknown_values)]\n",
    "\n",
    "    unknown_subset = unknown_subset[\n",
    "        unknown_subset[pred_col] != \"cell_line\"\n",
    "        unknown_subset[max_pred_col] > predScore_threshold\n",
    "    ]\n",
    "\n",
    "    df = pd.concat([df, unknown_subset])\n",
    "\n",
    "    print(f\"After:\\n{df[true_col].value_counts(dropna=False)}\\n{df[pred_col].value_counts(dropna=False)}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChIP-Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = (\n",
    "    predictions_dir / \"ChIP-Atlas_predictions_20240606_merge_metadata_freeze1.csv.xz\"\n",
    ")\n",
    "pred_df = pd.read_csv(preds_path, sep=\",\", low_memory=False, compression=\"xz\")\n",
    "print(pred_df.shape)\n",
    "\n",
    "pred_df.fillna(\"unknown\", inplace=True)\n",
    "pred_df.replace(\"indeterminate\", \"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "    col\n",
    "    for col in pred_df.columns\n",
    "    if any(l in col.lower() for l in [\"disease\", \"assay11\", \"assay13\"])\n",
    "]\n",
    "pred_df = pred_df.drop(columns=to_drop)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df[pred_df[\"is_EpiAtlas_EpiRR\"].astype(str) == \"0\"]\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df[\n",
    "    ~pred_df[\"core7_DBs_consensus\"].isin(\n",
    "        [\"Ignored - Potential non-core\", \"non-core/CTCF\"]\n",
    "    )\n",
    "]\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[BIOMATERIAL_TYPE] = pred_df[\"expected_biomat\"]\n",
    "\n",
    "to_replace = {\n",
    "    \"sex3\": \"sex\",\n",
    "    \"assay7\": \"assay\",\n",
    "    \"donorlife\": \"lifestage\",\n",
    "}\n",
    "pred_df = rename_columns(\n",
    "    df=pred_df,\n",
    "    remapper=to_replace,\n",
    "    exact_match=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_templates = {\n",
    "    \"true_col\": \"expected_{}\",\n",
    "    \"pred_col\": \"Predicted_class_{}\",\n",
    "    \"max_pred\": \"Max_pred_{}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "categories = [\"assay\", \"sex\", \"cancer\", \"biomat\"]\n",
    "\n",
    "pred_df = format_category_labels(pred_df, categories + [\"lifestage\"], verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = merge_life_stages(\n",
    "    df=pred_df,\n",
    "    lifestage_column_name=\"lifestage\",\n",
    "    column_name_templates=list(col_templates.values()),\n",
    "    verbose=verbose,\n",
    ")\n",
    "\n",
    "to_drop = [template.format(LIFE_STAGE) for template in col_templates.values()]\n",
    "pred_df = pred_df.drop(columns=to_drop, errors=\"ignore\")\n",
    "\n",
    "categories.extend([\"lifestage_merged\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_target_recall(\n",
    "    df=pred_df,\n",
    "    category_name=\"biomat\",\n",
    "    class_of_interest=\"cell_line\",\n",
    "    target_recall=0.9,\n",
    "    col_templates=col_templates,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_dfs = {}\n",
    "other_info = {}\n",
    "for category in categories:\n",
    "    print(\"TASK:\", category)\n",
    "    col_mapper = {k: v.format(category) for k, v in col_templates.items()}\n",
    "\n",
    "    df = pred_df.copy()\n",
    "\n",
    "    # Filter unknown/NA\n",
    "    df = df[~df[col_mapper[\"true_col\"]].isin(unknown_values)]\n",
    "\n",
    "    if category == \"assay\":\n",
    "        df = pred_df[pred_df[col_mapper[\"true_col\"]].isin(ASSAY_ORDER[0:7])]\n",
    "\n",
    "    elif category == \"lifestage_merged\":\n",
    "        cell_line_vals = [\"cell_line\"]\n",
    "        df = df[~df[BIOMATERIAL_TYPE].isin(cell_line_vals)]\n",
    "        if verbose:\n",
    "            print(\"Biomaterial type, lifestage and assay post cell line filter:\")\n",
    "            for col in [\"biomat\", \"lifestage_merged\", \"assay\"]:\n",
    "                print(\n",
    "                    df[col_templates[\"true_col\"].format(col)].value_counts(dropna=False),\n",
    "                    \"\\n\",\n",
    "                )\n",
    "\n",
    "    cat_name = category_remapper[category]\n",
    "\n",
    "    nb_samples = df.shape[0]\n",
    "    N_true_classes = len(set(df[col_mapper[\"true_col\"]]))\n",
    "    total_N_classes = len(\n",
    "        set(df[col_mapper[\"pred_col\"]]) | set(df[col_mapper[\"true_col\"]])\n",
    "    )\n",
    "    other_info[cat_name] = {\n",
    "        \"nb_samples\": nb_samples,\n",
    "        \"nb_classes\": N_true_classes,\n",
    "        \"total_possible_classes\": total_N_classes,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        for col in [col_mapper[\"true_col\"], col_mapper[\"pred_col\"]]:\n",
    "            print(df[col].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "    threshold_dfs[cat_name] = evaluate_thresholds_global(\n",
    "        df, thresholds, verbose=verbose, columns=col_mapper  # type: ignore\n",
    "    )\n",
    "\n",
    "all_threshold_results[\"ChIP-Atlas\"] = {\"results\": threshold_dfs, \"other_info\": other_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for task_name in categories:\n",
    "#     df = threshold_dfs[task_name]\n",
    "#     print(\"TASK:\", task_name)\n",
    "#     nb_samples = other_info[task_name][\"nb_samples\"]\n",
    "#     nb_classes = other_info[task_name][\"nb_classes\"]\n",
    "\n",
    "#     # create figure\n",
    "#     name = f\"{task_name} - {nb_classes} classes\"\n",
    "#     fig = create_thresholds_graph_global_plotly(df, f\"{name} - n={nb_samples}\", xrange=(max(0, 1.0/nb_classes-0.05), 1.001))\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENCODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = predictions_dir / \"encode_predictions_merge_metadata_2025-02_freeze1.csv.xz\"\n",
    "\n",
    "pred_df = pd.read_csv(preds_path, sep=\",\", low_memory=False, compression=\"xz\")\n",
    "print(pred_df.shape)\n",
    "\n",
    "pred_df.fillna(\"unknown\", inplace=True)\n",
    "pred_df.replace(\"indeterminate\", \"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "    col\n",
    "    for col in pred_df.columns\n",
    "    if any(\n",
    "        l in col.lower()\n",
    "        for l in [\"disease\", \"assay_epiclass_7c\", \"assay13\", \"biospecimen\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "pred_df = pred_df.drop(columns=to_drop)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(pred_df.columns):\n",
    "    if \"11c\" in col:\n",
    "        new_col = col.replace(\"assay_epiclass_11c\", \"assay_epiclass\")\n",
    "        pred_df = pred_df.rename(columns={col: new_col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df[~pred_df[\"in_epiatlas\"]]\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_templates = {\n",
    "    \"true_col\": \"{}\",\n",
    "    \"pred_col\": \"Predicted class ({})\",\n",
    "    \"max_pred\": \"Max pred ({})\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = []\n",
    "categories = [ASSAY, SEX, CANCER, BIOMATERIAL_TYPE]\n",
    "\n",
    "pred_df = format_category_labels(pred_df, categories + [LIFE_STAGE], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = merge_life_stages(\n",
    "    df=pred_df,\n",
    "    lifestage_column_name=LIFE_STAGE,\n",
    "    column_name_templates=list(col_templates.values()),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "to_drop = [template.format(LIFE_STAGE) for template in col_templates.values()]\n",
    "pred_df = pred_df.drop(columns=to_drop, errors=\"ignore\")\n",
    "\n",
    "categories.extend([f\"{LIFE_STAGE}_merged\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "threshold_dfs_core = {}\n",
    "threshold_dfs_noncore = {}\n",
    "other_info_core = {}\n",
    "other_info_noncore = {}\n",
    "\n",
    "for category in categories:\n",
    "    print(\"TASK:\", category)\n",
    "    col_mapper = {k: v.format(category) for k, v in col_templates.items()}\n",
    "\n",
    "    df: pd.DataFrame = pred_df.copy()  # type: ignore\n",
    "\n",
    "    # Filter unknown/NA\n",
    "    df = df[~(df[col_mapper[\"true_col\"]].isin(unknown_values))]\n",
    "\n",
    "    # Merge rna / wgbs pairs\n",
    "    if category == ASSAY:\n",
    "        true, pred = col_mapper[\"true_col\"], col_mapper[\"pred_col\"]\n",
    "        df.loc[:, [true, pred]] = df.loc[:, [true, pred]].replace(\n",
    "            ASSAY_MERGE_DICT, inplace=False\n",
    "        )\n",
    "    elif category == f\"{LIFE_STAGE}_merged\":\n",
    "        df = df[~df[BIOMATERIAL_TYPE].isin(cell_line_vals)]\n",
    "        if verbose:\n",
    "            print(\"Biomaterial type, lifestage and assay post cell line filter:\")\n",
    "            for col in [BIOMATERIAL_TYPE, ASSAY, f\"{LIFE_STAGE}_merged\"]:\n",
    "                print(\n",
    "                    df[col_templates[\"true_col\"].format(col)].value_counts(dropna=False),\n",
    "                    \"\\n\",\n",
    "                )\n",
    "\n",
    "    # split core/non-core\n",
    "    df.loc[:, ASSAY] = df.loc[:, ASSAY].replace(ASSAY_MERGE_DICT, inplace=False)\n",
    "    mask = df[ASSAY].isin(core9_assays)\n",
    "\n",
    "    df_core = df[mask]\n",
    "    df_noncore = df[~mask]\n",
    "\n",
    "    # Compute all thresholds\n",
    "    cat_name = category_remapper[category]\n",
    "    for name, container_results, container_other_info, set_df in zip(\n",
    "        [\"core\", \"noncore\"],\n",
    "        [threshold_dfs_core, threshold_dfs_noncore],\n",
    "        [other_info_core, other_info_noncore],\n",
    "        [df_core, df_noncore],\n",
    "    ):\n",
    "        if cat_name == ASSAY and \"ctcf\" in set_df[ASSAY].unique():\n",
    "            if verbose:\n",
    "                print(\"\\nSkipping assay non-core\\n\")\n",
    "            continue\n",
    "\n",
    "        nb_samples = set_df.shape[0]\n",
    "        N_true_classes = len(set(set_df[col_mapper[\"true_col\"]]))\n",
    "        total_N_classes = len(\n",
    "            set(set_df[col_mapper[\"pred_col\"]]) | set(set_df[col_mapper[\"true_col\"]])\n",
    "        )\n",
    "        container_other_info[cat_name] = {\n",
    "            \"nb_samples\": nb_samples,\n",
    "            \"nb_classes\": N_true_classes,\n",
    "            \"total_possible_classes\": total_N_classes,\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Set: {name}\")\n",
    "            for col in [col_mapper[\"true_col\"], col_mapper[\"pred_col\"]]:\n",
    "                print(set_df[col].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "        container_results[cat_name] = evaluate_thresholds_global(\n",
    "            set_df, thresholds, verbose=False, columns=col_mapper  # type: ignore\n",
    "        )\n",
    "\n",
    "all_threshold_results[\"ENCODE_core\"] = {\n",
    "    \"results\": threshold_dfs_core,\n",
    "    \"other_info\": other_info_core,\n",
    "}\n",
    "all_threshold_results[\"ENCODE_non-core\"] = {\n",
    "    \"results\": threshold_dfs_noncore,\n",
    "    \"other_info\": other_info_noncore,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for task_name in categories:\n",
    "#     df = threshold_dfs[task_name]\n",
    "#     print(\"TASK:\", task_name)\n",
    "#     nb_samples = other_info[task_name][\"nb_samples\"]\n",
    "#     nb_classes = other_info[task_name][\"nb_classes\"]\n",
    "\n",
    "#     # create figure\n",
    "#     name = f\"{task_name} - {nb_classes} classes\"\n",
    "#     fig = create_thresholds_graph_global_plotly(df, f\"{name} - n={nb_samples}\", xrange=(max(0, 1.0/nb_classes-0.05), 1.001))\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recount3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316228, 68)\n"
     ]
    }
   ],
   "source": [
    "preds_path = predictions_dir / \"recount3_merged_preds_metadata_freeze1.csv.xz\"\n",
    "pred_df = pd.read_csv(preds_path, sep=\",\", low_memory=False, compression=\"xz\")\n",
    "print(pred_df.shape)\n",
    "\n",
    "pred_df.fillna(\"unknown\", inplace=True)\n",
    "pred_df.replace(\"indeterminate\", \"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming expected_lifestage to harmonized_donor_life_stage\n",
      "Renaming expected_assay to assay_epiclass\n",
      "Renaming expected_cancer to harmonized_sample_cancer_high\n",
      "Renaming expected_biomat to harmonized_biomaterial_type\n",
      "Renaming expected_sex to harmonized_donor_sex\n"
     ]
    }
   ],
   "source": [
    "to_replace = {\n",
    "    \"expected_lifestage\": LIFE_STAGE,\n",
    "    \"expected_assay\": ASSAY,\n",
    "    \"expected_cancer\": CANCER,\n",
    "    \"expected_biomat\": BIOMATERIAL_TYPE,\n",
    "    \"expected_sex\": SEX,\n",
    "}\n",
    "pred_df = rename_columns(\n",
    "    df=pred_df,\n",
    "    remapper=to_replace,\n",
    "    exact_match=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_templates = {\n",
    "    \"true_col\": \"{}\",\n",
    "    \"pred_col\": \"Predicted class ({})\",\n",
    "    \"max_pred\": \"Max pred ({})\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    ASSAY,\n",
    "    SEX,\n",
    "    CANCER,\n",
    "    BIOMATERIAL_TYPE,\n",
    "]\n",
    "lifestage_categories = [LIFE_STAGE, f\"{LIFE_STAGE}_merged\"]\n",
    "pred_df = format_category_labels(pred_df, categories + lifestage_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: harmonized_donor_life_stage -> harmonized_donor_life_stage_merged\n",
      "Remapped successfully. New categories:\n",
      "harmonized_donor_life_stage_merged\n",
      "unknown      230255\n",
      "adult         57504\n",
      "perinatal     25278\n",
      "child          3191\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing: Predicted class (harmonized_donor_life_stage) -> Predicted class (harmonized_donor_life_stage_merged)\n",
      "Remapped successfully. New categories:\n",
      "Predicted class (harmonized_donor_life_stage_merged)\n",
      "adult        144427\n",
      "perinatal    121497\n",
      "child         50304\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing: Max pred (harmonized_donor_life_stage) -> Max pred (harmonized_donor_life_stage_merged)\n",
      "Copied scores (no remapping applied)\n"
     ]
    }
   ],
   "source": [
    "pred_df = merge_life_stages(\n",
    "    df=pred_df,\n",
    "    lifestage_column_name=LIFE_STAGE,\n",
    "    column_name_templates=list(col_templates.values()),\n",
    "    verbose=True,\n",
    "    exact_replace=True,\n",
    ")\n",
    "\n",
    "to_drop = [template.format(LIFE_STAGE) for template in col_templates.values()]\n",
    "pred_df = pred_df.drop(columns=to_drop, errors=\"ignore\")\n",
    "\n",
    "categories.extend([f\"{LIFE_STAGE}_merged\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 0.8382513661202186\n",
      "0.7012048192771084 0.8384733700044318\n",
      "0.7024096385542168 0.838686093268578\n",
      "0.7036144578313253 0.8393259015020673\n",
      "0.7048192771084337 0.8397913171140939\n",
      "0.7060240963855421 0.8400840005250033\n",
      "0.7072289156626506 0.8404101485473906\n",
      "0.708433734939759 0.8408144987092355\n",
      "0.7096385542168674 0.8413385411168585\n",
      "0.7108433734939759 0.8417014972752764\n",
      "0.7120481927710843 0.8423983664377205\n",
      "0.7132530120481927 0.8427459005501369\n",
      "0.7144578313253012 0.8434562641459193\n",
      "0.7156626506024096 0.8436666310959343\n",
      "0.716867469879518 0.8440957930186561\n",
      "0.7180722891566265 0.844458724931052\n",
      "0.7192771084337349 0.844779482777122\n",
      "0.7204819277108433 0.8451056281244961\n",
      "0.7216867469879518 0.8454890558113238\n",
      "0.7228915662650602 0.8457554422594481\n",
      "0.7240963855421686 0.846027027027027\n",
      "0.7253012048192771 0.846666486061821\n",
      "0.7265060240963855 0.8470559491815295\n",
      "0.7277108433734939 0.8471791523856156\n",
      "0.7289156626506024 0.8476530556615602\n",
      "0.7301204819277108 0.8479591279165073\n",
      "0.7313253012048192 0.8483604313789894\n",
      "0.7325301204819277 0.8486694101508916\n",
      "0.7337349397590361 0.8489694971145919\n",
      "0.7349397590361445 0.849186465875616\n",
      "0.736144578313253 0.849659131682813\n",
      "0.7373493975903614 0.8504564315352697\n",
      "0.7385542168674698 0.8508691674290942\n",
      "0.7397590361445783 0.8511678284777959\n",
      "0.7409638554216867 0.8516642547033285\n",
      "0.7421686746987951 0.8521360604961353\n",
      "0.7433734939759036 0.852469101280689\n",
      "0.744578313253012 0.852927991033903\n",
      "0.7457831325301204 0.8533853581854929\n",
      "0.7469879518072289 0.8539762018622183\n",
      "0.7481927710843373 0.8543481324876674\n",
      "0.7493975903614457 0.8548669566691147\n",
      "0.7506024096385542 0.8554111082788116\n",
      "0.7518072289156627 0.8560356575062458\n",
      "0.753012048192771 0.8567403739434816\n",
      "0.7542168674698795 0.8573058709419752\n",
      "0.755421686746988 0.8573797204196564\n",
      "0.7566265060240963 0.8576626651004212\n",
      "0.7578313253012048 0.8583165756966389\n",
      "0.7590361445783133 0.8587432323465038\n",
      "0.7602409638554216 0.859023207481815\n",
      "0.7614457831325301 0.8596481074198402\n",
      "0.7626506024096386 0.8600313297748897\n",
      "0.7638554216867469 0.860587550901687\n",
      "0.7650602409638554 0.860941052999825\n",
      "0.7662650602409639 0.8615308180144373\n",
      "0.7674698795180722 0.8618769961030209\n",
      "0.7686746987951807 0.8621743089621948\n",
      "0.7698795180722892 0.8625430564960108\n",
      "0.7710843373493975 0.863187088779394\n",
      "0.772289156626506 0.8636672878397965\n",
      "0.7734939759036145 0.8639665549856198\n",
      "0.7746987951807228 0.8644793152639088\n",
      "0.7759036144578313 0.8650178784266984\n",
      "0.7771084337349398 0.8655299071281393\n",
      "0.7783132530120481 0.8662393674374027\n",
      "0.7795180722891566 0.8666366366366366\n",
      "0.7807228915662651 0.8669075579644685\n",
      "0.7819277108433734 0.8673426277989016\n",
      "0.7831325301204819 0.8678280857756404\n",
      "0.7843373493975904 0.8680955556902928\n",
      "0.7855421686746987 0.8683906648839188\n",
      "0.7867469879518072 0.8688749390541199\n",
      "0.7879518072289157 0.8691680161324818\n",
      "0.789156626506024 0.8696424741889035\n",
      "0.7903614457831325 0.8701258827141541\n",
      "0.791566265060241 0.8703601108033241\n",
      "0.7927710843373493 0.8705697276288596\n",
      "0.7939759036144578 0.8709418094119102\n",
      "0.7951807228915663 0.8713187426374853\n",
      "0.7963855421686746 0.8715733200721079\n",
      "0.7975903614457831 0.8720585943587346\n",
      "0.7987951807228916 0.8724215526940867\n",
      "0.7999999999999999 0.8726879428177315\n",
      "0.8012048192771084 0.873070271969816\n",
      "0.8024096385542169 0.8735809788092835\n",
      "0.8036144578313252 0.873996079175362\n",
      "0.8048192771084337 0.8744408844335881\n",
      "0.8060240963855422 0.8749920458160992\n",
      "0.8072289156626505 0.8754908533665358\n",
      "0.808433734939759 0.8759126425003202\n",
      "0.8096385542168675 0.876453208298542\n",
      "0.8108433734939758 0.876948846798093\n",
      "0.8120481927710843 0.8772984327031831\n",
      "0.8132530120481928 0.8776423291401894\n",
      "0.8144578313253011 0.8778330569375346\n",
      "0.8156626506024096 0.8780559358497947\n",
      "0.8168674698795181 0.8784698381559588\n",
      "0.8180722891566264 0.8789489223501624\n",
      "0.8192771084337349 0.8793943383805135\n",
      "0.8204819277108434 0.8800462122462452\n",
      "0.8216867469879517 0.8802411075048023\n",
      "0.8228915662650602 0.8806897468270317\n",
      "0.8240963855421687 0.8811769018026724\n",
      "0.825301204819277 0.8816334714610347\n",
      "0.8265060240963855 0.8820808473553664\n",
      "0.8277108433734939 0.882778338378742\n",
      "0.8289156626506023 0.8833040777747773\n",
      "0.8301204819277108 0.883888663145063\n",
      "0.8313253012048192 0.8845147922964574\n",
      "0.8325301204819276 0.8850402291013227\n",
      "0.8337349397590361 0.8853938730853391\n",
      "0.8349397590361445 0.8860212101451762\n",
      "0.836144578313253 0.886737857388908\n",
      "0.8373493975903614 0.8873137166764634\n",
      "0.8385542168674698 0.8878206240671965\n",
      "0.8397590361445783 0.8884668989547039\n",
      "0.8409638554216867 0.8888422672121403\n",
      "0.8421686746987951 0.889169738458838\n",
      "0.8433734939759036 0.8895660138086515\n",
      "0.844578313253012 0.8901067816986069\n",
      "0.8457831325301204 0.8906482993680324\n",
      "0.8469879518072289 0.8909401953097156\n",
      "0.8481927710843373 0.8914956011730205\n",
      "0.8493975903614457 0.8919103383863783\n",
      "0.8506024096385542 0.8920847750865052\n",
      "0.8518072289156626 0.8922976737455229\n",
      "0.853012048192771 0.8927688953488372\n",
      "0.8542168674698795 0.8931033225135855\n",
      "0.8554216867469879 0.8935834951812085\n",
      "0.8566265060240963 0.8938759006028525\n",
      "0.8578313253012048 0.8940651396112279\n",
      "0.8590361445783132 0.8944366249351804\n",
      "0.8602409638554216 0.8945802179816241\n",
      "0.8614457831325301 0.8950730267827126\n",
      "0.8626506024096385 0.8952795170784748\n",
      "0.8638554216867469 0.8957392351701294\n",
      "0.8650602409638554 0.8964187119464508\n",
      "0.8662650602409638 0.897215786075132\n",
      "0.8674698795180722 0.8982371794871795\n",
      "0.8686746987951807 0.8985746034178864\n",
      "0.8698795180722891 0.8990366088631985\n",
      "0.8710843373493975 0.8992605211196717\n",
      "0.872289156626506 0.8992136406104018\n",
      "0.8734939759036144 0.8996087636932707\n",
      "0.8746987951807228 0.9000078548425104\n",
      "0.8746987951807228 0.9000078548425104\n"
     ]
    }
   ],
   "source": [
    "class_of_interest = \"cell_line\"\n",
    "\n",
    "threshold, recall = find_target_recall(\n",
    "    df=pred_df,\n",
    "    class_of_interest=class_of_interest,\n",
    "    category_name=BIOMATERIAL_TYPE,\n",
    "    col_templates=col_templates,\n",
    "    target_recall=0.9,\n",
    ")\n",
    "\n",
    "print(threshold, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/USHERBROOKE/rabj2301/Projects/envs/epiclass/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning:\n",
      "\n",
      "Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "\n",
      "/home/local/USHERBROOKE/rabj2301/Projects/envs/epiclass/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning:\n",
      "\n",
      "Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "           cell_line      0.601     0.900     0.721     25425\n",
      "        primary_cell      0.006     0.681     0.011       354\n",
      "primary_cell_culture      0.000     0.000     0.000         0\n",
      "      primary_tissue      0.001     0.607     0.001        28\n",
      "\n",
      "           micro avg      0.219     0.897     0.352     25807\n",
      "           macro avg      0.152     0.547     0.183     25807\n",
      "        weighted avg      0.592     0.897     0.710     25807\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/USHERBROOKE/rabj2301/Projects/envs/epiclass/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning:\n",
      "\n",
      "Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "true = col_templates[\"true_col\"].format(BIOMATERIAL_TYPE)\n",
    "pred = col_templates[\"pred_col\"].format(BIOMATERIAL_TYPE)\n",
    "max_pred = col_templates[\"max_pred\"].format(BIOMATERIAL_TYPE)\n",
    "\n",
    "tmp_df = pred_df[pred_df[max_pred] > 0.875]\n",
    "\n",
    "labels = sorted(tmp_df[pred].unique())\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true=tmp_df[true], y_pred=tmp_df[pred], labels=labels, digits=3\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[167]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_pred_col = f\"Predicted class ({ASSAY})\"\n",
    "assay_max_pred_col = f\"Max pred ({ASSAY})\"\n",
    "\n",
    "verbose = True\n",
    "\n",
    "threshold_dfs = {}\n",
    "other_info = {}\n",
    "\n",
    "for category in categories:\n",
    "    print(\"TASK:\", category)\n",
    "    col_mapper = {k: v.format(category) for k, v in col_templates.items()}\n",
    "\n",
    "    df = pred_df.copy()\n",
    "\n",
    "    # Filter unknown/NA\n",
    "    df = df[~df[col_mapper[\"true_col\"]].isin(unknown_values)]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Know labels distribution:\")\n",
    "        print(df[col_mapper[\"true_col\"]].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "    if category == ASSAY:\n",
    "        pred_col = col_mapper[\"pred_col\"]\n",
    "        df.loc[:, pred_col] = df.loc[:, pred_col].replace(ASSAY_MERGE_DICT, inplace=False)\n",
    "\n",
    "        # All supposed to be rna-seq-like assays\n",
    "        true_col = col_mapper[\"true_col\"]\n",
    "        df.loc[:, true_col] = \"rna_seq\"\n",
    "    else:\n",
    "        # Only keep \"similar to training\" dsets\n",
    "        # Predicted as m/rna-seq by assay classifier with high-pred (>0.6)\n",
    "        cond1 = df[assay_pred_col].isin([\"rna_seq\", \"mrna_seq\"])\n",
    "        cond2 = df[assay_max_pred_col] > 0.6\n",
    "        df = df[cond1 & cond2]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"All labels distribution after 11c filter:\")\n",
    "        print(df[col_mapper[\"true_col\"]].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "    if category == f\"{LIFE_STAGE}_merged\":\n",
    "        df = df[~df[BIOMATERIAL_TYPE].isin(cell_line_vals)]\n",
    "        if verbose:\n",
    "            print(\"Biomaterial type, lifestage and assay post cell line filter:\")\n",
    "            for col in [BIOMATERIAL_TYPE, ASSAY, f\"{LIFE_STAGE}_merged\"]:\n",
    "                print(df[col].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "    cat_name = category_remapper[category]\n",
    "\n",
    "    nb_samples = df.shape[0]\n",
    "    N_true_classes = len(set(df[col_mapper[\"true_col\"]]))\n",
    "    total_N_classes = len(\n",
    "        set(df[col_mapper[\"pred_col\"]]) | set(df[col_mapper[\"true_col\"]])\n",
    "    )\n",
    "    other_info[cat_name] = {\n",
    "        \"nb_samples\": nb_samples,\n",
    "        \"nb_classes\": N_true_classes,\n",
    "        \"total_possible_classes\": total_N_classes,\n",
    "    }\n",
    "\n",
    "    threshold_dfs[cat_name] = evaluate_thresholds_global(\n",
    "        df, thresholds, verbose=verbose, columns=col_mapper  # type: ignore\n",
    "    )\n",
    "\n",
    "all_threshold_results[\"recount3\"] = {\"results\": threshold_dfs, \"other_info\": other_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for task_name in categories:\n",
    "#     df = threshold_dfs[task_name]\n",
    "#     print(\"TASK:\", task_name)\n",
    "#     nb_samples = other_info[task_name][\"nb_samples\"]\n",
    "#     nb_classes = other_info[task_name][\"nb_classes\"]\n",
    "\n",
    "#     # create figure\n",
    "#     name = f\"{task_name} - {nb_classes} classes\"\n",
    "#     fig = create_thresholds_graph_global_plotly(df, f\"{name} - n={nb_samples}\", xrange=(max(0, 1.0/nb_classes-0.05), 1.001))\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph results for training and inference per database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2hex(r, g, b):\n",
    "    \"\"\"Convert rgb to hex.\"\"\"\n",
    "    return f\"#{r:02x}{g:02x}{b:02x}\"\n",
    "\n",
    "\n",
    "def hex2rgb(hex_str):\n",
    "    \"\"\"Convert hex to rgb.\"\"\"\n",
    "    return ImageColor.getrgb(hex_str)\n",
    "\n",
    "\n",
    "def add_acc_f1(\n",
    "    fig: go.Figure,\n",
    "    df: pd.DataFrame,\n",
    "    row: int,\n",
    "    col: int,\n",
    "    colors: List[str],\n",
    "    show_legend: bool = True,\n",
    "    label_modifier: str = \"\",\n",
    "    color_mod: int = 0,\n",
    ") -> None:\n",
    "    \"\"\"Add accuracy and F1 to the figure.\n",
    "\n",
    "    Args:\n",
    "        fig: The figure to add the traces to.\n",
    "        df: The dataframe containing the data.\n",
    "        row: The row of the subplot. (1 indexed)\n",
    "        col: The column of the subplot. (1 indexd)\n",
    "        colors: The colors to use for the traces (1 for accuracy, 2 for F1).\n",
    "        show_legend: Whether to show the legend.\n",
    "        label_modifier: A string to add to the legend.\n",
    "        color_mod: The RGB amount to modify the color by.\n",
    "    \"\"\"\n",
    "    acc_label = df.filter(like=\"Acc\").columns[0]\n",
    "    f1_label = df.filter(like=\"F1\").columns[0]\n",
    "\n",
    "    color_acc = colors[1]\n",
    "    color_f1 = colors[2]\n",
    "\n",
    "    name_acc = acc_label\n",
    "    name_f1 = f1_label\n",
    "\n",
    "    if label_modifier:\n",
    "        # Names\n",
    "        name_acc = f\"{name_acc} {label_modifier}\"\n",
    "        name_f1 = f\"{name_f1} {label_modifier}\"\n",
    "\n",
    "        N = color_mod\n",
    "        # Acc\n",
    "        rgb_color = hex2rgb(color_acc)\n",
    "        rgb_vals = [max(color_val - N, 0) for color_val in rgb_color]\n",
    "        color_acc = rgb2hex(*rgb_vals)\n",
    "\n",
    "        # F1\n",
    "        rgb_color = hex2rgb(color_f1)\n",
    "        rgb_vals = [max(color_val - N, 0) for color_val in rgb_color]\n",
    "        color_f1 = rgb2hex(*rgb_vals)\n",
    "\n",
    "    # Plot accuracy\n",
    "    acc_vals = df[acc_label]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"Threshold\"],\n",
    "            y=acc_vals,\n",
    "            name=name_acc,\n",
    "            line=dict(color=color_acc, dash=\"solid\"),\n",
    "            mode=\"lines\",\n",
    "            showlegend=show_legend,\n",
    "            legendgroup=\"Accuracy\",\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    # Plot F1\n",
    "    prec_vals = df[f1_label]\n",
    "    if not prec_vals.isna().all():\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"Threshold\"],\n",
    "                y=prec_vals,\n",
    "                name=name_f1,\n",
    "                line=dict(color=color_f1, dash=\"dot\"),\n",
    "                mode=\"lines\",\n",
    "                showlegend=show_legend,\n",
    "                legendgroup=\"F1-score\",\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "\n",
    "def add_subset_size(\n",
    "    fig: go.Figure,\n",
    "    df: pd.DataFrame,\n",
    "    row: int,\n",
    "    col: int,\n",
    "    colors: List[str],\n",
    "    show_legend: bool = True,\n",
    "    label_modifier: str = \"\",\n",
    "    color_mod: int = 1,\n",
    ") -> None:\n",
    "    \"\"\"Add file count relative size to the figure.\"\"\"\n",
    "    # Plot subset size on secondary Y-axis\n",
    "    subset_label = df.filter(like=\"Subset\").columns[0]\n",
    "    trace_name = subset_label.split(\"(\")[0].strip() + \" (%)\"\n",
    "\n",
    "    trace_color = colors[0]\n",
    "\n",
    "    if label_modifier:\n",
    "        trace_name = f\"{trace_name} {label_modifier}\"\n",
    "\n",
    "        N = color_mod\n",
    "        rgb_color = hex2rgb(trace_color)\n",
    "        rgb_vals = [min(color_val + N, 255) for color_val in rgb_color]\n",
    "        trace_color = rgb2hex(*rgb_vals)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"Threshold\"],\n",
    "            y=df[subset_label],\n",
    "            name=trace_name,\n",
    "            line=dict(color=trace_color, dash=\"dash\"),\n",
    "            yaxis=\"y2\",\n",
    "            mode=\"lines\",\n",
    "            showlegend=show_legend,\n",
    "            legendgroup=\"Subset Size\",\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "\n",
    "def graph_all_DB_threshold_graphs(\n",
    "    results_dict: Dict[str, Dict],\n",
    "    output_dir: Path | None = None,\n",
    "    filename: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a threshold graph for mutiple DBs and classifiers.\n",
    "\n",
    "    Args:\n",
    "        results_dict: A dictionary containing the results for each DB and classifier.\n",
    "        output_dir: The directory to save the graph to.\n",
    "        name: The name of the graph.\n",
    "\n",
    "    \"\"\"\n",
    "    category_order = [ASSAY, SEX, CANCER, LIFE_STAGE, BIOMATERIAL_TYPE]\n",
    "    DBs_order = [\"EpiATLAS\", \"ENCODE_core\", \"ENCODE_non-core\", \"ChIP-Atlas\", \"recount3\"]\n",
    "    graph_renamer = {\n",
    "        ASSAY: \"Assay\",\n",
    "        SEX: \"Sex\",\n",
    "        CANCER: \"Cancer status\",\n",
    "        LIFE_STAGE: \"Life stage\",\n",
    "        BIOMATERIAL_TYPE: \"Biomaterial type\",\n",
    "    }\n",
    "\n",
    "    # color-blind friendly\n",
    "    # black, blue, red\n",
    "    colors = [\"#000000\", \"#005AB5\", \"#DC3220\"]\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=5,\n",
    "        cols=5,\n",
    "        row_titles=DBs_order,\n",
    "        column_titles=[graph_renamer[category] for category in category_order],\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.025,\n",
    "        horizontal_spacing=0.04,\n",
    "        x_title=\"Prediction Score Threshold\",\n",
    "        y_title=\"Metric value\",\n",
    "    )\n",
    "\n",
    "    y_ranges = {\n",
    "        \"EpiATLAS\": [0.7, 1.01],\n",
    "        \"ChIP-Atlas\": [0.1, 1.01],\n",
    "        \"ENCODE_core\": [0.45, 1.01],\n",
    "        \"ENCODE_non-core\": [0.30, 1.01],\n",
    "        \"recount3\": [0, 1.01],\n",
    "    }\n",
    "\n",
    "    for i, DB in enumerate(DBs_order):\n",
    "        # Add empty subplot row, temporary\n",
    "        if DB == \"EpiATLAS\":\n",
    "            for j, _ in enumerate(category_order):\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=[],\n",
    "                        y=[],\n",
    "                        name=\"\",\n",
    "                    ),\n",
    "                    row=i + 1,\n",
    "                    col=j + 1,\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        data = results_dict[DB]\n",
    "        for j, category in enumerate(category_order):\n",
    "            show_legend = bool(j == 0 and i == 0)\n",
    "\n",
    "            try:\n",
    "                threshold_df = data[\"results\"][category]\n",
    "            except KeyError as e:\n",
    "                print(f\"Could not find results for {DB} {category}: {e}\")\n",
    "                continue\n",
    "\n",
    "            add_acc_f1(fig, threshold_df, i + 1, j + 1, colors, show_legend)\n",
    "            add_subset_size(fig, threshold_df, i + 1, j + 1, colors, show_legend)\n",
    "\n",
    "            # Nb files + classes\n",
    "            try:\n",
    "                other_info = data[\"other_info\"][category]\n",
    "            except KeyError as e:\n",
    "                print(f\"Could not find other info for {DB} {category}: {e}\")\n",
    "                continue\n",
    "\n",
    "            N = other_info[\"nb_samples\"]\n",
    "            c_true = other_info[\"nb_classes\"]\n",
    "            c_all = other_info[\"total_possible_classes\"]\n",
    "            annotation_text = f\"N = {N}<br>C = {c_true}/{c_all}\"\n",
    "            # print(DB, category, annotation_text)\n",
    "            fig.add_annotation(\n",
    "                text=annotation_text,\n",
    "                showarrow=False,\n",
    "                font=dict(size=10, color=\"black\"),\n",
    "                # y=0.1,\n",
    "                # xref=f\"x{i+1} domain\",\n",
    "                # yref=f\"y{j+1} domain\",\n",
    "                row=i + 1,\n",
    "                col=j + 1,\n",
    "            )\n",
    "\n",
    "    # Set y-axis ranges\n",
    "    for i, DB in enumerate(DBs_order):\n",
    "        y_range = y_ranges[DB]\n",
    "        for j in range(1, 6):\n",
    "            dtick = 0.2\n",
    "            if DB in [\"ENCODE_core\", \"ENCODE_non-core\", \"EpiATLAS\"]:\n",
    "                dtick = 0.1\n",
    "\n",
    "            fig.update_yaxes(range=y_range, row=i + 1, col=j, dtick=dtick)\n",
    "\n",
    "    fig.update_xaxes(range=[0.1, 1.01], dtick=0.2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=800,\n",
    "        height=800,\n",
    "        title=\"All Databases - 5 classifiers - Metrics at Different Pred. Score Thresholds\",\n",
    "    )\n",
    "\n",
    "    fig.update_layout(hovermode=\"x unified\", hoverlabel_namelength=-1)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    if output_dir:\n",
    "        if not filename:\n",
    "            filename = \"all_DBs_5_classifiers_thresholds\"\n",
    "        fig.write_image(output_dir / f\"{filename}.svg\")\n",
    "        fig.write_image(output_dir / f\"{filename}.png\")\n",
    "        fig.write_html(output_dir / f\"{filename}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = base_fig_dir / \"threshold_graphs\" / \"100kb_all_none\"\n",
    "\n",
    "graph_all_DB_threshold_graphs(\n",
    "    all_threshold_results,\n",
    "    output_dir=output_dir,\n",
    "    filename=\"3DBs_5_classifiers_thresholds_w_ENCODE_split\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
