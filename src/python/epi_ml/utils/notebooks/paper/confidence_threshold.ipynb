{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot accuracy, precision, and subset size for different probability thresholds.\"\"\"\n",
    "\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, pointless-statement, use-dict-literal, expression-not-assigned, unused-import, too-many-lines, too-many-branches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from PIL import ImageColor\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from epi_ml.utils.general_utility import get_valid_filename\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    BIOMATERIAL_TYPE,\n",
    "    CANCER,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    SplitResultsHandler,\n",
    "    format_labels,\n",
    "    merge_life_stages,\n",
    "    rename_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_seq = np.typing.NDArray[np.floating] | Sequence[float | np.floating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "paper_dir = base_dir\n",
    "if not paper_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {paper_dir} does not exist.\")\n",
    "\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "table_dir = paper_dir / \"tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core7_assays = ASSAY_ORDER[:7]\n",
    "core9_assays = ASSAY_ORDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence threshold impact on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB: {\"results\": dict, \"other_info\": dict}\n",
    "all_threshold_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing and co. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    threshold: float,\n",
    "    true_col: str,\n",
    "    pred_col: str,\n",
    "    pred_prob_cols: List[str],\n",
    "    target_class: str | None,\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, and subset size for a given probability threshold and class.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the true labels, predicted labels, and predicted probabilities.\n",
    "    threshold (float): The probability threshold for filtering the DataFrame.\n",
    "    true_col (str): The column name containing the true labels.\n",
    "    pred_col (str): The column name containing the predicted labels.\n",
    "    pred_prob_cols (List[str]): List of column names containing the predicted probabilities.\n",
    "    target_class (str|None): The class for which precision is to be calculated. Return np.nan if None.\n",
    "\n",
    "    Considers target class for computations if given, otherwise considers all samples.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, float, float, float]: A tuple containing the threshold, the calculated accuracy (%), the calculated precision (%),\n",
    "                                       and the subset size (%) respectively.\n",
    "    \"\"\"\n",
    "    # Targeting a class or not\n",
    "    if target_class in [None, \"all\"]:\n",
    "        total_size = len(df)\n",
    "    else:\n",
    "        total_size = len(df[true_col] == target_class)\n",
    "\n",
    "    # Filter rows where the max predicted probability is above the threshold\n",
    "    try:\n",
    "        subset_df = df[df[pred_prob_cols].max(axis=1) >= threshold]\n",
    "    except TypeError as e:\n",
    "        print(\n",
    "            f\"Error: Could not filter rows.\\npred_cols: {pred_prob_cols}\\nthreshold: {threshold}\"\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "    if len(subset_df) == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    # Calculate the accuracy for this subset\n",
    "    if target_class in [None, \"all\"]:\n",
    "        correct_preds = np.sum(subset_df[true_col] == subset_df[pred_col])\n",
    "        subset_size = len(subset_df)\n",
    "    else:\n",
    "        correct_preds = np.sum(\n",
    "            (subset_df[true_col] == subset_df[pred_col])\n",
    "            & (subset_df[true_col] == target_class)\n",
    "        )\n",
    "        subset_size = np.sum(subset_df[true_col] == target_class)\n",
    "    accuracy = (correct_preds / subset_size) * 100\n",
    "    subset_size_percent = (subset_size / total_size) * 100\n",
    "\n",
    "    # Calculate precision for the target class\n",
    "    if target_class in [None, \"all\"]:\n",
    "        precision = np.nan\n",
    "        return threshold, accuracy, precision, subset_size_percent\n",
    "\n",
    "    true_positives = np.sum(\n",
    "        (subset_df[true_col] == target_class) & (subset_df[pred_col] == target_class)\n",
    "    )\n",
    "    false_positives = np.sum(\n",
    "        (subset_df[true_col] != target_class) & (subset_df[pred_col] == target_class)\n",
    "    )\n",
    "\n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = np.nan\n",
    "    else:\n",
    "        precision = (true_positives / (true_positives + false_positives)) * 100\n",
    "\n",
    "    return threshold, accuracy, precision, subset_size_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_global(\n",
    "    df: pd.DataFrame,\n",
    "    threshold: float | np.floating,\n",
    "    true_col: str,\n",
    "    pred_col: str,\n",
    "    pred_prob_cols: List[str],\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, and subset size for a given probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the true labels, predicted labels, and predicted probabilities.\n",
    "    threshold (float): The probability threshold for filtering the DataFrame.\n",
    "    true_col (str): The column name containing the true labels.\n",
    "    pred_col (str): The column name containing the predicted labels.\n",
    "    pred_prob_cols (List[str]|str): List of column names containing the predicted probabilities.\n",
    "                                OR a Max PredScore column\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, float, float, float, int]: A tuple containing the threshold, the accuracy (%), the macro f1-score (%) and the subset size (%) respectively.\n",
    "    \"\"\"\n",
    "    total_size = len(df)\n",
    "\n",
    "    # Filter rows where the max predicted probability is above the threshold\n",
    "    # Normally expecting a matrix of probabilities\n",
    "    # But can deal with a Max PredScore column\n",
    "    if isinstance(pred_prob_cols, str):\n",
    "        pred_prob_cols = [pred_prob_cols]\n",
    "    try:\n",
    "        subset_df = df[df[pred_prob_cols].max(axis=1) >= threshold]\n",
    "    except TypeError as e:\n",
    "        print(\n",
    "            f\"Error: Could not filter rows.\\npred_cols: {pred_prob_cols}\\nthreshold: {threshold}\"\n",
    "        )\n",
    "        raise e\n",
    "\n",
    "    N = len(subset_df)\n",
    "    if N == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    # Metrics\n",
    "    existing_labels = sorted(df[true_col].unique())\n",
    "    acc: float = accuracy_score(subset_df[true_col], subset_df[pred_col])  # type: ignore\n",
    "    f1: float = f1_score(subset_df[true_col], subset_df[pred_col], average=\"macro\", labels=existing_labels)  # type: ignore\n",
    "    relative_size = N / total_size\n",
    "\n",
    "    return float(threshold), acc, f1, relative_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCURACY_NAME = \"rec\"\n",
    "PRECISION_NAME = \"prec\"\n",
    "SUBSET_SIZE_NAME = \"sz\"\n",
    "\n",
    "\n",
    "def find_columns(df: pd.DataFrame, verbose: bool = False) -> Dict[str, List[str] | str]:\n",
    "    \"\"\"\n",
    "    Find the columns containing true labels, predicted labels, and predicted probabilities in a DataFrame.\n",
    "    \"\"\"\n",
    "    df_cols = df.columns\n",
    "    df_cols = [col for col in df_cols if str(col) not in [\"TRUE\", \"FALSE\"]]\n",
    "\n",
    "    likely_true_class_cols = [\n",
    "        col for col in df_cols if \"true\" in col.lower() or \"expected\" in col.lower()\n",
    "    ]\n",
    "    likely_pred_class_cols = [col for col in df_cols if \"pred\" in col.lower()]\n",
    "\n",
    "    if not likely_true_class_cols or not likely_pred_class_cols:\n",
    "        raise ValueError(\n",
    "            \"Could not automatically detect 'True class' or 'Predicted class' columns.\"\n",
    "        )\n",
    "\n",
    "    true_col = likely_true_class_cols[0]\n",
    "    pred_col = likely_pred_class_cols[0]\n",
    "    if df[true_col].dtype != object or df[pred_col].dtype != object:\n",
    "        print(f\"{true_col} and {pred_col} are not string columns. Could cause issues.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"True class: {true_col}\")\n",
    "        print(f\"Predicted class: {pred_col}\")\n",
    "\n",
    "    classes = df[true_col].unique().tolist() + [\"all\"]\n",
    "    pred_prob_cols = classes[0:-1]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Classes: {classes}\")\n",
    "        print(f\"Predicted probability columns: {pred_prob_cols}\")\n",
    "\n",
    "    for col in pred_prob_cols:\n",
    "        if df[col].dtype != float:\n",
    "            print(f\"{col} is not a float column ({df[col].dtype}). Could cause issues.\")\n",
    "\n",
    "    return {\n",
    "        \"true_col\": true_col,\n",
    "        \"pred_col\": pred_col,\n",
    "        \"classes\": classes,\n",
    "        \"pred_prob_cols\": pred_prob_cols,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_thresholds(\n",
    "    df: pd.DataFrame, thresholds: List[float], verbose: bool = False\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy and subset size for different probability thresholds with improved automatic column detection.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe containing true labels and predicted probabilities.\n",
    "    thresholds (list): List of probability thresholds to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe containing the accuracy and subset size for each threshold.\n",
    "    \"\"\"\n",
    "    columns = find_columns(df, verbose=verbose)\n",
    "    true_col: str = columns[\"true_col\"]  # type: ignore\n",
    "    pred_col: str = columns[\"pred_col\"]  # type: ignore\n",
    "    classes: List[str] = columns[\"classes\"]  # type: ignore\n",
    "    pred_prob_cols: List[str] = columns[\"pred_prob_cols\"]  # type: ignore\n",
    "\n",
    "    # Evaluate each threshold over each class\n",
    "    results_dfs = {}\n",
    "    for class_label in classes:\n",
    "        results = []\n",
    "        filtered_df = (\n",
    "            df\n",
    "            if class_label == \"all\"\n",
    "            else df[(df[true_col] == class_label) | (df[pred_col] == class_label)]\n",
    "        )\n",
    "\n",
    "        for thresh in thresholds:\n",
    "            try:\n",
    "                result = compute_metrics(\n",
    "                    filtered_df,\n",
    "                    thresh,\n",
    "                    true_col,\n",
    "                    pred_col,\n",
    "                    pred_prob_cols,\n",
    "                    target_class=class_label,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Error. Could not compute metric with class {class_label}.\\ntrue_col: {true_col}\\npred_col: {pred_col}\\npred_prob_cols: {pred_prob_cols}\\n\"\n",
    "                )\n",
    "                raise e\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "        # Convert to DataFrame for easier manipulation\n",
    "        short_class_label = class_label[0:10]\n",
    "        results_df = pd.DataFrame(\n",
    "            results,\n",
    "            columns=[\n",
    "                \"Threshold\",\n",
    "                f\"{ACCURACY_NAME}_{short_class_label} (%)\",\n",
    "                f\"{PRECISION_NAME}_{short_class_label} (%)\",\n",
    "                f\"{SUBSET_SIZE_NAME}_{short_class_label} (%) ({filtered_df.shape[0]})\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        results_dfs[class_label] = results_df\n",
    "\n",
    "    return results_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_thresholds_global(\n",
    "    df: pd.DataFrame,\n",
    "    thresholds: float_seq,\n",
    "    verbose: bool = False,\n",
    "    columns: Dict[str, List[str] | str] | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy and subset size for different probability thresholds with improved automatic column detection.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe containing true labels and predicted probabilities.\n",
    "        thresholds (list): List of probability thresholds to evaluate.\n",
    "        verbose (bool): Whether to print verbose information.\n",
    "        columns (dict): A dictionary containing the column names for true labels, predicted labels, and predicted probabilities.\n",
    "                        Expecting entries: \"true_col\", \"pred_col\", \"pred_prob_cols\"|\"max_pred\".\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing the accuracy and subset size for each threshold.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = find_columns(df, verbose=verbose)\n",
    "        true_col: str = columns[\"true_col\"]  # type: ignore\n",
    "        pred_col: str = columns[\"pred_col\"]  # type: ignore\n",
    "        pred_prob_cols: List[str] = columns[\"pred_prob_cols\"]  # type: ignore\n",
    "    else:\n",
    "        true_col: str = columns[\"true_col\"]  # type: ignore\n",
    "        pred_col: str = columns[\"pred_col\"]  # type: ignore\n",
    "        try:\n",
    "            pred_prob_cols: List[str] = columns[\"pred_prob_cols\"]  # type: ignore\n",
    "        except KeyError:\n",
    "            pred_prob_cols: List[str] = [columns[\"max_pred\"]]  # type: ignore\n",
    "\n",
    "    # Evaluate each threshold over each class\n",
    "    results = []\n",
    "    for tresh in thresholds:\n",
    "        try:\n",
    "            result = compute_metrics_global(df, tresh, true_col, pred_col, pred_prob_cols)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error. Could not compute metrics.\\ntrue_col: {true_col}\\npred_col: {pred_col}\\npred_prob_cols: {pred_prob_cols}\\n\"\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    results_df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\n",
    "            \"Threshold\",\n",
    "            \"Accuracy (%)\",\n",
    "            \"F1-score\",\n",
    "            f\"Subset size (%) ({df.shape[0]})\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thresholds_graph_global_plotly(\n",
    "    metrics_df: pd.DataFrame, name: str, xrange: Tuple[float, float] | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Return graph of the accuracy and subset size at different probability thresholds for global results.\n",
    "\n",
    "    Parameters:\n",
    "    metrics_df (pd.DataFrame): DataFrame with metrics at different probability thresholds.\n",
    "    name (str): Graph title.\n",
    "\n",
    "    Returns:\n",
    "    go.Figure: Plotly figure object with the plotted graph.\n",
    "    \"\"\"\n",
    "    # color-blind friendly\n",
    "    # black, blue, red\n",
    "    colors = [\"#000000\", \"#005AB5\", \"#DC3220\"]\n",
    "    marker1 = \"square-open\"\n",
    "    marker2 = \"cross-open\"\n",
    "    marker3 = \"circle\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    acc_label = metrics_df.filter(like=\"Acc\").columns[0]\n",
    "    f1_score_label = metrics_df.filter(like=\"F1\").columns[0]\n",
    "    subset_size_label = metrics_df.filter(like=\"Subset\").columns[0]\n",
    "\n",
    "    # Plot accuracy\n",
    "    vals = metrics_df[acc_label]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=metrics_df[\"Threshold\"],\n",
    "            y=vals,\n",
    "            name=acc_label,\n",
    "            line=dict(color=colors[2]),\n",
    "            marker_symbol=marker1,\n",
    "            mode=\"lines+markers\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot f1_score\n",
    "    vals = metrics_df[f1_score_label]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=metrics_df[\"Threshold\"],\n",
    "            y=vals,\n",
    "            name=f1_score_label,\n",
    "            line=dict(color=colors[1], dash=\"dot\"),\n",
    "            marker_symbol=marker2,\n",
    "            mode=\"lines+markers\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot subset size on secondary Y-axis\n",
    "    vals = metrics_df[subset_size_label]\n",
    "    min_y2 = vals.min()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=metrics_df[\"Threshold\"],\n",
    "            y=vals,\n",
    "            name=subset_size_label.split(\"(\")[0].strip(),\n",
    "            line=dict(color=colors[0], dash=\"dash\"),\n",
    "            marker_symbol=marker3,\n",
    "            yaxis=\"y2\",\n",
    "            mode=\"lines+markers\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Adjusting the layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Metrics at Different Pred. Score Thresholds<br>{name}\",\n",
    "        xaxis_title=\"Prediction Score Threshold\",\n",
    "        xaxis=dict(\n",
    "            tickvals=np.linspace(0, 1, 11),\n",
    "            ticktext=[f\"{x:.1f}\" for x in np.linspace(0, 1, 11)],\n",
    "        ),\n",
    "        yaxis_title=\"Accuracy / F1-score (%)\",\n",
    "        yaxis2=dict(title=\"Subset Size (%)\", overlaying=\"y\", side=\"right\"),\n",
    "        legend=dict(orientation=\"v\", x=1.1, y=1),\n",
    "        height=500,\n",
    "        width=500,\n",
    "        yaxis2_range=[min_y2 - 0.001, 1.001],\n",
    "    )\n",
    "\n",
    "    if not xrange:\n",
    "        xrange = (-0.001, 1.001)\n",
    "    fig.update_xaxes(range=xrange)\n",
    "\n",
    "    fig.update_traces(line={\"width\": 1})\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thresholds_graph_plotly(threshold_dfs: Dict[str, pd.DataFrame], name: str):\n",
    "    \"\"\"\n",
    "    Return graph of the accuracy and subset size at different probability thresholds for all classes.\n",
    "\n",
    "    Parameters:\n",
    "    threshold_metrics_df (Dict[str, pd.DataFrame]): A dictionary containing dfs with metrics for each class label and the general case.\n",
    "    name (str): Graph title.\n",
    "\n",
    "    Returns:\n",
    "    go.Figure: Plotly figure object with the plotted graph.\n",
    "    \"\"\"\n",
    "    colors = px.colors.qualitative.Dark24\n",
    "    marker1 = \"circle\"\n",
    "    marker2 = \"cross-open\"\n",
    "    marker3 = \"circle-open\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for idx, (_, threshold_metrics) in enumerate(threshold_dfs.items()):\n",
    "        color = colors[idx % len(colors)]\n",
    "\n",
    "        acc_label = threshold_metrics.filter(like=f\"{ACCURACY_NAME}\").columns[0]\n",
    "        acc_subset = threshold_metrics.filter(like=f\"{SUBSET_SIZE_NAME}\").columns[0]\n",
    "        prec_label = threshold_metrics.filter(like=f\"{PRECISION_NAME}\").columns[0]\n",
    "\n",
    "        # Plot accuracy\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=threshold_metrics[\"Threshold\"],\n",
    "                y=threshold_metrics[acc_label],\n",
    "                name=acc_label,\n",
    "                line=dict(color=color),\n",
    "                marker_symbol=marker1,\n",
    "                mode=\"lines+markers\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Plot precision\n",
    "        prec_vals = threshold_metrics[prec_label]\n",
    "        if not prec_vals.isna().all():\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=threshold_metrics[\"Threshold\"],\n",
    "                    y=prec_vals,\n",
    "                    name=prec_label,\n",
    "                    line=dict(color=color, dash=\"dot\"),\n",
    "                    marker_symbol=marker2,\n",
    "                    mode=\"lines+markers\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Plot subset size on secondary Y-axis\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=threshold_metrics[\"Threshold\"],\n",
    "                y=threshold_metrics[acc_subset],\n",
    "                name=acc_subset,\n",
    "                line=dict(color=color, dash=\"dash\"),\n",
    "                marker_symbol=marker3,\n",
    "                yaxis=\"y2\",\n",
    "                mode=\"lines+markers\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Adjusting the layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Accuracy and Subset Size at Different Probability Thresholds<br>{name}\",\n",
    "        xaxis_title=\"Probability Threshold\",\n",
    "        xaxis=dict(\n",
    "            tickvals=np.linspace(0, 1, 11),\n",
    "            ticktext=[f\"{x:.1f}\" for x in np.linspace(0, 1, 11)],\n",
    "        ),\n",
    "        yaxis_title=\"Accuracy (%)\",\n",
    "        yaxis2=dict(title=\"Subset Size (%)\", overlaying=\"y\", side=\"right\"),\n",
    "        legend=dict(orientation=\"v\", x=1.05, y=1),\n",
    "        height=1000,\n",
    "        width=1600,\n",
    "    )\n",
    "    fig.update_xaxes(range=[-0.001, 1.001])\n",
    "    fig.update_traces(line={\"width\": 1})\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds: List[float] = [float(x) for x in np.arange(0, 1, 1 / 20)] + [0.99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP EpiAtlas cross-validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_remapper = {\n",
    "    \"assay\": ASSAY,\n",
    "    \"assay7\": ASSAY,\n",
    "    f\"{ASSAY}_11c\": ASSAY,\n",
    "    ASSAY: ASSAY,\n",
    "    \"sex\": SEX,\n",
    "    \"sex3\": SEX,\n",
    "    SEX: SEX,\n",
    "    \"harmonized_donor_sex_w-mixed\": SEX,\n",
    "    \"cancer\": CANCER,\n",
    "    CANCER: CANCER,\n",
    "    \"biomat\": BIOMATERIAL_TYPE,\n",
    "    BIOMATERIAL_TYPE: BIOMATERIAL_TYPE,\n",
    "}\n",
    "\n",
    "for l in [\"donorlife\", \"lifestage\", LIFE_STAGE]:\n",
    "    category_remapper[l] = LIFE_STAGE\n",
    "    category_remapper[f\"{l}_merged\"] = LIFE_STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    ASSAY,\n",
    "    CELL_TYPE,\n",
    "    SEX,\n",
    "    LIFE_STAGE,\n",
    "    BIOMATERIAL_TYPE,\n",
    "    CANCER,\n",
    "    \"paired_end\",\n",
    "    \"project\",\n",
    "]\n",
    "split_results_handler = SplitResultsHandler()\n",
    "\n",
    "data_dir_100kb = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select 10-fold oversampling runs\n",
    "# all_split_dfs = split_results_handler.general_split_metrics(\n",
    "#     results_dir=data_dir_100kb,\n",
    "#     merge_assays=False,\n",
    "#     include_categories=categories,\n",
    "#     exclude_names=[\"reg\", \"no-mixed\", \"chip\", \"16ct\", \"27ct\"],\n",
    "#     return_type=\"split_results\",\n",
    "#     oversampled_only=True,\n",
    "#     verbose=False,\n",
    "# )\n",
    "# all_split_dfs_concat: Dict = split_results_handler.concatenate_split_results(all_split_dfs, concat_first_level=True)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing special case \"paired_end\" which has bool values that aren't treated as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = [\"True class\", \"Predicted class\"]\n",
    "# df = all_split_dfs_concat[\"paired_end\"].copy()\n",
    "\n",
    "# # labels: bool -> str\n",
    "# df[cols] = df[cols].astype(str)\n",
    "# for col in cols:\n",
    "#     df[col] = df[col].str.lower()\n",
    "\n",
    "# # make sure column names = class names\n",
    "# df = df.rename(columns={\"TRUE\": \"true\", \"FALSE\": \"false\"})\n",
    "\n",
    "# all_split_dfs_concat[\"paired_end\"] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing all values separately from graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_dfs = {}\n",
    "# for task_name, df in all_split_dfs_concat.items():\n",
    "#     print(\"TASK:\",task_name)\n",
    "#     threshold_dfs[task_name] = evaluate_thresholds(df, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = base_fig_dir / \"threshold_graphs\" / \"100kb_all_none\"\n",
    "# if not output_dir.exists():\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for task_name, df in all_split_dfs_concat.items():\n",
    "#     print(\"TASK:\", task_name)\n",
    "#     nb_samples = len(df)\n",
    "#     nb_classes = df[\"True class\"].nunique()\n",
    "\n",
    "#     df = threshold_dfs[task_name]\n",
    "\n",
    "#     # create figure\n",
    "#     name = f\"{task_name} - {nb_classes} classes\"\n",
    "#     fig = create_thresholds_graph_plotly(df, f\"{name} - n={nb_samples}\")\n",
    "#     fig.show()\n",
    "\n",
    "#     # # save\n",
    "#     filename = f\"threshold_impact_graph_full_{get_valid_filename(name)}\".replace(\n",
    "#         \"_-_\", \"-\"\n",
    "#     )\n",
    "#     fig.write_image(output_dir / f\"{filename}.png\")\n",
    "#     fig.write_image(output_dir / f\"{filename}.svg\")\n",
    "#     fig.write_html(output_dir / f\"{filename}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_dfs = {}\n",
    "# other_info = {}\n",
    "# for task_name, df in all_split_dfs_concat.items():\n",
    "#     print(\"TASK:\", task_name)\n",
    "#     nb_samples = len(df)\n",
    "#     nb_classes = df[\"True class\"].nunique()\n",
    "\n",
    "#     other_info[task_name] = {\"nb_samples\": nb_samples, \"nb_classes\": nb_classes}\n",
    "\n",
    "#     threshold_dfs[task_name] = evaluate_thresholds_global(df, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = base_fig_dir / \"threshold_graphs\" / \"100kb_all_none\" / \"EpiATLAS\"\n",
    "# if not output_dir.exists():\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for task_name, df in all_split_dfs_concat.items():\n",
    "#     print(\"TASK:\", task_name)\n",
    "#     nb_samples = len(df)\n",
    "#     nb_classes = df[\"True class\"].nunique()\n",
    "\n",
    "#     df = threshold_dfs[task_name]\n",
    "\n",
    "#     # create figure\n",
    "#     name = f\"{task_name} - {nb_classes} classes\"\n",
    "#     fig = create_thresholds_graph_global_plotly(df, f\"{name} - n={nb_samples}\", xrange=(max(0, 1.0/nb_classes-0.05), 1.001))\n",
    "#     # fig.show()\n",
    "\n",
    "#     # # save\n",
    "#     filename = f\"threshold_impact_graph_global_{get_valid_filename(name)}\".replace(\n",
    "#         \"_-_\", \"-\"\n",
    "#     )\n",
    "#     fig.write_image(output_dir / f\"{filename}.png\")\n",
    "#     fig.write_image(output_dir / f\"{filename}.svg\")\n",
    "#     fig.write_html(output_dir / f\"{filename}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename / drop classifier metrics for future graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in [f\"{ASSAY}_7c\", \"project\", \"paired_end\"]:\n",
    "#     threshold_dfs.pop(label, None)\n",
    "\n",
    "# for name in list(threshold_dfs.keys()):\n",
    "#     try:\n",
    "#         new_name = category_remapper[name]\n",
    "#     except KeyError:\n",
    "#         # Undesired category for rest\n",
    "#         del threshold_dfs[name]\n",
    "#         continue\n",
    "\n",
    "#     threshold_dfs[new_name] = threshold_dfs.pop(name)\n",
    "\n",
    "# all_threshold_results[\"EpiATLAS\"] = {\"results\": threshold_dfs, \"other_info\": other_info}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE, ChIP-Atlas and recount3 inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'other'/'unknown' are too undefined, we exclude from life stage predictions\n",
    "cell_line_vals = [\"cell_line\", \"cell line\", \"unknown\", \"other\"]\n",
    "\n",
    "unmerged_life_stages = [\n",
    "    \"embryonic\",\n",
    "    \"fetal\",\n",
    "    \"newborn\",\n",
    "    \"embryo\",\n",
    "]\n",
    "\n",
    "unknown_values = [\"unknown\", \"other\", \"indeterminate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dir = table_dir / \"dfreeze_v2\" / \"predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not apply `life stage classifier` on `cell line` samples because it was not part of the training data,\n",
    "and the notion of life stage for a cell line is dubious. \n",
    "\n",
    "Also, we merge `perinatal stages` public DB inference (embryonic, fetal, newborn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_category_labels(\n",
    "    df: pd.DataFrame, categories: List[str], verbose: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Uniformize class labels for each category labels.\"\"\"\n",
    "    # Uniformize class labels\n",
    "    to_format = []\n",
    "    for col in df.columns:\n",
    "        cond1 = any(category in col.lower() for category in categories)\n",
    "        cond2 = any(l in col.lower() for l in [\"true\", \"expected\", \"predicted\"])\n",
    "        if cond1 and (cond2 or col in categories):\n",
    "            if verbose:\n",
    "                print(f\"Formatting {col}\")\n",
    "            to_format.append(col)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Formatting {len(to_format)} columns: {to_format}\")\n",
    "\n",
    "    df = format_labels(\n",
    "        df=df,\n",
    "        columns=to_format,\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChIP-Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = (\n",
    "    predictions_dir / \"ChIP-Atlas_predictions_20240606_merge_metadata_freeze1.csv.xz\"\n",
    ")\n",
    "pred_df = pd.read_csv(preds_path, sep=\",\", low_memory=False, compression=\"xz\")\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "    col\n",
    "    for col in pred_df.columns\n",
    "    if any(l in col.lower() for l in [\"disease\", \"assay11\", \"assay13\"])\n",
    "]\n",
    "pred_df = pred_df.drop(columns=to_drop)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df[pred_df[\"is_EpiAtlas_EpiRR\"].astype(str) == \"0\"]\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df[\n",
    "    ~pred_df[\"core7_DBs_consensus\"].isin(\n",
    "        [\"Ignored - Potential non-core\", \"non-core/CTCF\"]\n",
    "    )\n",
    "]\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df.fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[BIOMATERIAL_TYPE] = pred_df[\"expected_biomat\"]\n",
    "\n",
    "to_replace = {\n",
    "    \"sex3\": \"sex\",\n",
    "    \"assay7\": \"assay\",\n",
    "    \"donorlife\": \"lifestage\",\n",
    "}\n",
    "pred_df = rename_columns(\n",
    "    df=pred_df,\n",
    "    remapper=to_replace,\n",
    "    exact_match=False,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mapper_template = {\n",
    "    \"true_col\": \"expected_{}\",\n",
    "    \"pred_col\": \"Predicted_class_{}\",\n",
    "    \"max_pred\": \"Max_pred_{}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "categories = [\"assay\", \"sex\", \"cancer\", \"lifestage\", \"biomat\"]\n",
    "\n",
    "pred_df = format_category_labels(pred_df, categories, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_dfs = {}\n",
    "other_info = {}\n",
    "for category in categories:\n",
    "    print(\"TASK:\", category)\n",
    "    col_mapper = {k: v.format(category) for k, v in col_mapper_template.items()}\n",
    "\n",
    "    df = pred_df.copy()\n",
    "\n",
    "    # Filter unknown/NA\n",
    "    df = df[~df[col_mapper[\"true_col\"]].isin(unknown_values)]\n",
    "\n",
    "    if category == \"assay\":\n",
    "        df = pred_df[pred_df[col_mapper[\"true_col\"]].isin(ASSAY_ORDER[0:7])]\n",
    "\n",
    "    elif category == \"lifestage\":\n",
    "        df = df[~df[BIOMATERIAL_TYPE].isin(cell_line_vals)]\n",
    "        life_stages = set(df[col_mapper[\"true_col\"]].unique()) | set(\n",
    "            df[col_mapper[\"pred_col\"]].unique()\n",
    "        )\n",
    "        if any(label in life_stages for label in unmerged_life_stages):\n",
    "            df = merge_life_stages(\n",
    "                df=df,\n",
    "                lifestage_column_name=category,\n",
    "                column_name_templates=list(col_mapper.values()),\n",
    "            )\n",
    "            category = f\"{category}_merged\"\n",
    "            col_mapper = {k: v.format(category) for k, v in col_mapper_template.items()}\n",
    "\n",
    "    cat_name = category_remapper[category]\n",
    "\n",
    "    nb_samples = df.shape[0]\n",
    "    N_true_classes = len(set(df[col_mapper[\"true_col\"]]))\n",
    "    total_N_classes = len(\n",
    "        set(df[col_mapper[\"pred_col\"]]) | set(df[col_mapper[\"true_col\"]])\n",
    "    )\n",
    "    other_info[cat_name] = {\n",
    "        \"nb_samples\": nb_samples,\n",
    "        \"nb_classes\": N_true_classes,\n",
    "        \"total_possible_classes\": total_N_classes,\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        for col in [col_mapper[\"true_col\"], col_mapper[\"pred_col\"]]:\n",
    "            print(df[col].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "    threshold_dfs[cat_name] = evaluate_thresholds_global(\n",
    "        df, thresholds, verbose=verbose, columns=col_mapper  # type: ignore\n",
    "    )\n",
    "\n",
    "all_threshold_results[\"ChIP-Atlas\"] = {\"results\": threshold_dfs, \"other_info\": other_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for task_name in categories:\n",
    "#     df = threshold_dfs[task_name]\n",
    "#     print(\"TASK:\", task_name)\n",
    "#     nb_samples = other_info[task_name][\"nb_samples\"]\n",
    "#     nb_classes = other_info[task_name][\"nb_classes\"]\n",
    "\n",
    "#     # create figure\n",
    "#     name = f\"{task_name} - {nb_classes} classes\"\n",
    "#     fig = create_thresholds_graph_global_plotly(df, f\"{name} - n={nb_samples}\", xrange=(max(0, 1.0/nb_classes-0.05), 1.001))\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ENCODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = predictions_dir / \"encode_predictions_merge_metadata_2025-02_freeze1.csv.xz\"\n",
    "\n",
    "pred_df = pd.read_csv(preds_path, sep=\",\", low_memory=False, compression=\"xz\")\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in pred_df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\n",
    "    col\n",
    "    for col in pred_df.columns\n",
    "    if any(\n",
    "        l in col.lower()\n",
    "        for l in [\"disease\", \"assay_epiclass_7c\", \"assay13\", \"biospecimen\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "pred_df = pred_df.drop(columns=to_drop)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(pred_df.columns):\n",
    "    if \"11c\" in col:\n",
    "        new_col = col.replace(\"assay_epiclass_11c\", \"assay_epiclass\")\n",
    "        pred_df = pred_df.rename(columns={col: new_col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pred_df[~pred_df[\"in_epiatlas\"]]\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mapper_template = {\n",
    "    \"true_col\": \"{}\",\n",
    "    \"pred_col\": \"Predicted class ({})\",\n",
    "    \"max_pred\": \"Max pred ({})\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = []\n",
    "categories = [ASSAY, SEX, CANCER, LIFE_STAGE, BIOMATERIAL_TYPE]\n",
    "\n",
    "for category in categories:\n",
    "    relevant_columns.extend(\n",
    "        [\n",
    "            col_mapper_template[\"true_col\"].format(category),\n",
    "            col_mapper_template[\"pred_col\"].format(category),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "pred_df = format_category_labels(pred_df, relevant_columns, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "threshold_dfs_core = {}\n",
    "threshold_dfs_noncore = {}\n",
    "other_info_core = {}\n",
    "other_info_noncore = {}\n",
    "\n",
    "for category in categories:\n",
    "    print(\"TASK:\", category)\n",
    "    col_mapper = {k: v.format(category) for k, v in col_mapper_template.items()}\n",
    "\n",
    "    df: pd.DataFrame = pred_df.copy()  # type: ignore\n",
    "    df.fillna(\"unknown\", inplace=True)\n",
    "\n",
    "    # Filter unknown/NA\n",
    "    df = df[~(df[col_mapper[\"true_col\"]].isin(unknown_values))]\n",
    "\n",
    "    # Merge rna / wgbs pairs\n",
    "    if category == ASSAY:\n",
    "        true, pred = col_mapper[\"true_col\"], col_mapper[\"pred_col\"]\n",
    "        df.loc[:, [true, pred]] = df.loc[:, [true, pred]].replace(\n",
    "            ASSAY_MERGE_DICT, inplace=False\n",
    "        )\n",
    "    elif category == LIFE_STAGE:\n",
    "        df = df[~df[BIOMATERIAL_TYPE].isin(cell_line_vals)]\n",
    "\n",
    "        life_stages = set(df[col_mapper[\"true_col\"]].unique()) | set(\n",
    "            df[col_mapper[\"pred_col\"]].unique()\n",
    "        )\n",
    "        if any(label in life_stages for label in unmerged_life_stages):\n",
    "            df = merge_life_stages(\n",
    "                df=df,\n",
    "                lifestage_column_name=category,\n",
    "                column_name_templates=list(col_mapper.values()),\n",
    "            )\n",
    "            category = f\"{category}_merged\"\n",
    "            col_mapper = {k: v.format(category) for k, v in col_mapper_template.items()}\n",
    "            if verbose:\n",
    "                print(\"Biomaterial type and assay post cell line filter:\")\n",
    "                print(df[BIOMATERIAL_TYPE].value_counts(dropna=False), \"\\n\")\n",
    "                print(df[ASSAY].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "    # split core/non-core\n",
    "    df.loc[:, ASSAY] = df.loc[:, ASSAY].replace(ASSAY_MERGE_DICT, inplace=False)\n",
    "    mask = df[ASSAY].isin(core9_assays)\n",
    "\n",
    "    df_core = df[mask]\n",
    "    df_noncore = df[~mask]\n",
    "\n",
    "    # Compute all thresholds\n",
    "    cat_name = category_remapper[category]\n",
    "    for name, container_results, container_other_info, set_df in zip(\n",
    "        [\"core\", \"noncore\"],\n",
    "        [threshold_dfs_core, threshold_dfs_noncore],\n",
    "        [other_info_core, other_info_noncore],\n",
    "        [df_core, df_noncore],\n",
    "    ):\n",
    "        if cat_name == ASSAY and \"ctcf\" in set_df[ASSAY].unique():\n",
    "            if verbose:\n",
    "                print(\"\\nSkipping assay non-core\\n\")\n",
    "            continue\n",
    "\n",
    "        nb_samples = set_df.shape[0]\n",
    "        N_true_classes = len(set(set_df[col_mapper[\"true_col\"]]))\n",
    "        total_N_classes = len(\n",
    "            set(set_df[col_mapper[\"pred_col\"]]) | set(set_df[col_mapper[\"true_col\"]])\n",
    "        )\n",
    "        container_other_info[cat_name] = {\n",
    "            \"nb_samples\": nb_samples,\n",
    "            \"nb_classes\": N_true_classes,\n",
    "            \"total_possible_classes\": total_N_classes,\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Set: {name}\")\n",
    "            for col in [col_mapper[\"true_col\"], col_mapper[\"pred_col\"]]:\n",
    "                print(set_df[col].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "        container_results[cat_name] = evaluate_thresholds_global(\n",
    "            set_df, thresholds, verbose=False, columns=col_mapper  # type: ignore\n",
    "        )\n",
    "\n",
    "all_threshold_results[\"ENCODE_core\"] = {\n",
    "    \"results\": threshold_dfs_core,\n",
    "    \"other_info\": other_info_core,\n",
    "}\n",
    "all_threshold_results[\"ENCODE_non-core\"] = {\n",
    "    \"results\": threshold_dfs_noncore,\n",
    "    \"other_info\": other_info_noncore,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for task_name in categories:\n",
    "#     df = threshold_dfs[task_name]\n",
    "#     print(\"TASK:\", task_name)\n",
    "#     nb_samples = other_info[task_name][\"nb_samples\"]\n",
    "#     nb_classes = other_info[task_name][\"nb_classes\"]\n",
    "\n",
    "#     # create figure\n",
    "#     name = f\"{task_name} - {nb_classes} classes\"\n",
    "#     fig = create_thresholds_graph_global_plotly(df, f\"{name} - n={nb_samples}\", xrange=(max(0, 1.0/nb_classes-0.05), 1.001))\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recount3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = predictions_dir / \"recount3_merged_preds_metadata_freeze1.csv.xz\"\n",
    "pred_df = pd.read_csv(preds_path, sep=\",\", low_memory=False, compression=\"xz\")\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in pred_df.columns:\n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mapper_template = {\n",
    "    \"true_col\": \"{}\",\n",
    "    \"pred_col\": \"Predicted class ({})\",\n",
    "    \"max_pred\": \"Max pred ({})\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ASSAY, SEX, CANCER, f\"{LIFE_STAGE}_merged\", BIOMATERIAL_TYPE]\n",
    "pred_df = format_category_labels(pred_df, categories + [LIFE_STAGE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in pred_df.columns:\n",
    "    if \"donor_life\" in col.lower():\n",
    "        if pred_df[col].dtype == \"object\":\n",
    "            print(col)\n",
    "            print(pred_df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = merge_life_stages(\n",
    "    df=pred_df,\n",
    "    lifestage_column_name=LIFE_STAGE,\n",
    "    column_name_templates=[\"Predicted class ({})\", \"Max pred ({})\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = [f\"{LIFE_STAGE}_merged\"]\n",
    "\n",
    "assay_pred_col = f\"Predicted class ({ASSAY})\"\n",
    "assay_max_pred_col = f\"Max pred ({ASSAY})\"\n",
    "\n",
    "verbose = True\n",
    "\n",
    "threshold_dfs = {}\n",
    "other_info = {}\n",
    "\n",
    "for category in categories:\n",
    "    print(\"TASK:\", category)\n",
    "    col_mapper = {k: v.format(category) for k, v in col_mapper_template.items()}\n",
    "\n",
    "    df = pred_df.copy()\n",
    "    df.fillna(\"unknown\", inplace=True)\n",
    "\n",
    "    # Filter unknown/NA\n",
    "    df = df[~df[col_mapper[\"true_col\"]].isin(unknown_values)]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Know labels distribution:\")\n",
    "        print(df[col_mapper[\"true_col\"]].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "    if category == ASSAY:\n",
    "        pred_col = col_mapper[\"pred_col\"]\n",
    "        df.loc[:, pred_col] = df.loc[:, pred_col].replace(ASSAY_MERGE_DICT, inplace=False)\n",
    "\n",
    "        # All supposed to be rna-seq-like assays\n",
    "        true_col = col_mapper[\"true_col\"]\n",
    "        df.loc[:, true_col] = \"rna_seq\"\n",
    "    else:\n",
    "        # Only keep \"similar to training\" dsets\n",
    "        # Predicted as m/rna-seq by assay classifier with high-pred (>0.6)\n",
    "        cond1 = df[assay_pred_col].isin([\"rna_seq\", \"mrna_seq\"])\n",
    "        cond2 = df[assay_max_pred_col] > 0.6\n",
    "        df = df[cond1 & cond2]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"All labels distribution after 11c filter:\")\n",
    "        print(df[col_mapper[\"true_col\"]].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "    if LIFE_STAGE in category:\n",
    "        if verbose:\n",
    "            print(f\"Filtering out cell lines for `{category}`...\")\n",
    "\n",
    "        df = df[~df[BIOMATERIAL_TYPE].isin(cell_line_vals)]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Life stage labels distribution after cell line filter:\")\n",
    "            for col in [col_mapper[\"true_col\"], col_mapper[\"pred_col\"]]:\n",
    "                print(df[col].value_counts(dropna=False), \"\\n\")\n",
    "            print(df[BIOMATERIAL_TYPE].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "    cat_name = category_remapper[category]\n",
    "\n",
    "    nb_samples = df.shape[0]\n",
    "    N_true_classes = len(set(df[col_mapper[\"true_col\"]]))\n",
    "    total_N_classes = len(\n",
    "        set(df[col_mapper[\"pred_col\"]]) | set(df[col_mapper[\"true_col\"]])\n",
    "    )\n",
    "    other_info[cat_name] = {\n",
    "        \"nb_samples\": nb_samples,\n",
    "        \"nb_classes\": N_true_classes,\n",
    "        \"total_possible_classes\": total_N_classes,\n",
    "    }\n",
    "\n",
    "    threshold_dfs[cat_name] = evaluate_thresholds_global(\n",
    "        df, thresholds, verbose=verbose, columns=col_mapper  # type: ignore\n",
    "    )\n",
    "\n",
    "all_threshold_results[\"recount3\"] = {\"results\": threshold_dfs, \"other_info\": other_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for task_name in categories:\n",
    "#     df = threshold_dfs[task_name]\n",
    "#     print(\"TASK:\", task_name)\n",
    "#     nb_samples = other_info[task_name][\"nb_samples\"]\n",
    "#     nb_classes = other_info[task_name][\"nb_classes\"]\n",
    "\n",
    "#     # create figure\n",
    "#     name = f\"{task_name} - {nb_classes} classes\"\n",
    "#     fig = create_thresholds_graph_global_plotly(df, f\"{name} - n={nb_samples}\", xrange=(max(0, 1.0/nb_classes-0.05), 1.001))\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph results for training and inference per database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2hex(r, g, b):\n",
    "    \"\"\"Convert rgb to hex.\"\"\"\n",
    "    return f\"#{r:02x}{g:02x}{b:02x}\"\n",
    "\n",
    "\n",
    "def hex2rgb(hex_str):\n",
    "    \"\"\"Convert hex to rgb.\"\"\"\n",
    "    return ImageColor.getrgb(hex_str)\n",
    "\n",
    "\n",
    "def add_acc_f1(\n",
    "    fig: go.Figure,\n",
    "    df: pd.DataFrame,\n",
    "    row: int,\n",
    "    col: int,\n",
    "    colors: List[str],\n",
    "    show_legend: bool = True,\n",
    "    label_modifier: str = \"\",\n",
    "    color_mod: int = 0,\n",
    ") -> None:\n",
    "    \"\"\"Add accuracy and F1 to the figure.\n",
    "\n",
    "    Args:\n",
    "        fig: The figure to add the traces to.\n",
    "        df: The dataframe containing the data.\n",
    "        row: The row of the subplot. (1 indexed)\n",
    "        col: The column of the subplot. (1 indexd)\n",
    "        colors: The colors to use for the traces (1 for accuracy, 2 for F1).\n",
    "        show_legend: Whether to show the legend.\n",
    "        label_modifier: A string to add to the legend.\n",
    "        color_mod: The RGB amount to modify the color by.\n",
    "    \"\"\"\n",
    "    acc_label = df.filter(like=\"Acc\").columns[0]\n",
    "    f1_label = df.filter(like=\"F1\").columns[0]\n",
    "\n",
    "    color_acc = colors[1]\n",
    "    color_f1 = colors[2]\n",
    "\n",
    "    name_acc = acc_label\n",
    "    name_f1 = f1_label\n",
    "\n",
    "    if label_modifier:\n",
    "        # Names\n",
    "        name_acc = f\"{name_acc} {label_modifier}\"\n",
    "        name_f1 = f\"{name_f1} {label_modifier}\"\n",
    "\n",
    "        N = color_mod\n",
    "        # Acc\n",
    "        rgb_color = hex2rgb(color_acc)\n",
    "        rgb_vals = [max(color_val - N, 0) for color_val in rgb_color]\n",
    "        color_acc = rgb2hex(*rgb_vals)\n",
    "\n",
    "        # F1\n",
    "        rgb_color = hex2rgb(color_f1)\n",
    "        rgb_vals = [max(color_val - N, 0) for color_val in rgb_color]\n",
    "        color_f1 = rgb2hex(*rgb_vals)\n",
    "\n",
    "    # Plot accuracy\n",
    "    acc_vals = df[acc_label]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"Threshold\"],\n",
    "            y=acc_vals,\n",
    "            name=name_acc,\n",
    "            line=dict(color=color_acc, dash=\"solid\"),\n",
    "            mode=\"lines\",\n",
    "            showlegend=show_legend,\n",
    "            legendgroup=\"Accuracy\",\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    # Plot F1\n",
    "    prec_vals = df[f1_label]\n",
    "    if not prec_vals.isna().all():\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df[\"Threshold\"],\n",
    "                y=prec_vals,\n",
    "                name=name_f1,\n",
    "                line=dict(color=color_f1, dash=\"dot\"),\n",
    "                mode=\"lines\",\n",
    "                showlegend=show_legend,\n",
    "                legendgroup=\"F1-score\",\n",
    "            ),\n",
    "            row=row,\n",
    "            col=col,\n",
    "        )\n",
    "\n",
    "\n",
    "def add_subset_size(\n",
    "    fig: go.Figure,\n",
    "    df: pd.DataFrame,\n",
    "    row: int,\n",
    "    col: int,\n",
    "    colors: List[str],\n",
    "    show_legend: bool = True,\n",
    "    label_modifier: str = \"\",\n",
    "    color_mod: int = 1,\n",
    ") -> None:\n",
    "    \"\"\"Add file count relative size to the figure.\"\"\"\n",
    "    # Plot subset size on secondary Y-axis\n",
    "    subset_label = df.filter(like=\"Subset\").columns[0]\n",
    "    trace_name = subset_label.split(\"(\")[0].strip() + \" (%)\"\n",
    "\n",
    "    trace_color = colors[0]\n",
    "\n",
    "    if label_modifier:\n",
    "        trace_name = f\"{trace_name} {label_modifier}\"\n",
    "\n",
    "        N = color_mod\n",
    "        rgb_color = hex2rgb(trace_color)\n",
    "        rgb_vals = [min(color_val + N, 255) for color_val in rgb_color]\n",
    "        trace_color = rgb2hex(*rgb_vals)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df[\"Threshold\"],\n",
    "            y=df[subset_label],\n",
    "            name=trace_name,\n",
    "            line=dict(color=trace_color, dash=\"dash\"),\n",
    "            yaxis=\"y2\",\n",
    "            mode=\"lines\",\n",
    "            showlegend=show_legend,\n",
    "            legendgroup=\"Subset Size\",\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "\n",
    "def graph_all_DB_threshold_graphs(\n",
    "    results_dict: Dict[str, Dict],\n",
    "    output_dir: Path | None = None,\n",
    "    filename: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a threshold graph for mutiple DBs and classifiers.\n",
    "\n",
    "    Args:\n",
    "        results_dict: A dictionary containing the results for each DB and classifier.\n",
    "        output_dir: The directory to save the graph to.\n",
    "        name: The name of the graph.\n",
    "\n",
    "    \"\"\"\n",
    "    category_order = [ASSAY, SEX, CANCER, LIFE_STAGE, BIOMATERIAL_TYPE]\n",
    "    DBs_order = [\"EpiATLAS\", \"ENCODE_core\", \"ENCODE_non-core\", \"ChIP-Atlas\", \"recount3\"]\n",
    "    graph_renamer = {\n",
    "        ASSAY: \"Assay\",\n",
    "        SEX: \"Sex\",\n",
    "        CANCER: \"Cancer status\",\n",
    "        LIFE_STAGE: \"Life stage\",\n",
    "        BIOMATERIAL_TYPE: \"Biomaterial type\",\n",
    "    }\n",
    "\n",
    "    # color-blind friendly\n",
    "    # black, blue, red\n",
    "    colors = [\"#000000\", \"#005AB5\", \"#DC3220\"]\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=5,\n",
    "        cols=5,\n",
    "        row_titles=DBs_order,\n",
    "        column_titles=[graph_renamer[category] for category in category_order],\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.025,\n",
    "        horizontal_spacing=0.04,\n",
    "        x_title=\"Prediction Score Threshold\",\n",
    "        y_title=\"Metric value\",\n",
    "    )\n",
    "\n",
    "    y_ranges = {\n",
    "        \"EpiATLAS\": [0.7, 1.01],\n",
    "        \"ChIP-Atlas\": [0.1, 1.01],\n",
    "        \"ENCODE_core\": [0.45, 1.01],\n",
    "        \"ENCODE_non-core\": [0.30, 1.01],\n",
    "        \"recount3\": [0, 1.01],\n",
    "    }\n",
    "\n",
    "    for i, DB in enumerate(DBs_order):\n",
    "        # Add empty subplot row, temporary\n",
    "        if DB == \"EpiATLAS\":\n",
    "            for j, _ in enumerate(category_order):\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=[],\n",
    "                        y=[],\n",
    "                        name=\"\",\n",
    "                    ),\n",
    "                    row=i + 1,\n",
    "                    col=j + 1,\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        data = results_dict[DB]\n",
    "        for j, category in enumerate(category_order):\n",
    "            show_legend = bool(j == 0 and i == 0)\n",
    "\n",
    "            try:\n",
    "                threshold_df = data[\"results\"][category]\n",
    "            except KeyError as e:\n",
    "                print(f\"Could not find results for {DB} {category}: {e}\")\n",
    "                continue\n",
    "\n",
    "            add_acc_f1(fig, threshold_df, i + 1, j + 1, colors, show_legend)\n",
    "            add_subset_size(fig, threshold_df, i + 1, j + 1, colors, show_legend)\n",
    "\n",
    "            # Nb files + classes\n",
    "            try:\n",
    "                other_info = data[\"other_info\"][category]\n",
    "            except KeyError as e:\n",
    "                print(f\"Could not find other info for {DB} {category}: {e}\")\n",
    "                continue\n",
    "\n",
    "            N = other_info[\"nb_samples\"]\n",
    "            c_true = other_info[\"nb_classes\"]\n",
    "            c_all = other_info[\"total_possible_classes\"]\n",
    "            annotation_text = f\"N = {N}<br>C = {c_true}/{c_all}\"\n",
    "            # print(DB, category, annotation_text)\n",
    "            fig.add_annotation(\n",
    "                text=annotation_text,\n",
    "                showarrow=False,\n",
    "                font=dict(size=10, color=\"black\"),\n",
    "                # y=0.1,\n",
    "                # xref=f\"x{i+1} domain\",\n",
    "                # yref=f\"y{j+1} domain\",\n",
    "                row=i + 1,\n",
    "                col=j + 1,\n",
    "            )\n",
    "\n",
    "    # Set y-axis ranges\n",
    "    for i, DB in enumerate(DBs_order):\n",
    "        y_range = y_ranges[DB]\n",
    "        for j in range(1, 6):\n",
    "            dtick = 0.2\n",
    "            if DB in [\"ENCODE_core\", \"ENCODE_non-core\", \"EpiATLAS\"]:\n",
    "                dtick = 0.1\n",
    "\n",
    "            fig.update_yaxes(range=y_range, row=i + 1, col=j, dtick=dtick)\n",
    "\n",
    "    fig.update_xaxes(range=[0.1, 1.01], dtick=0.2)\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=800,\n",
    "        height=800,\n",
    "        title=\"All Databases - 5 classifiers - Metrics at Different Pred. Score Thresholds\",\n",
    "    )\n",
    "\n",
    "    fig.update_layout(hovermode=\"x unified\", hoverlabel_namelength=-1)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    if output_dir:\n",
    "        if not filename:\n",
    "            filename = \"all_DBs_5_classifiers_thresholds\"\n",
    "        fig.write_image(output_dir / f\"{filename}.svg\")\n",
    "        fig.write_image(output_dir / f\"{filename}.png\")\n",
    "        fig.write_html(output_dir / f\"{filename}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = base_fig_dir / \"threshold_graphs\" / \"100kb_all_none\"\n",
    "\n",
    "graph_all_DB_threshold_graphs(\n",
    "    all_threshold_results,\n",
    "    output_dir=output_dir,\n",
    "    filename=\"3DBs_5_classifiers_thresholds_w_ENCODE_split\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
