{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create figures destined for the paper.\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_ORDER,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    CELL_TYPE,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flagship paper figure\n",
    "\n",
    "cell type classifier:  \n",
    "\n",
    "  for each assay, have a violin plot for accuracy per cell type (16 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"flagship\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "split_results_handler = SplitResultsHandler()\n",
    "path_results_cell_type = base_data_dir / \"dfreeze_v2\" / \"hg38_10kb_all_none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load split results into one combined dataframe\n",
    "ct_split_dfs = split_results_handler.gather_split_results_across_categories(path_results_cell_type)[\"harmonized_sample_ontology_intermediate_1l_3000n_10fold-oversampling\"]\n",
    "ct_full_df = pd.concat(ct_split_dfs.values(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata and join with split results\n",
    "metadata_2 = metadata_handler.load_metadata(\"v2\")\n",
    "ct_full_df = metadata_handler.join_metadata(ct_full_df, metadata_2)\n",
    "ct_full_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### violin version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_flagship_ct(cell_type_df: pd.DataFrame, logdir: Path, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a figure showing the cell type classifier performance on different assays.\n",
    "\n",
    "    Args:\n",
    "        cell_type_df (pd.DataFrame): DataFrame containing the cell type prediction results.\n",
    "        logdir (Path): The directory path for saving the figure.\n",
    "        name (str): The name for the saved figure.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    # Assuming all classifiers have the same assays for simplicity\n",
    "    assay_labels = ASSAY_ORDER\n",
    "    num_assays = len(assay_labels)\n",
    "\n",
    "    ct_labels = sorted(cell_type_df[\"True class\"].unique())\n",
    "    if len(ct_labels) != 16:\n",
    "        raise AssertionError(f\"Expected 16 cell type labels, got {len(ct_labels)}\")\n",
    "    ct_colors = [cell_type_colors[ct_label] for ct_label in ct_labels]\n",
    "\n",
    "    scatter_offset = 0.1  # Scatter plot jittering\n",
    "\n",
    "    # Calculate the size of the grid\n",
    "    grid_size = int(np.ceil(np.sqrt(num_assays)))\n",
    "    rows, cols = grid_size, grid_size\n",
    "\n",
    "    # Compute assay acc values beforehand, to be able to sort the assays by mean acc\n",
    "    assay_acc_dict = defaultdict(dict)\n",
    "    subclass_sizes = defaultdict(dict)\n",
    "    for idx, assay_label in enumerate(assay_labels):\n",
    "        assay_df = cell_type_df[cell_type_df[ASSAY] == assay_label]\n",
    "\n",
    "        # cell type subclass accuracy\n",
    "        subclass_sizes[assay_label] = assay_df.groupby([\"True class\"]).agg(\"size\")\n",
    "        subclass_groupby_acc = assay_df.groupby([\"True class\", \"Predicted class\"]).agg(\n",
    "            \"size\"\n",
    "        )\n",
    "        ct_accuracies = {}\n",
    "        for ct_label in sorted(ct_labels):\n",
    "            acc = float(subclass_groupby_acc[ct_label][ct_label]) / float(\n",
    "                subclass_sizes[assay_label][ct_label]\n",
    "            )\n",
    "            ct_accuracies[ct_label] = acc\n",
    "\n",
    "        assay_acc_dict[assay_label] = ct_accuracies\n",
    "\n",
    "    # assay_sorted_by_mean_acc = sorted(\n",
    "    #     assay_acc_dict,\n",
    "    #     key=lambda x: np.mean(list(assay_acc_dict[x].values())),\n",
    "    #     reverse=True,\n",
    "    # )\n",
    "\n",
    "    # Create subplots with a square grid\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=ASSAY_ORDER,\n",
    "        shared_yaxes=\"all\",  # type: ignore\n",
    "        horizontal_spacing=0,\n",
    "        vertical_spacing=0.02,\n",
    "        y_title=\"Cell type subclass accuracy\",\n",
    "    )\n",
    "\n",
    "    for idx, assay_label in enumerate(ASSAY_ORDER):\n",
    "        row, col = divmod(idx, grid_size)\n",
    "\n",
    "        acc_values = list(assay_acc_dict[assay_label].values())\n",
    "        # Add violin plot with integer x positions\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[idx] * len(acc_values),\n",
    "                y=acc_values,\n",
    "                name=assay_label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=False,\n",
    "                fillcolor=assay_colors[assay_label],\n",
    "                line_color=\"white\",\n",
    "                line=dict(width=0.8),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=row + 1,  # Plotly rows are 1-indexed\n",
    "            col=col + 1,\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(showticklabels=False)\n",
    "\n",
    "        # Prepare data for scatter plots\n",
    "        jittered_x_positions = np.random.uniform(-scatter_offset, scatter_offset, size=len(acc_values)) + idx - 0.4  # type: ignore\n",
    "\n",
    "        scatter_marker_size = 10\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=jittered_x_positions,\n",
    "                y=acc_values,\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=scatter_marker_size, color=ct_colors),\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"{ct_label} ({assay_acc_dict[assay_label][ct_label]:.3f}, n={subclass_sizes[assay_label][ct_label]})\"\n",
    "                    for ct_label in assay_acc_dict[assay_label]\n",
    "                ],\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=row + 1,  # Plotly rows are 1-indexed\n",
    "            col=col + 1,\n",
    "        )\n",
    "\n",
    "    # Add a dummy scatter plot for legend\n",
    "    for ct_label in ct_labels:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[None],\n",
    "                y=[None],\n",
    "                mode=\"markers\",\n",
    "                name=ct_label,\n",
    "                marker=dict(color=cell_type_colors[ct_label], size=scatter_marker_size),\n",
    "                showlegend=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(range=[0.34, 1.01])\n",
    "\n",
    "    title_text = f\"{CELL_TYPE.replace('_', ' ').title()} classifier: Accuracy per assay\"\n",
    "    fig.update_layout(\n",
    "        title=title_text,\n",
    "        height=1500,\n",
    "        width=1500,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"flagship\" / \"ct_assay_accuracy\"\n",
    "fig_flagship_ct(ct_full_df, logdir=fig_dir, name=\"ct_assay_accuracy_violin_10kb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### boxplot version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_flagship_ct_boxplot(cell_type_df: pd.DataFrame, logdir: Path, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Generates a boxplot for cell type classification accuracy across different assays.\n",
    "\n",
    "    This function creates a single figure with boxplots for each assay, displaying the accuracy\n",
    "    of cell type classification. Each boxplot represents the distribution of accuracy for one assay\n",
    "    across different cell types.\n",
    "\n",
    "    Args:\n",
    "        cell_type_df: DataFrame containing the cell type prediction results.\n",
    "        logdir: The directory path for saving the figure.\n",
    "        name: The name for the saved figure.\n",
    "    \"\"\"\n",
    "    # Assuming all classifiers have the same assays for simplicity\n",
    "    assay_labels = sorted(ASSAY_ORDER)\n",
    "    ct_labels = sorted(cell_type_df[\"True class\"].unique())\n",
    "\n",
    "    if len(ct_labels) != 16:\n",
    "        raise AssertionError(f\"Expected 16 cell type labels, got {len(ct_labels)}\")\n",
    "\n",
    "    assay_acc_dict = defaultdict(list)\n",
    "    for assay_label in assay_labels:\n",
    "        assay_df = cell_type_df[cell_type_df[ASSAY] == assay_label]\n",
    "\n",
    "        # cell type subclass accuracy\n",
    "        subclass_size = assay_df.groupby([\"True class\"]).agg(\"size\")\n",
    "        subclass_groupby_acc = assay_df.groupby([\"True class\", \"Predicted class\"]).agg(\n",
    "            \"size\"\n",
    "        )\n",
    "        for ct_label in sorted(ct_labels):\n",
    "            acc = subclass_groupby_acc[ct_label][ct_label] / subclass_size[ct_label]\n",
    "            assay_acc_dict[assay_label].append(acc)\n",
    "\n",
    "    # assay_sorted_by_mean_acc = sorted(\n",
    "    #     assay_acc_dict, key=lambda x: np.mean(assay_acc_dict[x]), reverse=True\n",
    "    # )\n",
    "\n",
    "    # Create the boxplot\n",
    "    fig = go.Figure()\n",
    "    for assay_label in ASSAY_ORDER:\n",
    "        # Select accuracies corresponding to the current assay\n",
    "        assay_accuracies = assay_acc_dict[assay_label]\n",
    "        assert len(assay_accuracies) == 16\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=assay_accuracies,\n",
    "                name=assay_label,\n",
    "                boxpoints=\"outliers\",\n",
    "                boxmean=True,\n",
    "                fillcolor=assay_colors[assay_label],\n",
    "                line_color=\"black\",\n",
    "                showlegend=False,\n",
    "                marker=dict(size=2),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(range=[0.34, 1.01])\n",
    "\n",
    "    title_text = f\"{CELL_TYPE.replace('_', ' ').title()} classifier: Accuracy per assay\"\n",
    "    fig.update_layout(\n",
    "        title=title_text,\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        xaxis_title=\"Assay\",\n",
    "        height=600,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    # Save and display the figure\n",
    "    if not logdir.exists():\n",
    "        raise FileNotFoundError(f\"Could not find {logdir}\")\n",
    "\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_flagship_ct_boxplot(ct_full_df, logdir=fig_dir, name=\"ct_assay_accuracy_boxplot_10kb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata 3rd factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell type classification: check (input, ct) pairs for enrichment in any metadata category\n",
    "    - use:\n",
    "        - harmonized_donor_life_stage\n",
    "        - harmonized_donor_sex\n",
    "        - harmonized_sample_cancer_high\n",
    "        - harmonized_biomaterial\n",
    "        - paired_end\n",
    "        - project\n",
    "    - find a 3rd factor metric, e.g. if any pair (assay, ct) subclass is very different from global dist, it can use that info as 3rd factor, and we're looking at assay specifically\n",
    "        - one score per pair, pearson w accuracy vector (one vector per assay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metadata_distribution(\n",
    "    df: pd.DataFrame, columns: List[str]\n",
    ") -> Dict[str, pd.Series]:\n",
    "    \"\"\"\n",
    "    Calculates the percentage of metadata labels within specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: A pandas DataFrame containing the data.\n",
    "        columns: A list of column names to analyze.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are column names and values are Series objects containing\n",
    "        the percentage of each unique label in the respective column.\n",
    "    \"\"\"\n",
    "    distribution = {}\n",
    "    nb_samples = len(df)\n",
    "    for column in columns:\n",
    "        # Count the occurrences of each unique value in the column\n",
    "        value_counts = df[column].value_counts(dropna=False)\n",
    "        # Calculate the percentages\n",
    "        percentages = (value_counts / nb_samples) * 100\n",
    "        # Store the results in the dictionary\n",
    "        distribution[column] = percentages\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_label_ratios(\n",
    "    target_distribution: Dict[str, pd.Series],\n",
    "    comparison_distributions: List[Dict[str, pd.Series]],\n",
    "    labels: List[str],\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compares label ratios of a target distribution against multiple comparison distributions,\n",
    "    calculating the difference in percentage points for each label within each metadata category.\n",
    "\n",
    "    Args:\n",
    "        target_distribution: A dictionary of Series representing the target distribution for comparison.\n",
    "        comparison_distributions: A list of dictionaries of Series, where each dictionary\n",
    "                                  represents a distribution (e.g., assay, cell type, global) for comparison.\n",
    "        labels: A list of labels corresponding to each distribution in `comparison_distributions`,\n",
    "                used for labeling the columns in the result.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of DataFrames, where each DataFrame shows the difference in percentage points\n",
    "        for each label in a metadata category between the target distribution and each of the\n",
    "        comparison distributions.\n",
    "    \"\"\"\n",
    "    comparison_results = {}\n",
    "    for category, target_series in target_distribution.items():\n",
    "        # Initialize a DataFrame to store comparison results for this category\n",
    "        comparison_df = pd.DataFrame()\n",
    "\n",
    "        for label, comparison_distribution in zip(labels, comparison_distributions):\n",
    "            # Ensure the comparison distribution series for this category exists and align target with comparison\n",
    "            comparison_series = comparison_distribution.get(\n",
    "                category, pd.Series(dtype=\"float64\")\n",
    "            )\n",
    "            aligned_target, aligned_comparison = target_series.align(\n",
    "                comparison_series, fill_value=0\n",
    "            )\n",
    "\n",
    "            # Calculate difference in percentage points\n",
    "            difference = aligned_target - aligned_comparison\n",
    "\n",
    "            # Store the results in the comparison DataFrame\n",
    "            comparison_df[f\"Difference_vs_{label}\"] = difference\n",
    "\n",
    "        comparison_results[category] = comparison_df\n",
    "\n",
    "    return comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_categories = [\n",
    "    \"harmonized_donor_life_stage\",\n",
    "    \"harmonized_donor_sex\",\n",
    "    \"harmonized_sample_disease_high\",\n",
    "    \"harmonized_biomaterial_type\",\n",
    "    \"paired_end\",\n",
    "    \"project\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_third_factor_correlation(\n",
    "    ct_full_df: pd.DataFrame,\n",
    "    metadata_categories: List[str],\n",
    "    save_full_details: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the correlation between third factor influence and cell type classification accuracy for each assay.\n",
    "\n",
    "    This function operates on classification results to evaluate how a third factor, represented by metadata category distributions,\n",
    "    correlates with the accuracy of cell type classifications across assays. It involves comparing metadata distributions\n",
    "    within assay and cell type groups to a global distribution, and then correlating these comparisons with classification\n",
    "    accuracies.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with epigenomics data, including assays, cell types, and metadata for classification.\n",
    "        metadata_categories (List[str]): A list of metadata categories to analyze.\n",
    "    \"\"\"\n",
    "    global_dist = calculate_metadata_distribution(ct_full_df, metadata_categories)\n",
    "    subclass_distributions = {}\n",
    "    comparison_results = (\n",
    "        {}\n",
    "    )  # Initialize a dict to hold comparison results for each subgroup\n",
    "\n",
    "    for group in ct_full_df.groupby(ASSAY):\n",
    "        label = group[0]\n",
    "        sub_df = group[1]\n",
    "        subclass_distributions[label] = calculate_metadata_distribution(\n",
    "            sub_df, metadata_categories\n",
    "        )\n",
    "\n",
    "    for group in ct_full_df.groupby(CELL_TYPE):\n",
    "        label = group[0]\n",
    "        sub_df = group[1]\n",
    "        subclass_distributions[label] = calculate_metadata_distribution(\n",
    "            sub_df, metadata_categories\n",
    "        )\n",
    "\n",
    "    # Loop through each group and compare to global\n",
    "    for group in ct_full_df.groupby([ASSAY, CELL_TYPE]):\n",
    "        assay, cell_type = group[0]  # type: ignore\n",
    "        sub_df = group[1]\n",
    "        pair_subclass_dist = calculate_metadata_distribution(sub_df, metadata_categories)\n",
    "        subclass_distributions[(assay, cell_type)] = pair_subclass_dist\n",
    "\n",
    "        assay_dist = subclass_distributions[assay]\n",
    "        cell_type_dist = subclass_distributions[cell_type]\n",
    "\n",
    "        comparisons_dists = [assay_dist, cell_type_dist, global_dist]\n",
    "        comparison_labels = [assay, cell_type, \"global\"]\n",
    "\n",
    "        comparison_results[(assay, cell_type)] = compare_label_ratios(\n",
    "            target_distribution=pair_subclass_dist,\n",
    "            comparison_distributions=comparisons_dists,\n",
    "            labels=comparison_labels,\n",
    "        )\n",
    "\n",
    "    pair_dfs = {}\n",
    "    pairs_3rd_factor = {}\n",
    "    for (assay, cell_type), comparisons in comparison_results.items():\n",
    "        # Initialize an empty list to collect DataFrames for concatenation\n",
    "        dfs_to_concat = []\n",
    "\n",
    "        for category, df_comparison in comparisons.items():\n",
    "            df_comparison.columns = [\n",
    "                \"Difference vs Assay\",\n",
    "                \"Difference vs Cell Type\",\n",
    "                \"Difference vs Global\",\n",
    "            ]\n",
    "            # Add identifiers for the assay, cell type, and category\n",
    "            df_comparison[\"Assay\"] = assay\n",
    "            df_comparison[\"Cell Type\"] = cell_type\n",
    "            df_comparison[\"Category\"] = category\n",
    "\n",
    "            subclass_dist = subclass_distributions[(assay, cell_type)][category]\n",
    "            df_comparison[\"(assay, ct) subclass %\"] = subclass_dist\n",
    "\n",
    "            # Collect the DataFrame\n",
    "            dfs_to_concat.append(df_comparison.reset_index())\n",
    "\n",
    "        # Concatenate all DataFrames along rows\n",
    "        final_df = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "        final_df.fillna(0, inplace=True)\n",
    "\n",
    "        new_columns = final_df.columns.tolist()\n",
    "        new_first = [\"index\", \"Category\", \"(assay, ct) subclass %\"]\n",
    "        for label in new_first:\n",
    "            new_columns.remove(label)\n",
    "        new_columns = new_first + new_columns\n",
    "        final_df = final_df[new_columns]\n",
    "\n",
    "        pair_dfs[(assay, cell_type)] = final_df\n",
    "\n",
    "        # val_3rd_factor = (final_df[\"Difference vs Assay\"] - ).abs().sum()\n",
    "        # val_3rd_factor= final_df[\"Difference vs Global\"].abs().max()\n",
    "        val_3rd_factor = final_df[\"Difference vs Global\"].min()\n",
    "        pairs_3rd_factor[(assay, cell_type)] = val_3rd_factor\n",
    "\n",
    "    # Subclass accuracy per assay\n",
    "    assay_labels = sorted(ct_full_df[ASSAY].unique())\n",
    "    ct_labels = sorted(ct_full_df[CELL_TYPE].unique())\n",
    "    assay_accuracies = {}\n",
    "    for assay_label in assay_labels:\n",
    "        assay_df = ct_full_df[ct_full_df[ASSAY] == assay_label]\n",
    "\n",
    "        # cell type subclass accuracy\n",
    "        subclass_size = assay_df.groupby([\"True class\"]).agg(\"size\")\n",
    "        subclass_groupby_acc = assay_df.groupby([\"True class\", \"Predicted class\"]).agg(\n",
    "            \"size\"\n",
    "        )\n",
    "        accuracies = {}\n",
    "        for ct_label in sorted(ct_labels):\n",
    "            acc_label = subclass_groupby_acc[ct_label][ct_label] / subclass_size[ct_label]\n",
    "            accuracies[ct_label] = acc_label\n",
    "\n",
    "        assay_accuracies[assay_label] = accuracies\n",
    "\n",
    "    # Concatenate all DataFrames along rows\n",
    "    if save_full_details:\n",
    "        all_pairs_df = pd.concat(pair_dfs, axis=0, ignore_index=True)\n",
    "        for assay in assay_labels:\n",
    "            for ct in ct_labels:\n",
    "                all_pairs_df.loc[\n",
    "                    (all_pairs_df[\"Assay\"] == assay) & (all_pairs_df[\"Cell Type\"] == ct),\n",
    "                    \"Accuracy\",\n",
    "                ] = assay_accuracies[assay][ct]\n",
    "\n",
    "        all_pairs_df.columns = [\n",
    "            \"Label\" if x == \"index\" else x for x in all_pairs_df.columns\n",
    "        ]\n",
    "        file_path = base_fig_dir / \"flagship\" / \"metadata_comparison_all.csv\"\n",
    "        all_pairs_df.to_csv(file_path, index=False)\n",
    "\n",
    "    pearson_3rd_factor = {}\n",
    "    for assay, acc_dict in assay_accuracies.items():\n",
    "        acc_vector = {ct: acc_dict[ct] for ct in ct_labels}\n",
    "        acc_vector = pd.Series(acc_vector)\n",
    "\n",
    "        diff_metric = {ct: pairs_3rd_factor[(assay, ct)] for ct in ct_labels}\n",
    "        diff_metric = pd.Series(diff_metric)\n",
    "        pearson = acc_vector.corr(diff_metric, method=\"pearson\")\n",
    "        pearson_3rd_factor[assay] = pearson\n",
    "\n",
    "    return pearson_3rd_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_third_factor_correlation(ct_full_df, metadata_categories, save_full_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_series = []\n",
    "for metadata_category in metadata_categories:\n",
    "    pearson_dict = compute_third_factor_correlation(ct_full_df, [metadata_category])\n",
    "    pearson_df = pd.DataFrame(pearson_dict, index=[metadata_category])\n",
    "    pearson_series.append(pearson_df)\n",
    "\n",
    "full_pearson_df = pd.concat(pearson_series, axis=0)\n",
    "\n",
    "# Add  max for each row and column\n",
    "full_pearson_df[\"Max\"] = full_pearson_df.abs().max(axis=1)\n",
    "max_row = full_pearson_df.abs().max(axis=0)\n",
    "max_row.name = \"Max\"\n",
    "full_pearson_df = full_pearson_df.append(max_row)\n",
    "\n",
    "full_pearson_df.to_csv(\n",
    "    base_fig_dir / \"flagship\" / \"3rd_factor_min_diff_pearson_correlation.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_labels = sorted(ct_full_df[ASSAY].unique())\n",
    "ct_labels = sorted(ct_full_df[CELL_TYPE].unique())\n",
    "life_stages = ct_full_df[\"harmonized_donor_life_stage\"].unique().tolist()\n",
    "life_stages.remove(\"unknown\")\n",
    "\n",
    "acc_list = []\n",
    "for assay_label in assay_labels:\n",
    "    assay_df = ct_full_df[ct_full_df[ASSAY] == assay_label]\n",
    "    for ct in ct_labels:\n",
    "        ct_df = assay_df[assay_df[CELL_TYPE] == ct]\n",
    "        for life_stage in life_stages:\n",
    "            life_stage_df = ct_df[ct_df[\"harmonized_donor_life_stage\"] == life_stage]\n",
    "            acc = life_stage_df[\"Predicted class\"].eq(life_stage_df[\"True class\"]).mean()\n",
    "            size = len(life_stage_df)\n",
    "            acc_list.append((assay_label, ct, life_stage, size, acc))\n",
    "\n",
    "acc_df = pd.DataFrame(\n",
    "    acc_list, columns=[\"Assay\", \"Cell Type\", \"Life Stage\", \"Size\", \"Accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df.to_csv(base_fig_dir / \"flagship\" / \"assay_ct_life_stage_accuracy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input classification - Hdf5 values at top SHAP features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparer le signal de input dans les différents cell-types (e.g. boxplot des 40 régions \"input & t cell\" vs les 9 régions \"input & lymphocyte of b lineage\" dans ces 2 CT (donc 4 boxplots) ou même ajouter un autre CT externe comme muscle comme ctrl neg (6 boxplots))\n",
    "\n",
    "all input, check 40 and 9 regions in\n",
    "- t cell\n",
    "- b cell\n",
    "- muscle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flagship_supp_shap_input(paper_dir: Path) -> None:\n",
    "    \"\"\"\n",
    "    Plot hdf5 values of different (input, cell type) pairs for top SHAP values\n",
    "    of T cell and B cell. (only ct which produced common top shap values features)\n",
    "\n",
    "    Args:\n",
    "        paper_dir (Path): The directory containing the paper data.\n",
    "        metadata (Metadata): Metadata version 2 object.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    # Load relevant metadata\n",
    "    ct_labels = [\"T cell\", \"lymphocyte of B lineage\", \"neutrophil\", \"muscle organ\"]\n",
    "    metadata_2 = MetadataHandler(paper_dir).load_metadata(\"v2\")\n",
    "    metadata_2.select_category_subsets(ASSAY, [\"input\"])\n",
    "    metadata_2.select_category_subsets(CELL_TYPE, ct_labels)\n",
    "    md5_per_ct = metadata_2.md5_per_class(CELL_TYPE)\n",
    "\n",
    "    # Load feature bins\n",
    "    hdf5_val_dir = (\n",
    "        base_data_dir\n",
    "        / \"harmonized_sample_ontology_intermediate/all_splits/harmonized_sample_ontology_intermediate_1l_3000n/10fold-dfreeze-v2/global_shap_analysis/top303/input\"\n",
    "    )\n",
    "    feature_filepath = hdf5_val_dir / \"features_n8.json\"\n",
    "    with open(feature_filepath, \"r\", encoding=\"utf8\") as f:\n",
    "        features: Dict[str, List[int]] = json.load(f)\n",
    "\n",
    "    # Load feature values\n",
    "    hdf5_val_path = hdf5_val_dir / \"hdf5_values_100kb_all_none_input_4ct_features_n8.csv\"\n",
    "    df = pd.read_csv(hdf5_val_path, index_col=0, header=0)\n",
    "\n",
    "    df_ct_dict = {}\n",
    "    for ct in ct_labels:\n",
    "        md5s = md5_per_ct[ct]\n",
    "        df_ct_dict[ct] = df.loc[md5s]\n",
    "\n",
    "    # Make two groups of boxplots, four boxplot per group (one per cell type)\n",
    "    # Each boxplot will take the values of the columns in the features dict\n",
    "    fig = go.Figure()\n",
    "    for ct_label, df_ct in df_ct_dict.items():\n",
    "        for i, (cell_type, top_shap_bins) in enumerate(features.items()):\n",
    "            top_shap_bins = [str(b) for b in top_shap_bins]\n",
    "            mean_bin_values_per_md5 = df_ct[top_shap_bins].mean(axis=1)\n",
    "\n",
    "            hovertext = [\n",
    "                f\"{md5}. {val:02f}\"\n",
    "                for md5, val in zip(df_ct.index, mean_bin_values_per_md5)\n",
    "            ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    x=[f\"Top SHAP '{cell_type}' bins\"] * len(mean_bin_values_per_md5),\n",
    "                    y=mean_bin_values_per_md5,\n",
    "                    name=f\"{ct_label} (n={len(hovertext)})\",\n",
    "                    points=\"all\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    spanmode=\"hard\",\n",
    "                    fillcolor=cell_type_colors[ct_label],\n",
    "                    line_color=\"black\",\n",
    "                    showlegend=i == 0,\n",
    "                    marker=dict(size=2),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                    legendgroup=ct_label,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Input files bin z-score values for top SHAP features\",\n",
    "        yaxis_title=\"Mean z-score across bins\",\n",
    "        xaxis_title=\"Top SHAP features groups\",\n",
    "        boxmode=\"group\",\n",
    "        violinmode=\"group\",\n",
    "        height=1000,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    logdir = base_fig_dir / \"flagship\"\n",
    "    name = \"input_feature_values_top_shap_bins_per_md5\"\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagship_supp_shap_input(paper_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
