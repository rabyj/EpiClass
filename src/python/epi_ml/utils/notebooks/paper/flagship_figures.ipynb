{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create figures destined for the paper.\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, too-many-branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display  # pylint: disable=unused-import\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# from epi_ml.core.metadata import UUIDMetadata\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flagship assay/ct accuracy\n",
    "\n",
    "cell type classifier:  \n",
    "\n",
    "  for each assay, have a violin plot for accuracy per cell type (16 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"flagship\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_results_cell_type = (\n",
    "    base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_10kb_all_none\"\n",
    ")\n",
    "if not path_results_cell_type.exists():\n",
    "    raise FileNotFoundError(f\"{path_results_cell_type} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load split results into one combined dataframe\n",
    "ct_split_dfs = split_results_handler.gather_split_results_across_categories(\n",
    "    path_results_cell_type\n",
    ")[\"harmonized_sample_ontology_intermediate_1l_3000n_10fold-oversampling\"]\n",
    "ct_full_df = pd.concat(ct_split_dfs.values(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata and join with split results\n",
    "metadata_2 = metadata_handler.load_metadata(\"v2\")\n",
    "ct_full_df = metadata_handler.join_metadata(ct_full_df, metadata_2)\n",
    "ct_full_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### violin version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_flagship_ct(cell_type_df: pd.DataFrame, logdir: Path, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Create a figure showing the cell type classifier performance on different assays.\n",
    "\n",
    "    Args:\n",
    "        cell_type_df (pd.DataFrame): DataFrame containing the cell type prediction results.\n",
    "        logdir (Path): The directory path for saving the figure.\n",
    "        name (str): The name for the saved figure.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    # Assuming all classifiers have the same assays for simplicity\n",
    "    assay_labels = ASSAY_ORDER\n",
    "    num_assays = len(assay_labels)\n",
    "\n",
    "    ct_labels = sorted(cell_type_df[\"True class\"].unique())\n",
    "    if len(ct_labels) != 16:\n",
    "        raise AssertionError(f\"Expected 16 cell type labels, got {len(ct_labels)}\")\n",
    "    ct_colors = [cell_type_colors[ct_label] for ct_label in ct_labels]\n",
    "\n",
    "    scatter_offset = 0.1  # Scatter plot jittering\n",
    "\n",
    "    # Calculate the size of the grid\n",
    "    grid_size = int(np.ceil(np.sqrt(num_assays)))\n",
    "    rows, cols = grid_size, grid_size\n",
    "\n",
    "    # Compute assay acc values beforehand, to be able to sort the assays by mean acc\n",
    "    assay_acc_dict = {}\n",
    "    subclass_sizes = {}\n",
    "    for idx, assay_label in enumerate(assay_labels):\n",
    "        assay_df = cell_type_df[cell_type_df[ASSAY] == assay_label]\n",
    "\n",
    "        # cell type subclass accuracy\n",
    "        subclass_sizes[assay_label] = assay_df.groupby([\"True class\"]).agg(\"size\")\n",
    "        pred_confusion_matrix = assay_df.groupby([\"True class\", \"Predicted class\"]).agg(\n",
    "            \"size\"\n",
    "        )\n",
    "\n",
    "        ct_accuracies = {\n",
    "            ct_label: pred_confusion_matrix[ct_label][ct_label]\n",
    "            / float(subclass_sizes[assay_label][ct_label])\n",
    "            for ct_label in sorted(ct_labels)\n",
    "        }\n",
    "\n",
    "        assay_acc_dict[assay_label] = ct_accuracies\n",
    "\n",
    "    # assay_sorted_by_mean_acc = sorted(\n",
    "    #     assay_acc_dict,\n",
    "    #     key=lambda x: np.mean(list(assay_acc_dict[x].values())),\n",
    "    #     reverse=True,\n",
    "    # )\n",
    "\n",
    "    # Create subplots with a square grid\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=ASSAY_ORDER,\n",
    "        shared_yaxes=\"all\",  # type: ignore\n",
    "        horizontal_spacing=0,\n",
    "        vertical_spacing=0.02,\n",
    "        y_title=\"Cell type subclass accuracy\",\n",
    "    )\n",
    "\n",
    "    for idx, assay_label in enumerate(ASSAY_ORDER):\n",
    "        row, col = divmod(idx, grid_size)\n",
    "\n",
    "        acc_values = list(assay_acc_dict[assay_label].values())\n",
    "        # Add violin plot with integer x positions\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[idx] * len(acc_values),\n",
    "                y=acc_values,\n",
    "                name=assay_label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=False,\n",
    "                fillcolor=assay_colors[assay_label],\n",
    "                line_color=\"white\",\n",
    "                line=dict(width=0.8),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=row + 1,  # Plotly rows are 1-indexed\n",
    "            col=col + 1,\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(showticklabels=False)\n",
    "\n",
    "        # Prepare data for scatter plots\n",
    "        jittered_x_positions = np.random.uniform(-scatter_offset, scatter_offset, size=len(acc_values)) + idx - 0.4  # type: ignore\n",
    "\n",
    "        scatter_marker_size = 10\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=jittered_x_positions,\n",
    "                y=acc_values,\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=scatter_marker_size, color=ct_colors),\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"{ct_label} ({assay_acc_dict[assay_label][ct_label]:.3f}, n={subclass_sizes[assay_label][ct_label]})\"\n",
    "                    for ct_label in assay_acc_dict[assay_label]\n",
    "                ],\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=row + 1,  # Plotly rows are 1-indexed\n",
    "            col=col + 1,\n",
    "        )\n",
    "\n",
    "    # Add a dummy scatter plot for legend\n",
    "    for ct_label in ct_labels:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[None],\n",
    "                y=[None],\n",
    "                mode=\"markers\",\n",
    "                name=ct_label,\n",
    "                marker=dict(color=cell_type_colors[ct_label], size=scatter_marker_size),\n",
    "                showlegend=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(range=[0.34, 1.01])\n",
    "\n",
    "    title_text = f\"{CELL_TYPE.replace('_', ' ').title()} classifier: Accuracy per assay\"\n",
    "    fig.update_layout(\n",
    "        title=title_text,\n",
    "        height=1500,\n",
    "        width=1500,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"flagship\" / \"ct_assay_accuracy\"\n",
    "# fig_flagship_ct(ct_full_df, logdir=fig_dir, name=\"ct_assay_accuracy_violin_10kb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boxplot version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_flagship_ct_boxplot(cell_type_df: pd.DataFrame, logdir: Path, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Generates a boxplot for cell type classification accuracy across different assays.\n",
    "\n",
    "    This function creates a single figure with boxplots for each assay, displaying the accuracy\n",
    "    of cell type classification. Each boxplot represents the distribution of accuracy for one assay\n",
    "    across different cell types.\n",
    "\n",
    "    Args:\n",
    "        cell_type_df: DataFrame containing the cell type prediction results.\n",
    "        logdir: The directory path for saving the figure.\n",
    "        name: The name for the saved figure.\n",
    "    \"\"\"\n",
    "    # Assuming all classifiers have the same assays for simplicity\n",
    "    assay_labels = sorted(ASSAY_ORDER)\n",
    "    ct_labels = sorted(cell_type_df[\"True class\"].unique())\n",
    "\n",
    "    if len(ct_labels) != 16:\n",
    "        raise AssertionError(f\"Expected 16 cell type labels, got {len(ct_labels)}\")\n",
    "\n",
    "    assay_acc_dict = defaultdict(list)\n",
    "    for assay_label in assay_labels:\n",
    "        assay_df = cell_type_df[cell_type_df[ASSAY] == assay_label]\n",
    "\n",
    "        # cell type subclass accuracy\n",
    "        subclass_size = assay_df.groupby([\"True class\"]).agg(\"size\")\n",
    "        pred_confusion_matrix = assay_df.groupby([\"True class\", \"Predicted class\"]).agg(\n",
    "            \"size\"\n",
    "        )\n",
    "        for ct_label in sorted(ct_labels):\n",
    "            acc = pred_confusion_matrix[ct_label][ct_label] / subclass_size[ct_label]\n",
    "            assay_acc_dict[assay_label].append(acc)\n",
    "\n",
    "    # assay_sorted_by_mean_acc = sorted(\n",
    "    #     assay_acc_dict, key=lambda x: np.mean(assay_acc_dict[x]), reverse=True\n",
    "    # )\n",
    "\n",
    "    # Create the boxplot\n",
    "    fig = go.Figure()\n",
    "    for assay_label in ASSAY_ORDER:\n",
    "        # Select accuracies corresponding to the current assay\n",
    "        assay_accuracies = assay_acc_dict[assay_label]\n",
    "        assert len(assay_accuracies) == 16\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=assay_accuracies,\n",
    "                name=assay_label,\n",
    "                boxpoints=\"outliers\",\n",
    "                boxmean=True,\n",
    "                fillcolor=assay_colors[assay_label],\n",
    "                line_color=\"black\",\n",
    "                showlegend=False,\n",
    "                marker=dict(size=2),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(range=[0.34, 1.01])\n",
    "\n",
    "    title_text = f\"{CELL_TYPE.replace('_', ' ').title()} classifier: Accuracy per assay\"\n",
    "    fig.update_layout(\n",
    "        title=title_text,\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        xaxis_title=\"Assay\",\n",
    "        height=600,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    # Save and display the figure\n",
    "    if not logdir.exists():\n",
    "        raise FileNotFoundError(f\"Could not find {logdir}\")\n",
    "\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_flagship_ct_boxplot(ct_full_df, logdir=fig_dir, name=\"ct_assay_accuracy_boxplot_10kb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input classification - Hdf5 values at top SHAP features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparer le signal de input dans les différents cell-types (e.g. boxplot des 40 régions \"input & t cell\" vs les 9 régions \"input & lymphocyte of b lineage\" dans ces 2 CT (donc 4 boxplots) ou même ajouter un autre CT externe comme muscle comme ctrl neg (6 boxplots))\n",
    "\n",
    "all input, check 40 and 9 regions in\n",
    "- t cell\n",
    "- b cell\n",
    "- muscle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flagship_supp_shap_input(paper_dir: Path) -> None:\n",
    "    \"\"\"\n",
    "    Plot hdf5 values of different (input, cell type) pairs for top SHAP values\n",
    "    of T cell and B cell. (only ct which produced common top shap values features)\n",
    "\n",
    "    Args:\n",
    "        paper_dir (Path): The directory containing the paper data.\n",
    "        metadata (Metadata): Metadata version 2 object.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    # Load relevant metadata\n",
    "    ct_labels = [\"T cell\", \"lymphocyte of B lineage\", \"neutrophil\", \"muscle organ\"]\n",
    "    metadata_2 = MetadataHandler(paper_dir).load_metadata(\"v2\")\n",
    "    metadata_2.select_category_subsets(ASSAY, [\"input\"])\n",
    "    metadata_2.select_category_subsets(CELL_TYPE, ct_labels)\n",
    "    md5_per_ct = metadata_2.md5_per_class(CELL_TYPE)\n",
    "\n",
    "    # Load feature bins\n",
    "    hdf5_val_dir = (\n",
    "        base_data_dir\n",
    "        / \"harmonized_sample_ontology_intermediate/all_splits/harmonized_sample_ontology_intermediate_1l_3000n/10fold-dfreeze-v2/global_shap_analysis/top303/input\"\n",
    "    )\n",
    "    feature_filepath = hdf5_val_dir / \"features_n8.json\"\n",
    "    with open(feature_filepath, \"r\", encoding=\"utf8\") as f:\n",
    "        features: Dict[str, List[int]] = json.load(f)\n",
    "\n",
    "    # Load feature values\n",
    "    hdf5_val_path = hdf5_val_dir / \"hdf5_values_100kb_all_none_input_4ct_features_n8.csv\"\n",
    "    df = pd.read_csv(hdf5_val_path, index_col=0, header=0)\n",
    "\n",
    "    df_ct_dict = {}\n",
    "    for ct in ct_labels:\n",
    "        md5s = md5_per_ct[ct]\n",
    "        df_ct_dict[ct] = df.loc[md5s]\n",
    "\n",
    "    # Make two groups of boxplots, four boxplot per group (one per cell type)\n",
    "    # Each boxplot will take the values of the columns in the features dict\n",
    "    fig = go.Figure()\n",
    "    for ct_label, df_ct in df_ct_dict.items():\n",
    "        for i, (cell_type, top_shap_bins) in enumerate(features.items()):\n",
    "            top_shap_bins = [str(b) for b in top_shap_bins]\n",
    "            mean_bin_values_per_md5 = df_ct[top_shap_bins].mean(axis=1)\n",
    "\n",
    "            hovertext = [\n",
    "                f\"{md5}. {val:02f}\"\n",
    "                for md5, val in zip(df_ct.index, mean_bin_values_per_md5)\n",
    "            ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    x=[f\"Top SHAP '{cell_type}' bins\"] * len(mean_bin_values_per_md5),\n",
    "                    y=mean_bin_values_per_md5,\n",
    "                    name=f\"{ct_label} (n={len(hovertext)})\",\n",
    "                    points=\"all\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    spanmode=\"hard\",\n",
    "                    fillcolor=cell_type_colors[ct_label],\n",
    "                    line_color=\"black\",\n",
    "                    showlegend=i == 0,\n",
    "                    marker=dict(size=2),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                    legendgroup=ct_label,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Input files bin z-score values for top SHAP features\",\n",
    "        yaxis_title=\"Mean z-score across bins\",\n",
    "        xaxis_title=\"Top SHAP features groups\",\n",
    "        boxmode=\"group\",\n",
    "        violinmode=\"group\",\n",
    "        height=1000,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    logdir = base_fig_dir / \"flagship\"\n",
    "    name = \"input_feature_values_top_shap_bins_per_md5\"\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flagship_supp_shap_input(paper_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell type classification - Training per assay results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "results_dir_1 = (\n",
    "    base_results_dir\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / \"harmonized_sample_ontology_intermediate_1l_3000n\"\n",
    "    / \"10fold-oversampling-unique_assay\"\n",
    ")\n",
    "results_dir_2 = (\n",
    "    base_results_dir\n",
    "    / \"hg38_cpg_topvar_200bp_10kb_coord_n30k/harmonized_sample_ontology_intermediate_1l_3000n/10fold-oversampling_wgbs-only\"\n",
    ")\n",
    "\n",
    "assay_results = {}\n",
    "for assay_folder in results_dir_1.glob(\"*_only\"):\n",
    "    assay_results[assay_folder.name] = split_results_handler.read_split_results(\n",
    "        assay_folder\n",
    "    )\n",
    "\n",
    "assay_results[\n",
    "    \"wgbs_only-cpg_topvar_200bp_n30321\"\n",
    "] = split_results_handler.read_split_results(results_dir_2)\n",
    "\n",
    "assay_metrics = split_results_handler.compute_split_metrics(\n",
    "    assay_results, concat_first_level=True\n",
    ")\n",
    "assay_metrics = split_results_handler.invert_metrics_dict(assay_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_performance_per_assay(\n",
    "    assay_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    title_end: str = \"\",\n",
    "    y_range: None | List[float] = None,\n",
    "):\n",
    "    \"\"\"Create a box plot of assay accuracy for each classifier.\"\"\"\n",
    "    fig = go.Figure()\n",
    "    for assay in ASSAY_ORDER:\n",
    "        try:\n",
    "            metrics = assay_metrics[f\"{assay}_only\"]\n",
    "            assay_acc = {split: metrics[split][\"Accuracy\"] for split in metrics}\n",
    "        except KeyError:\n",
    "            print(f\"KeyError. Skipping '{assay}'\")\n",
    "            continue\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=list(assay_acc.values()),\n",
    "                name=assay,\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                showlegend=True,\n",
    "                marker=dict(size=3, color=\"black\"),\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                fillcolor=assay_colors[assay],\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[f\"{split}: {value:.4f}\" for split, value in assay_acc.items()],\n",
    "            )\n",
    "        )\n",
    "    name_other = \"wgbs_only-cpg_topvar_200bp_n30321\"\n",
    "    wgbs_extra = assay_metrics[name_other]\n",
    "    acc = {split: wgbs_extra[split][\"Accuracy\"] for split in wgbs_extra}\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=list(acc.values()),\n",
    "            name=\"wgbs_cpg*\",\n",
    "            boxmean=True,\n",
    "            boxpoints=\"all\",\n",
    "            showlegend=True,\n",
    "            marker=dict(size=3, color=\"black\"),\n",
    "            line=dict(width=1, color=\"black\"),\n",
    "            fillcolor=assay_colors[\"wgbs\"],\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=[f\"{split}: {value:.4f}\" for split, value in acc.items()],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if y_range is not None:\n",
    "        fig.update_yaxes(range=y_range)\n",
    "\n",
    "    title_text = (\n",
    "        \"NN classification (100kb_all_none) - Sample ontology - Training per assay\"\n",
    "    )\n",
    "    if title_end:\n",
    "        title_text += f\" - {title_end}\"\n",
    "    fig.update_layout(\n",
    "        title_text=title_text,\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        xaxis_title=\"Assay\",\n",
    "        width=1000,\n",
    "        height=700,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    this_name = name\n",
    "    fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "    fig.write_image(logdir / f\"{this_name}.png\")\n",
    "    fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"flagship\" / \"cell_type_unique_assay_training\"\n",
    "y_min = 0.4\n",
    "NN_performance_per_assay(\n",
    "    assay_metrics=assay_metrics,\n",
    "    logdir=logdir,\n",
    "    name=f\"cell_type_training_per_assay_Y_{y_min:.2f}\",\n",
    "    y_range=[y_min, 1.001],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
