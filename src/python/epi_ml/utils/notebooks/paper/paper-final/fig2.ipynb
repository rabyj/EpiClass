{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Figure core creation: Fig2\n",
    "\n",
    "Formatting of the figures may not be identical to the paper, but they contain the same data points.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, too-many-branches, consider-using-f-string, duplicate-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    BIOMATERIAL_TYPE,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    create_mislabel_corrector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE7_ASSAYS = ASSAY_ORDER[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "paper_dir = base_dir\n",
    "if not paper_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {paper_dir} does not exist.\")\n",
    "\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "table_dir = paper_dir / \"tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map\n",
    "sex_colors = IHECColorMap.sex_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()\n",
    "\n",
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata_v2 = metadata_handler.load_metadata(\"v2\")\n",
    "metadata_v2_df = metadata_handler.load_metadata_df(\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_metadata_dir = base_data_dir / \"metadata\" / \"epiatlas\" / \"official\"\n",
    "if not official_metadata_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {official_metadata_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2A, 2B (and Supp Fig 4A) - MLP performance on various classification tasks - 100kb resolution files\n",
    "\n",
    "- 2A: Accuracy\n",
    "- 2B: F1-Score\n",
    "- Supp 4A: AUC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_performance_across_classification_tasks(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    name: str | None = None,\n",
    "    logdir: Path | None = None,\n",
    "    exclude_categories: List[str] | None = None,\n",
    "    y_range: List[float] | None = None,\n",
    "    sort_by_acc: bool = False,\n",
    "    metric_names: Sequence[str] = (\"Accuracy\", \"F1_macro\"),\n",
    "    title: str | None = None,\n",
    ") -> List[str]:\n",
    "    \"\"\"Render box plots of metrics per classifier and split, each in its own subplot.\n",
    "\n",
    "    This function generates a figure with subplots, each representing a different\n",
    "    metric. Each subplot contains box plots for each classifier, ordered by accuracy.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A nested dictionary with structure {split: {classifier: {metric: score}}}.\n",
    "        logdir: The directory path to save the output plots. If None, only display the plot.\n",
    "        name: The base name for the output plot files.\n",
    "        exclude_categories: Task categories to exclude from the plot.\n",
    "        y_range: The y-axis range for the plots.\n",
    "        sort_by_acc: Whether to sort the classifiers by accuracy.\n",
    "        metric_names: The metrics to include in the plot.\n",
    "\n",
    "    Returns:\n",
    "        The list of classifier names in the order they appear in the plot.\n",
    "    \"\"\"\n",
    "    # Exclude some categories\n",
    "    classifier_names = list(split_metrics[\"split0\"].keys())\n",
    "    if exclude_categories is not None:\n",
    "        for category in exclude_categories:\n",
    "            classifier_names = [c for c in classifier_names if category not in c]\n",
    "\n",
    "    available_metrics = list(split_metrics[\"split0\"][classifier_names[0]].keys())\n",
    "    try:\n",
    "        available_metrics.remove(\"count\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "    if any(metric not in available_metrics for metric in metric_names):\n",
    "        raise ValueError(f\"Invalid metric. Metrics need to be in {available_metrics}\")\n",
    "\n",
    "    # Get classifier counts\n",
    "    classifiers_N = split_results_handler.extract_count_from_metrics(split_metrics)\n",
    "\n",
    "    # Sort classifiers by accuracy\n",
    "    if sort_by_acc:\n",
    "        mean_acc = {}\n",
    "        for classifier in classifier_names:\n",
    "            mean_acc[classifier] = np.mean(\n",
    "                [split_metrics[split][classifier][\"Accuracy\"] for split in split_metrics]\n",
    "            )\n",
    "        classifier_names = sorted(\n",
    "            classifier_names, key=lambda x: mean_acc[x], reverse=True\n",
    "        )\n",
    "\n",
    "    # Create subplots, one column for each metric\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(metric_names),\n",
    "        subplot_titles=metric_names,\n",
    "        horizontal_spacing=0.03,\n",
    "    )\n",
    "\n",
    "    color_group = px.colors.qualitative.Plotly\n",
    "    colors = {\n",
    "        classifier: color_group[i % len(color_group)]\n",
    "        for i, classifier in enumerate(classifier_names)\n",
    "    }\n",
    "\n",
    "    point_pos = 0\n",
    "    for i, metric in enumerate(metric_names):\n",
    "        for classifier_name in classifier_names:\n",
    "            values = [\n",
    "                split_metrics[split][classifier_name][metric] for split in split_metrics\n",
    "            ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=values,\n",
    "                    name=f\"{classifier_name} (N={classifiers_N[classifier_name]})\",\n",
    "                    fillcolor=colors[classifier_name],\n",
    "                    line=dict(color=\"black\", width=1.5),\n",
    "                    marker=dict(size=3, color=\"white\", line_width=1),\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    pointpos=point_pos,\n",
    "                    showlegend=i == 0,  # Only show legend in the first subplot\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in zip(split_metrics, values)\n",
    "                    ],\n",
    "                    legendgroup=classifier_name,\n",
    "                    width=0.5,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=i + 1,\n",
    "            )\n",
    "\n",
    "    title_text = (\n",
    "        \"Neural network classification - Metric distribution for 10-fold cross-validation\"\n",
    "    )\n",
    "    if title:\n",
    "        title_text = title\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title_text,\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode=\"group\",\n",
    "        height=1200 * 0.8,\n",
    "        width=1750 * 0.8,\n",
    "    )\n",
    "\n",
    "    # Acc, F1\n",
    "    fig.update_layout(yaxis=dict(range=[0.88, 1.001]))\n",
    "    fig.update_layout(yaxis2=dict(range=[0.80, 1.001]))\n",
    "\n",
    "    # AUC\n",
    "    range_auc = [0.986, 1.0001]\n",
    "    fig.update_layout(yaxis3=dict(range=range_auc))\n",
    "    fig.update_layout(yaxis4=dict(range=range_auc))\n",
    "\n",
    "    if y_range is not None:\n",
    "        fig.update_yaxes(range=y_range)\n",
    "\n",
    "    # Save figure\n",
    "    if logdir:\n",
    "        if name is None:\n",
    "            name = \"MLP_metrics_various_tasks\"\n",
    "        fig.write_image(logdir / f\"{name}.svg\")\n",
    "        fig.write_image(logdir / f\"{name}.png\")\n",
    "        fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return classifier_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\"track_type\", \"group\", \"disease\", \"PE\", \"martin\"]\n",
    "# exclude_categories = [\"track_type\", \"group\", \"disease\"]\n",
    "exclude_names = [\"chip-seq\", \"7c\", \"16ct\", \"no-mixed\"]\n",
    "\n",
    "hdf5_type = \"hg38_100kb_all_none\"\n",
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "\n",
    "mislabel_correction = True\n",
    "if mislabel_correction:\n",
    "    mislabel_corrector = create_mislabel_corrector(paper_dir)\n",
    "else:\n",
    "    mislabel_corrector = None\n",
    "\n",
    "split_results_metrics, all_split_results = split_results_handler.general_split_metrics(\n",
    "    results_dir,\n",
    "    merge_assays=True,\n",
    "    exclude_categories=exclude_categories,\n",
    "    exclude_names=exclude_names,\n",
    "    return_type=\"both\",\n",
    "    oversampled_only=True,\n",
    "    mislabel_corrections=mislabel_corrector,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\"\n",
    "fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "if mislabel_correction:\n",
    "    this_fig_logdir = fig_logdir / \"post_mislabel_correction\"\n",
    "    this_fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "else:\n",
    "    this_fig_logdir = fig_logdir / \"no_mislabel_correction\"\n",
    "    this_fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "metrics_full = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "fig_name = f\"{hdf5_type}_perf_across_categories_full\"\n",
    "sorted_task_names = NN_performance_across_classification_tasks(\n",
    "    split_results_metrics,  # type: ignore\n",
    "    sort_by_acc=True,\n",
    "    metric_names=metrics_full,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2C: Normalized Shannon Entropy on various metadata categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_imbalance(\n",
    "    all_split_results: Dict[str, Dict[str, pd.DataFrame]],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute class imbalance for each task and split.\n",
    "\n",
    "    Args:\n",
    "        all_split_results: A dictionary with structure {task_name: {split_name: split_results_df}}.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the following columns:\n",
    "            - avg(balance_ratio): The average balance ratio for each task.\n",
    "            - n: The number of classes for each task (used for the average).\n",
    "    \"\"\"\n",
    "    # combine md5 lists\n",
    "    task_md5s = {\n",
    "        classifier_task: [split_df.index for split_df in split_results.values()]\n",
    "        for classifier_task, split_results in all_split_results.items()\n",
    "    }\n",
    "    task_md5s = {\n",
    "        classifier_task: [list(split_md5s) for split_md5s in md5s]\n",
    "        for classifier_task, md5s in task_md5s.items()\n",
    "    }\n",
    "    task_md5s = {\n",
    "        classifier_task: list(itertools.chain(*md5s))\n",
    "        for classifier_task, md5s in task_md5s.items()\n",
    "    }\n",
    "\n",
    "    # get metadata\n",
    "    metadata_df = metadata_handler.load_metadata_df(\"v2-encode\")\n",
    "\n",
    "    label_counts = {}\n",
    "    for classifier_task, md5s in task_md5s.items():\n",
    "        try:\n",
    "            label_counts[classifier_task] = metadata_df.loc[md5s][\n",
    "                classifier_task\n",
    "            ].value_counts()\n",
    "        except KeyError as e:\n",
    "            category_name = classifier_task.rsplit(\"_\", maxsplit=1)[0]\n",
    "            try:\n",
    "                label_counts[classifier_task] = metadata_df.loc[md5s][\n",
    "                    category_name\n",
    "                ].value_counts()\n",
    "            except KeyError as e:\n",
    "                raise e\n",
    "\n",
    "    # Compute Shannon Entropy\n",
    "    class_balance = {}\n",
    "    for classifier_task, counts in label_counts.items():\n",
    "        total_count = counts.sum()\n",
    "        k = len(counts)\n",
    "        p_x = counts / total_count  # class proportions\n",
    "        p_x = p_x.values\n",
    "        shannon_entropy = -np.sum(p_x * np.log2(p_x))\n",
    "        balance = shannon_entropy / np.log2(k)\n",
    "        class_balance[classifier_task] = (balance, k)\n",
    "\n",
    "    df_class_balance = pd.DataFrame.from_dict(\n",
    "        class_balance, orient=\"index\", columns=[\"Normalized Shannon Entropy\", \"k\"]\n",
    "    ).sort_index()\n",
    "\n",
    "    return df_class_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = {\n",
    "    k: v\n",
    "    for k, v in all_split_results.items()  # type: ignore\n",
    "    if not any(label in k for label in [\"martin\", \"PE\"])\n",
    "}\n",
    "df_class_balance = compute_class_imbalance(subset)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_task_names = [\n",
    "    task_name for task_name in sorted_task_names if task_name in df_class_balance.index\n",
    "]\n",
    "df_class_balance = df_class_balance.loc[sorted_task_names]\n",
    "fig = px.scatter(\n",
    "    df_class_balance,\n",
    "    x=df_class_balance.index,\n",
    "    y=\"Normalized Shannon Entropy\",\n",
    "    labels={\n",
    "        \"k\": \"Number of classes\",\n",
    "        \"Normalized Shannon Entropy\": \"Normalized Shannon Entropy\",\n",
    "    },\n",
    "    title=\"Class imbalance across tasks (higher is more balanced)\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    yaxis=dict(range=[0, 1]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2D: Donor Sex, before and after label correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IHEC_sample_metadata_harmonization.v1.1.extended.csv contains 314 EpiRRs with unknown sex. We applied a fully trained sex classifier on those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to reproduce: https://docs.google.com/spreadsheets/d/1nMCEOZd16pWHfGY63vXcI2JzeOqGBR-tCoBuRZLZgQM/edit?gid=608314669#gid=608314669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_results_dir = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"dfreeze_v2\"\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / f\"{SEX}_1l_3000n\"\n",
    ")\n",
    "if not sex_results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {sex_results_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unknown Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_v1_1_path = (\n",
    "    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.1.extended.csv\"\n",
    ")\n",
    "metadata_v1_1 = pd.read_csv(metadata_v1_1_path, index_col=0)\n",
    "\n",
    "metadata_v1_2_path = (\n",
    "    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.2.extended.csv\"\n",
    ")\n",
    "metadata_v1_2 = pd.read_csv(metadata_v1_2_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata_df = metadata_v2_df\n",
    "full_metadata_df[\"md5sum\"] = full_metadata_df.index\n",
    "assert (\n",
    "    metadata_v2_df[metadata_v2_df[SEX].isin([\"unknown\"])][\"EpiRR\"].nunique()\n",
    "    == metadata_v1_1[metadata_v1_1[SEX] == \"unknown\"].index.nunique()\n",
    "    == 314\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_full_model_dir = sex_results_dir / \"complete_no_valid_oversample\"\n",
    "if not sex_full_model_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {sex_full_model_dir} does not exist\")\n",
    "\n",
    "pred_unknown_file_path = (\n",
    "    sex_full_model_dir\n",
    "    / \"predictions\"\n",
    "    / \"complete_no_valid_oversample_test_prediction_100kb_all_none_dfreeze_v2.1_sex_mixed_unknown.csv\"\n",
    ")\n",
    "pred_unknown_df = pd.read_csv(pred_unknown_file_path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_unknown_df = pred_unknown_df[pred_unknown_df[\"True class\"] == \"unknown\"]\n",
    "pred_unknown_df = split_results_handler.add_max_pred(pred_unknown_df)  # type: ignore\n",
    "pred_unknown_df = metadata_handler.join_metadata(pred_unknown_df, metadata_v2)\n",
    "pred_unknown_df[\"md5sum\"] = pred_unknown_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10fold cross-validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_10fold_dir = sex_results_dir / \"10fold-oversampling\"\n",
    "if not sex_10fold_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {sex_10fold_dir} does not exist\")\n",
    "\n",
    "split_results: Dict[str, pd.DataFrame] = split_results_handler.read_split_results(\n",
    "    sex_10fold_dir\n",
    ")\n",
    "concat_results_10fold: pd.DataFrame = split_results_handler.concatenate_split_results(split_results, depth=1)  # type: ignore\n",
    "concat_results_10fold = split_results_handler.add_max_pred(concat_results_10fold)\n",
    "concat_results_10fold = metadata_handler.join_metadata(concat_results_10fold, metadata_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average chrY values z-score distributions\n",
    "\n",
    "1. For each bigwig file, the chrY average value is computed. (with pyBigWig module, in chrY_bigwig_mean.py)  \n",
    "2. For each assay, the z-score distribution (of the mean chrY value) of the file group is computed.  \n",
    "3. Graph E is made by averaging for each EpiRR the z-score value in each assay distribution.  \n",
    "---\n",
    "1. Outputs `chrXY_coverage_all.csv`\n",
    "2. Outputs `chrY_coverage_zscores.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrY_coverage_dir = base_data_dir / \"chrY_coverage\"\n",
    "if not chrY_coverage_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {chrY_coverage_dir} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chrY_zscores(\n",
    "    chrY_coverage_dir: Path, version: str, save: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute z-scores for chrY coverage data.\n",
    "\n",
    "    Computes two distributions of z-scores:\n",
    "    1) Per assay group, excluding raw, pval, and Unique_raw tracks.\n",
    "    2) Per assay+track group.\n",
    "\n",
    "    In both cases, rna-seq/mrna-seq and wgbs-standard/wgbs-pbat are put as one assay.\n",
    "\n",
    "    Args:\n",
    "        chrY_coverage_dir: The directory containing the chrY coverage data.\n",
    "        version: The metadata version to use.\n",
    "        save: Whether to save the results.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The chrY coverage data with z-scores appended.\n",
    "    \"\"\"\n",
    "    output_dir = Path()\n",
    "    if save:\n",
    "        output_dir = chrY_coverage_dir / f\"dfreeze_{version}_stats\"\n",
    "        output_dir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "    # Get chrY coverage data\n",
    "    chrY_coverage_dir = base_data_dir / \"chrY_coverage\"\n",
    "    if not chrY_coverage_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {chrY_coverage_dir} does not exist.\")\n",
    "    chrY_coverage_df = pd.read_csv(chrY_coverage_dir / \"chrXY_coverage_all.csv\", header=0)\n",
    "\n",
    "    # Filter out md5s not in metadata version\n",
    "    metadata = MetadataHandler(paper_dir).load_metadata(version)\n",
    "    md5s = set(metadata.md5s)\n",
    "    chrY_coverage_df = chrY_coverage_df[chrY_coverage_df[\"filename\"].isin(md5s)]\n",
    "\n",
    "    # Make sure all values are non-zero\n",
    "    if not (chrY_coverage_df[\"chrY\"] != 0).all():\n",
    "        raise ValueError(\"Some chrY values are zero.\")\n",
    "\n",
    "    # Merge metadata\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n",
    "    metadata_df.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n",
    "    chrY_coverage_df = chrY_coverage_df.merge(\n",
    "        metadata_df, left_on=\"filename\", right_on=\"md5sum\"\n",
    "    )\n",
    "\n",
    "    # Compute stats for distributions\n",
    "    metric_name_1 = \"chrY_zscore_vs_assay_w_track_exclusion\"\n",
    "    metric_name_2 = \"chrY_zscore_vs_assay_track\"\n",
    "    files1 = chrY_coverage_df[\n",
    "        ~chrY_coverage_df[\"track_type\"].isin([\"raw\", \"pval\", \"Unique_raw\"])\n",
    "    ]\n",
    "    files2 = chrY_coverage_df\n",
    "    dist1 = files1.groupby(ASSAY).agg({\"chrY\": [\"mean\", \"std\", \"count\"]})\n",
    "    dist2 = files2.groupby([ASSAY, \"track_type\"]).agg({\"chrY\": [\"mean\", \"std\", \"count\"]})\n",
    "    if save:\n",
    "        output_dir: Path\n",
    "        dist1.to_csv(output_dir / \"chrY_coverage_stats_assay_w_track_exclusion.csv\")\n",
    "        dist2.to_csv(output_dir / \"chrY_coverage_stats_assay_and_track.csv\")\n",
    "\n",
    "    # Compute full z-score distributions\n",
    "    for groups in files1.groupby(ASSAY):\n",
    "        _, group_df = groups\n",
    "        group_df[\"zscore\"] = zscore(group_df[\"chrY\"])\n",
    "        chrY_coverage_df.loc[group_df.index, metric_name_1] = group_df[\"zscore\"]\n",
    "        chrY_coverage_df.loc[group_df.index, f\"N_{metric_name_1}\"] = groups[1].shape[0]\n",
    "    for groups in files2.groupby([ASSAY, \"track_type\"]):\n",
    "        _, group_df = groups\n",
    "        group_df[\"zscore\"] = zscore(group_df[\"chrY\"])\n",
    "        chrY_coverage_df.loc[group_df.index, metric_name_2] = group_df[\"zscore\"]\n",
    "        chrY_coverage_df.loc[group_df.index, f\"N_{metric_name_2}\"] = groups[1].shape[0]\n",
    "\n",
    "    # Fill in missing values\n",
    "    for N_name in [f\"N_{metric_name_1}\", f\"N_{metric_name_2}\"]:\n",
    "        chrY_coverage_df[N_name] = chrY_coverage_df[N_name].fillna(0).astype(int)\n",
    "    chrY_coverage_df.fillna(pd.NA, inplace=True)\n",
    "\n",
    "    if save:\n",
    "        output_cols = [\n",
    "            \"filename\",\n",
    "            ASSAY,\n",
    "            \"track_type\",\n",
    "            \"chrY\",\n",
    "            metric_name_1,\n",
    "            f\"N_{metric_name_1}\",\n",
    "            metric_name_2,\n",
    "            f\"N_{metric_name_2}\",\n",
    "        ]\n",
    "        chrY_coverage_df[output_cols].to_csv(\n",
    "            output_dir / \"chrY_coverage_zscores.csv\", index=False, na_rep=\"NA\"  # type: ignore\n",
    "        )\n",
    "    return chrY_coverage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrY_coverage_df = compute_chrY_zscores(chrY_coverage_dir, \"v2\", save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fig 2D - Inner portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of unknown, excluding mixed. same as v1.1 ihec metadata\n",
    "no_mixed = full_metadata_df[full_metadata_df[SEX] != \"mixed\"]\n",
    "\n",
    "with pd.option_context(\"display.float_format\", \"{:.2%}\".format):\n",
    "    print(\"file-wise:\")\n",
    "    print(no_mixed[SEX].value_counts(dropna=False) / no_mixed.shape[0])\n",
    "\n",
    "    print(\"\\nEpiRR-wise:\")\n",
    "    epirr_no_mixed = no_mixed.drop_duplicates(subset=[\"EpiRR\"])\n",
    "    print(epirr_no_mixed[SEX].value_counts(dropna=False) / epirr_no_mixed.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fig 2D - Outer portion\n",
    "\n",
    "Outer ring represents SEX metadata labnels v1.2 (without `mixed` labels), which had those modifications:\n",
    "- Some unknown SEX files were labelled, using (assay,track type) z-score in conjunction with fully trained model predictions.\n",
    "- Correction of some mislabels, using 10fold cross-validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_v1_2_no_mixed = metadata_v1_2[metadata_v1_2[SEX] != \"mixed\"]\n",
    "with pd.option_context(\"display.float_format\", \"{:.2%}\".format):\n",
    "    print(\"EpiRR-wise:\")\n",
    "    print(meta_v1_2_no_mixed.value_counts(SEX) / meta_v1_2_no_mixed.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unknown predictions analysis file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a similar table to what was used to determine which unknown sex to label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = [\n",
    "    \"EpiRR\",\n",
    "    \"project\",\n",
    "    \"harmonized_donor_type\",\n",
    "    CELL_TYPE,\n",
    "    SEX,\n",
    "    \"Predicted class\",\n",
    "]\n",
    "val_cols = [\"Max pred\", \"chrY_zscore_vs_assay_track\"]\n",
    "pred_unknown_analysis = pred_unknown_df.merge(\n",
    "    chrY_coverage_df, on=\"md5sum\", suffixes=(\"\", \"_DROP\")\n",
    ")\n",
    "pred_unknown_analysis.drop(\n",
    "    columns=[c for c in pred_unknown_analysis.columns if c.endswith(\"_DROP\")],\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "pivot_table = pred_unknown_analysis.pivot_table(\n",
    "    index=index_cols,\n",
    "    values=val_cols,\n",
    "    aggfunc=[\"mean\", \"median\", \"std\", \"count\"],\n",
    ")\n",
    "\n",
    "display(pivot_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10fold cross-validation predictions analysis file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a similar table to what was used to determine which EpiRR sex labels might be mistaken.\n",
    "It aggregates results from the 10 cross-validation classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_analysis = concat_results_10fold.merge(chrY_coverage_df, left_index=True, right_on=\"md5sum\", suffixes=(\"\", \"_DROP\"))  # type: ignore\n",
    "cross_val_analysis.drop(\n",
    "    columns=[c for c in cross_val_analysis.columns if c.endswith(\"_DROP\")], inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cols = [\n",
    "    \"EpiRR\",\n",
    "    \"project\",\n",
    "    \"harmonized_donor_type\",\n",
    "    CELL_TYPE,\n",
    "    SEX,\n",
    "    \"Predicted class\",\n",
    "]\n",
    "val_cols = [\"Max pred\", \"chrY_zscore_vs_assay_track\"]\n",
    "\n",
    "# not directly used in full mislabel analysis\n",
    "to_drop = [\n",
    "    \"N_chrY_zscore_vs_assay_w_track_exclusion\",\n",
    "    \"chrY_zscore_vs_assay_w_track_exclusion\",\n",
    "]\n",
    "cross_val_analysis_track = cross_val_analysis.drop(to_drop, axis=1)\n",
    "\n",
    "pivot_table = cross_val_analysis_track.pivot_table(\n",
    "    index=index_cols,\n",
    "    values=val_cols,\n",
    "    aggfunc=[\"mean\", \"median\", \"std\", \"count\"],\n",
    ")\n",
    "\n",
    "display(pivot_table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2E - Average EpiRR z-score for 10fold on v1.1 IHEC harmonized metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_merged_assays(\n",
    "    zscore_df: pd.DataFrame,\n",
    "    sex_mislabels: Dict[str, str],\n",
    "    name: str | None = None,\n",
    "    logdir: Path | None = None,\n",
    "    min_pred: float | None = None,\n",
    "    no_rna: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Male vs Female z-score distribution for merged assays, excluding wgbs.\n",
    "\n",
    "    Does not include pval and raw tracks.\n",
    "\n",
    "    Highlights mislabels in the plot.\n",
    "\n",
    "    Args:\n",
    "        zscore_df (pd.DataFrame): The dataframe with z-score data.\n",
    "        sex_mislabels (Dict[str, str]): {EpiRR_no-v: corrected_sex_label}\n",
    "        logdir (Path): The directory path to save the output plots. If None, only display the plot.\n",
    "        name (str): The base name for the output plot files.\n",
    "        min_pred (float|None): Minimum prediction value to include in the plot. Used on average EpiRR 'Max pred' values.\n",
    "        no_rna (bool): Whether to exclude rna_seq from the plot.\n",
    "    \"\"\"\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "\n",
    "    # Remove pval/raw tracks + rna unstranded\n",
    "    zscore_df = zscore_df[~zscore_df[\"track_type\"].isin([\"pval\", \"raw\", \"Unique_raw\"])]\n",
    "\n",
    "    # Merge rna protocols\n",
    "    zscore_df.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n",
    "\n",
    "    # wgbs reverses male/female chrY tendency, so removed here\n",
    "    zscore_df = zscore_df[~zscore_df[ASSAY].str.contains(\"wgb\")]\n",
    "\n",
    "    if no_rna:\n",
    "        zscore_df = zscore_df[~zscore_df[ASSAY].str.contains(\"rna\")]\n",
    "\n",
    "    N_assays = len(zscore_df[ASSAY].unique())\n",
    "    print(\n",
    "        f\"Average chrY z-score values computed from:\\n{zscore_df[ASSAY].value_counts(dropna=False)}\"\n",
    "    )\n",
    "\n",
    "    # Average chrY z-score values\n",
    "    metric_label = \"chrY_zscore_vs_assay_w_track_exclusion\"\n",
    "    zscore_df = zscore_df[zscore_df[metric_label] != \"NA\"]\n",
    "    mean_chrY_values_df = zscore_df.groupby([\"EpiRR\", SEX]).agg(\n",
    "        {metric_label: \"mean\", \"Max pred\": \"mean\"}\n",
    "    )\n",
    "    mean_chrY_values_df.reset_index(inplace=True)\n",
    "    if not mean_chrY_values_df[\"EpiRR\"].is_unique:\n",
    "        raise ValueError(\"EpiRR is not unique.\")\n",
    "\n",
    "    # Filter out low prediction values\n",
    "    if min_pred is not None:\n",
    "        mean_chrY_values_df = mean_chrY_values_df[\n",
    "            mean_chrY_values_df[\"Max pred\"] > min_pred\n",
    "        ]\n",
    "\n",
    "    mean_chrY_values_df.reset_index(drop=True, inplace=True)\n",
    "    mean_chrY_values_df[\"EpiRR_no_v\"] = mean_chrY_values_df[\"EpiRR\"].str.extract(\n",
    "        pat=r\"(\\w+\\d+).\\d+\"\n",
    "    )[0]\n",
    "\n",
    "    chrY_values = mean_chrY_values_df[metric_label]\n",
    "    female_idx = np.argwhere((mean_chrY_values_df[SEX] == \"female\").values).flatten()  # type: ignore\n",
    "    male_idx = np.argwhere((mean_chrY_values_df[SEX] == \"male\").values).flatten()  # type: ignore\n",
    "\n",
    "    # Mislabels\n",
    "    predicted_as_female = set(\n",
    "        epirr_no_v for epirr_no_v, label in sex_mislabels.items() if label == \"female\"\n",
    "    )\n",
    "    predicted_as_male = set(\n",
    "        epirr_no_v for epirr_no_v, label in sex_mislabels.items() if label == \"male\"\n",
    "    )\n",
    "    predicted_as_female_idx = np.argwhere(mean_chrY_values_df[\"EpiRR_no_v\"].isin(predicted_as_female).values).flatten()  # type: ignore\n",
    "    predicted_as_male_idx = np.argwhere(mean_chrY_values_df[\"EpiRR_no_v\"].isin(predicted_as_male).values).flatten()  # type: ignore\n",
    "\n",
    "    print(\n",
    "        f\"Adding mislabels to graph: {len(predicted_as_female_idx)} male->female, {len(predicted_as_male_idx)} female->male\"\n",
    "    )\n",
    "\n",
    "    # Hovertext\n",
    "    hovertext = [\n",
    "        f\"{epirr}: <z-score>={z_score:.3f}\"\n",
    "        for epirr, z_score in zip(\n",
    "            mean_chrY_values_df[\"EpiRR\"],\n",
    "            mean_chrY_values_df[metric_label],\n",
    "        )\n",
    "    ]\n",
    "    hovertext = np.array(hovertext)\n",
    "\n",
    "    # sanity check\n",
    "    if mean_chrY_values_df[\"EpiRR\"].nunique() != mean_chrY_values_df.shape[0]:\n",
    "        raise ValueError(\"EpiRR is not unique.\")\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            name=\"Female\",\n",
    "            x=[0] * len(female_idx),\n",
    "            y=chrY_values[female_idx],  # type: ignore\n",
    "            boxmean=True,\n",
    "            boxpoints=\"all\",\n",
    "            pointpos=-2,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[female_idx],\n",
    "            marker=dict(size=2),\n",
    "            line=dict(width=1, color=\"black\"),\n",
    "            fillcolor=sex_colors[\"female\"],\n",
    "            showlegend=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            name=\"Male\",\n",
    "            x=[1] * len(female_idx),\n",
    "            y=chrY_values[male_idx],  # type: ignore\n",
    "            boxmean=True,\n",
    "            boxpoints=\"all\",\n",
    "            pointpos=-2,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[male_idx],\n",
    "            marker=dict(size=2),\n",
    "            line=dict(width=1, color=\"black\"),\n",
    "            fillcolor=sex_colors[\"male\"],\n",
    "            showlegend=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Male\",\n",
    "            x=[-0.5] * len(predicted_as_male_idx),\n",
    "            y=chrY_values[predicted_as_male_idx],  # type: ignore\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=10, color=sex_colors[\"male\"], line=dict(width=1, color=\"black\")\n",
    "            ),\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[predicted_as_male_idx],\n",
    "            showlegend=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Female\",\n",
    "            x=[0.5] * len(predicted_as_female_idx),\n",
    "            y=chrY_values[predicted_as_female_idx],  # type: ignore\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=10, color=sex_colors[\"female\"], line=dict(width=1, color=\"black\")\n",
    "            ),\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[predicted_as_female_idx],\n",
    "            showlegend=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "\n",
    "    fig.update_yaxes(range=[-1.5, 3])\n",
    "    title = f\"z-score(mean chrY coverage per file) distribution - z-scores averaged over {N_assays} assays\"\n",
    "    if min_pred is not None:\n",
    "        title += f\"<br>avg_maxPred>{min_pred}\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(text=title, x=0.5),\n",
    "        xaxis_title=SEX,\n",
    "        yaxis_title=\"Average z-score\",\n",
    "        width=750,\n",
    "        height=750,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    if logdir:\n",
    "        this_name = f\"{metric_label}_n{mean_chrY_values_df.shape[0]}\"\n",
    "        if name:\n",
    "            this_name = f\"{name}_{this_name}\"\n",
    "        fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "        fig.write_image(logdir / f\"{this_name}.png\")\n",
    "        fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_chrY_zscore\"\n",
    "logdir.mkdir(exist_ok=True)\n",
    "\n",
    "for no_rna in [True, False]:\n",
    "    zscore_merged_assays(\n",
    "        zscore_df=cross_val_analysis,\n",
    "        sex_mislabels=create_mislabel_corrector(paper_dir)[1][SEX],\n",
    "        logdir=logdir,\n",
    "        name=\"no_RNA\" if no_rna else \"w_RNA\",\n",
    "        no_rna=no_rna,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 5B - Average EpiRR z-score per assay (for 10fold on v1.1 IHEC harmonized metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_per_assay(\n",
    "    zscore_df: pd.DataFrame, logdir: Path | None = None, name: str | None = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot the z-score distributions per assay.\n",
    "\n",
    "    Does not include pval and raw tracks.\n",
    "\n",
    "    Args:\n",
    "        zscore_df: The dataframe with z-score data.\n",
    "    \"\"\"\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "\n",
    "    # Remove pval/raw tracks + rna unstranded\n",
    "    zscore_df = zscore_df[~zscore_df[\"track_type\"].isin([\"pval\", \"raw\", \"Unique_raw\"])]\n",
    "\n",
    "    # Merge rna protocols\n",
    "    zscore_df.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n",
    "\n",
    "    # Remove NAs\n",
    "    metric_label = \"chrY_zscore_vs_assay_w_track_exclusion\"\n",
    "    zscore_df = zscore_df[zscore_df[metric_label] != \"NA\"]\n",
    "\n",
    "    assay_sizes = zscore_df[ASSAY].value_counts()\n",
    "    assays = sorted(assay_sizes.index)\n",
    "\n",
    "    x_title = \"Sex z-score distributions per assay\"\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(assays),\n",
    "        shared_yaxes=True,\n",
    "        x_title=x_title,\n",
    "        y_title=\"z-score\",\n",
    "        horizontal_spacing=0.02,\n",
    "        subplot_titles=[\n",
    "            f\"{assay_label} ({assay_sizes[assay_label]})\" for assay_label in assays\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    for i, assay_label in enumerate(sorted(assays)):\n",
    "        sub_df = zscore_df[zscore_df[ASSAY] == assay_label]\n",
    "\n",
    "        hovertext = [\n",
    "            f\"{epirr}: z-score={z_score:.3f}, pred={pred:.3f}\"\n",
    "            for epirr, pred, z_score in zip(\n",
    "                sub_df[\"EpiRR\"],\n",
    "                sub_df[\"Max pred\"],\n",
    "                sub_df[metric_label],\n",
    "            )\n",
    "        ]\n",
    "        hovertext = np.array(hovertext)\n",
    "\n",
    "        sub_df.reset_index(drop=True, inplace=True)\n",
    "        y_values = sub_df[metric_label].values\n",
    "\n",
    "        female_idx = np.argwhere((sub_df[SEX] == \"female\").values).flatten()\n",
    "        male_idx = np.argwhere((sub_df[SEX] == \"male\").values).flatten()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=assay_label,\n",
    "                y=y_values[female_idx],\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[female_idx],\n",
    "                marker=dict(\n",
    "                    size=2,\n",
    "                    color=sex_colors[\"female\"],\n",
    "                    line=dict(width=0.5, color=\"black\"),\n",
    "                ),\n",
    "                fillcolor=sex_colors[\"female\"],\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                showlegend=False,\n",
    "                legendgroup=\"Female\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=assay_label,\n",
    "                y=y_values[male_idx],\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[male_idx],\n",
    "                marker=dict(\n",
    "                    size=2, color=sex_colors[\"male\"], line=dict(width=0.5, color=\"black\")\n",
    "                ),\n",
    "                fillcolor=sex_colors[\"male\"],\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                showlegend=False,\n",
    "                legendgroup=\"Male\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "    # Add a dummy scatter plot for legend\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Female\",\n",
    "            marker=dict(color=sex_colors[\"female\"], size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Female\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Male\",\n",
    "            marker=dict(color=sex_colors[\"male\"], size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Male\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(range=[-1.5, 3])\n",
    "    title = \"z-score(mean chrY coverage per file) distribution per assay\"\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        width=3000,\n",
    "        height=1000,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    if logdir:\n",
    "        if name is None:\n",
    "            name = \"zscore_distributions_per_assay\"\n",
    "        fig.write_image(logdir / f\"{name}.svg\")\n",
    "        fig.write_image(logdir / f\"{name}.png\")\n",
    "        fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_per_assay(cross_val_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 5C - Female/Male chrY coverage z-score cluster separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_assays_separation_distance(\n",
    "    zscore_df: pd.DataFrame, logdir: Path | None = None, name: str | None = None\n",
    ") -> None:\n",
    "    \"\"\"Complement to figure 2E, showing separation distance (mean, median)\n",
    "    between male/female zscore clusters, for ChIP-seq (core7). Grouped by EpiRR.\n",
    "\n",
    "    Args:\n",
    "        zscore_df (pd.DataFrame): The dataframe with z-score data.\n",
    "        logdir (Path): The directory path to save the output plots.\n",
    "        name (str): The base name for the output plot files.\n",
    "    \"\"\"\n",
    "    metric_label = \"chrY_zscore_vs_assay_track\"\n",
    "\n",
    "    # Preprocessing\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "    zscore_df.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n",
    "\n",
    "    zscore_df = zscore_df[zscore_df[ASSAY].isin(CORE7_ASSAYS)]  # type: ignore\n",
    "\n",
    "    # Remove pval/raw tracks\n",
    "    zscore_df = zscore_df[~zscore_df[\"track_type\"].isin([\"pval\", \"raw\"])]\n",
    "\n",
    "    # Average chrY z-score values\n",
    "    mean_chrY_values_df = zscore_df.groupby([\"EpiRR\", SEX]).agg(\n",
    "        {metric_label: \"mean\", \"Max pred\": \"mean\"}\n",
    "    )\n",
    "    mean_chrY_values_df.reset_index(inplace=True)\n",
    "    if not mean_chrY_values_df[\"EpiRR\"].is_unique:\n",
    "        raise ValueError(\"EpiRR is not unique.\")\n",
    "\n",
    "    mean_chrY_values_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    distances = {\"mean\": [], \"median\": []}\n",
    "    min_preds = list(np.arange(0, 1.0, 0.01)) + [0.999]\n",
    "    sample_count = []\n",
    "    for min_pred in min_preds:\n",
    "        subset_chrY_values_df = mean_chrY_values_df[\n",
    "            mean_chrY_values_df[\"Max pred\"] > min_pred\n",
    "        ]\n",
    "        sample_count.append(subset_chrY_values_df.shape[0])\n",
    "\n",
    "        # Compute separation distances\n",
    "        chrY_vals_female = subset_chrY_values_df[subset_chrY_values_df[SEX] == \"female\"][\n",
    "            metric_label\n",
    "        ]\n",
    "        chrY_vals_male = subset_chrY_values_df[subset_chrY_values_df[SEX] == \"male\"][\n",
    "            metric_label\n",
    "        ]\n",
    "\n",
    "        if not chrY_vals_female.empty and not chrY_vals_male.empty:\n",
    "            mean_distance = np.abs(chrY_vals_female.mean() - chrY_vals_male.mean())\n",
    "            median_distance = np.abs(chrY_vals_female.median() - chrY_vals_male.median())\n",
    "\n",
    "            distances[\"mean\"].append(mean_distance)\n",
    "            distances[\"median\"].append(median_distance)\n",
    "        else:\n",
    "            distances[\"mean\"].append(np.nan)\n",
    "            distances[\"median\"].append(np.nan)\n",
    "\n",
    "    # Plotting the results\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for mean and median distances\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=min_preds,\n",
    "            y=distances[\"mean\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Mean Distance (left)\",\n",
    "            line=dict(color=\"blue\"),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=min_preds,\n",
    "            y=distances[\"median\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Median Distance (left)\",\n",
    "            line=dict(color=\"green\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add trace for number of files\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=min_preds,\n",
    "            y=np.array(sample_count) / max(sample_count),\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Proportion of samples (right)\",\n",
    "            line=dict(color=\"red\"),\n",
    "            yaxis=\"y2\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(range=[0.499, 1.0])\n",
    "\n",
    "    # Update layout for secondary y-axis\n",
    "    fig.update_layout(\n",
    "        title=\"Separation Distance of chrY z-scores male/female clusters - ChIP-Seq\",\n",
    "        xaxis_title=\"Average Prediction Score minimum threshold\",\n",
    "        yaxis_title=\"Z-score Distance\",\n",
    "        yaxis2=dict(title=\"Proportion of samples\", overlaying=\"y\", side=\"right\"),\n",
    "        yaxis2_range=[0, 1.001],\n",
    "        legend=dict(\n",
    "            x=1.08,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    if logdir:\n",
    "        if name is None:\n",
    "            name = \"zscore_cluster_separation_distance\"\n",
    "        fig.write_image(logdir / f\"{name}.svg\")\n",
    "        fig.write_image(logdir / f\"{name}.png\")\n",
    "        fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_assays_separation_distance(cross_val_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 5E - Sex genes chromatin states\n",
    "\n",
    "Images extracted from Epilogos viewer, using specified coordinates (XIST and FIRRE positions), and:\n",
    "- View mode: Paired\n",
    "- Dataset: IHEC\n",
    "- Pairwise: Male VS Female 100 samples\n",
    "- Saliency Metric: S1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2F, Supp Fig 5D - Donor life stage and GP-Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_v1_2_df = pd.read_csv(\n",
    "    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.2.extended.csv\"\n",
    ")\n",
    "meta_v1_1_df = pd.read_csv(\n",
    "    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.1.extended.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check, both metadata versions have the same cell line samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_lines_v11 = meta_v1_1_df[meta_v1_1_df[BIOMATERIAL_TYPE] == \"cell line\"][\n",
    "    \"epirr_id_without_version\"\n",
    "].unique()\n",
    "cell_lines_v12 = meta_v1_2_df[meta_v1_2_df[BIOMATERIAL_TYPE] == \"cell line\"][\n",
    "    \"epirr_id_without_version\"\n",
    "].unique()\n",
    "assert set(cell_lines_v11) == set(cell_lines_v12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_age_dir = base_data_dir / \"GP_age\"\n",
    "if not gp_age_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {gp_age_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gp_age = pd.read_csv(gp_age_dir / \"life_stage_prediction.GPage.20250513.tsv\", sep=\"\\t\")\n",
    "df_gp_age[\"graph_age\"] = df_gp_age[\"model30\"]\n",
    "\n",
    "epiclass_pred_col = \"epiclass_predicted_lifestage\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `epiclass_predicted_lifestage` column is manually curated, from analyzing all predictions for all samples.  \n",
    "When an expected label is known, and epiclass predictions are inconclusive (low average max pred and/or no majority consensus), the expected label is kept.\n",
    "\n",
    "Columns `tissue` and `tissue_subtype` are formatting of `harmonized_sample_organ_system_order_AnetaMikulasova` and `harmonized_sample_organ_order_AnetaMikulasova`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gp_age.loc[:, \"graph_age_categories\"] = df_gp_age[epiclass_pred_col].str.removesuffix(\n",
    "    \"_pred\"\n",
    ")\n",
    "\n",
    "gp_age_categories = {\n",
    "    \"adult\": \"adult\",\n",
    "    \"child\": \"pediatric\",\n",
    "    \"embryonic\": \"perinatal\",\n",
    "    \"fetal\": \"perinatal\",\n",
    "    \"newborn\": \"perinatal\",\n",
    "    \"unknown\": \"unknown\",\n",
    "}\n",
    "df_gp_age.loc[:, \"graph_age_categories\"] = df_gp_age[\"graph_age_categories\"].map(\n",
    "    gp_age_categories\n",
    ")\n",
    "display(df_gp_age[\"graph_age_categories\"].value_counts(dropna=False))\n",
    "print(df_gp_age.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epirr_col = \"epirr_id_without_version\"\n",
    "\n",
    "merged_dp_age = pd.merge(\n",
    "    df_gp_age,\n",
    "    meta_v1_2_df[[epirr_col, CELL_TYPE, BIOMATERIAL_TYPE]],\n",
    "    left_on=\"epirr_noV\",\n",
    "    right_on=epirr_col,\n",
    "    how=\"left\",\n",
    ")\n",
    "merged_dp_age.drop_duplicates(subset=[epirr_col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dp_age.drop(columns=[\"epirr_noV\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing cell lines: life stage makes less sense\n",
    "# type: ignore\n",
    "N_before = merged_dp_age.shape[0]\n",
    "merged_dp_age: pd.DataFrame = merged_dp_age[\n",
    "    merged_dp_age[BIOMATERIAL_TYPE] != \"cell line\"\n",
    "]\n",
    "print(f\"Removed {N_before - merged_dp_age.shape[0]} cell lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the graph categories.\n",
    "\n",
    "We need to categorize separately whole blood since from other tissues since GP-Age training is only made of whole blood. We keep 'immune system' tissues, specifically venous and umbilical blood since they match the most closely to the training data. unsure about blood marrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_vals = [\"IMMU\"]\n",
    "layer2_vals = [\"blood-umbilical-cord\", \"blood-venous\"]\n",
    "\n",
    "merged_dp_age.loc[:, \"tissue_group\"] = [\n",
    "    \"blood\" if (val1 in layer1_vals and val2 in layer2_vals) else \"other\"\n",
    "    for val1, val2 in merged_dp_age.loc[:, [\"tissue\", \"tissue_subtype\"]].values\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check, no NaN present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_cols = [\n",
    "    \"epirr_id_without_version\",\n",
    "    \"tissue_group\",\n",
    "    epiclass_pred_col,\n",
    "    \"graph_age\",\n",
    "]\n",
    "missing_N = merged_dp_age.loc[:, important_cols].isna().sum().sum()\n",
    "if missing_N > 0:\n",
    "    raise ValueError(f\"Missing values in merged_dp_age: {missing_N}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_gp_age(\n",
    "    df_gp_age: pd.DataFrame,\n",
    "    logdir: Path | None = None,\n",
    "    name: str | None = None,\n",
    "    title: str | None = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot the GP age predictions.\n",
    "\n",
    "    Args:\n",
    "        df_gp_age: The dataframe with GP age data.\n",
    "    \"\"\"\n",
    "    df = df_gp_age.copy(deep=True)\n",
    "\n",
    "    tissue_colors = {\"blood\": \"red\", \"other\": \"gray\"}\n",
    "\n",
    "    age_cat_label = \"graph_age_categories\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for tissue_group in sorted(df[\"tissue_group\"].unique()):\n",
    "        sub_df = df[df[\"tissue_group\"] == tissue_group]\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=f\"{tissue_group} (n={len(sub_df)})\",\n",
    "                x=sub_df[age_cat_label],\n",
    "                y=sub_df[\"graph_age\"],\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"{ct}: {age:.3f}\"\n",
    "                    for ct, age in zip(sub_df[CELL_TYPE], sub_df[\"graph_age\"])\n",
    "                ],\n",
    "                marker=dict(size=2, color=tissue_colors[tissue_group]),\n",
    "                showlegend=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"GP age predictions - Using MLP predicted labels ({title})\",\n",
    "        xaxis_title=\"Life stage\",\n",
    "        yaxis_title=\"GP-Age : Predicted age\",\n",
    "        width=750,\n",
    "        height=750,\n",
    "        boxmode=\"group\",\n",
    "    )\n",
    "\n",
    "    # Order x-axis\n",
    "    label_order = [\"perinatal\", \"pediatric\", \"adult\"]\n",
    "    axis_labels = [\n",
    "        f\"{age_cat} (n={(df[age_cat_label] == age_cat).sum()})\" for age_cat in label_order\n",
    "    ]\n",
    "\n",
    "    fig.update_xaxes(categoryorder=\"array\", categoryarray=label_order)\n",
    "    fig.update_xaxes(tickvals=[0, 1, 2], ticktext=axis_labels)\n",
    "\n",
    "    # Save figure\n",
    "    if logdir:\n",
    "        if name is None:\n",
    "            name = \"GP_age_predictions\"\n",
    "        fig.write_image(logdir / f\"{name}.svg\")\n",
    "        fig.write_image(logdir / f\"{name}.png\")\n",
    "        fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_fig_logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"GP-Age\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove inconclusive predictions\n",
    "gp_graph_df = merged_dp_age[merged_dp_age[epiclass_pred_col] != \"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tissue_age_df(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Display tissue group count breakdown.\"\"\"\n",
    "    count = df.groupby([\"tissue_group\", \"graph_age_categories\"]).size()\n",
    "    print(\"Tissue group summary\")\n",
    "    print(f\"Blood: {count.loc['blood'].sum()}\")  # type: ignore\n",
    "    print(f\"Other: {count.loc['other'].sum()}\")  # type: ignore\n",
    "    print(f\"Total: {count.sum()}\")\n",
    "    print(\"Detail:\")\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supp Fig 5D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering all samples that had a conclusive epiclass prediction (or existing label and no conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tissue_age_df(gp_graph_df)\n",
    "\n",
    "graph_gp_age(\n",
    "    df_gp_age=gp_graph_df,\n",
    "    name=\"GP_age_predictions_all_samples_MLP_predicted_labels\",\n",
    "    title=\"All samples (has a life stage pred + gp-age)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fig 2F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For initially unknown labels, only predictions that were manually confirmed as mislabels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_unknown_df: pd.DataFrame = gp_graph_df[gp_graph_df[LIFE_STAGE] == \"unknown\"]\n",
    "display_tissue_age_df(only_unknown_df)\n",
    "\n",
    "graph_gp_age(\n",
    "    df_gp_age=only_unknown_df,\n",
    "    name=\"GP_age_predictions_unknown_only_MLP_predicted_labels\",\n",
    "    title=\"Samples with unknown life stage\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Life stage label already existed in v1.1.\n",
    "known_expected_label_df = gp_graph_df[gp_graph_df[LIFE_STAGE] != \"unknown\"]\n",
    "display_tissue_age_df(known_expected_label_df)\n",
    "\n",
    "graph_gp_age(\n",
    "    df_gp_age=known_expected_label_df,\n",
    "    name=\"GP_age_predictions_known_samples_MLP_predicted_labels\",\n",
    "    title=\"All samples w expected label\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2G + Supp Fig 7 - Biospecimen classifier - ChromScore for high-SHAP regions\n",
    "\n",
    "See analyze_hdf5_vals.ipynb, particularly section \"ChromScore hdf5 values\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2H - Gene Ontology and SHAP values\n",
    "\n",
    "See profile_bed.ipynb for creation of gene ontology files.  \n",
    "The code compares important SHAP values regions of different cell types with gene gff (using the gProfiler module)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_type = \"hg38_100kb_all_none\"\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"GO-biospecimens\"\n",
    "if not logdir.exists():\n",
    "    logdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cell_types = [\n",
    "    \"T_cell\",\n",
    "    \"neutrophil\",\n",
    "    \"lymphocyte_of_B_lineage\",\n",
    "    \"brain\",\n",
    "    \"hepatocyte\",\n",
    "]\n",
    "go_terms_table = [\n",
    "    \"T cell receptor complex\",\n",
    "    \"plasma membrane signaling receptor complex\",\n",
    "    \"adaptive immune response\",\n",
    "    \"receptor complex\",\n",
    "    \"secretory granule\",\n",
    "    \"secretory vesicle\",\n",
    "    \"secretory granule membrane\",\n",
    "    \"intracellular vesicle\",\n",
    "    \"immunoglobulin complex\",\n",
    "    \"immune response\",\n",
    "    \"immune system process\",\n",
    "    \"homophilic cell adhesion via plasma membrane adhesion molecules\",\n",
    "    \"DNA binding\",\n",
    "    \"cell-cell adhesion via plasma-membrane adhesion molecules\",\n",
    "    \"RNA polymerase II cis-regulatory region sequence-specific DNA binding\",\n",
    "    \"blood microparticle\",\n",
    "    \"platelet alpha granule lumen\",\n",
    "    \"fibrinogen complex\",\n",
    "    \"endoplasmic reticulum lumen\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAP_dir = base_data_dir / \"SHAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_shap_dir = (\n",
    "    SHAP_dir / hdf5_type / f\"{CELL_TYPE}_1l_3000n\" / \"10fold-oversampling\"\n",
    ")\n",
    "beds_file = cell_type_shap_dir / \"select_beds_top303.tar.gz\"\n",
    "if not beds_file.exists():\n",
    "    raise FileNotFoundError(f\"File {beds_file} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_go_dfs: Dict[str, pd.DataFrame] = {}\n",
    "with tarfile.open(beds_file, \"r:gz\") as tar:\n",
    "    for member in tar.getmembers():\n",
    "        filename = member.name\n",
    "        if filename.endswith(\"profiler.tsv\") and \"merge_samplings\" in filename:\n",
    "            with tar.extractfile(member) as f:  # type: ignore\n",
    "                go_df = pd.read_csv(f, sep=\"\\t\", index_col=0)\n",
    "                all_go_dfs[member.name] = go_df\n",
    "\n",
    "assert len(all_go_dfs) == 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in all_go_dfs.items():\n",
    "    sub_df = df.copy()\n",
    "    sub_df.loc[:, \"shap_source\"] = re.match(r\".*/merge_samplings_(.*)_features_intersect_gff_gprofiler.tsv\", name).group(1)  # type: ignore\n",
    "    sub_df.loc[:, \"table_val\"] = -np.log10(sub_df.loc[:, \"p_value\"])\n",
    "    all_go_dfs[name] = sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_concat_df = pd.concat(all_go_dfs.values())\n",
    "full_concat_df = full_concat_df.drop([\"significant\", \"query\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_write = full_concat_df.copy()\n",
    "to_write.rename(\n",
    "    columns={\"shap_source\": \"biospecimen\", \"table_val\": \"-log10(p_value)\"}, inplace=True\n",
    ")\n",
    "to_write.to_csv(logdir / \"GO_biospecimen_table.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(go_term in full_concat_df[\"name\"].values for go_term in go_terms_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = full_concat_df.pivot_table(\n",
    "    index=\"name\", columns=\"shap_source\", values=\"table_val\", aggfunc=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_terms_graph = [\n",
    "    \"T cell receptor complex\",\n",
    "    \"plasma membrane<br>signaling receptor complex\",\n",
    "    \"adaptive immune response\",\n",
    "    \"receptor complex\",\n",
    "    \"secretory granule\",\n",
    "    \"secretory vesicle\",\n",
    "    \"secretory granule membrane\",\n",
    "    \"intracellular vesicle\",\n",
    "    \"immunoglobulin complex\",\n",
    "    \"immune response\",\n",
    "    \"immune system process\",\n",
    "    \"homophilic cell adhesion via<br>plasma membrane adhesion molecules\",\n",
    "    \"DNA binding\",\n",
    "    \"cell-cell adhesion via<br>plasma-membrane adhesion molecules\",\n",
    "    \"RNA polymerase II cis-regulatory<br>region sequence-specific DNA binding\",\n",
    "    \"blood microparticle\",\n",
    "    \"platelet alpha granule lumen\",\n",
    "    \"fibrinogen complex\",\n",
    "    \"endoplasmic reticulum lumen\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_5_ct = table.loc[go_terms_table, selected_cell_types].copy()\n",
    "assert table_5_ct.shape == (len(go_terms_graph), len(selected_cell_types))\n",
    "\n",
    "# Rename index\n",
    "table_5_ct = pd.DataFrame(table_5_ct, index=go_terms_graph, columns=table_5_ct.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = \"\\u03c3\"\n",
    "\n",
    "colorbar = dict(\n",
    "    title=\"-log<sub>10</sub>(p-value)\",\n",
    "    tickvals=[0, 1.30, 2, 3, 5, 6.53, 10],\n",
    "    ticktext=[\n",
    "        \"0\",\n",
    "        f\"1.30: p=0.05~2{sigma}\",\n",
    "        \"2: p=0.01\",\n",
    "        f\"3: p=0.001~3{sigma}\",\n",
    "        \"5\",\n",
    "        f\"6.53: p=3x10<sup>-7</sup> = 5{sigma}\",\n",
    "        \"10\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill z with NaNs (for color) and make custom text labels\n",
    "z = table_5_ct.values  # keep NaNs in for color\n",
    "text = np.where(\n",
    "    np.isnan(z), \"NS\", np.char.mod(\"%.2f\", z)  # format floats to 2 decimal places\n",
    ")\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=np.nan_to_num(\n",
    "            z, nan=0\n",
    "        ),  # replace NaNs with 0 for coloring (or use a custom colormap later)\n",
    "        x=table_5_ct.columns,\n",
    "        y=table_5_ct.index,\n",
    "        colorscale=\"Blues\",\n",
    "        zmin=0,\n",
    "        zmax=10,\n",
    "        colorbar=colorbar,\n",
    "        text=text,\n",
    "        texttemplate=\"%{text}\",\n",
    "        hovertemplate=\"GO Term: %{y}<br>Class: %{x}<br>Value: %{z:.2f}<extra></extra>\",\n",
    "        showscale=True,\n",
    "        xgap=2,\n",
    "        ygap=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Top SHAP regions: GO term enrichment\",\n",
    "    width=600,\n",
    "    height=1000,\n",
    "    plot_bgcolor=\"black\",\n",
    ")\n",
    "\n",
    "# Fix gridlines\n",
    "fig.update_xaxes(showgrid=False)\n",
    "fig.update_yaxes(showgrid=False)\n",
    "\n",
    "name = \"GO_5ct_term_enrichment\"\n",
    "fig.write_image(logdir / f\"{name}.svg\")\n",
    "fig.write_image(logdir / f\"{name}.png\")\n",
    "fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 top pval terms for each biospecimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 5 more significant terms for each cell type, preserve order\n",
    "top_5_terms = []\n",
    "for name, df in all_go_dfs.items():\n",
    "    df.sort_values(\"p_value\", inplace=True)\n",
    "    top_5 = df[\"name\"].head(5).to_list()\n",
    "    top_5_terms.extend(top_5)\n",
    "\n",
    "top_5_terms = list(dict.fromkeys(top_5_terms))  # preserve order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_top_5_GO = table.loc[top_5_terms, :].copy()\n",
    "\n",
    "z = table_top_5_GO.values\n",
    "text = np.where(np.isnan(z), \"NS\", np.char.mod(\"%.2f\", z))\n",
    "\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=np.nan_to_num(\n",
    "            z, nan=0\n",
    "        ),  # replace NaNs with 0 for coloring (or use a custom colormap later)\n",
    "        x=table_top_5_GO.columns,\n",
    "        y=table_top_5_GO.index,\n",
    "        colorscale=\"Blues\",\n",
    "        zmin=0,\n",
    "        zmax=10,\n",
    "        colorbar=colorbar,\n",
    "        text=text,\n",
    "        texttemplate=\"%{text}\",\n",
    "        hovertemplate=\"GO Term: %{y}<br>Class: %{x}<br>Value: %{z:.2f}<extra></extra>\",\n",
    "        showscale=True,\n",
    "        xgap=2,\n",
    "        ygap=2,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Top SHAP regions: GO term enrichment\",\n",
    "    width=1300,\n",
    "    height=2000,\n",
    "    plot_bgcolor=\"black\",\n",
    ")\n",
    "\n",
    "# Fix gridlines\n",
    "fig.update_xaxes(showgrid=False)\n",
    "fig.update_yaxes(showgrid=False)\n",
    "\n",
    "name = \"GO_top_5_term_enrichment_all_ct\"\n",
    "fig.write_image(logdir / f\"{name}.svg\")\n",
    "fig.write_image(logdir / f\"{name}.png\", scale=2)\n",
    "fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important regions per cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = table_dir / \"dfreeze_v2/100kb_all_none/SHAP-MLP/cell_type\"\n",
    "input_file = input_dir / \"select_beds_top303.tar.gz\"\n",
    "\n",
    "if not input_file.exists():\n",
    "    raise FileNotFoundError(f\"File {input_file} does not exist.\")\n",
    "\n",
    "sizes = []\n",
    "with tarfile.open(input_file, \"r:gz\") as tar:\n",
    "    for member in tar.getmembers():\n",
    "        filename = member.name\n",
    "        if \"merge_samplings\" in filename and filename.endswith(\"bed\"):\n",
    "            with tar.extractfile(member) as f:  # type: ignore\n",
    "                N = len(f.readlines())\n",
    "                print(f\"{filename.split('/')[1]}: {N}\")\n",
    "                sizes.append(N)\n",
    "\n",
    "print(f\"Mean: {np.mean(sizes):.1f}\")\n",
    "print(f\"Median: {np.median(sizes):.1f}\")\n",
    "print(f\"Std: {np.std(sizes):.1f}\")\n",
    "print(f\"Range: [{min(sizes)}, {max(sizes)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2I - Biospecimen important regions chromatin states\n",
    "\n",
    "Images extracted from Epilogos viewer, using:\n",
    "- Region: chr11:118300000-118400000\n",
    "- View mode: Single\n",
    "- Dataset: IHEC\n",
    "- All biosamples\n",
    "- Saliency Metric: S1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2J - Copy Number Variant (CNV) Signatures and SHAP values\n",
    "\n",
    "See CNV_treatment.ipynb for the creation of the CNV stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnv_dir = base_data_dir / \"CNV\"\n",
    "cnv_intersection_results = (\n",
    "    cnv_dir\n",
    "    / \"signature_analysis\"\n",
    "    / \"epiatlas_cancer_types\"\n",
    "    / \"important_cancer_features_z_scores_vs_random200.tsv\"\n",
    ")\n",
    "if not cnv_intersection_results.exists():\n",
    "    raise FileNotFoundError(f\"File {cnv_intersection_results} does not exist.\")\n",
    "\n",
    "cnv_df = pd.read_csv(cnv_intersection_results, sep=\"\\t\", index_col=0)\n",
    "cnv_df.name = cnv_intersection_results.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cnv_zscores(cnv_df: pd.DataFrame, logdir: Path | None = None) -> None:\n",
    "    \"\"\"Plot z-scores of top SHAP features vs random feature sets, grouped by CNV groups.\n",
    "\n",
    "    Args:\n",
    "        cnv_df: The DataFrame with z-scores.\n",
    "        logdir: The output directory to save the plot.\n",
    "    \"\"\"\n",
    "    n_beds = int(cnv_df.name.split(\"random\")[1])\n",
    "    signature_subset_name = \"EpiATLAS cancer types\"\n",
    "\n",
    "    CN_groups = [\n",
    "        [f\"CN{i}\" for i in range(1, 4)],\n",
    "        [f\"CN{i}\" for i in range(9, 13)],\n",
    "        [f\"CN{i}\" for i in range(13, 17)],\n",
    "        [f\"CN{i}\" for i in range(17, 18)],\n",
    "        [f\"CN{i}\" for i in range(18, 22)],\n",
    "        [f\"CN{i}\" for i in range(4, 9)],\n",
    "    ]\n",
    "    CN_names = [\n",
    "        \"CN1-CN3\",\n",
    "        \"CN9-CN12\",\n",
    "        \"CN13-CN16\",\n",
    "        \"CN17\",\n",
    "        \"CN18-CN21\",\n",
    "        \"CN4-CN8\",\n",
    "    ]\n",
    "\n",
    "    # Assign groups to the DataFrame\n",
    "    cnv_df[\"group\"] = \"Other\"\n",
    "    for i, group in enumerate(CN_groups):\n",
    "        cnv_df.loc[cnv_df.index.isin(group), \"group\"] = CN_names[i]\n",
    "\n",
    "    # Sort groups\n",
    "    group_medians = (\n",
    "        cnv_df.groupby(\"group\")[\"z_score\"].median().sort_values(ascending=False)\n",
    "    )\n",
    "    sorted_CN_names = group_medians.index.tolist()\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for group in sorted_CN_names:\n",
    "        group_data = cnv_df[cnv_df[\"group\"] == group]\n",
    "        marker_size = 4 if group != \"CN17\" else 6\n",
    "\n",
    "        # Add the box plot without points\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=group_data[\"z_score\"],\n",
    "                name=group,\n",
    "                boxmean=True,\n",
    "                boxpoints=False,  # Don't show points in the box plot\n",
    "                line=dict(color=\"black\"),\n",
    "                fillcolor=\"rgba(255,255,255,0)\",\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add scatter plot for individual points\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[group] * len(group_data),\n",
    "                y=group_data[\"z_score\"],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    color=\"red\",\n",
    "                    size=marker_size,\n",
    "                ),\n",
    "                name=group,\n",
    "                showlegend=False,\n",
    "                text=group_data.index,  # Use CN names as hover text\n",
    "                hoverinfo=\"text+y\",  # Show CN name and y-value on hover\n",
    "            )\n",
    "        )\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            \"text\": f\"Z-scores of top SHAP features (N=336) vs {n_beds} random feature sets of same size<br>on {signature_subset_name}\"\n",
    "        },\n",
    "        xaxis_title=\"Cancer Type Group\",\n",
    "        yaxis_title=\"Z-score\",\n",
    "    )\n",
    "\n",
    "    # Add a horizontal line at y=0 for reference\n",
    "    fig.add_hline(y=0, line_color=\"grey\", line_width=1)\n",
    "\n",
    "    # Show and save the figure\n",
    "    if logdir:\n",
    "        name = \"important_cancer_features_z_scores_boxplot\"\n",
    "        fig.write_image(logdir / f\"{name}.png\")\n",
    "        fig.write_image(logdir / f\"{name}.svg\")\n",
    "        fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cnv_zscores(cnv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cnv_zscores_alt(cnv_df: pd.DataFrame, logdir: Path | None = None) -> None:\n",
    "    \"\"\"Plot z-scores of top SHAP features vs random feature sets, grouped by focal vs chromosomal events.\n",
    "\n",
    "    Args:\n",
    "        cnv_df: The DataFrame with z-scores.\n",
    "        logdir: The output directory to save the plot.\n",
    "    \"\"\"\n",
    "    n_beds = int(cnv_df.name.split(\"random\")[1])\n",
    "    signature_subset_name = \"EpiATLAS cancer types\"\n",
    "\n",
    "    # Assign groups\n",
    "    CN_groups = [\n",
    "        [f\"CN{i}\" for i in range(1, 9)] + [f\"CN{i}\" for i in range(13, 17)],\n",
    "        [f\"CN{i}\" for i in range(9, 13)] + [f\"CN{i}\" for i in range(17, 22)],\n",
    "    ]\n",
    "    CN_names = [\n",
    "        \"Chromosomal events (CN1-CN8, CN13-CN16)\",\n",
    "        \"Focal events (CN9-CN12, CN17-CN21)\",\n",
    "    ]\n",
    "\n",
    "    # Assign groups to the DataFrame\n",
    "    cnv_df[\"group\"] = \"Other\"\n",
    "    for i, group in enumerate(CN_groups):\n",
    "        cnv_df.loc[cnv_df.index.isin(group), \"group\"] = CN_names[i]\n",
    "\n",
    "    # Sort groups\n",
    "    group_medians = (\n",
    "        cnv_df.groupby(\"group\")[\"z_score\"].median().sort_values(ascending=False)\n",
    "    )\n",
    "    sorted_CN_names = group_medians.index.tolist()\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for group in sorted_CN_names:\n",
    "        group_data = cnv_df[cnv_df[\"group\"] == group]\n",
    "        hover_text = [\n",
    "            f\"{CN_name}: Z={val:.3f}\"\n",
    "            for CN_name, val in zip(group_data.index, group_data[\"z_score\"])\n",
    "        ]\n",
    "\n",
    "        # Add the box plot without points\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=group_data[\"z_score\"],\n",
    "                name=group,\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                line=dict(color=\"black\"),\n",
    "                fillcolor=\"rgba(255,255,255,0)\",\n",
    "                showlegend=False,\n",
    "                hoverinfo=\"text\",  # Show CN name and y-value on hover\n",
    "                hovertext=hover_text,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            \"text\": f\"Z-scores of top SHAP features (N=336) vs {n_beds} random feature sets of same size<br>on {signature_subset_name}\"\n",
    "        },\n",
    "        xaxis_title=\"Copy Number Alteration Event Type\",\n",
    "        yaxis_title=\"CNA Enrichment (Z-score)\",\n",
    "    )\n",
    "\n",
    "    # Add a horizontal line at y=0 for reference\n",
    "    fig.add_hline(y=0, line_color=\"grey\", line_width=1)\n",
    "\n",
    "    # Show and save the figure\n",
    "    if logdir:\n",
    "        name = \"important_cancer_features_z_scores_boxplot_V2\"\n",
    "        fig.write_image(logdir / f\"{name}.png\")\n",
    "        fig.write_image(logdir / f\"{name}.svg\")\n",
    "        fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cnv_zscores_alt(cnv_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
