{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create supplementary prediction files destined for the paper.\n",
    "\n",
    "Includes most data predictions used to create paper figures.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import functools\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import classification_report, confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.utils.classification_merging_utils import merge_dataframes\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANCER = \"harmonized_sample_cancer_high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "metadata_dir = base_data_dir / \"metadata\"\n",
    "paper_dir = base_dir\n",
    "table_dir = paper_dir / \"tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()\n",
    "metadata_handler = MetadataHandler(paper_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Official metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = metadata_handler.load_metadata_df(version=\"v2\", merge_assays=False)\n",
    "md5sum_to_epirr = meta_df[\"epirr_id_without_version\"].to_dict()\n",
    "md5sum_to_uuid = meta_df[\"uuid\"].to_dict()\n",
    "del meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_metadata_dir = base_data_dir / \"metadata\" / \"official\"\n",
    "\n",
    "metadata_v1_1_path = (\n",
    "    official_metadata_dir / \"IHEC_metadata_harmonization.v1.1.extended.csv\"\n",
    ")\n",
    "metadata_v1_1 = pd.read_csv(metadata_v1_1_path, index_col=False)\n",
    "metadata_v1_1.set_index(\"epirr_id_without_version\", inplace=True)\n",
    "\n",
    "metadata_v1_2_path = (\n",
    "    official_metadata_dir / \"IHEC_metadata_harmonization.v1.2.extended.csv\"\n",
    ")\n",
    "metadata_v1_2 = pd.read_csv(metadata_v1_2_path, index_col=False)\n",
    "metadata_v1_2.set_index(\"epirr_id_without_version\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect experiment keys for all trained classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_experiment_info(line: str) -> Tuple[str, str] | None:\n",
    "    \"\"\"Extract split and experiment key from a line containing checkpoint information.\n",
    "\n",
    "    Line should have format: .../splitX/EpiLaP/[exp_key]/checkpoints/...\n",
    "    \"\"\"\n",
    "    if \"EpiLaP\" not in line:\n",
    "        return None\n",
    "\n",
    "    parts = line.strip().split(\"/\")\n",
    "    for i, part in enumerate(parts):\n",
    "        if part == \"EpiLaP\" and i > 0:\n",
    "            return (parts[i - 1], parts[i + 1])\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_log_file(file_path: Path) -> Set[Tuple[str, str]]:\n",
    "    \"\"\"Process a single log file and extract experiment information.\"\"\"\n",
    "    experiment_info = set()\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                if result := extract_experiment_info(line):\n",
    "                    experiment_info.add(result)\n",
    "    except Exception as e:  # pylint: disable=broad-exception-caught\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    return experiment_info\n",
    "\n",
    "\n",
    "def collect_exp_keys(folder: Path) -> Dict[str, Set[Tuple[str, str]]]:\n",
    "    \"\"\"Collect experiment keys from log files (.o files), recursively from a given folder.\"\"\"\n",
    "    experiments_keys = defaultdict(set)\n",
    "\n",
    "    log_files = folder.glob(\"*1l_3000n/**/*.o\")\n",
    "    for file in log_files:\n",
    "        if experiment_info := process_log_file(file):\n",
    "            experiments_keys[file.parent].update(experiment_info)\n",
    "\n",
    "    return experiments_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_exp_key_context(\n",
    "    experiments_keys_dict: Dict[str, Set[Tuple[str, str]]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Format experiment keys context for saving to a DataFrame.\"\"\"\n",
    "    data = []\n",
    "    for exp_folder, exp_keys in experiments_keys_dict.items():\n",
    "        for split, exp_key in exp_keys:\n",
    "            data.append((str(exp_folder), split, exp_key))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"exp_folder\", \"split\", \"exp_key\"])\n",
    "\n",
    "    # comet-ml experiment url\n",
    "    df[\"comet-url\"] = \"https://www.comet.com/rabyj/epiclass/\" + df[\"exp_key\"].astype(str)\n",
    "\n",
    "    # Remove useless part of paths\n",
    "    to_remove_path = (\n",
    "        str(Path.home() / \"Projects/epiclass/output/paper/data/training_results\") + \"/\"\n",
    "    )\n",
    "    df[\"complete_experiment_context\"] = df[\"exp_folder\"].str.replace(to_remove_path, \"\")\n",
    "    df.drop(columns=\"exp_folder\", inplace=True)\n",
    "\n",
    "    # Split path into named parts\n",
    "    df[[\"release\", \"feature_set_name\", \"metadata_category\"]] = (\n",
    "        df[\"complete_experiment_context\"].str.split(\"/\", expand=True).loc[:, [0, 1, 2]]\n",
    "    )\n",
    "\n",
    "    df[\"experiment_specification\"] = (\n",
    "        df[\"complete_experiment_context\"]\n",
    "        .str.split(\"/\", n=3, expand=True)[3]\n",
    "        .str.replace(\"/\", \",\")\n",
    "    )\n",
    "\n",
    "    # Remove redundant info (all MLP exp are 1 hidden layer 3000 nodes)\n",
    "    df[\"metadata_category\"] = df[\"metadata_category\"].str.replace(\"_1l_3000n\", \"\")\n",
    "\n",
    "    # Reorder columns\n",
    "    df_new_col_order = df.columns.to_list()[-4:] + df.columns.to_list()[:-4]\n",
    "    df = df[df_new_col_order]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exp_keys_dfs = []\n",
    "for folder in [\"dfreeze_v2\", \"2023-01-epiatlas-freeze\", \"imputation\"]:\n",
    "    data_dir = base_data_dir / \"training_results\" / folder\n",
    "    for subfolder in data_dir.glob(\"*\"):\n",
    "        if subfolder.is_file():\n",
    "            continue\n",
    "        # print(subfolder)\n",
    "        exp_key_dict = collect_exp_keys(subfolder)\n",
    "        df = format_exp_key_context(exp_key_dict)\n",
    "        all_exp_keys_dfs.append(df)\n",
    "\n",
    "exp_keys_df = pd.concat(all_exp_keys_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in [\"release\", \"feature_set_name\", \"metadata_category\", \"split\"]:\n",
    "#     display(exp_keys_df[col].value_counts(dropna=False))\n",
    "\n",
    "exp_keys_df.to_csv(table_dir / \"training_experiment_keys.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assay_epiclass + sample ontology for all 5 model types - 100kb_all_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df_for_save(\n",
    "    df: pd.DataFrame, md5sum_to_epirr: Dict[str, str], md5sum_to_uuid: Dict[str, str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Prepare DataFrame for saving to CSV. Return a modified DataFrame, with:\n",
    "    - Index set to md5sum\n",
    "    - Expected class as the first column\n",
    "    - epirr_without_version column added\n",
    "    - uuid column added\n",
    "    - Sorted by epirr_without_version and uuid\n",
    "    \"\"\"\n",
    "    df.insert(0, \"Expected class\", df.pop(\"True class\"))\n",
    "    df.set_index(\"md5sum\", inplace=True)\n",
    "\n",
    "    df[\"epirr_without_version\"] = df.index.map(md5sum_to_epirr)\n",
    "    df[\"uuid\"] = df.index.map(md5sum_to_uuid)\n",
    "    df.sort_values([\"epirr_without_version\", \"uuid\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_100kb = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "logdir = table_dir / \"dfreeze_v2\" / \"100kb_all_none\"\n",
    "if not logdir.exists():\n",
    "    logdir.mkdir(parents=True)\n",
    "\n",
    "split_md5sums = []\n",
    "for category in [ASSAY, CELL_TYPE]:\n",
    "    all_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "        results_dir=data_dir_100kb,\n",
    "        label_category=category,\n",
    "        only_NN=False,\n",
    "    )\n",
    "\n",
    "    # Sanity check, same shape, same input files for each method\n",
    "    for split_dict in all_split_dfs.values():\n",
    "        ref_dict = split_dict[\"NN\"]\n",
    "        ref_md5sums = sorted(ref_dict.index.values.tolist())\n",
    "        ref_shape = ref_dict.shape\n",
    "        for method, df in split_dict.items():\n",
    "            if not ref_md5sums == sorted(df.index.values.tolist()):\n",
    "                raise ValueError(\"MD5sums do not match\")\n",
    "            if ref_shape != df.shape:\n",
    "                raise ValueError(\"Shapes do not match\")\n",
    "\n",
    "    all_split_dfs_concat: Dict = split_results_handler.concatenate_split_results(all_split_dfs)  # type: ignore\n",
    "\n",
    "    # Save to file\n",
    "    for method, df in all_split_dfs_concat.items():\n",
    "        df = prepare_df_for_save(df, md5sum_to_epirr, md5sum_to_uuid)\n",
    "\n",
    "        if method == \"NN\":\n",
    "            method = \"MLP\"\n",
    "\n",
    "        filename = f\"10fold_predictions_{category}_{method}.csv\"\n",
    "        df.to_csv(logdir / filename, index=True, sep=\",\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other MLP results - 100kb_all_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"paired_end\",\n",
    "    \"harmonized_sample_cancer_high\",\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    \"harmonized_biomaterial_type\",\n",
    "    \"project\",\n",
    "]\n",
    "\n",
    "# Select 10-fold oversampling runs\n",
    "all_split_dfs = split_results_handler.general_split_metrics(\n",
    "    results_dir=data_dir_100kb,\n",
    "    merge_assays=False,\n",
    "    include_categories=categories,\n",
    "    exclude_names=[\"reg\", \"no-mixed\", \"chip\"],\n",
    "    return_type=\"split_results\",\n",
    "    oversampled_only=True,\n",
    "    verbose=False,\n",
    ")\n",
    "all_split_dfs_concat: Dict = split_results_handler.concatenate_split_results(all_split_dfs, concat_first_level=True)  # type: ignore\n",
    "\n",
    "# Save to file\n",
    "for category, df in all_split_dfs_concat.items():\n",
    "    df = prepare_df_for_save(df, md5sum_to_epirr, md5sum_to_uuid)\n",
    "    if category in [LIFE_STAGE, SEX]:\n",
    "        for version, metadata in [(\"v1.2\", metadata_v1_2), (\"v1.1\", metadata_v1_1)]:\n",
    "            idx = df.index.map(md5sum_to_epirr).values\n",
    "            values = metadata.loc[idx, category].to_list()  # type: ignore\n",
    "            df.insert(loc=0, column=f\"Expected class {version}\", value=values)\n",
    "\n",
    "        df.drop(columns=\"Expected class\", inplace=True)\n",
    "\n",
    "    filename = f\"10fold_predictions_{category}_MLP.csv\"\n",
    "    df.to_csv(logdir / filename, index=True, sep=\",\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for other feature sets (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_splits_identity(\n",
    "    all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]],\n",
    "    task_names: List[str],\n",
    "    verbose: bool | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Verify that the splits are identical between feature sets for each task.\n",
    "\n",
    "    all_results: {feature_set: {task_name: {split_name: results_dataframe}}}\n",
    "    task_names: list of task names to verify\n",
    "    verbose: print additional information\n",
    "    \"\"\"\n",
    "    # Sanity check : MD5sums and shapes should match between reference and other feature sets, for each split\n",
    "    for task_name in task_names:\n",
    "        if verbose:\n",
    "            print(f\"Verifying task '{task_name}'\")\n",
    "        # Select a reference feature set and use its splits as the baseline for comparison\n",
    "        reference_feature_set = \"hg38_100kb_all_none\"\n",
    "        reference_splits = all_results[reference_feature_set][task_name]\n",
    "\n",
    "        # Create reference MD5sums and shapes for each split in the reference feature set\n",
    "        reference_md5sums = {\n",
    "            split_name: sorted(df.index.tolist())\n",
    "            for split_name, df in reference_splits.items()\n",
    "        }\n",
    "        reference_shapes = {\n",
    "            split_name: df.shape for split_name, df in reference_splits.items()\n",
    "        }\n",
    "\n",
    "        # Iterate over each feature set and compare its splits against the reference\n",
    "        for feature_set_name, tasks_dict in all_results.items():\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Verifying feature set '{feature_set_name}' against reference feature set '{reference_feature_set}'\"\n",
    "                )\n",
    "            for split_name, df in tasks_dict[task_name].items():\n",
    "                if reference_shapes[split_name] != df.shape:\n",
    "                    print(\n",
    "                        f\"WARNING: Shape mismatch in task '{task_name}', split '{split_name}', \"\n",
    "                        f\"between reference feature set '{reference_feature_set}' and feature set '{feature_set_name}'\",\n",
    "                    )\n",
    "                if reference_md5sums[split_name] != sorted(df.index.tolist()):\n",
    "                    print(\n",
    "                        f\"WARNING: MD5sums mismatch in task '{task_name}', split '{split_name}', \"\n",
    "                        f\"between reference feature set '{reference_feature_set}' and feature set '{feature_set_name}'\",\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ASSAY, CELL_TYPE]\n",
    "include_sets = [\n",
    "    \"hg38_10mb_all_none_1mb_coord\",\n",
    "    \"hg38_100kb_random_n316_none\",\n",
    "    \"hg38_1mb_all_none\",\n",
    "    \"hg38_100kb_random_n3044_none\",\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_gene_regions_100kb_coord_n19864\",\n",
    "    \"hg38_10kb_random_n30321_none\",\n",
    "    \"hg38_regulatory_regions_n30321\",\n",
    "    \"hg38_1kb_random_n30321_none\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n",
    "    \"hg38_10kb_all_none\",\n",
    "    \"hg38_regulatory_regions_n303114\",\n",
    "    \"hg38_1kb_random_n303114_none\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n300k\",\n",
    "]\n",
    "exclude_names = [\"7c\", \"chip-seq-only\", \"27ct\", \"16ct\"]\n",
    "\n",
    "# Select 10-fold oversampling runs\n",
    "# expected result shape: {feature_set: {task_name: {split_name: results_dataframe}}}\n",
    "all_results: Dict[\n",
    "    str, Dict[str, Dict[str, pd.DataFrame]]\n",
    "] = split_results_handler.obtain_all_feature_set_data(\n",
    "    return_type=\"split_results\",\n",
    "    parent_folder=data_dir_100kb.parent,\n",
    "    merge_assays=False,\n",
    "    include_categories=categories,\n",
    "    include_sets=include_sets,\n",
    "    exclude_names=exclude_names,\n",
    "    verbose=False,\n",
    ")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_set_name in all_results.keys():\n",
    "    try:\n",
    "        all_results[feature_set_name][ASSAY] = all_results[feature_set_name][\"assay_epiclass_11c\"]  # type: ignore\n",
    "        del all_results[feature_set_name][\"assay_epiclass_11c\"]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_splits_identity(all_results, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = table_dir / \"dfreeze_v2\" / \"other_feature_sets\"\n",
    "logdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for feature_set_name, tasks_dict in all_results.items():\n",
    "    if feature_set_name == \"hg38_100kb_all_none\":\n",
    "        continue\n",
    "    all_split_dfs_concat: Dict = split_results_handler.concatenate_split_results(\n",
    "        tasks_dict, concat_first_level=True\n",
    "    )  # type: ignore\n",
    "    for task_name, df in all_split_dfs_concat.items():\n",
    "        df = prepare_df_for_save(df, md5sum_to_epirr, md5sum_to_uuid)\n",
    "\n",
    "        filename = f\"{feature_set_name}_10fold_predictions_{task_name}.csv\"\n",
    "        df.to_csv(logdir / filename, index=True, sep=\",\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winsorized files and/or blacklist zeroed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ASSAY, CELL_TYPE, SEX, \"harmonized_biomaterial_type\"]\n",
    "include_sets = [\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_100kb_all_none_0blklst\",\n",
    "    \"hg38_100kb_all_none_0blklst_winsorized\",\n",
    "]\n",
    "\n",
    "results_folder = base_data_dir / \"training_results\" / \"2023-01-epiatlas-freeze\"\n",
    "if not results_folder.exists():\n",
    "    raise FileNotFoundError(f\"Folder '{results_folder}' not found\")\n",
    "\n",
    "logdir = table_dir / \"2023-01-epiatlas-freeze\"\n",
    "if not logdir.exists():\n",
    "    logdir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10-fold oversampling runs\n",
    "# expected result shape: {feature_set: {task_name: {split_name: results_dataframe}}}\n",
    "all_results: Dict[\n",
    "    str, Dict[str, Dict[str, pd.DataFrame]]\n",
    "] = split_results_handler.obtain_all_feature_set_data(\n",
    "    return_type=\"split_results\",\n",
    "    parent_folder=results_folder,\n",
    "    merge_assays=False,\n",
    "    include_categories=categories,\n",
    "    include_sets=include_sets,\n",
    "    oversampled_only=False,\n",
    "    verbose=False,\n",
    ")  # type: ignore\n",
    "\n",
    "display(all_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_collected = list(all_results[\"hg38_100kb_all_none\"].keys())\n",
    "verify_splits_identity(all_results, tasks_collected, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save concatenated result\n",
    "for feature_set_name, tasks_dict in all_results.items():\n",
    "    concatenated_dfs = split_results_handler.concatenate_split_results(\n",
    "        tasks_dict, concat_first_level=True\n",
    "    )\n",
    "    for task_name, concatenated_df in concatenated_dfs.items():\n",
    "        concatenated_df = prepare_df_for_save(concatenated_df, md5sum_to_epirr, md5sum_to_uuid)  # type: ignore\n",
    "        filename = f\"{feature_set_name}_10fold_predictions_{task_name}.csv\"\n",
    "        print(f\"Saving {filename}\")\n",
    "        concatenated_df.to_csv(\n",
    "            logdir / filename, index=True, sep=\",\", float_format=\"%.4f\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate input dataset discrepancy in assay_epiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata_df = metadata_handler.load_metadata_df(\"v2\", merge_assays=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_counts = {}\n",
    "for feature_set_name, tasks_dict in all_results.items():\n",
    "    concatenated_dfs = split_results_handler.concatenate_split_results(\n",
    "        tasks_dict, concat_first_level=True\n",
    "    )\n",
    "    md5sums = concatenated_dfs[ASSAY].index.tolist()\n",
    "    print(f\"{feature_set_name}: {len(md5sums)}\")\n",
    "\n",
    "    metadata_subset = metadata_df[metadata_df.index.isin(md5sums)]\n",
    "    values_counts[feature_set_name] = metadata_subset[ASSAY].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    values_counts[\"hg38_100kb_all_none_0blklst\"] - values_counts[\"hg38_100kb_all_none\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODE predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata cleanup/merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_dir = base_data_dir / \"metadata\" / \"encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata_path = encode_metadata_dir / \"encode_metadata_2023-10-25.csv\"\n",
    "full_metadata_df = pd.read_csv(full_metadata_path)\n",
    "full_metadata_df[\"filename\"] = full_metadata_df[\"md5sum\"]\n",
    "full_metadata_df.drop(columns=\"md5sum\", inplace=True)\n",
    "full_metadata_df.reset_index(drop=True, inplace=True)\n",
    "print(full_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file does not contain ctcf/non-core, so gotta fill out the missing values when merging\n",
    "encode_epiatlas_mapping_path = encode_metadata_dir / \"ENCODE_IHEC_keys.tsv\"\n",
    "encode_epiatlas_mapping_df = pd.read_csv(encode_epiatlas_mapping_path, sep=\"\\t\")\n",
    "print(encode_epiatlas_mapping_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"is_EpiAtlas_EpiRR\" not in full_metadata_df.columns:\n",
    "    full_metadata_df = full_metadata_df.merge(\n",
    "        encode_epiatlas_mapping_df, left_on=\"filename\", right_on=\"ENC_ID\", how=\"left\"\n",
    "    )\n",
    "    full_metadata_df.replace(np.nan, \"unknown\", inplace=True)\n",
    "\n",
    "# merge duplicate column names from previous merging when possible\n",
    "for col in full_metadata_df.columns:\n",
    "    if col.endswith(\"_x\"):\n",
    "        name1 = col\n",
    "        name2 = col.replace(\"_x\", \"_y\")\n",
    "        if name2 not in full_metadata_df.columns:\n",
    "            full_metadata_df.rename(\n",
    "                columns={name1: name1.replace(\"_x\", \"\")}, inplace=True\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        col1 = full_metadata_df[name1]\n",
    "        col2 = full_metadata_df[name2]\n",
    "        if not (col1 == col2).all():\n",
    "            diff_mask = col1 != col2\n",
    "            if (col1[diff_mask] == \"unknown\").all():\n",
    "                full_metadata_df.drop(columns=name1, inplace=True)\n",
    "                full_metadata_df.rename(\n",
    "                    columns={name2: name2.replace(\"_y\", \"\")}, inplace=True\n",
    "                )\n",
    "            elif (col2[diff_mask] == \"unknown\").all():\n",
    "                full_metadata_df.drop(columns=name2, inplace=True)\n",
    "                full_metadata_df.rename(\n",
    "                    columns={name1: name1.replace(\"_x\", \"\")}, inplace=True\n",
    "                )\n",
    "        else:\n",
    "            full_metadata_df.drop(columns=name2, inplace=True)\n",
    "            full_metadata_df.rename(\n",
    "                columns={name1: name1.replace(\"_x\", \"\")}, inplace=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in full_metadata_df.columns:\n",
    "#     if \"lab\" in col.lower():\n",
    "#         print(full_metadata_df[col].value_counts(dropna=False), \"\\n\")\n",
    "try:\n",
    "    full_metadata_df.drop(columns=[\"lab_x\", \"lab_y\"], inplace=True)\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curie_def_df = pd.read_csv(\n",
    "    encode_metadata_dir / \"EpiAtlas_list-curie_term_HSOI.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"code\", \"term\", CELL_TYPE],\n",
    ")\n",
    "encode_ontology_df = pd.read_csv(encode_metadata_dir / \"encode_ontol+assay.tsv\", sep=\"\\t\")\n",
    "partial_meta = encode_ontology_df.merge(\n",
    "    curie_def_df, left_on=\"Biosample term id\", right_on=\"code\", how=\"left\"\n",
    ")\n",
    "partial_meta.drop(columns=[\"code\", \"term\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CELL_TYPE not in full_metadata_df.columns:\n",
    "    full_metadata_df = full_metadata_df.merge(\n",
    "        partial_meta,\n",
    "        left_on=\"filename\",\n",
    "        right_on=\"ENC_ID\",\n",
    "        suffixes=(\"_DROP\", \"\"),\n",
    "        how=\"left\",\n",
    "    )\n",
    "    for col in full_metadata_df.columns:\n",
    "        if col.endswith(\"_DROP\"):\n",
    "            full_metadata_df.drop(columns=col, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_predictions_dir = base_data_dir / \"training_results\" / \"predictions\" / \"encode\"\n",
    "pred_dfs = {}\n",
    "for folder in encode_predictions_dir.glob(\"*1l_3000n\"):\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "    # match categories with dir names of format: [cat_name]_1l_3000n\n",
    "    cat = folder.name.split(\"_1l_3000n\")[0]\n",
    "    pred_file = list(folder.rglob(\"complete_no_valid_oversample_*.csv\"))[0]\n",
    "    encode_df = pd.read_csv(pred_file)\n",
    "    pred_dfs[cat] = encode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_all_encode_preds(\n",
    "    pred_dfs: Dict[str, pd.DataFrame], full_metadata_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Merge all ENCODE predictions into a single DataFrame.\"\"\"\n",
    "    # define correct column names for prediction tasks\n",
    "    metadata_mapping = {\n",
    "        ASSAY: ASSAY,\n",
    "        CELL_TYPE: CELL_TYPE,\n",
    "        CANCER: \"cancer_status\",\n",
    "        SEX: \"donor_sex\",\n",
    "        LIFE_STAGE: \"life_stage\",\n",
    "    }\n",
    "\n",
    "    same_col_len = 8\n",
    "    # Make all different columns have unique relevant names except for the pred vector\n",
    "    new_dfs = {}\n",
    "    for cat, df in pred_dfs.items():\n",
    "        metadata_colname = metadata_mapping[cat]\n",
    "        df = df.copy()\n",
    "        df = df.drop(columns=[\"Same?\"])\n",
    "        df = df.merge(\n",
    "            full_metadata_df[[\"filename\", metadata_colname]],\n",
    "            left_on=\"md5sum\",\n",
    "            right_on=\"filename\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        df[\"True class\"] = df[metadata_mapping[cat]]\n",
    "        df = df.rename(columns={\"True class\": \"Expected class\"})\n",
    "        df = df.drop(columns=[\"filename\", metadata_colname])\n",
    "\n",
    "        old_names = df.columns[1 : same_col_len - 1]\n",
    "        new_names = [f\"{old_name} ({cat})\" for old_name in old_names]\n",
    "        df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "        new_dfs[cat] = df\n",
    "\n",
    "    df_order = [ASSAY, CELL_TYPE, SEX, LIFE_STAGE, CANCER]\n",
    "    df_list = [new_dfs[cat] for cat in df_order]\n",
    "    full_merged_df = functools.reduce(merge_dataframes, df_list)\n",
    "    full_merged_df.reset_index(drop=True, inplace=True)\n",
    "    if \"md5sum\" in full_merged_df.columns and \"ENC\" in full_merged_df.loc[0, \"md5sum\"]:\n",
    "        full_merged_df.rename(columns={\"md5sum\": \"filename\"}, inplace=True)\n",
    "\n",
    "    full_merged_df = full_merged_df.merge(\n",
    "        full_metadata_df,\n",
    "        left_on=\"filename\",\n",
    "        right_on=\"filename\",\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"\", \"_DROP\"),\n",
    "    )\n",
    "    for col in full_merged_df.columns:\n",
    "        if col.endswith(\"_DROP\"):\n",
    "            full_merged_df.drop(columns=col, inplace=True)\n",
    "    return full_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_encode_preds_df = merged_all_encode_preds(pred_dfs, full_metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = table_dir / \"dfreeze_v2\" / \"encode\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "merged_all_encode_preds_df.to_csv(\n",
    "    output_dir / \"encode_predictions_5task.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChIP-Atlas predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChIP-Seq_imputed_with_RNA-Seq_only predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = table_dir / \"dfreeze_v2\" / \"epiatlas_imputed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are from epiclass_11c complete training (with oversampling) MLP classifer  \n",
    "Training details at 0f8e5eb996114868a17057bebe64f87c (comet-ml id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_folder = base_data_dir / \"training_results\" / \"predictions\" / \"epiatlas_imputed\"\n",
    "pred_file = \"complete_no_valid_oversample_test_prediction_100kb_all_none_ChIP-Seq_imputed_with_RNA-Seq_only.csv\"\n",
    "pred_df = pd.read_csv(pred_folder / pred_file)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.rename(columns={\"Unnamed: 0\": \"filename\"}, inplace=True)\n",
    "\n",
    "# filename of format 'impute_[ihec-id]_[expected-class]_[resolution]_[filter_in]_[filter_out].csv'\n",
    "pred_df[\"True class\"] = pred_df[\"filename\"].str.split(\"_\", expand=True)[2].str.lower()\n",
    "pred_df.rename(columns={\"True class\": \"Expected class\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_pred_col = np.where(pred_df.columns == \"Predicted class\")[0][0]\n",
    "pred_df.insert(\n",
    "    loc=int(idx_pred_col + 1),\n",
    "    column=\"Same?\",\n",
    "    value=pred_df[\"Expected class\"] == pred_df[\"Predicted class\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {pred_df['Same?'].sum() / pred_df.shape[0]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_pred_vector_cols = 4\n",
    "nb_classes = 11\n",
    "pred_df.insert(\n",
    "    loc=non_pred_vector_cols,\n",
    "    column=\"Max pred\",\n",
    "    value=pred_df.iloc[:, non_pred_vector_cols : non_pred_vector_cols + nb_classes].max(\n",
    "        axis=1\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(output_dir / \"epiatlas_imputed_w_rna_only_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recount3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "recount3_folder = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"predictions\"\n",
    "    / \"recount3\"\n",
    "    / \"hg38_100kb_all_none\"\n",
    ")\n",
    "if not recount3_folder.exists():\n",
    "    raise FileNotFoundError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_pred_files = {}\n",
    "for cat in [ASSAY, SEX, LIFE_STAGE, CANCER]:\n",
    "    pred_files = list(recount3_folder.rglob(f\"{cat}*/**/recount3/complete_*.csv\"))\n",
    "    split_pred_files[cat] = pred_files\n",
    "\n",
    "assert len(split_pred_files) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs = {}\n",
    "for cat, pred_files in split_pred_files.items():\n",
    "    dfs = []\n",
    "    for pred_file in pred_files:\n",
    "        df = pd.read_csv(pred_file, low_memory=False)\n",
    "        dfs.append(df)\n",
    "    concat_df = pd.concat(dfs, ignore_index=True)\n",
    "    pred_dfs[cat] = concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat, pred_df in list(pred_dfs.items()):\n",
    "    try:\n",
    "        pred_df = pred_df.drop(\"True class\", axis=1)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    pred_df = pred_df.rename(columns={\"Unnamed: 0\": \"filename\"})\n",
    "\n",
    "    # Add max pred + move it to front\n",
    "    pred_df = split_results_handler.add_max_pred(pred_df, target_label=\"Predicted class\")\n",
    "    pred_df.insert(2, \"Max pred\", pred_df.pop(\"Max pred\"))\n",
    "    pred_df = pred_df[pred_df[\"Max pred\"] >= 0]\n",
    "\n",
    "    # Get id columns\n",
    "    id_cols = (\n",
    "        pred_df[\"filename\"].str.split(\".\", expand=True)[2].str.split(\"_\", expand=True)\n",
    "    )\n",
    "\n",
    "    pred_df.insert(1, \"id1\", id_cols.loc[:, 0])\n",
    "    pred_df.insert(2, \"id2\", id_cols.loc[:, 1])\n",
    "\n",
    "    pred_dfs[cat] = pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(pred_dfs[ASSAY][\"filename\"].str.split(\".\", expand=True)[2].str.split(\"_\",expand=True).head())\n",
    "# display(pred_dfs[ASSAY][\"id1\"].nunique(), pred_dfs[ASSAY][\"id2\"].nunique())\n",
    "# display(pred_dfs[ASSAY][\"id2\"].str.slice(0,3).value_counts())\n",
    "assert pred_dfs[ASSAY][\"id2\"].nunique() == pred_dfs[ASSAY].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_name = \"harmonized_metadata_20250110\"\n",
    "metadata_file = metadata_dir / f\"recount_{meta_name}.tsv\"\n",
    "recount_metadata_df = pd.read_csv(metadata_file, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "recount_metadata_df.rename(\n",
    "    mapper={\n",
    "        \"harmonized_assay\": ASSAY,\n",
    "        \"harmonized_lifestage\": LIFE_STAGE,\n",
    "        \"harmonized_sex\": SEX,\n",
    "        \"harmonized_cancer\": CANCER,\n",
    "    },\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "recount_metadata_df.fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_recount3_preds(\n",
    "    pred_dfs: Dict[str, pd.DataFrame], full_metadata_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Merge all recount3 predictions into a single DataFrame.\"\"\"\n",
    "    same_col_len = 5\n",
    "    # Make all different columns have unique relevant names except for the pred vector\n",
    "    new_dfs = {}\n",
    "    for cat, df in pred_dfs.items():\n",
    "        df = df.copy()\n",
    "        df[\"ID\"] = df[\"id2\"]\n",
    "        df = df.drop([\"id1\", \"id2\"], axis=1)\n",
    "        try:\n",
    "            df = df.drop(columns=[\"Same?\"])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        df = df.merge(\n",
    "            full_metadata_df[[\"ID\", cat]],\n",
    "            left_on=\"ID\",\n",
    "            right_on=\"ID\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        df.insert(1, \"Expected class\", df[cat])\n",
    "        df = df.drop(columns=[cat])\n",
    "\n",
    "        old_names = df.columns[1 : same_col_len - 1]\n",
    "        new_names = [f\"{old_name} ({cat})\" for old_name in old_names]\n",
    "        df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "\n",
    "        new_dfs[cat] = df\n",
    "\n",
    "    df_order = [ASSAY, SEX, CANCER, LIFE_STAGE]\n",
    "    df_list = [new_dfs[cat] for cat in df_order]\n",
    "\n",
    "    merge_dataframes_func = functools.partial(merge_dataframes, on=\"external_id\")\n",
    "    full_merged_df = functools.reduce(merge_dataframes_func, df_list)\n",
    "    full_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    full_merged_df = full_merged_df.merge(\n",
    "        full_metadata_df,\n",
    "        on=\"ID\",\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"\", \"_DROP\"),\n",
    "    )\n",
    "    for col in full_merged_df.columns:\n",
    "        if col.endswith(\"_DROP\"):\n",
    "            full_merged_df.drop(columns=col, inplace=True)\n",
    "    return full_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = merge_all_recount3_preds(pred_dfs, recount_metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.insert(1, \"ID\", final_df.pop(\"ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = recount3_folder / f\"recount3_merged_preds_{meta_name}.tsv.gz\"\n",
    "final_df.to_csv(out_path, sep=\"\\t\", index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = recount3_folder / f\"recount3_merged_preds_{meta_name}.tsv.gz\"\n",
    "full_df = pd.read_csv(preds_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = full_df.shape[0]\n",
    "for max_pred in [0, 0.6, 0.8]:\n",
    "    subset = full_df[full_df[f\"Max pred ({ASSAY})\"] >= max_pred]\n",
    "    counts = subset[f\"Predicted class ({ASSAY})\"].value_counts()\n",
    "\n",
    "    N_subset = counts.sum()\n",
    "    counts_perc = counts / N_subset\n",
    "    correct_perc = counts_perc[\"rna_seq\"] + counts_perc[\"mrna_seq\"]\n",
    "    print(f\"min_PredScore >= {max_pred} ({N_subset/N:.2%}% left): {correct_perc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for max_pred in [0, 0.6, 0.8]:\n",
    "    subset = full_df[full_df[f\"Max pred ({ASSAY})\"] >= max_pred]\n",
    "    print(f\"min_PredScore >= {max_pred}\")\n",
    "\n",
    "    for cat in [SEX, CANCER, LIFE_STAGE]:\n",
    "        pred_label = f\"Predicted class ({cat})\"\n",
    "        true_label = f\"Expected class ({cat})\"\n",
    "\n",
    "        if cat == CANCER:\n",
    "            subset = subset.replace(\"healthy\", \"non-cancer\")\n",
    "\n",
    "        known_pred = subset[subset[true_label] != \"unknown\"]\n",
    "        if cat == LIFE_STAGE:\n",
    "            known_pred = known_pred[known_pred[true_label] != \"children\"]\n",
    "        # print(known_pred[true_label].value_counts(dropna=False))\n",
    "\n",
    "        classes = sorted(\n",
    "            set(known_pred[pred_label].unique()) | set(known_pred[pred_label].unique())\n",
    "        )\n",
    "\n",
    "        N_known = known_pred.shape[0]\n",
    "        N_unknown = subset.shape[0] - N_known\n",
    "        # print(f\"Unknown (%): {(N_unknown)/subset.shape[0]*100:.2f}\")\n",
    "\n",
    "        y_pred = known_pred[pred_label]\n",
    "        y_true = known_pred[true_label]\n",
    "        N_correct = (y_pred == y_true).sum()\n",
    "        print(f\"{cat} prediction match (%): {N_correct/N_known*100:.2f}\")\n",
    "\n",
    "        print(classification_report(y_true, y_pred, target_names=classes) + \"\\n\")  # type: ignore\n",
    "\n",
    "        print(f\"confusion matrix classes row order: {classes}\")\n",
    "        cm = sk_cm(y_true, y_pred, normalize=\"true\", labels=classes)\n",
    "        print(str(cm) + \"\\n\")\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
