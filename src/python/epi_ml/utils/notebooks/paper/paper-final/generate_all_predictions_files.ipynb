{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create supplementary prediction files destined for the paper.\n",
    "\n",
    "Includes most data predictions used to create paper figures.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, too-many-branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import functools\n",
    "import gc\n",
    "import json\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from epi_ml.utils.classification_merging_utils import merge_dataframes\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assays = ASSAY_ORDER[0:7] + [\"rna_seq\", \"wgbs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISEASE = \"harmonized_sample_disease_high\"\n",
    "CANCER = \"harmonized_sample_cancer_high\"\n",
    "BIOMAT = \"harmonized_biomaterial_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "base_metadata_dir = base_data_dir / \"metadata\"\n",
    "table_dir = base_dir / \"tables\"\n",
    "\n",
    "paper_dir = base_dir\n",
    "\n",
    "predictions_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"predictions\"\n",
    "full_preds_table_dir = table_dir / \"dfreeze_v2\" / \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()\n",
    "metadata_handler = MetadataHandler(paper_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df_for_save(\n",
    "    df: pd.DataFrame, md5sum_to_epirr: Dict[str, str], md5sum_to_uuid: Dict[str, str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Prepare predictions DataFrame for saving to CSV. Return a modified DataFrame, with:\n",
    "    - Index set to md5sum\n",
    "    - Expected class as the first column\n",
    "    - epirr_without_version column added\n",
    "    - uuid column added\n",
    "    - Sorted by epirr_without_version and uuid\n",
    "    \"\"\"\n",
    "    df = df.copy(deep=True)\n",
    "    df.insert(0, \"Expected class\", df.pop(\"True class\"))\n",
    "    df.set_index(\"md5sum\", inplace=True)\n",
    "\n",
    "    df[\"epirr_without_version\"] = df.index.map(md5sum_to_epirr)\n",
    "    df[\"uuid\"] = df.index.map(md5sum_to_uuid)\n",
    "    df.sort_values(by=[\"epirr_without_version\", \"uuid\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Official metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = metadata_handler.load_metadata_df(version=\"v2\", merge_assays=False)\n",
    "md5sum_to_epirr = meta_df[\"epirr_id_without_version\"].to_dict()\n",
    "md5sum_to_uuid = meta_df[\"uuid\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_merged_assays = metadata_handler.load_metadata_df(version=\"v2\", merge_assays=True)\n",
    "md5sum_to_assay = meta_df_merged_assays[ASSAY].rename(ASSAY_MERGE_DICT).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_metadata_dir = base_metadata_dir / \"epiatlas\" / \"official\"\n",
    "\n",
    "metadata_v1_1_path = (\n",
    "    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.1.extended.csv\"\n",
    ")\n",
    "metadata_v1_1 = pd.read_csv(metadata_v1_1_path, index_col=False)\n",
    "metadata_v1_1.set_index(\"epirr_id_without_version\", inplace=True)\n",
    "\n",
    "metadata_post_correction_path = (\n",
    "    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.4.extended.csv\"\n",
    ")\n",
    "metadata_post_correction = pd.read_csv(metadata_post_correction_path, index_col=False)\n",
    "metadata_post_correction.set_index(\"epirr_id_without_version\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important fct: metrics per assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_per_assay(\n",
    "    dataframe: pd.DataFrame,\n",
    "    md5sum_to_assay: Dict[str, str],\n",
    "    is_assay: bool = False,\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute metrics for all assays for each split.\n",
    "\n",
    "    Expects a dataframe with the following columns [optional]:\n",
    "    index=md5sum, True class, Predicted class, pred vector columns, split, [uuid], [epirr]\n",
    "\n",
    "    Expecting md5sum_to_assay dict to contain md5sum -> assay, with assay being one of 9 (wgbs/rna merged)\n",
    "\n",
    "    Is made to work well with the output of prepare_df_for_save\n",
    "\n",
    "    Returns a dataframe with the following columns:\n",
    "    split, assay, metrics x5\n",
    "    \"\"\"\n",
    "\n",
    "    df = dataframe.copy()\n",
    "    for col in [\"uuid\", \"epirr\", \"epirr_without_version\"]:\n",
    "        try:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    # For metrics fct\n",
    "    if \"Expected class\" in df.columns:\n",
    "        df.rename({\"Expected class\": \"True class\"}, axis=1, inplace=True)\n",
    "\n",
    "    # Columns to keep for metric calculation\n",
    "    desired_columns = list(df.columns)\n",
    "    desired_columns.remove(\"split\")\n",
    "\n",
    "    df[ASSAY] = df.index.map(md5sum_to_assay)\n",
    "    if np.nan in df[ASSAY].unique():\n",
    "        raise ValueError(\"NaN found in ASSAY column\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    results = defaultdict(dict)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"All splits, all assays\")\n",
    "\n",
    "    metrics = split_results_handler.calculate_metrics_for_single_df(\n",
    "        df=df[desired_columns],\n",
    "        logging_name=\"all_splits-all_assays\",\n",
    "    )\n",
    "\n",
    "    results[(\"all\", \"all\")] = metrics\n",
    "    if verbose:\n",
    "        print(metrics)\n",
    "\n",
    "    # All folds, split per assay\n",
    "    for assay, assay_df in df.groupby(by=ASSAY):\n",
    "        if verbose:\n",
    "            print(f\"All splits, {assay}\")\n",
    "\n",
    "        metrics = split_results_handler.calculate_metrics_for_single_df(\n",
    "            df=assay_df[desired_columns],\n",
    "            logging_name=f\"all_splits-{assay}\",\n",
    "        )\n",
    "        if is_assay:\n",
    "            metrics[\"F1_macro\"] = np.nan\n",
    "\n",
    "        results[(\"all\", assay)] = metrics\n",
    "        if verbose:\n",
    "            print(metrics)\n",
    "\n",
    "    # Split per fold\n",
    "    for split_idx, split_df in df.groupby(by=\"split\"):\n",
    "        if verbose:\n",
    "            print(f\"Split {split_idx}, all assays\")\n",
    "\n",
    "        metrics = split_results_handler.calculate_metrics_for_single_df(\n",
    "            df=split_df[desired_columns],\n",
    "            logging_name=f\"split{split_idx}-all_assays\",\n",
    "        )\n",
    "        results[(split_idx, \"all\")] = metrics\n",
    "        if verbose:\n",
    "            print(metrics)\n",
    "\n",
    "        # Split per fold, per assay\n",
    "        for assay_label, assay_df in split_df.groupby(by=ASSAY):\n",
    "            if verbose:\n",
    "                print(f\"Split {split_idx}, {assay_label}\")\n",
    "\n",
    "            metrics = split_results_handler.calculate_metrics_for_single_df(\n",
    "                df=assay_df[desired_columns],\n",
    "                logging_name=f\"split{split_idx}-{assay_label}\",\n",
    "            )\n",
    "\n",
    "            if is_assay:\n",
    "                metrics[\"F1_macro\"] = np.nan\n",
    "\n",
    "            results[(split_idx, assay_label)] = metrics\n",
    "            if verbose:\n",
    "                print(metrics)\n",
    "\n",
    "    values_as_rows = []\n",
    "    for key, values in results.items():\n",
    "        split_idx, assay_label = key\n",
    "        row = [split_idx, assay_label]\n",
    "        try:\n",
    "            row.extend(\n",
    "                [\n",
    "                    values[\"Accuracy\"],\n",
    "                    values[\"F1_macro\"],\n",
    "                    values[\"AUC_micro\"],\n",
    "                    values[\"AUC_macro\"],\n",
    "                    values[\"count\"],\n",
    "                ]\n",
    "            )\n",
    "        except KeyError as err:\n",
    "            raise ValueError(f\"Missing key {key} in metrics: {values}\") from err\n",
    "        values_as_rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        values_as_rows,\n",
    "        columns=[\n",
    "            \"split\",\n",
    "            \"assay\",\n",
    "            \"Accuracy\",\n",
    "            \"F1_macro\",\n",
    "            \"AUC_micro\",\n",
    "            \"AUC_macro\",\n",
    "            \"count\",\n",
    "        ],\n",
    "    )\n",
    "    df.sort_values([\"split\", \"assay\"], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EpiATLAS training metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = table_dir / \"datasets_composition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_meta = (\n",
    "    base_metadata_dir\n",
    "    / \"epiatlas\"\n",
    "    / \"hg38_2023-epiatlas-dfreeze-pospurge-nodup_filterCtl.json\"\n",
    ")\n",
    "with open(path_meta, \"r\", encoding=\"utf8\") as f:\n",
    "    records = json.load(f)[\"datasets\"]\n",
    "\n",
    "epiatlas_meta_df = pd.DataFrame(records)\n",
    "del records\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epiatlas_meta_df[CANCER] = epiatlas_meta_df[DISEASE].map(\n",
    "    {\"Disease\": \"non-cancer\", \"Healthy/None\": \"non-cancer\", \"Cancer\": \"cancer\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check, one epirr = one biospecimen (cell type)\n",
    "for epirr, cell_type in epiatlas_meta_df.groupby(\"epirr_id\")[CELL_TYPE].unique().items():\n",
    "    if len(cell_type) != 1:\n",
    "        raise ValueError(f\"Dataset with multiple cell types ({epirr}): {cell_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biospecimens = epiatlas_meta_df.fillna(\"unknown\").copy(deep=True)\n",
    "\n",
    "df_biospecimens = df_biospecimens.groupby(CELL_TYPE, dropna=False).agg(\n",
    "    {\n",
    "        \"epirr_id\": \"nunique\",\n",
    "        \"uuid\": \"nunique\",\n",
    "        \"md5sum\": \"nunique\",\n",
    "    }\n",
    ")\n",
    "\n",
    "df_biospecimens.rename(\n",
    "    columns={\n",
    "        \"epirr_id\": \"Biospecimen count\",\n",
    "        \"uuid\": \"uuid/experiment count\",\n",
    "        \"md5sum\": \"File Count\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "df_biospecimens = df_biospecimens.sort_values(\"Biospecimen count\", ascending=False)\n",
    "\n",
    "df_biospecimens.to_csv(output_dir / \"EpiATLAS_biospecimens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check, one uuid = one experiment\n",
    "\n",
    "# Group by UUID and get unique assays per UUID\n",
    "groupby_uuid = epiatlas_meta_df.groupby(\"uuid\")[ASSAY].unique()\n",
    "\n",
    "for uuid, assay in groupby_uuid.items():\n",
    "    if len(assay) != 1:\n",
    "        print(f\"uuid with multiple assays ({uuid}): {assay}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one uuid `!=` one experiment --> Input files need to be handled separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each assay\n",
    "experiment_counter = groupby_uuid.explode().value_counts()\n",
    "\n",
    "# Detailled composition\n",
    "N_exp = experiment_counter.sum()\n",
    "N_uuid = len(groupby_uuid)\n",
    "N_input_alone = N_uuid + experiment_counter[\"input\"] - N_exp\n",
    "\n",
    "print(f\"Total experiments: {N_exp}\")\n",
    "print(f\"Total uuids (can include assay+input): {N_uuid}\")\n",
    "print(f\"Total input: {experiment_counter.get('input', 0)}\")\n",
    "print(f\"Total input alone: {N_input_alone}\")\n",
    "print(\n",
    "    f\"#exp - #uuid = #input - #input alone: {N_exp}-{N_uuid} = {experiment_counter.get('input', 0)}-{N_input_alone} = {N_exp - N_uuid}\"\n",
    ")\n",
    "\n",
    "assert (N_exp - N_uuid) == (experiment_counter.get(\"input\", 0) - N_input_alone)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_exp = experiment_counter.rename_axis(\"Experiment Assay\").reset_index(\n",
    "    name=\"Experiment Count\"\n",
    ")\n",
    "df_exp = df_exp.sort_values(\"Experiment Count\", ascending=False).set_index(\n",
    "    \"Experiment Assay\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of files per assay\n",
    "unique_files = (\n",
    "    epiatlas_meta_df.groupby(ASSAY)[\"md5sum\"]\n",
    "    .count()\n",
    "    .rename(\"File Count\")\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# Merge file counts with experiment counts\n",
    "df_exp = df_exp.merge(\n",
    "    unique_files, left_on=\"Experiment Assay\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# Compute the track type average count\n",
    "df_exp[\"Track type average count\"] = df_exp[\"File Count\"] / df_exp[\"Experiment Count\"]\n",
    "\n",
    "df_exp.to_csv(output_dir / \"EpiATLAS_assays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_types = (\n",
    "    epiatlas_meta_df.groupby(\"track_type\")[\"md5sum\"]\n",
    "    .count()\n",
    "    .rename(\"File Count\")\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "track_types.to_csv(output_dir / \"EpiATLAS_track_types.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del groupby_uuid, df_biospecimens, experiment_counter, df_exp, unique_files, track_types\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect experiment keys for all trained classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_experiment_info(line: str) -> Tuple[str, str] | None:\n",
    "    \"\"\"Extract split and experiment key from a line containing checkpoint information.\n",
    "\n",
    "    Line should have format: .../splitX/EpiLaP/[exp_key]/checkpoints/...\n",
    "    \"\"\"\n",
    "    if \"EpiLaP\" not in line:\n",
    "        return None\n",
    "\n",
    "    parts = line.strip().split(\"/\")\n",
    "    for i, part in enumerate(parts):\n",
    "        if part == \"EpiLaP\" and i > 0:\n",
    "            return (parts[i - 1], parts[i + 1])\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_log_file(file_path: Path) -> Set[Tuple[str, str]]:\n",
    "    \"\"\"Process a single log file and extract experiment information.\"\"\"\n",
    "    experiment_info = set()\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                if result := extract_experiment_info(line):\n",
    "                    experiment_info.add(result)\n",
    "    except Exception as e:  # pylint: disable=broad-exception-caught\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    return experiment_info\n",
    "\n",
    "\n",
    "def collect_exp_keys(folder: Path) -> Dict[str, Set[Tuple[str, str]]]:\n",
    "    \"\"\"Collect experiment keys from log files (.o files), recursively from a given folder.\"\"\"\n",
    "    experiments_keys = defaultdict(set)\n",
    "\n",
    "    log_files = folder.glob(\"*1l_3000n/**/*.o\")\n",
    "    for file in log_files:\n",
    "        if experiment_info := process_log_file(file):\n",
    "            experiments_keys[file.parent].update(experiment_info)\n",
    "\n",
    "    return experiments_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_exp_key_context(\n",
    "    experiments_keys_dict: Dict[str, Set[Tuple[str, str]]],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Format experiment keys context for saving to a DataFrame.\"\"\"\n",
    "    data = []\n",
    "    for exp_folder, exp_keys in experiments_keys_dict.items():\n",
    "        for split, exp_key in exp_keys:\n",
    "            data.append((str(exp_folder), split, exp_key))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"exp_folder\", \"split\", \"exp_key\"])\n",
    "\n",
    "    # comet-ml experiment url\n",
    "    df[\"comet-url\"] = \"https://www.comet.com/rabyj/epiclass/\" + df[\"exp_key\"].astype(str)\n",
    "\n",
    "    # Remove useless part of paths\n",
    "    to_remove_path = (\n",
    "        str(Path.home() / \"Projects/epiclass/output/paper/data/training_results\") + \"/\"\n",
    "    )\n",
    "    df[\"complete_experiment_context\"] = df[\"exp_folder\"].str.replace(to_remove_path, \"\")\n",
    "    df.drop(columns=\"exp_folder\", inplace=True)\n",
    "\n",
    "    # Split path into named parts\n",
    "    df[[\"release\", \"feature_set_name\", \"metadata_category\"]] = (\n",
    "        df[\"complete_experiment_context\"].str.split(\"/\", expand=True).loc[:, [0, 1, 2]]\n",
    "    )\n",
    "\n",
    "    df[\"experiment_specification\"] = (\n",
    "        df[\"complete_experiment_context\"]\n",
    "        .str.split(\"/\", n=3, expand=True)[3]\n",
    "        .str.replace(\"/\", \",\")\n",
    "    )\n",
    "\n",
    "    # Remove redundant info (all MLP exp are 1 hidden layer 3000 nodes)\n",
    "    df[\"metadata_category\"] = df[\"metadata_category\"].str.replace(\"_1l_3000n\", \"\")\n",
    "\n",
    "    # Reorder columns\n",
    "    df_new_col_order = df.columns.to_list()[-4:] + df.columns.to_list()[:-4]\n",
    "    df = df[df_new_col_order]\n",
    "\n",
    "    return df  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exp_keys_dfs = []\n",
    "for folder in [\"dfreeze_v2\", \"2023-01-epiatlas-freeze\", \"imputation\"]:\n",
    "    data_dir = base_data_dir / \"training_results\" / folder\n",
    "    for subfolder in data_dir.glob(\"*\"):\n",
    "        if subfolder.is_file():\n",
    "            continue\n",
    "        # print(subfolder)\n",
    "        exp_key_dict = collect_exp_keys(subfolder)\n",
    "        df = format_exp_key_context(exp_key_dict)\n",
    "        all_exp_keys_dfs.append(df)\n",
    "\n",
    "exp_keys_df = pd.concat(all_exp_keys_dfs, ignore_index=True)\n",
    "\n",
    "exp_keys_df.to_csv(table_dir / \"training_experiment_keys.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `collect_all.ipynb` for download of training results from comet-ml.  \n",
    "Next, merging some comet-ml info with the chosen experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_info_path = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"all_results_cometml_filtered_oversampling-fixed.csv\"\n",
    ")\n",
    "exp_info = pd.read_csv(exp_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_info = pd.merge(\n",
    "    exp_keys_df,\n",
    "    exp_info,\n",
    "    how=\"left\",\n",
    "    left_on=\"exp_key\",\n",
    "    right_on=\"Experience key\",\n",
    "    suffixes=(\"\", \"_DROP\"),\n",
    ")\n",
    "for col in exp_info.columns:\n",
    "    if col.endswith(\"_DROP\"):\n",
    "        exp_info.drop(columns=col, inplace=True)\n",
    "\n",
    "exp_info = exp_info.dropna(axis=0, how=\"all\")\n",
    "exp_info = exp_info.dropna(axis=1, how=\"all\")\n",
    "\n",
    "exp_info = exp_info.sort_values([\"SLURM_JOB_ID\", \"split\"])\n",
    "\n",
    "exp_info.to_csv(table_dir / \"detailled_training_info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assay_epiclass + sample ontology for all 5 model types - 100kb_all_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_100kb = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "logdir = table_dir / \"dfreeze_v2\" / \"100kb_all_none\"\n",
    "if not logdir.exists():\n",
    "    logdir.mkdir(parents=True)\n",
    "\n",
    "split_md5sums = []\n",
    "all_metrics = {}\n",
    "all_assay_metrics = {}\n",
    "for category in [ASSAY, CELL_TYPE]:\n",
    "    all_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "        results_dir=data_dir_100kb,\n",
    "        label_category=category,\n",
    "        only_NN=False,\n",
    "    )\n",
    "\n",
    "    # Sanity check, same shape, same input files for each method\n",
    "    for split_dict in all_split_dfs.values():\n",
    "        ref_dict = split_dict[\"NN\"]\n",
    "        ref_md5sums = sorted(ref_dict.index.values.tolist())\n",
    "        ref_shape = ref_dict.shape\n",
    "        for method, df in split_dict.items():\n",
    "            if not ref_md5sums == sorted(df.index.values.tolist()):\n",
    "                raise ValueError(\"MD5sums do not match\")\n",
    "            if ref_shape != df.shape:\n",
    "                raise ValueError(\"Shapes do not match\")\n",
    "\n",
    "    all_split_dfs_concat: Dict = split_results_handler.concatenate_split_results(all_split_dfs)  # type: ignore\n",
    "\n",
    "    # Save to file\n",
    "    for method, df in all_split_dfs_concat.items():\n",
    "        # continue\n",
    "        print(f\"Method: {method}. Category: {category}.\")\n",
    "        df = prepare_df_for_save(df, md5sum_to_epirr, md5sum_to_uuid)\n",
    "\n",
    "        if method == \"NN\":\n",
    "            method = \"MLP\"\n",
    "\n",
    "        is_assay = ASSAY in category\n",
    "        # print(f\"Saving {method} predictions for {category}. Assay: {is_assay}\")\n",
    "\n",
    "        assay_metrics = metrics_per_assay(\n",
    "            df, md5sum_to_assay, is_assay=is_assay, verbose=False\n",
    "        )\n",
    "        assay_metrics.insert(0, \"Metadata category\", category)\n",
    "        assay_metrics.insert(0, \"Classifier type\", method)\n",
    "        all_assay_metrics[(method, category)] = assay_metrics\n",
    "\n",
    "        filename = f\"10fold_predictions_{category}_{method}.csv\"\n",
    "        df.to_csv(logdir / filename, index=True, sep=\",\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving aggregated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_metrics_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Rename columns and values for more legibility.\"\"\"\n",
    "    # Rename columns\n",
    "    df = df.rename(\n",
    "        {\n",
    "            \"count\": \"Validation size\",\n",
    "            \"assay\": \"target/assay\",\n",
    "            \"split\": \"Fold idx\",\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Rename classifiers\n",
    "    df = df.replace(\n",
    "        {\n",
    "            \"Classifier type\": {\n",
    "                \"RF\": \"Random Forest\",\n",
    "                \"LGBM\": \"LightGBM\",\n",
    "                \"LR\": \"Logistic Regression\",\n",
    "                \"LinearSVC\": \"Linear SVM\",\n",
    "                \"NN\": \"Multilayer Perceptron\",\n",
    "                \"MLP\": \"Multilayer Perceptron\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to long format\n",
    "full_assay_metrics_df = pd.concat(\n",
    "    all_assay_metrics.values(), keys=all_assay_metrics.keys(), ignore_index=True\n",
    ")\n",
    "\n",
    "# Rename columns/values\n",
    "full_assay_metrics_df = rename_metrics_df(full_assay_metrics_df)\n",
    "full_assay_metrics_df.fillna(\"NA\", inplace=True)\n",
    "\n",
    "# Save\n",
    "full_assay_metrics_df.to_csv(\n",
    "    table_dir / \"dfreeze_v2\" / \"dfreeze_v2_100kb_all_none_5algo_metrics.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other MLP results - 100kb_all_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_100kb = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "logdir = table_dir / \"dfreeze_v2\" / \"100kb_all_none\"\n",
    "if not logdir.exists():\n",
    "    logdir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    ASSAY,\n",
    "    CELL_TYPE,\n",
    "    \"paired_end\",\n",
    "    \"harmonized_sample_cancer_high\",\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    \"harmonized_biomaterial_type\",\n",
    "    \"project\",\n",
    "]\n",
    "\n",
    "# Select 10-fold oversampling runs\n",
    "all_split_dfs = split_results_handler.general_split_metrics(\n",
    "    results_dir=data_dir_100kb,\n",
    "    merge_assays=False,\n",
    "    include_categories=categories,\n",
    "    exclude_names=[\"reg\", \"no-mixed\", \"chip\", \"16ct\", \"27ct\"],\n",
    "    return_type=\"split_results\",\n",
    "    oversampled_only=True,\n",
    "    verbose=False,\n",
    ")\n",
    "all_split_dfs_concat: Dict = split_results_handler.concatenate_split_results(all_split_dfs, concat_first_level=True)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for task names that don't exactly fit metadata categories\n",
    "cat_mapper = {\n",
    "    key: (SEX if \"sex\" in key else ASSAY if \"assay\" in key else key)\n",
    "    for key in all_split_dfs_concat\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "new_dfs = {}\n",
    "for category, df in all_split_dfs_concat.items():\n",
    "    new_df = prepare_df_for_save(df, md5sum_to_epirr, md5sum_to_uuid)\n",
    "\n",
    "    # we want to write down the expected class for both metadata versions,\n",
    "    # since there were changes in-between\n",
    "    if category in [LIFE_STAGE, SEX]:\n",
    "        for version, metadata in [\n",
    "            (\"v1.4\", metadata_post_correction),\n",
    "            (\"v1.1\", metadata_v1_1),\n",
    "        ]:\n",
    "            idx = new_df.index.map(md5sum_to_epirr).values\n",
    "            values = metadata.loc[idx, category].to_list()  # type: ignore\n",
    "            new_df.insert(loc=0, column=f\"Expected class {version}\", value=values)\n",
    "        new_df.drop(\"Expected class\", axis=1, inplace=True)\n",
    "\n",
    "    # using training metadata\n",
    "    else:\n",
    "        cat = cat_mapper[category]\n",
    "        values = epiatlas_meta_df[[\"md5sum\", cat]].set_index(\"md5sum\")\n",
    "        values = values.loc[new_df.index, cat]\n",
    "        new_df[\"Expected class\"] = values\n",
    "\n",
    "    new_dfs[category] = new_df\n",
    "\n",
    "    print(f\"Saving predictions for {category}\")\n",
    "    filename = f\"10fold_predictions_{category}_MLP.csv\"\n",
    "    new_df.to_csv(logdir / filename, index=True, sep=\",\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing metrics for all classification tasks. We compute metrics post-correction for sex and life stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assay_metrics = {}\n",
    "for category, df in new_dfs.items():\n",
    "    df = df.copy(deep=True)\n",
    "    if category in [LIFE_STAGE, SEX]:\n",
    "        df = df.drop(\"Expected class v1.1\", axis=1)\n",
    "        df.rename(columns={\"Expected class v1.4\": \"Expected class\"}, inplace=True)\n",
    "\n",
    "    is_assay = \"assay\" in category\n",
    "    assay_metrics = metrics_per_assay(df, md5sum_to_assay, is_assay)\n",
    "\n",
    "    assay_metrics.insert(0, \"Metadata category\", category)\n",
    "    assay_metrics.insert(0, \"Classifier type\", \"MLP\")\n",
    "\n",
    "    all_assay_metrics[category] = assay_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_assay_metrics_df = pd.concat(\n",
    "    all_assay_metrics.values(), keys=all_assay_metrics.keys(), ignore_index=True\n",
    ")\n",
    "\n",
    "full_assay_metrics_df = rename_metrics_df(full_assay_metrics_df)\n",
    "full_assay_metrics_df.fillna(\"NA\", inplace=True)\n",
    "\n",
    "full_assay_metrics_df.to_csv(\n",
    "    table_dir / \"dfreeze_v2\" / \"dfreeze_v2_100kb_all_none_all_MLP_metrics.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging all predictions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_col_len = 4\n",
    "\n",
    "final_dfs = {}\n",
    "for category, df in new_dfs.items():\n",
    "    new_df = df.copy(deep=True)\n",
    "    new_df.drop([\"uuid\", \"epirr_without_version\"], axis=1, inplace=True)\n",
    "    new_df.insert(0, \"split\", new_df.pop(\"split\"))\n",
    "\n",
    "    try:\n",
    "        new_df = new_df.reset_index()\n",
    "    except ValueError:\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "\n",
    "    # All dataframes need to have same shape for rest of code to work\n",
    "    if category in [LIFE_STAGE, SEX]:\n",
    "        new_df.drop(\"Expected class v1.1\", axis=1, inplace=True)\n",
    "        new_df.rename(columns={\"Expected class v1.4\": \"Expected class\"}, inplace=True)\n",
    "\n",
    "    new_df.insert(loc=0, column=\"md5sum\", value=new_df.pop(\"md5sum\"))\n",
    "\n",
    "    # this only works if all the columns to the right correspond to pred vector\n",
    "    pred_cols = new_df.columns[same_col_len:].tolist()\n",
    "    new_df.insert(\n",
    "        loc=same_col_len, column=\"Max pred\", value=new_df[pred_cols].max(axis=1)\n",
    "    )\n",
    "\n",
    "    old_names = new_df.columns[1 : same_col_len + 1]\n",
    "    new_names = [f\"{old_name} ({category})\" for old_name in old_names]\n",
    "    new_df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "\n",
    "    final_dfs[category] = new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For proper merging of full predictions values, assay classifiers need to have their column names modified, since they share output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = f\"{ASSAY}_11c\"\n",
    "assays = set(final_dfs[category][f\"Expected class ({category})\"].unique())\n",
    "\n",
    "for suffix in [\"_7c\", \"_11c\"]:\n",
    "    df = final_dfs[f\"{ASSAY}{suffix}\"]\n",
    "\n",
    "    renamer = {assay: f\"{assay}{suffix}\" for assay in assays}\n",
    "\n",
    "    df = df.rename(columns=renamer)\n",
    "    final_dfs[f\"{ASSAY}{suffix}\"] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge every df together, horizontally, with the md5sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = functools.reduce(merge_dataframes, final_dfs.values())\n",
    "\n",
    "final_df[\"epirr\"] = final_df.loc[:, \"md5sum\"].map(md5sum_to_epirr)  # type: ignore\n",
    "final_df[\"uuid\"] = final_df.loc[:, \"md5sum\"].map(md5sum_to_uuid)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\n",
    "    logdir / \"all_10fold_predictions_MLP.csv\",\n",
    "    index=False,\n",
    "    sep=\",\",\n",
    "    float_format=\"%.4f\",\n",
    "    na_rep=\"NA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_imbalance = []\n",
    "for category, df in all_split_dfs_concat.items():\n",
    "    for col in [\"Expected class\", \"True class\", \"Expected class v1.4\"]:\n",
    "        if col in df.columns:\n",
    "            class_counts = df[col].value_counts()\n",
    "            break\n",
    "\n",
    "    N = class_counts.sum()\n",
    "    for i, (class_label, count) in enumerate(sorted(class_counts.items())):\n",
    "        class_imbalance.append((category, i, class_label, count, count / N * 100))\n",
    "\n",
    "class_imbalance_df = pd.DataFrame(\n",
    "    class_imbalance,\n",
    "    columns=[\n",
    "        \"Classification task\",\n",
    "        \"Class index\",\n",
    "        \"Class label\",\n",
    "        \"Count\",\n",
    "        \"Fraction (%)\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "class_imbalance_df.to_csv(\n",
    "    table_dir / \"dfreeze_v2_100kb_all_none_MLP_class_imbalance.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for other feature sets (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_splits_identity(\n",
    "    all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]],\n",
    "    task_names: List[str],\n",
    "    verbose: bool | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Verify that the splits are identical between feature sets for each task.\n",
    "\n",
    "    all_results: {feature_set: {task_name: {split_name: results_dataframe}}}\n",
    "    task_names: list of task names to verify\n",
    "    verbose: print additional information\n",
    "    \"\"\"\n",
    "    # Sanity check : MD5sums and shapes should match between reference and other feature sets, for each split\n",
    "    for task_name in task_names:\n",
    "        if verbose:\n",
    "            print(f\"Verifying task '{task_name}'\")\n",
    "        # Select a reference feature set and use its splits as the baseline for comparison\n",
    "        reference_feature_set = \"hg38_100kb_all_none\"\n",
    "        reference_splits = all_results[reference_feature_set][task_name]\n",
    "\n",
    "        # Create reference MD5sums and shapes for each split in the reference feature set\n",
    "        reference_md5sums = {\n",
    "            split_name: sorted(df.index.tolist())\n",
    "            for split_name, df in reference_splits.items()\n",
    "        }\n",
    "        reference_shapes = {\n",
    "            split_name: df.shape for split_name, df in reference_splits.items()\n",
    "        }\n",
    "\n",
    "        # Iterate over each feature set and compare its splits against the reference\n",
    "        for feature_set_name, tasks_dict in all_results.items():\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Verifying feature set '{feature_set_name}' against reference feature set '{reference_feature_set}'\"\n",
    "                )\n",
    "            for split_name, df in tasks_dict[task_name].items():\n",
    "                if reference_shapes[split_name] != df.shape:\n",
    "                    print(\n",
    "                        f\"WARNING: Shape mismatch in task '{task_name}', split '{split_name}', \"\n",
    "                        f\"between reference feature set '{reference_feature_set}' and feature set '{feature_set_name}'\",\n",
    "                    )\n",
    "                if reference_md5sums[split_name] != sorted(df.index.tolist()):\n",
    "                    print(\n",
    "                        f\"WARNING: MD5sums mismatch in task '{task_name}', split '{split_name}', \"\n",
    "                        f\"between reference feature set '{reference_feature_set}' and feature set '{feature_set_name}'\",\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ASSAY, CELL_TYPE]\n",
    "include_sets = [\n",
    "    \"hg38_10mb_all_none_1mb_coord\",\n",
    "    \"hg38_100kb_random_n316_none\",\n",
    "    \"hg38_1mb_all_none\",\n",
    "    \"hg38_100kb_random_n3044_none\",\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_gene_regions_100kb_coord_n19864\",\n",
    "    \"hg38_10kb_random_n30321_none\",\n",
    "    \"hg38_regulatory_regions_n30321\",\n",
    "    \"hg38_1kb_random_n30321_none\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n",
    "    \"hg38_10kb_all_none\",\n",
    "    \"hg38_regulatory_regions_n303114\",\n",
    "    \"hg38_1kb_random_n303114_none\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n300k\",\n",
    "]\n",
    "exclude_names = [\"7c\", \"chip-seq-only\", \"27ct\", \"16ct\"]\n",
    "\n",
    "# Select 10-fold oversampling runs\n",
    "# expected result shape: {feature_set: {task_name: {split_name: results_dataframe}}}\n",
    "all_results: Dict[\n",
    "    str, Dict[str, Dict[str, pd.DataFrame]]\n",
    "] = split_results_handler.obtain_all_feature_set_data(\n",
    "    return_type=\"split_results\",\n",
    "    parent_folder=data_dir_100kb.parent,\n",
    "    merge_assays=False,\n",
    "    include_categories=categories,\n",
    "    include_sets=include_sets,\n",
    "    exclude_names=exclude_names,\n",
    "    verbose=False,\n",
    ")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace assay_11c with assay\n",
    "for feature_set in all_results.values():\n",
    "    try:\n",
    "        feature_set[ASSAY] = feature_set.pop(f\"{ASSAY}_11c\")\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_splits_identity(all_results, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = table_dir / \"dfreeze_v2\" / \"other_feature_sets\"\n",
    "logdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_assay_metrics = {}\n",
    "for feature_set_name, tasks_dict in all_results.items():\n",
    "    # continue\n",
    "\n",
    "    # if feature_set_name == \"hg38_100kb_all_none\":\n",
    "    #     continue\n",
    "    all_split_dfs_concat: Dict = split_results_handler.concatenate_split_results(\n",
    "        tasks_dict, concat_first_level=True\n",
    "    )  # type: ignore\n",
    "    for task_name, df in all_split_dfs_concat.items():\n",
    "        is_assay = \"assay\" in task_name\n",
    "        print(f\"Task: {task_name}. Feature set: {feature_set_name}. Assay: {is_assay}\")\n",
    "\n",
    "        df = prepare_df_for_save(df, md5sum_to_epirr, md5sum_to_uuid)\n",
    "\n",
    "        metrics = metrics_per_assay(df, md5sum_to_assay, is_assay)\n",
    "        metrics.insert(0, \"Classifier type\", \"MLP\")\n",
    "        metrics.insert(1, \"Metadata category\", task_name)\n",
    "        metrics.insert(2, \"Feature set\", feature_set_name)\n",
    "\n",
    "        all_assay_metrics[(feature_set_name, task_name)] = metrics\n",
    "\n",
    "        filename = f\"{feature_set_name}_10fold_predictions_{task_name}.csv\"\n",
    "        df.to_csv(logdir / filename, index=True, sep=\",\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_assay_metrics_df = pd.concat(\n",
    "    all_assay_metrics.values(), keys=all_assay_metrics.keys(), ignore_index=True\n",
    ")\n",
    "full_assay_metrics_df = rename_metrics_df(full_assay_metrics_df)\n",
    "full_assay_metrics_df.fillna(\"NA\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_assay_metrics_df.to_csv(\n",
    "    table_dir / \"dfreeze_v2\" / \"dfreeze_v2_100kb_all_none_all_feature_sets_metrics.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winsorized files and/or blacklist zeroed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winsorization was applied after blacklisted regions were put to zero, we did not test a winsorized only version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ASSAY, CELL_TYPE, SEX, \"harmonized_biomaterial_type\"]\n",
    "include_sets = [\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_100kb_all_none_0blklst\",\n",
    "    \"hg38_100kb_all_none_0blklst_winsorized\",\n",
    "]\n",
    "\n",
    "results_folder = base_data_dir / \"training_results\" / \"2023-01-epiatlas-freeze\"\n",
    "if not results_folder.exists():\n",
    "    raise FileNotFoundError(f\"Folder '{results_folder}' not found\")\n",
    "\n",
    "logdir = table_dir / \"2023-01-epiatlas-freeze\"\n",
    "if not logdir.exists():\n",
    "    logdir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10-fold oversampling runs\n",
    "# expected result shape: {feature_set: {task_name: {split_name: results_dataframe}}}\n",
    "all_results: Dict[\n",
    "    str, Dict[str, Dict[str, pd.DataFrame]]\n",
    "] = split_results_handler.obtain_all_feature_set_data(\n",
    "    return_type=\"split_results\",\n",
    "    parent_folder=results_folder,\n",
    "    merge_assays=False,\n",
    "    include_categories=categories,\n",
    "    include_sets=include_sets,\n",
    "    oversampled_only=False,\n",
    "    verbose=False,\n",
    ")  # type: ignore\n",
    "\n",
    "display(all_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_collected = list(all_results[\"hg38_100kb_all_none\"])\n",
    "verify_splits_identity(all_results, tasks_collected, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save concatenated result\n",
    "all_assay_metrics = {}\n",
    "for feature_set_name, tasks_dict in all_results.items():\n",
    "    # continue\n",
    "\n",
    "    concatenated_dfs = split_results_handler.concatenate_split_results(\n",
    "        tasks_dict, concat_first_level=True\n",
    "    )\n",
    "    for task_name, concatenated_df in concatenated_dfs.items():\n",
    "        concatenated_df = prepare_df_for_save(concatenated_df, md5sum_to_epirr, md5sum_to_uuid)  # type: ignore\n",
    "\n",
    "        is_assay = \"assay\" in task_name  # type: ignore\n",
    "        metrics = metrics_per_assay(concatenated_df, md5sum_to_assay, is_assay=is_assay)\n",
    "        metrics.insert(0, \"Classifier type\", \"MLP\")\n",
    "        metrics.insert(1, \"Metadata category\", task_name)  # type: ignore\n",
    "        metrics.insert(2, \"Input type\", feature_set_name)\n",
    "\n",
    "        all_assay_metrics[(feature_set_name, task_name)] = metrics\n",
    "\n",
    "        filename = f\"{feature_set_name}_10fold_predictions_{task_name}.csv\"\n",
    "        print(f\"Saving {filename}\")\n",
    "        concatenated_df.to_csv(\n",
    "            logdir / filename, index=True, sep=\",\", float_format=\"%.4f\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_assay_metrics_df = pd.concat(\n",
    "    all_assay_metrics.values(), keys=all_assay_metrics.keys(), ignore_index=True\n",
    ")\n",
    "full_assay_metrics_df = rename_metrics_df(full_assay_metrics_df)\n",
    "full_assay_metrics_df.fillna(\"NA\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_assay_metrics_df.to_csv(\n",
    "    table_dir\n",
    "    / \"2023-01-epiatlas-freeze\"\n",
    "    / \"2023-01-epiatlas-freeze_100kb_all_none_2variants_MLP_metrics.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate input dataset discrepancy in assay_epiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata_df = metadata_handler.load_metadata_df(\"v2\", merge_assays=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_counts = {}\n",
    "for feature_set_name, tasks_dict in all_results.items():\n",
    "    concatenated_dfs = split_results_handler.concatenate_split_results(\n",
    "        tasks_dict, concat_first_level=True\n",
    "    )\n",
    "    md5sums = concatenated_dfs[ASSAY].index.tolist()\n",
    "    print(f\"{feature_set_name}: {len(md5sums)}\")\n",
    "\n",
    "    metadata_subset = metadata_df[metadata_df.index.isin(md5sums)]\n",
    "    values_counts[feature_set_name] = metadata_subset[ASSAY].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    values_counts[\"hg38_100kb_all_none_0blklst\"] - values_counts[\"hg38_100kb_all_none\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODE predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See:\n",
    "- src/python/epi_ml/utils/notebooks/paper/encode_metadata_creation.ipynb\n",
    "- src/python/epi_ml/utils/notebooks/paper/encode_pred_analysis.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = table_dir / \"datasets_composition\"\n",
    "metadata_dir = base_metadata_dir / \"encode\"\n",
    "\n",
    "encode_preds_path = (\n",
    "    full_preds_table_dir / \"complete_encode_predictions_augmented_2025-02_metadata.csv.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_preds_df = pd.read_csv(\n",
    "    encode_preds_path, encoding=\"utf8\", low_memory=False, compression=\"gzip\"\n",
    ")\n",
    "print(encode_preds_df.head())\n",
    "print(encode_preds_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_id_cols = [\"EXPERIMENT_accession\", \"BIOSAMPLE_accession\", \"FILE_accession\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_files = encode_preds_df.shape[0]\n",
    "N_exp = encode_preds_df[\"EXPERIMENT_accession\"].nunique()\n",
    "N_biospecimen = encode_preds_df[\"BIOSAMPLE_accession\"].nunique()\n",
    "print(N_files, N_exp, N_biospecimen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assays_df = encode_preds_df.groupby(ASSAY, dropna=False).agg(\n",
    "    {\n",
    "        \"EXPERIMENT_accession\": \"nunique\",\n",
    "        \"FILE_accession\": \"nunique\",\n",
    "    }\n",
    ")\n",
    "for label in encode_id_cols:\n",
    "    try:\n",
    "        assays_df.rename(\n",
    "            columns={label: label.replace(\"_accession\", \" count\")}, inplace=True\n",
    "        )\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "\n",
    "assays_df[\"file_per_experiment\"] = assays_df[\"FILE count\"] / assays_df[\"EXPERIMENT count\"]\n",
    "\n",
    "assays_df.to_csv(output_dir / \"ENCODE_assays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biospecimens = encode_preds_df.fillna(\"unknown\").copy(deep=True)\n",
    "\n",
    "groupby = df_biospecimens.groupby(\n",
    "    [\"BIOSAMPLE_TYPE_name\", \"BIOSAMPLE_TYPE_term_name\"], dropna=False\n",
    ").agg(\n",
    "    {\n",
    "        \"BIOSAMPLE_accession\": \"nunique\",\n",
    "        \"EXPERIMENT_accession\": \"nunique\",\n",
    "        \"FILE_accession\": \"nunique\",\n",
    "    }\n",
    ")\n",
    "\n",
    "for label in encode_id_cols:\n",
    "    try:\n",
    "        groupby.rename(\n",
    "            columns={label: label.replace(\"_accession\", \" count\")}, inplace=True\n",
    "        )\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "groupby.to_csv(output_dir / \"ENCODE_biospecimens.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChIP-Atlas predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dir = base_metadata_dir / \"chip_atlas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using\n",
    "- downloaded metadata (see `src/python/epi_ml/utils/notebooks/paper/c-a_metadata.ipynb`)\n",
    "- 4DB target and ID metadata created by Gabriella\n",
    "- Extracted metadata by Frede\n",
    "- Pre-joined predictions from multiple classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = predictions_dir / \"C-A\" / \"ChIP-Atlas_merged_predictions_20240606.csv\"\n",
    "full_df = pd.read_csv(pred_path, sep=\",\", low_memory=False)\n",
    "display(full_df.head())\n",
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_meta_path = metadata_dir / \"CA.full_info_metadata.freeze1.tsv\"\n",
    "meta_df = pd.read_csv(extracted_meta_path, sep=\"\\t\", low_memory=False)\n",
    "display(meta_df.head())\n",
    "print(meta_df.shape)\n",
    "\n",
    "full_df = pd.merge(full_df, meta_df, how=\"left\", left_on=\"Experimental-id\", right_on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_meta_path = metadata_dir / \"CA_metadata_downloaded_20250306.tsv\"\n",
    "meta_df = pd.read_csv(downloaded_meta_path, sep=\"\\t\", low_memory=False)\n",
    "display(meta_df.head())\n",
    "print(meta_df.shape)\n",
    "\n",
    "full_df = pd.merge(full_df, meta_df, how=\"left\", on=\"Experimental-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_meta_path = metadata_dir / \"CA_minimal_4DB_metadata_20240606_mod.tsv\"\n",
    "meta_df = pd.read_csv(target_meta_path, sep=\"\\t\", low_memory=False)\n",
    "display(meta_df.head())\n",
    "print(meta_df.shape, \"\\n\")\n",
    "\n",
    "meta_df[\"in_epiatlas\"] = meta_df[\"is_EpiAtlas_EpiRR\"].astype(str) != \"0\"\n",
    "print(meta_df[\"in_epiatlas\"].value_counts(dropna=False))\n",
    "\n",
    "full_df = pd.merge(full_df, meta_df, how=\"left\", on=\"Experimental-id\")\n",
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.to_csv(\n",
    "    full_preds_table_dir\n",
    "    / \"ChIP-Atlas_predictions_20240606_merge_metadata_freeze1.csv.gz\",\n",
    "    compression=\"gzip\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChIP-Seq_imputed_with_RNA-Seq_only predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = table_dir / \"dfreeze_v2\" / \"epiatlas_imputed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are from epiclass_11c complete training (with oversampling) MLP classifer  \n",
    "Training details at 0f8e5eb996114868a17057bebe64f87c (comet-ml id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_folder = base_data_dir / \"training_results\" / \"predictions\" / \"epiatlas_imputed\"\n",
    "pred_file = \"complete_no_valid_oversample_test_prediction_100kb_all_none_ChIP-Seq_imputed_with_RNA-Seq_only.csv\"\n",
    "pred_df = pd.read_csv(pred_folder / pred_file)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.rename(columns={\"Unnamed: 0\": \"filename\"}, inplace=True)\n",
    "\n",
    "# filename of format 'impute_[ihec-id]_[expected-class]_[resolution]_[filter_in]_[filter_out].csv'\n",
    "pred_df[\"True class\"] = pred_df[\"filename\"].str.split(\"_\", expand=True)[2].str.lower()\n",
    "pred_df.rename(columns={\"True class\": \"Expected class\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_pred_col = np.where(pred_df.columns == \"Predicted class\")[0][0]\n",
    "pred_df.insert(\n",
    "    loc=int(idx_pred_col + 1),\n",
    "    column=\"Same?\",\n",
    "    value=pred_df[\"Expected class\"] == pred_df[\"Predicted class\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {pred_df['Same?'].sum() / pred_df.shape[0]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_pred_vector_cols = 4\n",
    "nb_classes = 11\n",
    "pred_df.insert(\n",
    "    loc=non_pred_vector_cols,\n",
    "    column=\"Max pred\",\n",
    "    value=pred_df.iloc[:, non_pred_vector_cols : non_pred_vector_cols + nb_classes].max(\n",
    "        axis=1  # type: ignore\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(output_dir / \"epiatlas_imputed_w_rna_only_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recount3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recount3_folder = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"predictions\"\n",
    "    / \"recount3\"\n",
    "    / \"hg38_100kb_all_none\"\n",
    ")\n",
    "if not recount3_folder.exists():\n",
    "    raise FileNotFoundError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recount3 predictions were initially done in chunk because there are too many files.  \n",
    "The below code handles pre-concatenated or still split, with a variable that needs to be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_concat = False\n",
    "nb_cat = 5\n",
    "categories = [ASSAY, SEX, LIFE_STAGE, CANCER, BIOMAT]\n",
    "\n",
    "if to_concat:\n",
    "    nb_split = 7\n",
    "\n",
    "    split_pred_files = {}\n",
    "    for cat in categories:\n",
    "        pred_files = list(\n",
    "            recount3_folder.rglob(f\"{cat}*/**/recount3/complete_*split*.csv\")\n",
    "        )\n",
    "        if len(pred_files) != nb_split:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Expected {nb_split} files for {cat}, found {len(pred_files)}\"\n",
    "            )\n",
    "        split_pred_files[cat] = pred_files\n",
    "\n",
    "    if len(split_pred_files) != nb_cat:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Expected {nb_cat} categories, found {len(split_pred_files)}\"\n",
    "        )\n",
    "\n",
    "    pred_dfs = {}\n",
    "    for cat, pred_files in split_pred_files.items():\n",
    "        dfs = []\n",
    "        for pred_file in pred_files:\n",
    "            df = pd.read_csv(pred_file, low_memory=False)\n",
    "            dfs.append(df)\n",
    "        concat_df = pd.concat(dfs, ignore_index=True)\n",
    "        pred_dfs[cat] = concat_df\n",
    "\n",
    "else:\n",
    "    pred_dfs = {}\n",
    "    for cat in categories:\n",
    "        pred_files = list(recount3_folder.rglob(f\"{cat}*/**/recount3/complete_*.csv\"))\n",
    "        if len(pred_files) != 1:\n",
    "            raise FileNotFoundError(f\"Expected 1 file for {cat}, found {len(pred_files)}\")\n",
    "        pred_df = pd.read_csv(pred_files[0], low_memory=False)\n",
    "        pred_dfs[cat] = pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat, pred_df in list(pred_dfs.items()):\n",
    "    try:\n",
    "        pred_df = pred_df.drop(\"True class\", axis=1)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    pred_df = pred_df.rename(columns={\"Unnamed: 0\": \"filename\"})\n",
    "\n",
    "    # Add max pred + move it to front\n",
    "    pred_df = split_results_handler.add_max_pred(pred_df, target_label=\"Predicted class\")\n",
    "    pred_df.insert(2, \"Max pred\", pred_df.pop(\"Max pred\"))\n",
    "    pred_df = pred_df[pred_df[\"Max pred\"] >= 0]  # in case of empty rows\n",
    "\n",
    "    # Get id columns\n",
    "    id_cols = (\n",
    "        pred_df[\"filename\"].str.split(\".\", expand=True)[2].str.split(\"_\", expand=True)\n",
    "    )\n",
    "\n",
    "    pred_df.insert(1, \"id1\", id_cols.loc[:, 0])\n",
    "    pred_df.insert(2, \"id2\", id_cols.loc[:, 1])\n",
    "\n",
    "    pred_dfs[cat] = pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(pred_dfs[ASSAY][\"filename\"].str.split(\".\", expand=True)[2].str.split(\"_\",expand=True).head())\n",
    "# display(pred_dfs[ASSAY][\"id1\"].nunique(), pred_dfs[ASSAY][\"id2\"].nunique())\n",
    "# display(pred_dfs[ASSAY][\"id2\"].str.slice(0,3).value_counts())\n",
    "assert pred_dfs[ASSAY][\"id2\"].nunique() == pred_dfs[ASSAY].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_filename = \"recount.full_info_metadata.freeze1.tsv\"\n",
    "metadata_file = base_metadata_dir / \"recount3\" / meta_filename\n",
    "recount_metadata_df = pd.read_csv(metadata_file, sep=\"\\t\", low_memory=False)\n",
    "recount_metadata_df.fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recount_metadata_df.rename(\n",
    "    mapper={\n",
    "        \"expected_assay\": ASSAY,\n",
    "        \"expected_lifestage\": LIFE_STAGE,\n",
    "        \"expected_sex\": SEX,\n",
    "        \"expected_cancer\": CANCER,\n",
    "        \"expected_biomat\": BIOMAT,\n",
    "    },\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recount_metadata_df[\"extracted_terms_biospecimen\"].value_counts(dropna=False).to_csv(\n",
    "    table_dir / \"datasets_composition\" / \"recount3_biospecimen.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recount_metadata_df[ASSAY].value_counts(dropna=False).to_csv(\n",
    "    table_dir / \"datasets_composition\" / \"recount3_assays.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_recount3_preds(\n",
    "    pred_dfs: Dict[str, pd.DataFrame], full_metadata_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Merge all recount3 predictions into a single DataFrame.\"\"\"\n",
    "    same_col_len = 5\n",
    "    # Make all different columns have unique relevant names except for the pred vector\n",
    "    new_dfs = {}\n",
    "    for cat, df in pred_dfs.items():\n",
    "        df = df.copy(deep=True)\n",
    "\n",
    "        df[\"ID\"] = df[\"id2\"]  # needs to be at end\n",
    "\n",
    "        df = df.drop([\"id1\", \"id2\"], axis=1)\n",
    "        try:\n",
    "            df = df.drop(columns=[\"Same?\"])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        df = df.merge(\n",
    "            full_metadata_df[[\"ID\", cat]],\n",
    "            left_on=\"ID\",\n",
    "            right_on=\"ID\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        df.insert(1, \"Expected class\", df[cat])\n",
    "        df = df.drop(columns=[cat])\n",
    "\n",
    "        old_names = df.columns[1 : same_col_len - 1]\n",
    "        new_names = [f\"{old_name} ({cat})\" for old_name in old_names]\n",
    "        df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "\n",
    "        new_dfs[cat] = df\n",
    "\n",
    "    df_order = [ASSAY, SEX, CANCER, LIFE_STAGE, BIOMAT]\n",
    "    df_list = [new_dfs[cat] for cat in df_order]\n",
    "\n",
    "    merge_dataframes_func = functools.partial(merge_dataframes, on=\"external_id\")\n",
    "    full_merged_df = functools.reduce(merge_dataframes_func, df_list)\n",
    "    full_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    full_merged_df = full_merged_df.merge(\n",
    "        full_metadata_df,\n",
    "        on=\"ID\",\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"\", \"_DROP\"),\n",
    "    )\n",
    "    for col in full_merged_df.columns:\n",
    "        if col.endswith(\"_DROP\"):\n",
    "            full_merged_df.drop(columns=col, inplace=True)\n",
    "\n",
    "    full_merged_df.insert(1, \"ID\", full_merged_df.pop(\"ID\"))\n",
    "\n",
    "    return full_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the merging process when prediction files are all available\n",
    "# final_df = merge_all_recount3_preds(pred_dfs, recount_metadata_df)\n",
    "# print(f\"Final df shape: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual merge\n",
    "\n",
    "If didn't merge with `merge_all_recount3_preds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Life stage metadata already has embryo/fetal/newborn merged into perinatal\n",
    "recount_metadata_df[LIFE_STAGE].value_counts(dropna=False)\n",
    "recount_metadata_df.rename(columns={LIFE_STAGE: f\"{LIFE_STAGE}_merged\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_only_path = table_dir / \"dfreeze_v2\" / \"predictions\" / \"recount3_merged_preds.csv\"\n",
    "preds_df = pd.read_csv(preds_only_path, sep=\",\", low_memory=False)\n",
    "display(preds_df.head())\n",
    "print(preds_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(\n",
    "    preds_df,\n",
    "    recount_metadata_df,\n",
    "    left_on=\"ID\",\n",
    "    right_on=\"ID\",\n",
    "    how=\"inner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"recount3_merged_preds_metadata_freeze1.csv.gz\"\n",
    "out_path = table_dir / \"dfreeze_v2\" / \"predictions\" / filename\n",
    "merged_df.to_csv(out_path, sep=\",\", index=False, compression=\"gzip\")\n",
    "print(f\"Saved to {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
