{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create supplementary prediction files destined for the paper.\n",
    "\n",
    "Includes most data predictions used to create paper figures.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import functools\n",
    "import gc\n",
    "import json\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import classification_report, confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.utils.classification_merging_utils import merge_dataframes\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISEASE = \"harmonized_sample_disease_high\"\n",
    "CANCER = \"harmonized_sample_cancer_high\"\n",
    "BIOMAT = \"harmonized_biomaterial_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "metadata_dir = base_data_dir / \"metadata\"\n",
    "paper_dir = base_dir\n",
    "table_dir = paper_dir / \"tables\"\n",
    "predictions_dir = base_data_dir / \"training_results\" / \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()\n",
    "metadata_handler = MetadataHandler(paper_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df_for_save(\n",
    "    df: pd.DataFrame, md5sum_to_epirr: Dict[str, str], md5sum_to_uuid: Dict[str, str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Prepare predictions DataFrame for saving to CSV. Return a modified DataFrame, with:\n",
    "    - Index set to md5sum\n",
    "    - Expected class as the first column\n",
    "    - epirr_without_version column added\n",
    "    - uuid column added\n",
    "    - Sorted by epirr_without_version and uuid\n",
    "    \"\"\"\n",
    "    df = df.copy(deep=True)\n",
    "    df.insert(0, \"Expected class\", df.pop(\"True class\"))\n",
    "    df.set_index(\"md5sum\", inplace=True)\n",
    "\n",
    "    df[\"epirr_without_version\"] = df.index.map(md5sum_to_epirr)\n",
    "    df[\"uuid\"] = df.index.map(md5sum_to_uuid)\n",
    "    df.sort_values(by=[\"epirr_without_version\", \"uuid\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Official metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = metadata_handler.load_metadata_df(version=\"v2\", merge_assays=False)\n",
    "md5sum_to_epirr = meta_df[\"epirr_id_without_version\"].to_dict()\n",
    "md5sum_to_uuid = meta_df[\"uuid\"].to_dict()\n",
    "del meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_metadata_dir = base_data_dir / \"metadata\" / \"official\"\n",
    "\n",
    "metadata_v1_1_path = (\n",
    "    official_metadata_dir / \"IHEC_metadata_harmonization.v1.1.extended.csv\"\n",
    ")\n",
    "metadata_v1_1 = pd.read_csv(metadata_v1_1_path, index_col=False)\n",
    "metadata_v1_1.set_index(\"epirr_id_without_version\", inplace=True)\n",
    "\n",
    "metadata_v1_2_path = (\n",
    "    official_metadata_dir / \"IHEC_metadata_harmonization.v1.2.extended.csv\"\n",
    ")\n",
    "metadata_v1_2 = pd.read_csv(metadata_v1_2_path, index_col=False)\n",
    "metadata_v1_2.set_index(\"epirr_id_without_version\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EpiATLAS training metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = table_dir / \"datasets_composition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_meta = metadata_dir / \"hg38_2023-epiatlas-dfreeze-pospurge-nodup_filterCtl.json\"\n",
    "with open(path_meta, \"r\", encoding=\"utf8\") as f:\n",
    "    records = json.load(f)[\"datasets\"]\n",
    "\n",
    "epiatlas_meta_df = pd.DataFrame(records)\n",
    "del records\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epiatlas_meta_df[CANCER] = epiatlas_meta_df[DISEASE].map(\n",
    "    {\"Disease\": \"non-cancer\", \"Healthy/None\": \"non-cancer\", \"Cancer\": \"cancer\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check, one epirr = one biospecimen (cell type)\n",
    "for epirr, cell_type in epiatlas_meta_df.groupby(\"epirr_id\")[CELL_TYPE].unique().items():\n",
    "    if len(cell_type) != 1:\n",
    "        raise ValueError(f\"Dataset with multiple cell types ({epirr}): {cell_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biospecimens = epiatlas_meta_df.fillna(\"unknown\").copy(deep=True)\n",
    "\n",
    "df_biospecimens = df_biospecimens.groupby(CELL_TYPE, dropna=False).agg(\n",
    "    {\n",
    "        \"epirr_id\": \"nunique\",\n",
    "        \"uuid\": \"nunique\",\n",
    "        \"md5sum\": \"nunique\",\n",
    "    }\n",
    ")\n",
    "\n",
    "df_biospecimens.rename(\n",
    "    columns={\n",
    "        \"epirr_id\": \"Biospecimen count\",\n",
    "        \"uuid\": \"uuid/experiment count\",\n",
    "        \"md5sum\": \"File Count\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "df_biospecimens = df_biospecimens.sort_values(\"Biospecimen count\", ascending=False)\n",
    "\n",
    "df_biospecimens.to_csv(output_dir / \"EpiATLAS_biospecimens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check, one uuid = one experiment\n",
    "\n",
    "# Group by UUID and get unique assays per UUID\n",
    "groupby_uuid = epiatlas_meta_df.groupby(\"uuid\")[ASSAY].unique()\n",
    "\n",
    "for uuid, assay in groupby_uuid.items():\n",
    "    if len(assay) != 1:\n",
    "        print(f\"uuid with multiple assays ({uuid}): {assay}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one uuid `!=` one experiment --> Input files need to be handled separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each assay\n",
    "experiment_counter = groupby_uuid.explode().value_counts()\n",
    "\n",
    "# Detailled composition\n",
    "N_exp = experiment_counter.sum()\n",
    "N_uuid = len(groupby_uuid)\n",
    "N_input_alone = N_uuid + experiment_counter[\"input\"] - N_exp\n",
    "\n",
    "print(f\"Total experiments: {N_exp}\")\n",
    "print(f\"Total uuids (can include assay+input): {N_uuid}\")\n",
    "print(f\"Total input: {experiment_counter.get('input', 0)}\")\n",
    "print(f\"Total input alone: {N_input_alone}\")\n",
    "print(\n",
    "    f\"#exp - #uuid = #input - #input alone: {N_exp}-{N_uuid} = {experiment_counter.get('input', 0)}-{N_input_alone} = {N_exp - N_uuid}\"\n",
    ")\n",
    "\n",
    "assert (N_exp - N_uuid) == (experiment_counter.get(\"input\", 0) - N_input_alone)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_exp = experiment_counter.rename_axis(\"Experiment Assay\").reset_index(\n",
    "    name=\"Experiment Count\"\n",
    ")\n",
    "df_exp = df_exp.sort_values(\"Experiment Count\", ascending=False).set_index(\n",
    "    \"Experiment Assay\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of files per assay\n",
    "unique_files = (\n",
    "    epiatlas_meta_df.groupby(ASSAY)[\"md5sum\"]\n",
    "    .count()\n",
    "    .rename(\"File Count\")\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# Merge file counts with experiment counts\n",
    "df_exp = df_exp.merge(\n",
    "    unique_files, left_on=\"Experiment Assay\", right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "# Compute the track type average count\n",
    "df_exp[\"Track type average count\"] = df_exp[\"File Count\"] / df_exp[\"Experiment Count\"]\n",
    "\n",
    "df_exp.to_csv(output_dir / \"EpiATLAS_assays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_types = (\n",
    "    epiatlas_meta_df.groupby(\"track_type\")[\"md5sum\"]\n",
    "    .count()\n",
    "    .rename(\"File Count\")\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "track_types.to_csv(output_dir / \"EpiATLAS_track_types.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del groupby_uuid, df_biospecimens, experiment_counter, df_exp, unique_files, track_types\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect experiment keys for all trained classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_experiment_info(line: str) -> Tuple[str, str] | None:\n",
    "    \"\"\"Extract split and experiment key from a line containing checkpoint information.\n",
    "\n",
    "    Line should have format: .../splitX/EpiLaP/[exp_key]/checkpoints/...\n",
    "    \"\"\"\n",
    "    if \"EpiLaP\" not in line:\n",
    "        return None\n",
    "\n",
    "    parts = line.strip().split(\"/\")\n",
    "    for i, part in enumerate(parts):\n",
    "        if part == \"EpiLaP\" and i > 0:\n",
    "            return (parts[i - 1], parts[i + 1])\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_log_file(file_path: Path) -> Set[Tuple[str, str]]:\n",
    "    \"\"\"Process a single log file and extract experiment information.\"\"\"\n",
    "    experiment_info = set()\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                if result := extract_experiment_info(line):\n",
    "                    experiment_info.add(result)\n",
    "    except Exception as e:  # pylint: disable=broad-exception-caught\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    return experiment_info\n",
    "\n",
    "\n",
    "def collect_exp_keys(folder: Path) -> Dict[str, Set[Tuple[str, str]]]:\n",
    "    \"\"\"Collect experiment keys from log files (.o files), recursively from a given folder.\"\"\"\n",
    "    experiments_keys = defaultdict(set)\n",
    "\n",
    "    log_files = folder.glob(\"*1l_3000n/**/*.o\")\n",
    "    for file in log_files:\n",
    "        if experiment_info := process_log_file(file):\n",
    "            experiments_keys[file.parent].update(experiment_info)\n",
    "\n",
    "    return experiments_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_exp_key_context(\n",
    "    experiments_keys_dict: Dict[str, Set[Tuple[str, str]]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Format experiment keys context for saving to a DataFrame.\"\"\"\n",
    "    data = []\n",
    "    for exp_folder, exp_keys in experiments_keys_dict.items():\n",
    "        for split, exp_key in exp_keys:\n",
    "            data.append((str(exp_folder), split, exp_key))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"exp_folder\", \"split\", \"exp_key\"])\n",
    "\n",
    "    # comet-ml experiment url\n",
    "    df[\"comet-url\"] = \"https://www.comet.com/rabyj/epiclass/\" + df[\"exp_key\"].astype(str)\n",
    "\n",
    "    # Remove useless part of paths\n",
    "    to_remove_path = (\n",
    "        str(Path.home() / \"Projects/epiclass/output/paper/data/training_results\") + \"/\"\n",
    "    )\n",
    "    df[\"complete_experiment_context\"] = df[\"exp_folder\"].str.replace(to_remove_path, \"\")\n",
    "    df.drop(columns=\"exp_folder\", inplace=True)\n",
    "\n",
    "    # Split path into named parts\n",
    "    df[[\"release\", \"feature_set_name\", \"metadata_category\"]] = (\n",
    "        df[\"complete_experiment_context\"].str.split(\"/\", expand=True).loc[:, [0, 1, 2]]\n",
    "    )\n",
    "\n",
    "    df[\"experiment_specification\"] = (\n",
    "        df[\"complete_experiment_context\"]\n",
    "        .str.split(\"/\", n=3, expand=True)[3]\n",
    "        .str.replace(\"/\", \",\")\n",
    "    )\n",
    "\n",
    "    # Remove redundant info (all MLP exp are 1 hidden layer 3000 nodes)\n",
    "    df[\"metadata_category\"] = df[\"metadata_category\"].str.replace(\"_1l_3000n\", \"\")\n",
    "\n",
    "    # Reorder columns\n",
    "    df_new_col_order = df.columns.to_list()[-4:] + df.columns.to_list()[:-4]\n",
    "    df = df[df_new_col_order]\n",
    "\n",
    "    return df  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exp_keys_dfs = []\n",
    "for folder in [\"dfreeze_v2\", \"2023-01-epiatlas-freeze\", \"imputation\"]:\n",
    "    data_dir = base_data_dir / \"training_results\" / folder\n",
    "    for subfolder in data_dir.glob(\"*\"):\n",
    "        if subfolder.is_file():\n",
    "            continue\n",
    "        # print(subfolder)\n",
    "        exp_key_dict = collect_exp_keys(subfolder)\n",
    "        df = format_exp_key_context(exp_key_dict)\n",
    "        all_exp_keys_dfs.append(df)\n",
    "\n",
    "exp_keys_df = pd.concat(all_exp_keys_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in [\"release\", \"feature_set_name\", \"metadata_category\", \"split\"]:\n",
    "#     display(exp_keys_df[col].value_counts(dropna=False))\n",
    "\n",
    "# exp_keys_df.to_csv(table_dir / \"training_experiment_keys.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assay_epiclass + sample ontology for all 5 model types - 100kb_all_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_100kb = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "logdir = table_dir / \"dfreeze_v2\" / \"100kb_all_none\"\n",
    "if not logdir.exists():\n",
    "    logdir.mkdir(parents=True)\n",
    "\n",
    "split_md5sums = []\n",
    "all_metrics = {}\n",
    "for category in [ASSAY, CELL_TYPE]:\n",
    "    all_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "        results_dir=data_dir_100kb,\n",
    "        label_category=category,\n",
    "        only_NN=False,\n",
    "    )\n",
    "\n",
    "    # Sanity check, same shape, same input files for each method\n",
    "    for split_dict in all_split_dfs.values():\n",
    "        ref_dict = split_dict[\"NN\"]\n",
    "        ref_md5sums = sorted(ref_dict.index.values.tolist())\n",
    "        ref_shape = ref_dict.shape\n",
    "        for method, df in split_dict.items():\n",
    "            if not ref_md5sums == sorted(df.index.values.tolist()):\n",
    "                raise ValueError(\"MD5sums do not match\")\n",
    "            if ref_shape != df.shape:\n",
    "                raise ValueError(\"Shapes do not match\")\n",
    "\n",
    "    split_metrics = split_results_handler.compute_split_metrics(all_split_dfs)\n",
    "    all_metrics[category] = split_metrics\n",
    "\n",
    "    all_split_dfs_concat: Dict = split_results_handler.concatenate_split_results(all_split_dfs)  # type: ignore\n",
    "\n",
    "    # Save to file\n",
    "    for method, df in all_split_dfs_concat.items():\n",
    "        df = prepare_df_for_save(df, md5sum_to_epirr, md5sum_to_uuid)\n",
    "\n",
    "        if method == \"NN\":\n",
    "            method = \"MLP\"\n",
    "\n",
    "        filename = f\"10fold_predictions_{category}_{method}.csv\"\n",
    "        df.to_csv(logdir / filename, index=True, sep=\",\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier type, Classification task, split, acc, F!, AUC-micro, AUC-macro, N\n",
    "metrics_as_rows = []\n",
    "for category, split_metrics in all_metrics.items():\n",
    "    for split_name, metrics in split_metrics.items():\n",
    "        for method, metric_dict in metrics.items():\n",
    "            metric_values = []\n",
    "            for metric_name in [\n",
    "                \"Accuracy\",\n",
    "                \"F1_macro\",\n",
    "                \"AUC_micro\",\n",
    "                \"AUC_macro\",\n",
    "                \"count\",\n",
    "            ]:\n",
    "                metric_values.append(metric_dict[metric_name])\n",
    "            metrics_as_rows.append((method, category, split_name, *metric_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(\n",
    "    metrics_as_rows,\n",
    "    columns=[\n",
    "        \"Classifier type\",\n",
    "        \"Classification task\",\n",
    "        \"Split name\",\n",
    "        \"Accuracy\",\n",
    "        \"F1-score macro\",\n",
    "        \"AUC micro\",\n",
    "        \"AUC macro\",\n",
    "        \"Validation size\",\n",
    "    ],\n",
    ")\n",
    "print(metrics_df.shape)\n",
    "\n",
    "metrics_df.replace(\"NN\", \"MLP\", inplace=True)\n",
    "metrics_df.replace(\"RF\", \"RandomForest\", inplace=True)\n",
    "metrics_df.replace(\"LGBM\", \"LightGBM\", inplace=True)\n",
    "metrics_df.replace(\"LR\", \"LogisticRegression\", inplace=True)\n",
    "metrics_df.replace(\"LinearSVC\", \"LinearSVM\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_df.to_csv(table_dir / \"dfreeze_v2_100kb_all_none_5algo_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other MLP results - 100kb_all_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_100kb = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "logdir = table_dir / \"dfreeze_v2\" / \"100kb_all_none\"\n",
    "if not logdir.exists():\n",
    "    logdir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"paired_end\",\n",
    "    \"harmonized_sample_cancer_high\",\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    \"harmonized_biomaterial_type\",\n",
    "    \"project\",\n",
    "    CELL_TYPE,\n",
    "]\n",
    "\n",
    "# Select 10-fold oversampling runs\n",
    "all_split_dfs = split_results_handler.general_split_metrics(\n",
    "    results_dir=data_dir_100kb,\n",
    "    merge_assays=False,\n",
    "    include_categories=categories,\n",
    "    exclude_names=[\"reg\", \"no-mixed\", \"chip\", \"16ct\", \"27ct\"],\n",
    "    return_type=\"split_results\",\n",
    "    oversampled_only=True,\n",
    "    verbose=False,\n",
    ")\n",
    "all_split_dfs_concat: Dict = split_results_handler.concatenate_split_results(all_split_dfs, concat_first_level=True)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_mapper = {key: (key if \"sex\" not in key else SEX) for key in all_split_dfs_concat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "new_dfs = {}\n",
    "for category, df in all_split_dfs_concat.items():\n",
    "    new_df = prepare_df_for_save(df, md5sum_to_epirr, md5sum_to_uuid)\n",
    "\n",
    "    # we want to write down the expected class for both metadata versions,\n",
    "    # since there were changes in-between\n",
    "    if category in [LIFE_STAGE, SEX]:\n",
    "        for version, metadata in [(\"v1.2\", metadata_v1_2), (\"v1.1\", metadata_v1_1)]:\n",
    "            idx = new_df.index.map(md5sum_to_epirr).values\n",
    "            values = metadata.loc[idx, category].to_list()  # type: ignore\n",
    "            new_df.insert(loc=0, column=f\"Expected class {version}\", value=values)\n",
    "        new_df.drop(\"Expected class\", axis=1, inplace=True)\n",
    "\n",
    "    # using training metadata\n",
    "    # necessary because some cell line samples got life stage labels\n",
    "    # in further versions, which would make the 10fold files\n",
    "    # different than was was actually used for training\n",
    "    else:\n",
    "        cat = cat_mapper[category]\n",
    "        values = epiatlas_meta_df[[\"md5sum\", cat]].set_index(\"md5sum\")\n",
    "        values = values.loc[new_df.index, cat]\n",
    "        new_df[\"Expected class\"] = values\n",
    "\n",
    "    new_dfs[category] = new_df\n",
    "\n",
    "    filename = f\"10fold_predictions_{category}_MLP.csv\"\n",
    "    new_df.to_csv(logdir / filename, index=True, sep=\",\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_col_len = 4\n",
    "\n",
    "final_dfs = {}\n",
    "for category, df in new_dfs.items():\n",
    "    new_df = df.copy(deep=True)\n",
    "    new_df.drop([\"uuid\", \"epirr_without_version\"], axis=1, inplace=True)\n",
    "    new_df.insert(0, \"split\", new_df.pop(\"split\"))\n",
    "\n",
    "    try:\n",
    "        new_df = new_df.reset_index()\n",
    "    except ValueError:\n",
    "        new_df = new_df.reset_index(drop=True)\n",
    "\n",
    "    # All dataframes need to have same shape for rest of code to work\n",
    "    if category in [LIFE_STAGE, SEX]:\n",
    "        new_df.drop(\"Expected class v1.1\", axis=1, inplace=True)\n",
    "        new_df.rename(columns={\"Expected class v1.2\": \"Expected class\"}, inplace=True)\n",
    "\n",
    "    new_df.insert(loc=0, column=\"md5sum\", value=new_df.pop(\"md5sum\"))\n",
    "\n",
    "    # this only works if all the columns to the right correspond to pred vector\n",
    "    pred_cols = new_df.columns[same_col_len:].tolist()\n",
    "    new_df.insert(\n",
    "        loc=same_col_len, column=\"Max pred\", value=new_df[pred_cols].max(axis=1)\n",
    "    )\n",
    "\n",
    "    old_names = new_df.columns[1 : same_col_len + 1]\n",
    "    new_names = [f\"{old_name} ({category})\" for old_name in old_names]\n",
    "    new_df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "\n",
    "    final_dfs[category] = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = functools.reduce(merge_dataframes, final_dfs.values())\n",
    "\n",
    "final_df[\"epirr\"] = final_df.loc[:, \"md5sum\"].map(md5sum_to_epirr)\n",
    "final_df[\"uuid\"] = final_df.loc[:, \"md5sum\"].map(md5sum_to_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\n",
    "    logdir / \"all_10fold_predictions_MLP.csv\", index=False, sep=\",\", float_format=\"%.4f\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for other feature sets (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_splits_identity(\n",
    "    all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]],\n",
    "    task_names: List[str],\n",
    "    verbose: bool | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Verify that the splits are identical between feature sets for each task.\n",
    "\n",
    "    all_results: {feature_set: {task_name: {split_name: results_dataframe}}}\n",
    "    task_names: list of task names to verify\n",
    "    verbose: print additional information\n",
    "    \"\"\"\n",
    "    # Sanity check : MD5sums and shapes should match between reference and other feature sets, for each split\n",
    "    for task_name in task_names:\n",
    "        if verbose:\n",
    "            print(f\"Verifying task '{task_name}'\")\n",
    "        # Select a reference feature set and use its splits as the baseline for comparison\n",
    "        reference_feature_set = \"hg38_100kb_all_none\"\n",
    "        reference_splits = all_results[reference_feature_set][task_name]\n",
    "\n",
    "        # Create reference MD5sums and shapes for each split in the reference feature set\n",
    "        reference_md5sums = {\n",
    "            split_name: sorted(df.index.tolist())\n",
    "            for split_name, df in reference_splits.items()\n",
    "        }\n",
    "        reference_shapes = {\n",
    "            split_name: df.shape for split_name, df in reference_splits.items()\n",
    "        }\n",
    "\n",
    "        # Iterate over each feature set and compare its splits against the reference\n",
    "        for feature_set_name, tasks_dict in all_results.items():\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Verifying feature set '{feature_set_name}' against reference feature set '{reference_feature_set}'\"\n",
    "                )\n",
    "            for split_name, df in tasks_dict[task_name].items():\n",
    "                if reference_shapes[split_name] != df.shape:\n",
    "                    print(\n",
    "                        f\"WARNING: Shape mismatch in task '{task_name}', split '{split_name}', \"\n",
    "                        f\"between reference feature set '{reference_feature_set}' and feature set '{feature_set_name}'\",\n",
    "                    )\n",
    "                if reference_md5sums[split_name] != sorted(df.index.tolist()):\n",
    "                    print(\n",
    "                        f\"WARNING: MD5sums mismatch in task '{task_name}', split '{split_name}', \"\n",
    "                        f\"between reference feature set '{reference_feature_set}' and feature set '{feature_set_name}'\",\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ASSAY, CELL_TYPE]\n",
    "include_sets = [\n",
    "    \"hg38_10mb_all_none_1mb_coord\",\n",
    "    \"hg38_100kb_random_n316_none\",\n",
    "    \"hg38_1mb_all_none\",\n",
    "    \"hg38_100kb_random_n3044_none\",\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_gene_regions_100kb_coord_n19864\",\n",
    "    \"hg38_10kb_random_n30321_none\",\n",
    "    \"hg38_regulatory_regions_n30321\",\n",
    "    \"hg38_1kb_random_n30321_none\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n",
    "    \"hg38_10kb_all_none\",\n",
    "    \"hg38_regulatory_regions_n303114\",\n",
    "    \"hg38_1kb_random_n303114_none\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n300k\",\n",
    "]\n",
    "exclude_names = [\"7c\", \"chip-seq-only\", \"27ct\", \"16ct\"]\n",
    "\n",
    "# Select 10-fold oversampling runs\n",
    "# expected result shape: {feature_set: {task_name: {split_name: results_dataframe}}}\n",
    "all_results: Dict[\n",
    "    str, Dict[str, Dict[str, pd.DataFrame]]\n",
    "] = split_results_handler.obtain_all_feature_set_data(\n",
    "    return_type=\"split_results\",\n",
    "    parent_folder=data_dir_100kb.parent,\n",
    "    merge_assays=False,\n",
    "    include_categories=categories,\n",
    "    include_sets=include_sets,\n",
    "    exclude_names=exclude_names,\n",
    "    verbose=False,\n",
    ")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace assay_11c with assay\n",
    "for feature_set in all_results.values():\n",
    "    try:\n",
    "        feature_set[ASSAY] = feature_set.pop(f\"{ASSAY}_11c\")\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_splits_identity(all_results, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = table_dir / \"dfreeze_v2\" / \"other_feature_sets\"\n",
    "logdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for feature_set_name, tasks_dict in all_results.items():\n",
    "    if feature_set_name == \"hg38_100kb_all_none\":\n",
    "        continue\n",
    "    all_split_dfs_concat: Dict = split_results_handler.concatenate_split_results(\n",
    "        tasks_dict, concat_first_level=True\n",
    "    )  # type: ignore\n",
    "    for task_name, df in all_split_dfs_concat.items():\n",
    "        df = prepare_df_for_save(df, md5sum_to_epirr, md5sum_to_uuid)\n",
    "\n",
    "        filename = f\"{feature_set_name}_10fold_predictions_{task_name}.csv\"\n",
    "        df.to_csv(logdir / filename, index=True, sep=\",\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winsorized files and/or blacklist zeroed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ASSAY, CELL_TYPE, SEX, \"harmonized_biomaterial_type\"]\n",
    "include_sets = [\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_100kb_all_none_0blklst\",\n",
    "    \"hg38_100kb_all_none_0blklst_winsorized\",\n",
    "]\n",
    "\n",
    "results_folder = base_data_dir / \"training_results\" / \"2023-01-epiatlas-freeze\"\n",
    "if not results_folder.exists():\n",
    "    raise FileNotFoundError(f\"Folder '{results_folder}' not found\")\n",
    "\n",
    "logdir = table_dir / \"2023-01-epiatlas-freeze\"\n",
    "if not logdir.exists():\n",
    "    logdir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10-fold oversampling runs\n",
    "# expected result shape: {feature_set: {task_name: {split_name: results_dataframe}}}\n",
    "all_results: Dict[\n",
    "    str, Dict[str, Dict[str, pd.DataFrame]]\n",
    "] = split_results_handler.obtain_all_feature_set_data(\n",
    "    return_type=\"split_results\",\n",
    "    parent_folder=results_folder,\n",
    "    merge_assays=False,\n",
    "    include_categories=categories,\n",
    "    include_sets=include_sets,\n",
    "    oversampled_only=False,\n",
    "    verbose=False,\n",
    ")  # type: ignore\n",
    "\n",
    "display(all_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_collected = list(all_results[\"hg38_100kb_all_none\"])\n",
    "verify_splits_identity(all_results, tasks_collected, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save concatenated result\n",
    "for feature_set_name, tasks_dict in all_results.items():\n",
    "    concatenated_dfs = split_results_handler.concatenate_split_results(\n",
    "        tasks_dict, concat_first_level=True\n",
    "    )\n",
    "    for task_name, concatenated_df in concatenated_dfs.items():\n",
    "        concatenated_df = prepare_df_for_save(concatenated_df, md5sum_to_epirr, md5sum_to_uuid)  # type: ignore\n",
    "        filename = f\"{feature_set_name}_10fold_predictions_{task_name}.csv\"\n",
    "        print(f\"Saving {filename}\")\n",
    "        concatenated_df.to_csv(\n",
    "            logdir / filename, index=True, sep=\",\", float_format=\"%.4f\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate input dataset discrepancy in assay_epiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata_df = metadata_handler.load_metadata_df(\"v2\", merge_assays=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_counts = {}\n",
    "for feature_set_name, tasks_dict in all_results.items():\n",
    "    concatenated_dfs = split_results_handler.concatenate_split_results(\n",
    "        tasks_dict, concat_first_level=True\n",
    "    )\n",
    "    md5sums = concatenated_dfs[ASSAY].index.tolist()\n",
    "    print(f\"{feature_set_name}: {len(md5sums)}\")\n",
    "\n",
    "    metadata_subset = metadata_df[metadata_df.index.isin(md5sums)]\n",
    "    values_counts[feature_set_name] = metadata_subset[ASSAY].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    values_counts[\"hg38_100kb_all_none_0blklst\"] - values_counts[\"hg38_100kb_all_none\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODE predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_preds_dir = table_dir / \"dfreeze_v2\" / \"predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See:\n",
    "- src/python/epi_ml/utils/notebooks/paper/encode_metadata_creation.ipynb\n",
    "- src/python/epi_ml/utils/notebooks/paper/encode_pred_analysis.ipynb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = table_dir / \"datasets_composition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_id_cols = [\"EXPERIMENT_accession\", \"BIOSAMPLE_accession\", \"FILE_accession\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_preds_path = (\n",
    "    predictions_dir\n",
    "    / \"encode\"\n",
    "    / \"complete_encode_predictions_augmented_2025-02_metadata.csv\"\n",
    ")\n",
    "shutil.copy(encode_preds_path, other_preds_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_preds_df = pd.read_csv(encode_preds_path, encoding=\"utf8\", low_memory=False)\n",
    "print(encode_preds_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_files = encode_preds_df.shape[0]\n",
    "N_exp = encode_preds_df[\"EXPERIMENT_accession\"].nunique()\n",
    "N_biospecimen = encode_preds_df[\"BIOSAMPLE_accession\"].nunique()\n",
    "print(N_files, N_exp, N_biospecimen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assays_df = encode_preds_df.groupby(ASSAY, dropna=False).agg(\n",
    "    {\n",
    "        \"EXPERIMENT_accession\": \"nunique\",\n",
    "        \"FILE_accession\": \"nunique\",\n",
    "    }\n",
    ")\n",
    "for label in encode_id_cols:\n",
    "    try:\n",
    "        assays_df.rename(\n",
    "            columns={label: label.replace(\"_accession\", \" count\")}, inplace=True\n",
    "        )\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "\n",
    "assays_df[\"file_per_experiment\"] = assays_df[\"FILE count\"] / assays_df[\"EXPERIMENT count\"]\n",
    "\n",
    "assays_df.to_csv(output_dir / \"ENCODE_assays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biospecimens = encode_preds_df.fillna(\"unknown\").copy(deep=True)\n",
    "\n",
    "groupby = df_biospecimens.groupby(\n",
    "    [\"BIOSAMPLE_TYPE_name\", \"BIOSAMPLE_TYPE_term_name\"], dropna=False\n",
    ").agg(\n",
    "    {\n",
    "        \"BIOSAMPLE_accession\": \"nunique\",\n",
    "        \"EXPERIMENT_accession\": \"nunique\",\n",
    "        \"FILE_accession\": \"nunique\",\n",
    "    }\n",
    ")\n",
    "\n",
    "for label in encode_id_cols:\n",
    "    try:\n",
    "        groupby.rename(\n",
    "            columns={label: label.replace(\"_accession\", \" count\")}, inplace=True\n",
    "        )\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "groupby.to_csv(output_dir / \"ENCODE_biospecimens.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChIP-Atlas predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See\n",
    "- `src/python/epi_ml/utils/notebooks/paper/c-a_pred_analysis.ipynb`\n",
    "- `src/python/epi_ml/utils/notebooks/paper/c-a_metadata.ipynb`\n",
    "\n",
    "Predictions pre-joined and then merged with custom metadata (with info on 4 databases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = metadata_dir / \"chip_atlas\" / \"CA_metadata_joined_20250306.tsv\"\n",
    "meta_df = pd.read_csv(meta_path, sep=\"\\t\", low_memory=False)\n",
    "print(meta_df.shape)\n",
    "\n",
    "pred_path = predictions_dir / \"C-A\" / \"CA_only_pred_20240606.tsv\"\n",
    "pred_df = pd.read_csv(pred_path, sep=\"\\t\", low_memory=False)\n",
    "print(pred_df.shape, pred_df.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = meta_df.columns[0]\n",
    "col2 = pred_df.columns[0]\n",
    "\n",
    "full_df = pd.merge(pred_df, meta_df, how=\"left\", left_on=col1, right_on=col2)\n",
    "full_df.to_csv(other_preds_dir / \"ChIP-Atlas_predictions.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChIP-Seq_imputed_with_RNA-Seq_only predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = table_dir / \"dfreeze_v2\" / \"epiatlas_imputed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are from epiclass_11c complete training (with oversampling) MLP classifer  \n",
    "Training details at 0f8e5eb996114868a17057bebe64f87c (comet-ml id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_folder = base_data_dir / \"training_results\" / \"predictions\" / \"epiatlas_imputed\"\n",
    "pred_file = \"complete_no_valid_oversample_test_prediction_100kb_all_none_ChIP-Seq_imputed_with_RNA-Seq_only.csv\"\n",
    "pred_df = pd.read_csv(pred_folder / pred_file)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.rename(columns={\"Unnamed: 0\": \"filename\"}, inplace=True)\n",
    "\n",
    "# filename of format 'impute_[ihec-id]_[expected-class]_[resolution]_[filter_in]_[filter_out].csv'\n",
    "pred_df[\"True class\"] = pred_df[\"filename\"].str.split(\"_\", expand=True)[2].str.lower()\n",
    "pred_df.rename(columns={\"True class\": \"Expected class\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_pred_col = np.where(pred_df.columns == \"Predicted class\")[0][0]\n",
    "pred_df.insert(\n",
    "    loc=int(idx_pred_col + 1),\n",
    "    column=\"Same?\",\n",
    "    value=pred_df[\"Expected class\"] == pred_df[\"Predicted class\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {pred_df['Same?'].sum() / pred_df.shape[0]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_pred_vector_cols = 4\n",
    "nb_classes = 11\n",
    "pred_df.insert(\n",
    "    loc=non_pred_vector_cols,\n",
    "    column=\"Max pred\",\n",
    "    value=pred_df.iloc[:, non_pred_vector_cols : non_pred_vector_cols + nb_classes].max(\n",
    "        axis=1  # type: ignore\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_df.to_csv(output_dir / \"epiatlas_imputed_w_rna_only_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recount3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recount3_folder = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"predictions\"\n",
    "    / \"recount3\"\n",
    "    / \"hg38_100kb_all_none\"\n",
    ")\n",
    "if not recount3_folder.exists():\n",
    "    raise FileNotFoundError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_pred_files = {}\n",
    "for cat in [ASSAY, SEX, LIFE_STAGE, CANCER]:\n",
    "    pred_files = list(recount3_folder.rglob(f\"{cat}*/**/recount3/complete_*.csv\"))\n",
    "    if len(pred_files) != 7:\n",
    "        raise FileNotFoundError(f\"Expected 7 files for {cat}, found {len(pred_files)}\")\n",
    "    split_pred_files[cat] = pred_files\n",
    "\n",
    "if len(split_pred_files) != 4:\n",
    "    raise FileNotFoundError(f\"Expected 4 categories, found {len(split_pred_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs = {}\n",
    "for cat, pred_files in split_pred_files.items():\n",
    "    dfs = []\n",
    "    for pred_file in pred_files:\n",
    "        df = pd.read_csv(pred_file, low_memory=False)\n",
    "        dfs.append(df)\n",
    "    concat_df = pd.concat(dfs, ignore_index=True)\n",
    "    pred_dfs[cat] = concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat, pred_df in list(pred_dfs.items()):\n",
    "    try:\n",
    "        pred_df = pred_df.drop(\"True class\", axis=1)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    pred_df = pred_df.rename(columns={\"Unnamed: 0\": \"filename\"})\n",
    "\n",
    "    # Add max pred + move it to front\n",
    "    pred_df = split_results_handler.add_max_pred(pred_df, target_label=\"Predicted class\")\n",
    "    pred_df.insert(2, \"Max pred\", pred_df.pop(\"Max pred\"))\n",
    "    pred_df = pred_df[pred_df[\"Max pred\"] >= 0]\n",
    "\n",
    "    # Get id columns\n",
    "    id_cols = (\n",
    "        pred_df[\"filename\"].str.split(\".\", expand=True)[2].str.split(\"_\", expand=True)\n",
    "    )\n",
    "\n",
    "    pred_df.insert(1, \"id1\", id_cols.loc[:, 0])\n",
    "    pred_df.insert(2, \"id2\", id_cols.loc[:, 1])\n",
    "\n",
    "    pred_dfs[cat] = pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(pred_dfs[ASSAY][\"filename\"].str.split(\".\", expand=True)[2].str.split(\"_\",expand=True).head())\n",
    "# display(pred_dfs[ASSAY][\"id1\"].nunique(), pred_dfs[ASSAY][\"id2\"].nunique())\n",
    "# display(pred_dfs[ASSAY][\"id2\"].str.slice(0,3).value_counts())\n",
    "assert pred_dfs[ASSAY][\"id2\"].nunique() == pred_dfs[ASSAY].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_name = \"harmonized_metadata_20250122_leuk2\"\n",
    "metadata_file = metadata_dir / \"recount3\" / f\"recount_{meta_name}.tsv\"\n",
    "recount_metadata_df = pd.read_csv(metadata_file, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(recount_metadata_df[\"cell_line_flag\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recount_metadata_df.rename(\n",
    "    mapper={\n",
    "        \"harmonized_assay\": ASSAY,\n",
    "        \"harmonized_lifestage\": LIFE_STAGE,\n",
    "        \"harmonized_sex\": SEX,\n",
    "        \"harmonized_cancer\": CANCER,\n",
    "    },\n",
    "    axis=1,\n",
    "    inplace=True,\n",
    ")\n",
    "recount_metadata_df.fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recount_metadata_df[\"tissue_keyword\"].str.split(\":\", expand=True)[0].value_counts(\n",
    "    dropna=False\n",
    ").to_csv(table_dir / \"datasets_composition\" / \"recount3_biospecimen.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recount_metadata_df[\"assay_epiclass\"].value_counts(dropna=False).to_csv(\n",
    "    table_dir / \"datasets_composition\" / \"recount3_assays.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_recount3_preds(\n",
    "    pred_dfs: Dict[str, pd.DataFrame], full_metadata_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Merge all recount3 predictions into a single DataFrame.\"\"\"\n",
    "    same_col_len = 5\n",
    "    # Make all different columns have unique relevant names except for the pred vector\n",
    "    new_dfs = {}\n",
    "    for cat, df in pred_dfs.items():\n",
    "        df = df.copy()\n",
    "        df[\"ID\"] = df[\"id2\"]\n",
    "        df = df.drop([\"id1\", \"id2\"], axis=1)\n",
    "        try:\n",
    "            df = df.drop(columns=[\"Same?\"])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        df = df.merge(\n",
    "            full_metadata_df[[\"ID\", cat]],\n",
    "            left_on=\"ID\",\n",
    "            right_on=\"ID\",\n",
    "            how=\"inner\",\n",
    "        )\n",
    "        df.insert(1, \"Expected class\", df[cat])\n",
    "        df = df.drop(columns=[cat])\n",
    "\n",
    "        old_names = df.columns[1 : same_col_len - 1]\n",
    "        new_names = [f\"{old_name} ({cat})\" for old_name in old_names]\n",
    "        df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "\n",
    "        new_dfs[cat] = df\n",
    "\n",
    "    df_order = [ASSAY, SEX, CANCER, LIFE_STAGE]\n",
    "    df_list = [new_dfs[cat] for cat in df_order]\n",
    "\n",
    "    merge_dataframes_func = functools.partial(merge_dataframes, on=\"external_id\")\n",
    "    full_merged_df = functools.reduce(merge_dataframes_func, df_list)\n",
    "    full_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    full_merged_df = full_merged_df.merge(\n",
    "        full_metadata_df,\n",
    "        on=\"ID\",\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"\", \"_DROP\"),\n",
    "    )\n",
    "    for col in full_merged_df.columns:\n",
    "        if col.endswith(\"_DROP\"):\n",
    "            full_merged_df.drop(columns=col, inplace=True)\n",
    "    return full_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = merge_all_recount3_preds(pred_dfs, recount_metadata_df)\n",
    "print(f\"Final df shape: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.insert(1, \"ID\", final_df.pop(\"ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = recount3_folder / f\"recount3_merged_preds_{meta_name}.tsv.gz\"\n",
    "# final_df.to_csv(out_path, sep=\"\\t\", index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = recount3_folder / f\"recount3_merged_preds_{meta_name}.tsv.gz\"\n",
    "full_df = pd.read_csv(preds_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_df = full_df[full_df[ASSAY] != \"unknown\"]\n",
    "N = assay_df.shape[0]\n",
    "\n",
    "for max_pred in [0, 0.6, 0.8]:\n",
    "    subset = assay_df[assay_df[f\"Max pred ({ASSAY})\"] >= max_pred]\n",
    "    counts = subset[f\"Predicted class ({ASSAY})\"].value_counts()\n",
    "\n",
    "    N_subset = counts.sum()\n",
    "    counts_perc = counts / N_subset\n",
    "    correct_perc = counts_perc[\"rna_seq\"] + counts_perc[\"mrna_seq\"]\n",
    "    print(f\"min_PredScore >= {max_pred} ({N_subset/N:.2%} left): {correct_perc:.2%}\\n\")\n",
    "\n",
    "    print(\"Predictions grouped, assay types left as is\")\n",
    "    groupby = (\n",
    "        subset.groupby([ASSAY, f\"Predicted class ({ASSAY})\"])\n",
    "        .size()\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"Count\"})\n",
    "        .sort_values(by=[ASSAY, \"Count\"], ascending=[True, False])\n",
    "    )\n",
    "    print(groupby, \"\\n\")\n",
    "\n",
    "    print(\"Predictions grouped, all rna types = rna\")\n",
    "    tmp_df = subset.copy()\n",
    "    tmp_df.loc[:, ASSAY] = \"rna_seq\"\n",
    "    tmp_df.loc[:, f\"Predicted class ({ASSAY})\"].replace(\n",
    "        \"mrna_seq\", \"rna_seq\", inplace=True\n",
    "    )\n",
    "    groupby = (\n",
    "        tmp_df.groupby([ASSAY, f\"Predicted class ({ASSAY})\"])\n",
    "        .size()\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"Count\"})\n",
    "        .sort_values(by=[ASSAY, \"Count\"], ascending=[True, False])\n",
    "    )\n",
    "    print(groupby, \"\\n\")\n",
    "\n",
    "    print(\"Breakdown by assay type\")\n",
    "    assay_breakdown = subset[ASSAY].value_counts(dropna=False)\n",
    "    print(assay_breakdown / assay_breakdown.sum(), \"\\n\")\n",
    "    for assay_type in assay_breakdown.index:\n",
    "        assay_type_subset = subset[subset[ASSAY] == assay_type].copy()\n",
    "\n",
    "        counts = assay_type_subset[f\"Predicted class ({ASSAY})\"].value_counts()\n",
    "        N_subset = counts.sum()\n",
    "        counts_perc = counts / N_subset\n",
    "        correct_perc = counts_perc[\"rna_seq\"] + counts_perc[\"mrna_seq\"]\n",
    "        print(f\"{assay_type} acc: {correct_perc:.2%}\\n\")\n",
    "        print(f\"{assay_type} preds:\\n{counts_perc}\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for max_pred in [0, 0.6, 0.8]:\n",
    "    subset = full_df[full_df[f\"Max pred ({ASSAY})\"] >= max_pred]\n",
    "    print(f\"min_PredScore >= {max_pred}\")\n",
    "\n",
    "    for cat in [SEX, CANCER, LIFE_STAGE]:\n",
    "        pred_label = f\"Predicted class ({cat})\"\n",
    "        true_label = f\"Expected class ({cat})\"\n",
    "\n",
    "        if cat == CANCER:\n",
    "            subset = subset.replace(\"healthy\", \"non-cancer\")\n",
    "\n",
    "        known_pred = subset[subset[true_label] != \"unknown\"]\n",
    "        if cat == LIFE_STAGE:\n",
    "            known_pred = known_pred[known_pred[true_label] != \"children\"]\n",
    "        # print(known_pred[true_label].value_counts(dropna=False))\n",
    "\n",
    "        classes = sorted(\n",
    "            set(known_pred[pred_label].unique()) | set(known_pred[pred_label].unique())\n",
    "        )\n",
    "\n",
    "        N_known = known_pred.shape[0]\n",
    "        N_unknown = subset.shape[0] - N_known\n",
    "        # print(f\"Unknown (%): {(N_unknown)/subset.shape[0]*100:.2f}\")\n",
    "\n",
    "        y_pred = known_pred[pred_label]\n",
    "        y_true = known_pred[true_label]\n",
    "        N_correct = (y_pred == y_true).sum()\n",
    "        print(f\"{cat} prediction match (%): {N_correct/N_known*100:.2f}\")\n",
    "\n",
    "        print(classification_report(y_true, y_pred, target_names=classes, zero_division=0) + \"\\n\")  # type: ignore\n",
    "\n",
    "        print(f\"confusion matrix classes row order: {classes}\")\n",
    "        cm = sk_cm(y_true, y_pred, normalize=\"true\", labels=classes)\n",
    "        print(str(cm) + \"\\n\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check for RNA Unique_raw tracks (unstranded data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = table_dir / \"experiments_including_unique_raw_files.list\"\n",
    "outfile.unlink(missing_ok=True)\n",
    "outfile.touch()\n",
    "\n",
    "v2_meta_df = metadata_handler.load_metadata_df(\"v2\", merge_assays=False)\n",
    "md5_unique_raw = v2_meta_df[v2_meta_df[\"track_type\"] == \"Unique_raw\"].index.unique()\n",
    "\n",
    "with outfile.open(\"w\", encoding=\"utf-8\") as out:\n",
    "    print(f\"Total Unique_raw md5sums: {len(md5_unique_raw)}\", file=out)\n",
    "    for pred_file in table_dir.rglob(\"*pred*.csv\"):\n",
    "        if any(label in str(pred_file) for label in [\"recount3\", \"encode\"]):\n",
    "            continue\n",
    "        df = pd.read_csv(pred_file)\n",
    "\n",
    "        # Get md5sums\n",
    "        try:\n",
    "            md5sums = set(df[\"md5sum\"])\n",
    "        except KeyError:\n",
    "            if isinstance(df.index[0], str) and len(df.index[0]) == 32:\n",
    "                md5sums = set(df.index)\n",
    "            else:\n",
    "                print(f\"Could not find md5sum column in {pred_file}\", file=out)\n",
    "                continue\n",
    "\n",
    "        shared_md5sums = md5sums.intersection(md5_unique_raw)\n",
    "\n",
    "        pred_file_relpath = pred_file.relative_to(table_dir)\n",
    "        print(f\"{pred_file_relpath}: {len(shared_md5sums)}\", file=out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
