{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Figure core creation: Fig1\n",
    "\n",
    "Formatting of the figures may not be identical to the paper, but they contain the same data points.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    merge_similar_assays,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Have a slimed down data directory that only uses the necessary files for figure creation, compress it, and use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "paper_dir = base_dir\n",
    "if not paper_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {paper_dir} does not exist.\")\n",
    "\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()\n",
    "\n",
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata_v2 = metadata_handler.load_metadata(\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL_CLASSIFIERS = [\"NN\", \"LR\", \"LGBM\", \"LinearSVC\", \"RF\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supp Fig 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 1A,1B - All classifiers metrics on EpiAtlas - Assay and Sample Ontology - 100kb resolution\n",
    "\n",
    "Fig 1A,1B: data points are included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_split_metrics(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    label_category: str,\n",
    "    logdir: Path | None = None,\n",
    "    filename: str = \"fig1_all_classifiers_metrics\",\n",
    ") -> None:\n",
    "    \"\"\"Render to box plots the metrics per classifier and split, each in its own subplot.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A dictionary containing metric scores for each classifier and split.\n",
    "        label_category: The label category for the classification task.\n",
    "        name: The name of the figure.\n",
    "        logdir: The directory to save the figure to. If None, the figure is only displayed.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the figure and saves it to the logdir if provided.\n",
    "    \"\"\"\n",
    "    metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "    classifier_names = list(next(iter(split_metrics.values())).keys())\n",
    "    classifier_names = [\"NN\", \"LR\", \"LGBM\", \"LinearSVC\", \"RF\"]\n",
    "\n",
    "    # Create subplots, one row for each metric\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(metrics),\n",
    "        subplot_titles=metrics,\n",
    "        horizontal_spacing=0.075,\n",
    "    )\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        for classifier in classifier_names:\n",
    "            values = [split_metrics[split][classifier][metric] for split in split_metrics]\n",
    "            if classifier == \"NN\":\n",
    "                classifier = \"MLP\"\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=values,\n",
    "                    name=classifier,\n",
    "                    line=dict(color=\"black\", width=1.5),\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",  # or \"outliers\" to show only outliers\n",
    "                    pointpos=-1.4,\n",
    "                    showlegend=False,\n",
    "                    width=0.5,\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in zip(split_metrics, values)\n",
    "                    ],\n",
    "                ),\n",
    "                row=1,\n",
    "                col=i + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{label_category} classification - Metric distribution for 10fold cross-validation\",\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode=\"group\",\n",
    "    )\n",
    "\n",
    "    # Adjust y-axis\n",
    "    if label_category == ASSAY:\n",
    "        range_acc = [0.95, 1.001]\n",
    "        range_AUC = [0.992, 1.0001]\n",
    "    elif label_category == CELL_TYPE:\n",
    "        range_acc = [0.81, 1]\n",
    "        range_AUC = [0.96, 1]\n",
    "    else:\n",
    "        range_acc = [0.6, 1.001]\n",
    "        range_AUC = [0.9, 1.0001]\n",
    "\n",
    "    fig.update_layout(yaxis=dict(range=range_acc))\n",
    "    fig.update_layout(yaxis2=dict(range=range_acc))\n",
    "    fig.update_layout(yaxis3=dict(range=range_AUC))\n",
    "    fig.update_layout(yaxis4=dict(range=range_AUC))\n",
    "\n",
    "    # Save figure\n",
    "    if logdir:\n",
    "        fig.write_image(logdir / f\"{filename}.svg\")\n",
    "        fig.write_image(logdir / f\"{filename}.png\")\n",
    "        fig.write_html(logdir / f\"{filename}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_100kb = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "if not data_dir_100kb.exists():\n",
    "    raise FileNotFoundError(f\"Directory {data_dir_100kb} does not exist.\")\n",
    "\n",
    "merge_assays = False\n",
    "\n",
    "for label_category in [ASSAY, CELL_TYPE]:\n",
    "    all_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "        results_dir=data_dir_100kb, label_category=label_category\n",
    "    )\n",
    "\n",
    "    if merge_assays and label_category == ASSAY:\n",
    "        for split_name, split_dfs in all_split_dfs.items():\n",
    "            for classifier_type, df in split_dfs.items():\n",
    "                all_split_dfs[split_name][classifier_type] = merge_similar_assays(df)\n",
    "\n",
    "    split_metrics = split_results_handler.compute_split_metrics(all_split_dfs)\n",
    "\n",
    "    plot_split_metrics(\n",
    "        split_metrics,\n",
    "        label_category=label_category,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Going forward, all results are for MLP classifiers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 1C - Metrics for zeroed blacklist values and winsorized files - 100kb resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blklst_graphs(\n",
    "    feature_set_metrics_dict: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n",
    "    logdir: Path | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Create boxplots for blacklisted related feature sets.\n",
    "\n",
    "    Args:\n",
    "        feature_set_metrics_dict (Dict[str, Dict[str, Dict[str, Dict[str, float]]]]): The dictionary containing all metrics for all blklst related feature sets.\n",
    "            format: {feature_set: {task_name: {split_name: metric_dict}}}\n",
    "        logdir (Path, Optional): The directory to save the figure to. If None, the figure is only displayed.\n",
    "    \"\"\"\n",
    "    # Assume names exist in all feature sets\n",
    "    task_names = list(feature_set_metrics_dict.values())[0].keys()\n",
    "\n",
    "    traces_names_dict = {\n",
    "        \"hg38_100kb_all_none\": \"observed\",\n",
    "        \"hg38_100kb_all_none_0blklst\": \"0blklst\",\n",
    "        \"hg38_100kb_all_none_0blklst_winsorized\": \"0blklst_winsorized\",\n",
    "    }\n",
    "\n",
    "    for task_name in task_names:\n",
    "        category_fig = make_subplots(\n",
    "            rows=1,\n",
    "            cols=2,\n",
    "            shared_yaxes=False,\n",
    "            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n",
    "            x_title=\"Feature set\",\n",
    "            y_title=\"Metric value\",\n",
    "            horizontal_spacing=0.1,\n",
    "        )\n",
    "        for feature_set_name, tasks_dicts in feature_set_metrics_dict.items():\n",
    "            task_dict = tasks_dicts[task_name]\n",
    "            trace_name = traces_names_dict[feature_set_name]\n",
    "\n",
    "            # Accuracy\n",
    "            metric = \"Accuracy\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "            metric = \"F1_macro\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=2,\n",
    "            )\n",
    "\n",
    "        category_fig.update_xaxes(\n",
    "            categoryorder=\"array\",\n",
    "            categoryarray=list(traces_names_dict.values()),\n",
    "        )\n",
    "        category_fig.update_yaxes(range=[0.85, 1.001])\n",
    "\n",
    "        task_name = task_name.replace(\"_1l_3000n-10fold\", \"\")\n",
    "        category_fig.update_layout(\n",
    "            title=f\"MLP performance<br>{task_name}\",\n",
    "        )\n",
    "\n",
    "        width = 500\n",
    "        height = width * 1.5\n",
    "        category_fig.update_layout(\n",
    "            autosize=False,\n",
    "            width=width,\n",
    "            height=height,\n",
    "        )\n",
    "        # Save figure\n",
    "        if logdir:\n",
    "            base_name = f\"metrics_{task_name}\"\n",
    "            category_fig.write_html(logdir / f\"{base_name}.html\")\n",
    "            category_fig.write_image(logdir / f\"{base_name}.svg\")\n",
    "            category_fig.write_image(logdir / f\"{base_name}.png\")\n",
    "\n",
    "        category_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_sets = [\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_100kb_all_none_0blklst\",\n",
    "    \"hg38_100kb_all_none_0blklst_winsorized\",\n",
    "]\n",
    "\n",
    "results_folder_blklst = base_data_dir / \"training_results\" / \"2023-01-epiatlas-freeze\"\n",
    "if not results_folder_blklst.exists():\n",
    "    raise FileNotFoundError(f\"Folder '{results_folder_blklst}' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10-fold oversampling runs\n",
    "# expected result shape: {feature_set: {task_name: {split_name: metrics_dict}}}\n",
    "all_metrics: Dict[\n",
    "    str, Dict[str, Dict[str, Dict[str, float]]]\n",
    "] = split_results_handler.obtain_all_feature_set_data(\n",
    "    return_type=\"metrics\",\n",
    "    parent_folder=results_folder_blklst,\n",
    "    merge_assays=False,\n",
    "    include_categories=[ASSAY, CELL_TYPE],\n",
    "    include_sets=include_sets,\n",
    "    oversampled_only=False,\n",
    "    verbose=False,\n",
    ")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_blklst_graphs(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 1D - Accuracy per assay + confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_performance_per_assay_across_categories(\n",
    "    all_split_results: Dict[str, Dict[str, pd.DataFrame]],\n",
    "    title_end: str = \"\",\n",
    "    exclude_categories: List[str] | None = None,\n",
    "    y_range: None | List[float] = None,\n",
    "    logdir: Path | None = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"Create a box plot of assay accuracy for each classifier.\n",
    "\n",
    "    all_split_results (Dict[str, Dict[str, pd.DataFrame]]): The dictionary containing all split results for each classifier.\n",
    "    title_end (str, optional): The title to append to the figure title.\n",
    "    exclude_categories (List[str], optional): The categories to exclude from the figure.\n",
    "    y_range (None | List[float], optional): The y-axis range for the figure.\n",
    "    logdir (Path, optional): The directory to save the figure to. If None, the figure is only displayed.\n",
    "    verbose (bool, optional): Whether to print progress information.\n",
    "    \"\"\"\n",
    "    all_split_results = copy.deepcopy(all_split_results)\n",
    "\n",
    "    # Exclude some categories\n",
    "    classifier_names = list(all_split_results.keys())\n",
    "    if exclude_categories is not None:\n",
    "        for category in exclude_categories:\n",
    "            classifier_names = [c for c in classifier_names if category not in c]\n",
    "\n",
    "    metadata_df = MetadataHandler(paper_dir).load_metadata_df(\"v2-encode\")\n",
    "\n",
    "    # One graph per metadata category\n",
    "    for task_name in classifier_names:\n",
    "        if verbose:\n",
    "            print(f\"Processing {task_name}\")\n",
    "        split_results = all_split_results[task_name]\n",
    "        if ASSAY in task_name:\n",
    "            for split_name in split_results:\n",
    "                try:\n",
    "                    split_results[split_name] = merge_similar_assays(\n",
    "                        split_results[split_name]\n",
    "                    )\n",
    "                except ValueError as e:\n",
    "                    print(f\"Skipping {task_name} assay merging: {e}\")\n",
    "                    break\n",
    "\n",
    "        assay_acc_df = split_results_handler.compute_acc_per_assay(\n",
    "            split_results, metadata_df\n",
    "        )\n",
    "\n",
    "        fig = go.Figure()\n",
    "        for assay in ASSAY_ORDER:\n",
    "            try:\n",
    "                assay_accuracies = assay_acc_df[assay]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=assay_accuracies.values,\n",
    "                    name=assay,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=True,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=assay_colors[assay],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in assay_accuracies.items()\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        yrange = [assay_acc_df.min(), 1.001]  # type: ignore\n",
    "        if y_range is not None:\n",
    "            yrange = y_range\n",
    "\n",
    "        fig.update_yaxes(range=yrange)\n",
    "\n",
    "        title_text = f\"NN classification - {task_name}\"\n",
    "        if title_end:\n",
    "            title_text += f\" - {title_end}\"\n",
    "        fig.update_layout(\n",
    "            title_text=title_text,\n",
    "            yaxis_title=\"Accuracy\",\n",
    "            xaxis_title=\"Assay\",\n",
    "            width=1000,\n",
    "            height=700,\n",
    "        )\n",
    "\n",
    "        # Save figure\n",
    "        if logdir:\n",
    "            filename = \"NN_assay_performance_\" + task_name\n",
    "            fig.write_image(logdir / f\"{filename}.svg\")\n",
    "            fig.write_image(logdir / f\"{filename}.png\")\n",
    "            fig.write_html(logdir / f\"{filename}.html\")\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    min_pred_score: float = 0,\n",
    "    majority: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Create a confusion matrix for the given DataFrame and save it to the logdir.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the results.\n",
    "        logdir (Path): The directory path for saving the figures.\n",
    "        name (str): The name for the saved figures.\n",
    "        min_pred_score (float): The minimum prediction score to consider.\n",
    "        majority (bool): Whether to use majority vote (uuid-wise) for the predicted class.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    classes = sorted(df[\"True class\"].unique())\n",
    "    if \"Max pred\" not in df.columns:\n",
    "        df[\"Max pred\"] = df[classes].max(axis=1)  # type: ignore\n",
    "    filtered_df = df[df[\"Max pred\"] > min_pred_score]\n",
    "\n",
    "    if majority:\n",
    "        # Majority vote for predicted class\n",
    "        groupby_uuid = filtered_df.groupby([\"uuid\", \"True class\", \"Predicted class\"])[\n",
    "            \"Max pred\"\n",
    "        ].aggregate([\"size\", \"mean\"])\n",
    "\n",
    "        if groupby_uuid[\"size\"].max() > 3:\n",
    "            raise ValueError(\"More than three predictions for the same uuid.\")\n",
    "\n",
    "        groupby_uuid = groupby_uuid.reset_index().sort_values(\n",
    "            [\"uuid\", \"True class\", \"size\"], ascending=[True, True, False]\n",
    "        )\n",
    "        groupby_uuid = groupby_uuid.drop_duplicates(\n",
    "            subset=[\"uuid\", \"True class\"], keep=\"first\"\n",
    "        )\n",
    "        filtered_df = groupby_uuid\n",
    "\n",
    "    confusion_mat = sk_cm(\n",
    "        filtered_df[\"True class\"], filtered_df[\"Predicted class\"], labels=classes\n",
    "    )\n",
    "\n",
    "    mat_writer = ConfusionMatrixWriter(labels=classes, confusion_matrix=confusion_mat)\n",
    "    files = mat_writer.to_all_formats(logdir, name=f\"{name}_n{len(filtered_df)}\")\n",
    "    print(f\"Saved confusion matrix to {logdir}:\")\n",
    "    for file in files:\n",
    "        print(Path(file).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "    results_dir=data_dir_100kb, label_category=ASSAY, only_NN=True\n",
    ")\n",
    "concat_assay_df = split_results_handler.concatenate_split_results(assay_split_dfs)[\"NN\"]\n",
    "\n",
    "df_with_meta = metadata_handler.join_metadata(concat_assay_df, metadata_v2)\n",
    "if \"Predicted class\" not in df_with_meta.columns:\n",
    "    raise ValueError(\"`Predicted class` not in DataFrame\")\n",
    "\n",
    "classifier_name = \"MLP\"\n",
    "min_pred_score = 0\n",
    "majority = False\n",
    "\n",
    "name = f\"{classifier_name}_pred>{min_pred_score}\"\n",
    "\n",
    "logdir = base_fig_dir / \"fig1_EpiAtlas_assay\" / \"fig1_supp_D-assay_c11_confusion_matrices\"\n",
    "if majority:\n",
    "    logdir = logdir / \"per_uuid\"\n",
    "else:\n",
    "    logdir = logdir / \"per_file\"\n",
    "logdir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_confusion_matrix(\n",
    "    df=df_with_meta,\n",
    "    min_pred_score=min_pred_score,\n",
    "    logdir=logdir,\n",
    "    name=name,\n",
    "    majority=majority,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_task: Dict[str, Dict[str, pd.DataFrame]] = {ASSAY: split_results_handler.invert_metrics_dict(assay_split_dfs)[\"NN\"]}  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_performance_per_assay_across_categories(all_split_results=results_per_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 1E,1F,1G - Prediction score per assay\n",
    "\n",
    "- E: assay_epiclass 10fold validation\n",
    "- F: assay_epiclass complete training, prediction scores on imputed data\n",
    "- G: sample_ontology 10fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
