{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Figure core creation: Fig1\n",
    "\n",
    "Formatting of the figures may not be identical to the paper, but they contain the same data points.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, too-many-branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import auc, confusion_matrix as sk_cm, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.metrics_per_assay import MetricsPerAssay\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    SEX,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    extract_input_sizes_from_output_files,\n",
    "    merge_similar_assays,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Have a slimed down data directory that only uses the necessary files for figure creation, compress it, and use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "paper_dir = base_dir\n",
    "if not paper_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {paper_dir} does not exist.\")\n",
    "\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()\n",
    "\n",
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata_v2 = metadata_handler.load_metadata(\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "if not gen_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {gen_data_dir} does not exist.\")\n",
    "\n",
    "data_dir_100kb = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "if not data_dir_100kb.exists():\n",
    "    raise FileNotFoundError(f\"Directory {data_dir_100kb} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 1A, 1B\n",
    "\n",
    "See `Supp Fig 1A,1B` for `Fig 1A,1B` results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 1C,1D - Various feature sets performance - Assay and Sample Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "\n",
    "feature_sets_14 = [\n",
    "    \"hg38_10mb_all_none_1mb_coord\",\n",
    "    \"hg38_100kb_random_n316_none\",\n",
    "    \"hg38_1mb_all_none\",\n",
    "    \"hg38_100kb_random_n3044_none\",\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_gene_regions_100kb_coord_n19864\",\n",
    "    \"hg38_10kb_random_n30321_none\",\n",
    "    \"hg38_regulatory_regions_n30321\",\n",
    "    \"hg38_1kb_random_n30321_none\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n",
    "    \"hg38_10kb_all_none\",\n",
    "    \"hg38_regulatory_regions_n303114\",\n",
    "    \"hg38_1kb_random_n303114_none\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n300k\",\n",
    "]\n",
    "fig1_sets = [\n",
    "    \"hg38_10mb_all_none_1mb_coord\",\n",
    "    \"hg38_100kb_random_n316_none\",\n",
    "    \"hg38_1mb_all_none\",\n",
    "    \"hg38_100kb_random_n3044_none\",\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_10kb_random_n30321_none\",\n",
    "    \"hg38_1kb_random_n30321_none\",\n",
    "    \"hg38_10kb_all_none\",\n",
    "    \"hg38_1kb_random_n303114_none\",\n",
    "]\n",
    "flagship_selection_4cat = [\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_gene_regions_100kb_coord_n19864\",\n",
    "    \"hg38_regulatory_regions_n30321\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n",
    "]\n",
    "metric_orders_map = {\n",
    "    \"feature_sets_14\": feature_sets_14,\n",
    "    \"fig1_sets\": fig1_sets,\n",
    "    \"flagship_selection_4cat\": flagship_selection_4cat,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sizes = extract_input_sizes_from_output_files(gen_data_dir)  # type: ignore\n",
    "input_sizes: Dict[str, int] = {k: v.pop() for k, v in input_sizes.items() if len(v) == 1}  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_selection_name = \"fig1_sets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = (\n",
    "    base_fig_dir\n",
    "    / \"fig2_EpiAtlas_other\"\n",
    "    / \"fig2--reduced_feature_sets\"\n",
    "    / set_selection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = split_results_handler.obtain_all_feature_set_data(\n",
    "    parent_folder=gen_data_dir,\n",
    "    merge_assays=True,\n",
    "    return_type=\"metrics\",\n",
    "    include_categories=[ASSAY, CELL_TYPE],\n",
    "    include_sets=metric_orders_map[set_selection_name],\n",
    "    exclude_names=[\"16ct\", \"27ct\", \"7c\", \"chip-seq-only\"],\n",
    ")\n",
    "all_metrics = {\n",
    "    name: all_metrics[name]  # type: ignore\n",
    "    for name in metric_orders_map[set_selection_name]\n",
    "    if name in all_metrics\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct a name\n",
    "try:\n",
    "    all_metrics[\"hg38_100kb_all_none\"][ASSAY] = all_metrics[\"hg38_100kb_all_none\"][  # type: ignore\n",
    "        f\"{ASSAY}_11c\"\n",
    "    ]\n",
    "    del all_metrics[\"hg38_100kb_all_none\"][f\"{ASSAY}_11c\"]\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_colors = {\n",
    "    \"100kb\": px.colors.qualitative.Safe[0],\n",
    "    \"10kb\": px.colors.qualitative.Safe[1],\n",
    "    \"1kb\": px.colors.qualitative.Safe[2],\n",
    "    \"regulatory\": px.colors.qualitative.Safe[3],\n",
    "    \"gene\": px.colors.qualitative.Safe[4],\n",
    "    \"cpg\": px.colors.qualitative.Safe[5],\n",
    "    \"1mb\": px.colors.qualitative.Safe[6],\n",
    "    \"5mb\": px.colors.qualitative.Safe[7],\n",
    "    \"10mb\": px.colors.qualitative.Safe[8],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_feature_set_metrics(\n",
    "    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n",
    "    input_sizes: Dict[str, int],\n",
    "    logdir: Path | None = None,\n",
    "    sort_by_input_size: bool = False,\n",
    "    name: str | None = None,\n",
    "    y_range: Tuple[float, float] | None = None,\n",
    "    boxpoints: str = \"all\",\n",
    ") -> None:\n",
    "    \"\"\"Graph the metrics for all feature sets.\n",
    "\n",
    "    Args:\n",
    "        all_metrics (Dict[str, Dict[str, Dict[str, Dict[str, float]]]): A dictionary containing all metrics for all feature sets.\n",
    "            Format: {feature_set: {task_name: {split_name: metric_dict}}}\n",
    "        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n",
    "        logdir (Path): The directory where the figure will be saved. If None, the figure will only be displayed.\n",
    "        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n",
    "        name (str|None): The name of the figure.\n",
    "        y_range (Tuple[float, float]|None): The y-axis range for the figure.\n",
    "        boxpoints (str): The type of boxpoints to display. Can be \"all\" or \"outliers\". Defaults to \"all\".\n",
    "    \"\"\"\n",
    "    if boxpoints not in [\"all\", \"outliers\"]:\n",
    "        raise ValueError(\"Invalid boxpoints value.\")\n",
    "\n",
    "    reference_hdf5_type = \"hg38_100kb_all_none\"\n",
    "    metadata_categories = list(all_metrics[reference_hdf5_type].keys())\n",
    "\n",
    "    non_standard_names = {ASSAY: f\"{ASSAY}_11c\", SEX: f\"{SEX}_w-mixed\"}\n",
    "    non_standard_assay_task_names = [\"hg38_100kb_all_none\"]\n",
    "    non_standard_sex_task_name = [\n",
    "        \"hg38_100kb_all_none\",\n",
    "        \"hg38_regulatory_regions_n30321\",\n",
    "        \"hg38_regulatory_regions_n303114\",\n",
    "    ]\n",
    "    used_resolutions = set()\n",
    "    for i in range(len(metadata_categories)):\n",
    "        category_idx = i\n",
    "        category_fig = make_subplots(\n",
    "            rows=1,\n",
    "            cols=2,\n",
    "            shared_yaxes=True,\n",
    "            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n",
    "            x_title=\"Feature set\",\n",
    "            y_title=\"Metric value\",\n",
    "        )\n",
    "\n",
    "        trace_names = []\n",
    "        order = list(all_metrics.keys())\n",
    "        if sort_by_input_size:\n",
    "            order = sorted(\n",
    "                all_metrics.keys(),\n",
    "                key=lambda x: input_sizes[x],\n",
    "            )\n",
    "        for feature_set_name in order:\n",
    "            # print(feature_set_name)\n",
    "            tasks_dicts = all_metrics[feature_set_name]\n",
    "            meta_categories = copy.deepcopy(metadata_categories)\n",
    "\n",
    "            if feature_set_name not in input_sizes:\n",
    "                print(f\"Skipping {feature_set_name}, no input size found.\")\n",
    "                continue\n",
    "\n",
    "            task_name = meta_categories[category_idx]\n",
    "            if \"split\" in task_name:\n",
    "                raise ValueError(\"Split in task name. Wrong metrics dict.\")\n",
    "\n",
    "            try:\n",
    "                task_dict = tasks_dicts[task_name]\n",
    "            except KeyError as err:\n",
    "                if SEX in str(err) and feature_set_name in non_standard_sex_task_name:\n",
    "                    task_dict = tasks_dicts[non_standard_names[SEX]]\n",
    "                elif (\n",
    "                    ASSAY in str(err)\n",
    "                    and feature_set_name in non_standard_assay_task_names\n",
    "                ):\n",
    "                    task_dict = tasks_dicts[non_standard_names[ASSAY]]\n",
    "                else:\n",
    "                    print(\"Skipping\", feature_set_name, task_name)\n",
    "                    continue\n",
    "\n",
    "            input_size = input_sizes[feature_set_name]\n",
    "\n",
    "            feature_set_name = feature_set_name.replace(\"_none\", \"\")\n",
    "            feature_set_name = feature_set_name.replace(\"hg38_\", \"\")\n",
    "\n",
    "            resolution = feature_set_name.split(\"_\")[0]\n",
    "            used_resolutions.add(resolution)\n",
    "\n",
    "            trace_name = f\"{input_size}|{feature_set_name}\"\n",
    "            trace_names.append(trace_name)\n",
    "\n",
    "            # Accuracy\n",
    "            metric = \"Accuracy\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=boxpoints,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=resolution_colors[resolution],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                    legendgroup=resolution,\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "            metric = \"F1_macro\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=boxpoints,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=resolution_colors[resolution],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                    legendgroup=resolution,\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=2,\n",
    "            )\n",
    "\n",
    "        title = f\"Neural network performance - {metadata_categories[category_idx]}\"\n",
    "        if name is not None:\n",
    "            title += f\" - {name}\"\n",
    "        category_fig.update_layout(\n",
    "            width=1500,\n",
    "            height=1500,\n",
    "            title=title,\n",
    "        )\n",
    "\n",
    "        # dummy scatters for resolution colors\n",
    "        for resolution, color in resolution_colors.items():\n",
    "            if resolution not in used_resolutions:\n",
    "                continue\n",
    "            category_fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[None],\n",
    "                    y=[None],\n",
    "                    mode=\"markers\",\n",
    "                    name=resolution,\n",
    "                    marker=dict(color=color, size=5),\n",
    "                    showlegend=True,\n",
    "                    legendgroup=resolution,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        category_fig.update_layout(legend=dict(itemsizing=\"constant\"))\n",
    "\n",
    "        # y-axis\n",
    "        if y_range:\n",
    "            category_fig.update_yaxes(range=y_range)\n",
    "        else:\n",
    "            if ASSAY in task_name:\n",
    "                category_fig.update_yaxes(range=[0.96, 1.001])\n",
    "            if CELL_TYPE in task_name:\n",
    "                category_fig.update_yaxes(range=[0.75, 1])\n",
    "\n",
    "        # Save figure\n",
    "        if logdir:\n",
    "            base_name = f\"feature_set_metrics_{metadata_categories[category_idx]}\"\n",
    "            if name is not None:\n",
    "                base_name = base_name + f\"_{name}\"\n",
    "            category_fig.write_html(logdir / f\"{base_name}.html\")\n",
    "            category_fig.write_image(logdir / f\"{base_name}.svg\")\n",
    "            category_fig.write_image(logdir / f\"{base_name}.png\")\n",
    "\n",
    "        category_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_feature_set_metrics(\n",
    "    all_metrics=all_metrics,  # type: ignore\n",
    "    input_sizes=input_sizes,\n",
    "    boxpoints=\"all\",\n",
    "    name=\"feature_sets_boxplots\",\n",
    "    logdir=logdir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bin_size(feature_set_name: str) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Parses the feature set name to extract a numerical bin size in base pairs.\n",
    "    Handles formats like '100kb', '5mb', 'regulatory', 'gene', 'cpg'.\n",
    "\n",
    "    Returns numerical size (float) or None if unparseable or non-numeric.\n",
    "    Assigns placeholder values for non-genomic-range types if needed,\n",
    "    but for a continuous axis, it's better to return None or filter later.\n",
    "    \"\"\"\n",
    "    name_parts = feature_set_name.replace(\"hg38_\", \"\").split(\"_\")\n",
    "    if not name_parts:\n",
    "        return None\n",
    "\n",
    "    resolution_str = name_parts[0].lower()\n",
    "\n",
    "    # Handle standard genomic ranges\n",
    "    match_kb = re.match(r\"(\\d+)kb\", resolution_str)\n",
    "    if match_kb:\n",
    "        return float(match_kb.group(1)) * 1_000\n",
    "    match_mb = re.match(r\"(\\d+)mb\", resolution_str)\n",
    "    if match_mb:\n",
    "        return float(match_mb.group(1)) * 1_000_000\n",
    "\n",
    "    # Handle non-range types - decide how to represent them.\n",
    "    # Option 1: Return None (they won't be plotted on the numeric axis)\n",
    "    # Option 2: Assign arbitrary numbers (might distort scale)\n",
    "    # Option 3: Could use different marker symbols later if needed\n",
    "    if resolution_str in [\"regulatory\", \"gene\", \"cpg\"]:\n",
    "        # For now, let's return None so they are filtered out from the numeric plot\n",
    "        # Or assign a placeholder if you want to handle them differently:\n",
    "        # if resolution_str == 'regulatory': return 1e1 # Example placeholder\n",
    "        # if resolution_str == 'gene': return 1e2 # Example placeholder\n",
    "        # if resolution_str == 'cpg': return 1e0 # Example placeholder\n",
    "        return None  # Returning None is cleaner for a pure numeric axis\n",
    "\n",
    "    # Fallback for unrecognised formats\n",
    "    try:\n",
    "        # Maybe it's just a number (e.g., representing window size)?\n",
    "        return float(resolution_str)\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_feature_set_scatter(\n",
    "    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n",
    "    input_sizes: Dict[str, int],\n",
    "    logdir: Optional[Path] = None,\n",
    "    metric_to_plot: str = \"Accuracy\",\n",
    "    name: Optional[str] = None,\n",
    "    metric_range: Optional[Tuple[float, float]] = None,\n",
    "    assay_task_key: str = ASSAY,\n",
    "    sex_task_key: str = SEX,\n",
    "    cell_type_task_key: str = CELL_TYPE,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Graphs performance metrics as a scatter plot with modifications.\n",
    "\n",
    "    X-axis: Number of Features (log scale).\n",
    "    Y-axis: Average performance metric (e.g., Accuracy, F1_macro) across folds.\n",
    "            Vertical lines indicate the min/max range across folds.\n",
    "    Color: Bin Size (bp, log scale).\n",
    "\n",
    "    Args:\n",
    "        all_metrics: Nested dict {feature_set: {task_name: {split_name: metric_dict}}}.\n",
    "        input_sizes: Dict {feature_set: num_features}.\n",
    "        logdir: Directory to save figures. If None, display only.\n",
    "        metric_to_plot: The metric key to use for the Y-axis ('Accuracy', 'F1_macro').\n",
    "        name: Optional suffix for figure titles and filenames.\n",
    "        metric_range: Optional tuple (min, max) to set the Y-axis range.\n",
    "        assay_task_key: Key used for the assay prediction task.\n",
    "        sex_task_key: Key used for the sex prediction task.\n",
    "        cell_type_task_key: Key used for the cell type prediction task.\n",
    "    \"\"\"\n",
    "    if metric_to_plot not in [\"Accuracy\", \"F1_macro\"]:\n",
    "        raise ValueError(\"metric_to_plot must be 'Accuracy' or 'F1_macro'\")\n",
    "\n",
    "    # --- Standard Name Handling (simplified from original) ---\n",
    "    non_standard_names = {ASSAY: f\"{ASSAY}_11c\", SEX: f\"{SEX}_w-mixed\"}\n",
    "    # These lists are no longer strictly needed by the simplified lookup, but kept for context\n",
    "    # non_standard_assay_task_names = [\"hg38_100kb_all_none\"]\n",
    "    # non_standard_sex_task_name = [\n",
    "    #     \"hg38_100kb_all_none\",\n",
    "    #     \"hg38_regulatory_regions_n30321\",\n",
    "    #     \"hg38_regulatory_regions_n303114\",\n",
    "    # ]\n",
    "\n",
    "    # --- Find reference and task names ----\n",
    "    reference_hdf5_type = next(iter(all_metrics), None)\n",
    "    if reference_hdf5_type is None or not all_metrics.get(reference_hdf5_type):\n",
    "        print(\n",
    "            \"Warning: Could not determine tasks from all_metrics. Trying default tasks.\"\n",
    "        )\n",
    "        cleaned_metadata_categories = {assay_task_key, sex_task_key, cell_type_task_key}\n",
    "    else:\n",
    "        metadata_categories = list(all_metrics[reference_hdf5_type].keys())\n",
    "        cleaned_metadata_categories = set()\n",
    "        for cat in metadata_categories:\n",
    "            original_name = cat\n",
    "            for standard, non_standard in non_standard_names.items():\n",
    "                if cat == non_standard:\n",
    "                    original_name = standard\n",
    "                    break\n",
    "            cleaned_metadata_categories.add(original_name)\n",
    "\n",
    "    # --- Define Bin size categories and Colors ---\n",
    "    bin_category_names = [\"1Kb\", \"10Kb\", \"100Kb\", \"1Mb\", \"10Mb\"]\n",
    "    bin_category_values = [1000, 10000, 100 * 1000, 1000 * 1000, 10000 * 1000]\n",
    "    discrete_colors = px.colors.sequential.Viridis_r\n",
    "    color_map = {\n",
    "        name: discrete_colors[i * 2] for i, name in enumerate(bin_category_names)\n",
    "    }\n",
    "\n",
    "    print(f\"Plotting for tasks: {list(cleaned_metadata_categories)}\")\n",
    "    for category_name in cleaned_metadata_categories:\n",
    "        plot_data_points = []\n",
    "\n",
    "        for feature_set_name_orig in all_metrics.keys():\n",
    "            try:\n",
    "                num_features = input_sizes[feature_set_name_orig]\n",
    "            except KeyError as e:\n",
    "                raise ValueError(\n",
    "                    f\"Feature set '{feature_set_name_orig}' not found in input_sizes\"\n",
    "                ) from e\n",
    "\n",
    "            # Parse Bin Size\n",
    "            bin_size = parse_bin_size(feature_set_name_orig)\n",
    "            if bin_size is None:\n",
    "                print(\n",
    "                    f\"Skipping {feature_set_name_orig}, could not parse numeric bin size.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # 3. Get Metric Values (Average, Min, Max)\n",
    "            tasks_dicts = all_metrics[feature_set_name_orig]\n",
    "\n",
    "            # --- Task Name Lookup ---\n",
    "            # 1. Try the standard category name first\n",
    "            # 2. If standard name not found, use non-standard name\n",
    "            task_dict = None\n",
    "            task_name = category_name\n",
    "            if category_name in tasks_dicts:\n",
    "                task_dict = tasks_dicts[category_name]\n",
    "            else:\n",
    "                non_standard_task_name = non_standard_names.get(category_name)\n",
    "                if non_standard_task_name and non_standard_task_name in tasks_dicts:\n",
    "                    task_name = non_standard_task_name\n",
    "                    task_dict = tasks_dicts[non_standard_task_name]\n",
    "\n",
    "                if task_dict is None:\n",
    "                    raise ValueError(\n",
    "                        f\"Task '{category_name}' not found in feature set '{feature_set_name_orig}'\"\n",
    "                    )\n",
    "            # --- End Task Name Lookup ---\n",
    "\n",
    "            # Calculate average, min, max metric value across splits\n",
    "            try:\n",
    "                metric_values = []\n",
    "                for split, split_data in task_dict.items():\n",
    "                    if metric_to_plot in split_data:\n",
    "                        metric_values.append(split_data[metric_to_plot])\n",
    "                    else:\n",
    "                        print(\n",
    "                            f\"Warning: Metric '{metric_to_plot}' not found in split '{split}' for {feature_set_name_orig} / {task_name}\"\n",
    "                        )\n",
    "\n",
    "                if not metric_values:\n",
    "                    print(\n",
    "                        f\"Warning: No metric values found for {feature_set_name_orig} / {task_name} / {metric_to_plot}\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                avg_metric = np.mean(metric_values)\n",
    "                min_metric = np.min(metric_values)\n",
    "                max_metric = np.max(metric_values)\n",
    "\n",
    "            except Exception as e:  # pylint: disable=broad-except\n",
    "                raise ValueError(\n",
    "                    f\"Error calculating metrics for {feature_set_name_orig} / {task_name}: {e}\"\n",
    "                ) from e\n",
    "\n",
    "            # Clean feature set name for hover text\n",
    "            clean_name = feature_set_name_orig.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n",
    "\n",
    "            # Store data for this point\n",
    "            plot_data_points.append(\n",
    "                {\n",
    "                    \"bin_size\": bin_size,\n",
    "                    \"num_features\": num_features,\n",
    "                    \"metric_value\": avg_metric,\n",
    "                    \"min_metric\": min_metric,  # For error bar low\n",
    "                    \"max_metric\": max_metric,  # For error bar high\n",
    "                    \"name\": clean_name,\n",
    "                    \"raw_name\": feature_set_name_orig,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if not plot_data_points:\n",
    "            raise ValueError(\n",
    "                f\"No suitable data points found to plot for task: {category_name}\"\n",
    "            )\n",
    "\n",
    "        # --- Determine Marker Symbols ---\n",
    "        marker_symbols = []\n",
    "        default_symbol = \"circle\"\n",
    "        random_symbol = \"cross\"\n",
    "        for p in plot_data_points:\n",
    "            if \"random\" in p[\"raw_name\"]:\n",
    "                marker_symbols.append(random_symbol)\n",
    "            else:\n",
    "                marker_symbols.append(default_symbol)\n",
    "\n",
    "        # --- Group Data by Category ---\n",
    "        points_by_category = {name: [] for name in bin_category_names}\n",
    "        for i, point_data in enumerate(plot_data_points):\n",
    "            bin_size = point_data[\"bin_size\"]\n",
    "            assigned_category = None\n",
    "            for cat_name, cat_value in zip(bin_category_names, bin_category_values):\n",
    "                if bin_size == cat_value:\n",
    "                    assigned_category = cat_name\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(f\"Could not find category for bin size: {bin_size}\")\n",
    "\n",
    "            points_by_category[assigned_category].append(\n",
    "                {\n",
    "                    \"x\": point_data[\"num_features\"],  # X is Num Features\n",
    "                    \"y\": point_data[\"metric_value\"],\n",
    "                    \"error_up\": point_data[\"max_metric\"] - point_data[\"metric_value\"],\n",
    "                    \"error_down\": point_data[\"metric_value\"] - point_data[\"min_metric\"],\n",
    "                    \"text\": point_data[\"name\"],\n",
    "                    \"customdata\": [\n",
    "                        point_data[\"min_metric\"],\n",
    "                        point_data[\"max_metric\"],\n",
    "                        point_data[\"bin_size\"],\n",
    "                    ],  # Keep bin size for hover\n",
    "                    \"symbol\": marker_symbols[i],  # Assign symbol determined earlier\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # --- Create Figure and Add Traces PER CATEGORY ---\n",
    "        fig = go.Figure()\n",
    "        traces = []\n",
    "\n",
    "        for cat_name in bin_category_names:  # Iterate in defined order for legend\n",
    "            points_in_cat = points_by_category[cat_name]\n",
    "            if not points_in_cat:\n",
    "                continue\n",
    "\n",
    "            category_color = color_map[cat_name]\n",
    "\n",
    "            # Extract data for all points in this category\n",
    "            x_vals = [p[\"x\"] for p in points_in_cat]\n",
    "            y_vals = [p[\"y\"] for p in points_in_cat]\n",
    "            error_up_vals = [p[\"error_up\"] for p in points_in_cat]\n",
    "            error_down_vals = [p[\"error_down\"] for p in points_in_cat]\n",
    "            text_vals = [p[\"text\"] for p in points_in_cat]\n",
    "            customdata_vals = [p[\"customdata\"] for p in points_in_cat]\n",
    "            symbols_vals = [p[\"symbol\"] for p in points_in_cat]\n",
    "\n",
    "            trace = go.Scatter(\n",
    "                x=x_vals,\n",
    "                y=y_vals,\n",
    "                mode=\"markers\",\n",
    "                name=cat_name,\n",
    "                showlegend=False,\n",
    "                legendgroup=cat_name,  # Group legend entries\n",
    "                marker=dict(\n",
    "                    color=category_color,\n",
    "                    size=15,\n",
    "                    symbol=symbols_vals,\n",
    "                    line=dict(width=1, color=\"DarkSlateGrey\"),\n",
    "                ),\n",
    "                error_y=dict(\n",
    "                    type=\"data\",\n",
    "                    symmetric=False,\n",
    "                    array=error_up_vals,\n",
    "                    arrayminus=error_down_vals,\n",
    "                    visible=True,\n",
    "                    thickness=1.5,\n",
    "                    width=15,\n",
    "                    color=category_color,\n",
    "                ),\n",
    "                text=text_vals,\n",
    "                customdata=customdata_vals,\n",
    "                hovertemplate=(\n",
    "                    f\"<b>%{{text}}</b><br><br>\"\n",
    "                    f\"Num Features: %{{x:,.0f}}<br>\"\n",
    "                    f\"{metric_to_plot}: %{{y:.4f}}<br>\"\n",
    "                    f\"Bin Size: %{{customdata[2]:,.0f}} bp<br>\"\n",
    "                    f\"{metric_to_plot} Range (10-fold): %{{customdata[0]:.4f}} - %{{customdata[1]:.4f}}\"\n",
    "                    \"<extra></extra>\"\n",
    "                ),\n",
    "            )\n",
    "            traces.append(trace)\n",
    "\n",
    "        fig.add_traces(traces)\n",
    "\n",
    "        # --- Add Legend ---\n",
    "        # Add a hidden scatter trace with square markers for legend\n",
    "        for cat_name in bin_category_names:\n",
    "            category_color = color_map[cat_name]\n",
    "            legend_trace = go.Scatter(\n",
    "                x=[None],\n",
    "                y=[None],\n",
    "                mode=\"markers\",\n",
    "                name=cat_name,\n",
    "                marker=dict(\n",
    "                    color=category_color,\n",
    "                    size=15,\n",
    "                    symbol=\"square\",\n",
    "                    line=dict(width=1, color=\"DarkSlateGrey\"),\n",
    "                ),\n",
    "                legendgroup=cat_name,\n",
    "                showlegend=True,\n",
    "            )\n",
    "            fig.add_trace(legend_trace)\n",
    "\n",
    "        # --- Update layout ---\n",
    "        plot_title = f\"{metric_to_plot} vs Number of Features - {category_name}\"\n",
    "        if name:\n",
    "            plot_title += f\" - {name}\"\n",
    "        xaxis_title = \"Number of Features (log scale)\"\n",
    "        xaxis_type = \"log\"\n",
    "\n",
    "        yaxis_title = metric_to_plot.replace(\"_\", \" \").title()\n",
    "        yaxis_type = \"linear\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=plot_title,\n",
    "            xaxis_title=xaxis_title,\n",
    "            yaxis_title=yaxis_title,\n",
    "            xaxis_type=xaxis_type,\n",
    "            yaxis_type=yaxis_type,\n",
    "            yaxis_range=metric_range,\n",
    "            width=900,\n",
    "            height=750,\n",
    "            hovermode=\"closest\",\n",
    "            legend_title_text=\"Bin Size\",\n",
    "        )\n",
    "\n",
    "        if category_name == CELL_TYPE:\n",
    "            fig.update_yaxes(range=[0.75, 1.005])\n",
    "        elif category_name == ASSAY:\n",
    "            fig.update_yaxes(range=[0.96, 1.001])\n",
    "\n",
    "        # --- Save or show figure ---\n",
    "        if logdir:\n",
    "            logdir.mkdir(parents=True, exist_ok=True)\n",
    "            # Include \"modified\" or similar in filename to distinguish\n",
    "            base_name = f\"feature_scatter_MODIFIED_v2_{category_name}_{metric_to_plot}\"\n",
    "            if name:\n",
    "                base_name += f\"_{name}\"\n",
    "            html_path = logdir / f\"{base_name}.html\"\n",
    "            svg_path = logdir / f\"{base_name}.svg\"\n",
    "            png_path = logdir / f\"{base_name}.png\"\n",
    "\n",
    "            print(f\"Saving modified plot for {category_name} to {html_path}\")\n",
    "            fig.write_html(html_path)\n",
    "            fig.write_image(svg_path)\n",
    "            fig.write_image(png_path)\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"Accuracy\", \"F1_macro\"]:\n",
    "    graph_feature_set_scatter(\n",
    "        all_metrics=all_metrics,  # type: ignore\n",
    "        input_sizes=input_sizes,\n",
    "        metric_to_plot=metric,\n",
    "        name=\"feature_set_scatter\",\n",
    "        logdir=logdir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supp Fig 4 - Various feature sets performance per assay - Sample Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_metric_sets_per_assay(\n",
    "    all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]], verbose: bool = False\n",
    ") -> Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]:\n",
    "    \"\"\"Prepare metric sets per assay.\n",
    "\n",
    "    Args:\n",
    "        all_results (Dict[str, Dict[str, Dict[str, pd.DataFrame]]]): A dictionary containing all results for all feature sets.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, Dict[str, float]]]]: A dictionary containing all metrics per assay for all feature sets.\n",
    "            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Loading metadata.\")\n",
    "    metadata = metadata_handler.load_metadata(\"v2\")\n",
    "    metadata.convert_classes(ASSAY, ASSAY_MERGE_DICT)\n",
    "    md5_per_assay = metadata.md5_per_class(ASSAY)\n",
    "    md5_per_assay = {k: set(v) for k, v in md5_per_assay.items()}\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Getting results per assay.\")\n",
    "    results_per_assay = {}\n",
    "    for assay_label in ASSAY_ORDER:\n",
    "        if verbose:\n",
    "            print(assay_label)\n",
    "        results_per_assay[assay_label] = {}\n",
    "        for feature_set, task_dict in all_results.items():\n",
    "            if verbose:\n",
    "                print(feature_set)\n",
    "            results_per_assay[assay_label][feature_set] = {}\n",
    "            for task_name, split_dict in task_dict.items():\n",
    "                if verbose:\n",
    "                    print(task_name)\n",
    "                results_per_assay[assay_label][feature_set][task_name] = {}\n",
    "\n",
    "                # Only keep the relevant assay\n",
    "                for split_name, split_df in split_dict.items():\n",
    "                    if verbose:\n",
    "                        print(split_name)\n",
    "                    assay_df = split_df[split_df.index.isin(md5_per_assay[assay_label])]\n",
    "                    results_per_assay[assay_label][feature_set][task_name][\n",
    "                        split_name\n",
    "                    ] = assay_df\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Finished getting results per assay. Now computing metrics.\")\n",
    "    metrics_per_assay = {}\n",
    "    for assay_label in ASSAY_ORDER:\n",
    "        if verbose:\n",
    "            print(assay_label)\n",
    "        metrics_per_assay[assay_label] = {}\n",
    "        for feature_set, task_dict in results_per_assay[assay_label].items():\n",
    "            if verbose:\n",
    "                print(feature_set)\n",
    "            assay_metrics = split_results_handler.compute_split_metrics(\n",
    "                task_dict, concat_first_level=True\n",
    "            )\n",
    "            inverted_dict = split_results_handler.invert_metrics_dict(assay_metrics)\n",
    "            metrics_per_assay[assay_label][feature_set] = inverted_dict\n",
    "\n",
    "    return metrics_per_assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_feature_set_metrics_per_assay(\n",
    "    all_metrics_per_assay: Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]],\n",
    "    input_sizes: Dict[str, int],\n",
    "    logdir: Path | None = None,\n",
    "    sort_by_input_size: bool = False,\n",
    "    name: str | None = None,\n",
    "    y_range: Tuple[float, float] | None = None,\n",
    "    boxpoints: str = \"outliers\",\n",
    ") -> None:\n",
    "    \"\"\"Graph the metrics for all feature sets, per assay, with separate plots for accuracy and F1-score.\n",
    "\n",
    "    Args:\n",
    "        all_metrics_per_assay (Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]): A dictionary containing all metrics per assay for all feature sets.\n",
    "            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n",
    "        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n",
    "        logdir (Path): The directory where the figures will be saved. If None, the figures will only be displayed.\n",
    "        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n",
    "        name (str|None): The name of the figure.\n",
    "        y_range (Tuple[float, float]|None): The y-axis range for the plots.\n",
    "        boxpoints (str): The type of points to display in the box plots. Defaults to \"outliers\".\n",
    "    \"\"\"\n",
    "    valid_boxpoints = [\"all\", \"outliers\"]\n",
    "    if boxpoints not in valid_boxpoints:\n",
    "        raise ValueError(f\"Invalid boxpoints value. Choose from {valid_boxpoints}.\")\n",
    "\n",
    "    fig_assay_order = [\n",
    "        \"rna_seq\",\n",
    "        \"h3k27ac\",\n",
    "        \"h3k4me1\",\n",
    "        \"h3k4me3\",\n",
    "        \"h3k36me3\",\n",
    "        \"h3k27me3\",\n",
    "        \"h3k9me3\",\n",
    "        \"input\",\n",
    "        \"wgbs\",\n",
    "    ]\n",
    "\n",
    "    reference_assay = next(iter(all_metrics_per_assay))\n",
    "    reference_feature_set = next(iter(all_metrics_per_assay[reference_assay]))\n",
    "    metadata_categories = list(\n",
    "        all_metrics_per_assay[reference_assay][reference_feature_set].keys()\n",
    "    )\n",
    "\n",
    "    for _, category in enumerate(metadata_categories):\n",
    "        for metric, metric_name in [\n",
    "            (\"Accuracy\", \"Accuracy\"),\n",
    "            (\"F1_macro\", \"F1-score (macro)\"),\n",
    "        ]:\n",
    "            fig = go.Figure()\n",
    "\n",
    "            feature_sets = list(all_metrics_per_assay[reference_assay].keys())\n",
    "            unique_feature_sets = set(feature_sets)\n",
    "            for assay in fig_assay_order:\n",
    "                if set(all_metrics_per_assay[assay].keys()) != unique_feature_sets:\n",
    "                    raise ValueError(\"Different feature sets through assays.\")\n",
    "\n",
    "            feature_set_order = feature_sets\n",
    "            if sort_by_input_size:\n",
    "                feature_set_order = sorted(\n",
    "                    feature_set_order, key=lambda x: input_sizes[x]\n",
    "                )\n",
    "\n",
    "            # Adjust spacing so each assay group has dedicated space based on the number of feature sets\n",
    "            spacing_multiplier = (\n",
    "                1.1  # Increase this multiplier if needed to add more spacing\n",
    "            )\n",
    "            x_positions = {\n",
    "                assay: i * len(feature_set_order) * spacing_multiplier\n",
    "                for i, assay in enumerate(fig_assay_order)\n",
    "            }\n",
    "\n",
    "            for i, feature_set_name in enumerate(feature_set_order):\n",
    "                resolution = (\n",
    "                    feature_set_name.replace(\"_none\", \"\")\n",
    "                    .replace(\"hg38_\", \"\")\n",
    "                    .split(\"_\")[0]\n",
    "                )\n",
    "                color = resolution_colors[resolution]\n",
    "                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n",
    "\n",
    "                for assay in fig_assay_order:\n",
    "                    if feature_set_name not in all_metrics_per_assay[assay]:\n",
    "                        continue\n",
    "\n",
    "                    tasks_dicts = all_metrics_per_assay[assay][feature_set_name]\n",
    "\n",
    "                    if feature_set_name not in input_sizes:\n",
    "                        print(f\"Skipping {feature_set_name}, no input size found.\")\n",
    "                        continue\n",
    "\n",
    "                    task_name = category\n",
    "                    if \"split\" in task_name:\n",
    "                        raise ValueError(\"Split in task name. Wrong metrics dict.\")\n",
    "\n",
    "                    try:\n",
    "                        task_dict = tasks_dicts[task_name]\n",
    "                    except KeyError:\n",
    "                        print(\n",
    "                            f\"Skipping {feature_set_name}, {task_name} for assay {assay}\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    y_vals = [task_dict[split][metric] for split in task_dict]\n",
    "                    hovertext = [\n",
    "                        f\"{assay} - {display_name} - {split}: {metrics_dict[metric]:.4f}\"\n",
    "                        for split, metrics_dict in task_dict.items()\n",
    "                    ]\n",
    "\n",
    "                    x_position = x_positions[assay] + i\n",
    "                    fig.add_trace(\n",
    "                        go.Box(\n",
    "                            x=[x_position] * len(y_vals),\n",
    "                            y=y_vals,\n",
    "                            name=f\"{assay}|{display_name}\",\n",
    "                            boxmean=True,\n",
    "                            boxpoints=boxpoints,\n",
    "                            marker=dict(size=3, color=\"black\"),\n",
    "                            line=dict(width=1, color=\"black\"),\n",
    "                            fillcolor=color,\n",
    "                            hovertemplate=\"%{text}\",\n",
    "                            text=hovertext,\n",
    "                            showlegend=False,\n",
    "                            legendgroup=display_name,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # separate box groups\n",
    "                    fig.add_vline(\n",
    "                        x=x_positions[assay] - 1, line_width=1, line_color=\"black\"\n",
    "                    )\n",
    "\n",
    "            # Add dummy traces for the legend\n",
    "            for feature_set_name in feature_set_order:\n",
    "                resolution = (\n",
    "                    feature_set_name.replace(\"_none\", \"\")\n",
    "                    .replace(\"hg38_\", \"\")\n",
    "                    .split(\"_\")[0]\n",
    "                )\n",
    "                color = resolution_colors[resolution]\n",
    "                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        name=display_name,\n",
    "                        x=[None],\n",
    "                        y=[None],\n",
    "                        mode=\"markers\",\n",
    "                        marker=dict(size=10, color=color),\n",
    "                        showlegend=True,\n",
    "                        legendgroup=display_name,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            title = f\"Neural network performance - {category} - {metric_name} (per assay)\"\n",
    "            if name is not None:\n",
    "                title += f\" - {name}\"\n",
    "            fig.update_layout(\n",
    "                width=1500,\n",
    "                height=1000,\n",
    "                title=title,\n",
    "                xaxis_title=\"Assay\",\n",
    "                yaxis_title=metric_name,\n",
    "            )\n",
    "\n",
    "            # Create x-axis labels\n",
    "            fig.update_xaxes(\n",
    "                tickmode=\"array\",\n",
    "                tickvals=[\n",
    "                    x_positions[assay] + len(feature_set_order) / 2\n",
    "                    for assay in fig_assay_order\n",
    "                ],\n",
    "                ticktext=list(x_positions.keys()),\n",
    "                title=\"Assay\",\n",
    "            )\n",
    "\n",
    "            fig.update_layout(\n",
    "                legend=dict(\n",
    "                    title=\"Feature Sets\", itemsizing=\"constant\", traceorder=\"normal\"\n",
    "                )\n",
    "            )\n",
    "            if y_range:\n",
    "                fig.update_yaxes(range=y_range)\n",
    "\n",
    "            if logdir:\n",
    "                base_name = f\"feature_set_metrics_{category}_{metric}_per_assay\"\n",
    "                if name is not None:\n",
    "                    base_name = base_name + f\"_{name}\"\n",
    "                fig.write_html(logdir / f\"{base_name}.html\")\n",
    "                fig.write_image(logdir / f\"{base_name}.svg\")\n",
    "                fig.write_image(logdir / f\"{base_name}.png\")\n",
    "\n",
    "            fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_selection_name = \"flagship_selection_4cat\"\n",
    "all_results = split_results_handler.obtain_all_feature_set_data(\n",
    "    parent_folder=gen_data_dir,\n",
    "    merge_assays=True,\n",
    "    return_type=\"split_results\",\n",
    "    include_categories=[CELL_TYPE],\n",
    "    include_sets=metric_orders_map[set_selection_name],\n",
    "    exclude_names=[\"16ct\", \"27ct\", \"7c\", \"chip-seq-only\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.ERROR)\n",
    "metrics_per_assay = prepare_metric_sets_per_assay(all_results)  # type: ignore\n",
    "root_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder feature sets\n",
    "feature_set_order = metric_orders_map[set_selection_name]\n",
    "for assay, feature_sets in list(metrics_per_assay.items()):\n",
    "    metrics_per_assay[assay] = {\n",
    "        feature_set_name: metrics_per_assay[assay][feature_set_name]\n",
    "        for feature_set_name in feature_set_order\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_feature_set_metrics_per_assay(\n",
    "    all_metrics_per_assay=metrics_per_assay,  # type: ignore\n",
    "    input_sizes=input_sizes,\n",
    "    boxpoints=\"all\",\n",
    "    sort_by_input_size=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = metadata_v2.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_selection_name = \"flagship_selection_4cat\"\n",
    "all_results = split_results_handler.obtain_all_feature_set_data(\n",
    "    parent_folder=gen_data_dir,\n",
    "    merge_assays=True,\n",
    "    return_type=\"split_results\",\n",
    "    include_categories=[CELL_TYPE],\n",
    "    include_sets=metric_orders_map[set_selection_name],\n",
    "    exclude_names=[\"16ct\", \"27ct\", \"7c\", \"chip-seq-only\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate split results\n",
    "concat_results = {}\n",
    "relevant_cols = [\"EpiRR\", ASSAY, \"md5sum\"]\n",
    "for set_name in flagship_selection_4cat:\n",
    "    results_dfs = all_results[set_name]\n",
    "\n",
    "    # Concatenate\n",
    "    new_dfs = split_results_handler.concatenate_split_results(\n",
    "        split_dfs=results_dfs,  # type: ignore\n",
    "        concat_first_level=True,\n",
    "    )\n",
    "    # Flatten\n",
    "    new_df: pd.DataFrame = new_dfs[CELL_TYPE]  # type: ignore\n",
    "\n",
    "    # Add max pred, metadata, merge rna/wgb\n",
    "    new_df = split_results_handler.add_max_pred(new_df)\n",
    "    new_df = pd.merge(\n",
    "        new_df, meta_df[relevant_cols], how=\"left\", left_index=True, right_on=\"md5sum\"\n",
    "    )\n",
    "    new_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    concat_results[set_name] = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_score_boxplot_multi(\n",
    "    results_dict: Dict[str, pd.DataFrame],\n",
    "    name: str,\n",
    "    min_y: float | None = None,\n",
    "    use_aggregate_vote: bool = True,\n",
    "    group_by_column: str = ASSAY,\n",
    "    logdir: Path | None = None,\n",
    "    title: str | None = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with boxplots for multiple result DataFrames,\n",
    "    grouped by assay and colored per result set.\n",
    "\n",
    "    Args:\n",
    "        results_dict (Dict[str, pd.DataFrame]): Dictionary mapping legend label to a results DataFrame.\n",
    "            Dataframes require the following columns: `EpiRR`, `Predicted class`, `True class`, 'group_by_column'.\n",
    "        name (str): Base name of the plot.\n",
    "        min_y (float, optional): Minimum y-axis value.\n",
    "        use_aggregate_vote (bool): Whether to use EpiRR-level aggregation.\n",
    "        group_by_column (str): Column to group assays by.\n",
    "        logdir (Path, optional): Directory to save the plot.\n",
    "        title (str, optional): Title of the plot.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Determine all unique assays across all datasets\n",
    "    all_assays = sorted(\n",
    "        set().union(*[df[group_by_column].unique() for df in results_dict.values()])\n",
    "    )\n",
    "    assay_index = {assay: i for i, assay in enumerate(all_assays)}\n",
    "    nb_labels = len(results_dict)\n",
    "\n",
    "    label_offset = 0.8 / nb_labels if nb_labels > 1 else 0.0  # to space groups\n",
    "\n",
    "    color_map = {\n",
    "        df_name: px.colors.qualitative.Dark24[i]\n",
    "        for i, df_name in enumerate(results_dict.keys())\n",
    "    }\n",
    "\n",
    "    for j, (label, df) in enumerate(results_dict.items()):\n",
    "        for assay in all_assays:\n",
    "            df_assay = df[df[group_by_column] == assay]\n",
    "            if df_assay.empty:\n",
    "                continue\n",
    "\n",
    "            if use_aggregate_vote:\n",
    "                groupby = (\n",
    "                    df_assay.groupby([\"EpiRR\", \"Predicted class\"])[\"Max pred\"]\n",
    "                    .agg([\"size\", \"mean\"])\n",
    "                    .reset_index()\n",
    "                    .sort_values([\"EpiRR\", \"size\"], ascending=[True, False])\n",
    "                    .drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "                )\n",
    "                pred_scores = groupby[\"mean\"]\n",
    "            else:\n",
    "                pred_scores = df_assay[\"Max pred\"]\n",
    "\n",
    "            # X-position = assay base + small offset for each result label\n",
    "            base_x = assay_index[assay]\n",
    "            x_pos = base_x + (j - nb_labels / 2) * label_offset + label_offset / 2\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=pred_scores,\n",
    "                    x=[x_pos] * len(pred_scores),\n",
    "                    name=label,\n",
    "                    fillcolor=color_map[label],\n",
    "                    line_color=\"black\",\n",
    "                    width=label_offset * 0.9,\n",
    "                    legendgroup=label,\n",
    "                    showlegend=(assay == all_assays[0]),  # Show legend once per label\n",
    "                    marker=dict(\n",
    "                        size=1e-6, opacity=0\n",
    "                    ),  # normal IQR whiskers, but no boxpoints visible\n",
    "                    hoverinfo=\"skip\",\n",
    "                    boxmean=True,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # X-axis ticks centered per assay group\n",
    "    fig.update_xaxes(\n",
    "        tickvals=list(assay_index.values()),\n",
    "        ticktext=list(assay_index.keys()),\n",
    "        title_text=group_by_column,\n",
    "    )\n",
    "\n",
    "    if min_y is None:\n",
    "        min_y = min(\n",
    "            min(df[\"Max pred\"])\n",
    "            for df in results_dict.values()\n",
    "            if not df[\"Max pred\"].empty\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(range=[min_y, 1.001], title_text=\"Prediction score\")\n",
    "\n",
    "    plot_title = \"Prediction score distribution\"\n",
    "    if use_aggregate_vote:\n",
    "        plot_title += \" (EpiRR majority vote)\"\n",
    "        filename = f\"{name}_epirr\"\n",
    "    else:\n",
    "        plot_title += \" (per file)\"\n",
    "        filename = name\n",
    "\n",
    "    if title:\n",
    "        plot_title += f\" - {title}\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=plot_title,\n",
    "        width=max(600, len(all_assays) * 120),\n",
    "        height=600,\n",
    "        boxmode=\"group\",\n",
    "    )\n",
    "\n",
    "    if logdir:\n",
    "        fig.write_html(logdir / f\"{filename}.html\")\n",
    "        fig.write_image(logdir / f\"{filename}.svg\")\n",
    "        fig.write_image(logdir / f\"{filename}.png\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = (\n",
    "    base_fig_dir\n",
    "    / \"fig2_EpiAtlas_other/fig2--reduced_feature_sets\"\n",
    "    / set_selection_name\n",
    "    / \"prediction_scores\"\n",
    ")\n",
    "if not output_dir.exists():\n",
    "    raise ValueError(f\"Output directory {output_dir} does not exist\")\n",
    "\n",
    "output_dir = output_dir / \"merged\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for do_vote in [True, False]:\n",
    "    pred_score_boxplot_multi(\n",
    "        results_dict=concat_results,\n",
    "        name=\"4regions_boxplot\",\n",
    "        use_aggregate_vote=do_vote,\n",
    "        min_y=0.1,\n",
    "        logdir=output_dir,\n",
    "        title=CELL_TYPE,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table XXX - Metrics per assay, various feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_selection_name = \"feature_sets_14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = split_results_handler.obtain_all_feature_set_data(\n",
    "    parent_folder=gen_data_dir,\n",
    "    merge_assays=True,\n",
    "    return_type=\"split_results\",\n",
    "    include_categories=[ASSAY, CELL_TYPE],\n",
    "    include_sets=metric_orders_map[set_selection_name],\n",
    "    exclude_names=[\"16ct\", \"27ct\", \"7c\", \"chip-seq-only\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = metadata_v2.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all results for MetricsPerAssay, which only deals with one DF at a time\n",
    "# That class was built for dealing with merged dfs of all predictions, not for this\n",
    "all_results_concat = {}\n",
    "for set_name, split_results in all_results.items():\n",
    "    concat_results = split_results_handler.concatenate_split_results(\n",
    "        split_results, concat_first_level=True  # type: ignore\n",
    "    )\n",
    "    new_concat_results = {}\n",
    "    for task_name, task_df in list(concat_results.items()):\n",
    "        if task_name == \"assay_epiclass_11c\":\n",
    "            new_task_name = ASSAY\n",
    "        else:\n",
    "            new_task_name = task_name\n",
    "        assert isinstance(task_df, pd.DataFrame)\n",
    "        new_concat_df = split_results_handler.add_max_pred(task_df)\n",
    "        new_concat_df = new_concat_df.merge(\n",
    "            metadata_df[[\"md5sum\", ASSAY, CELL_TYPE]],\n",
    "            left_index=True,\n",
    "            right_on=\"md5sum\",\n",
    "            how=\"inner\",\n",
    "            suffixes=(\"\", \"_DROP\"),\n",
    "        )\n",
    "        new_concat_df = new_concat_df.drop(\n",
    "            columns=[col for col in new_concat_df.columns if col.endswith(\"_DROP\")]\n",
    "        )\n",
    "        new_concat_results[new_task_name] = new_concat_df\n",
    "    all_results_concat[set_name] = new_concat_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_per_assay = defaultdict(dict)\n",
    "for set_name, all_tasks in all_results_concat.items():\n",
    "    for task_name, task_df in all_tasks.items():\n",
    "        print(set_name, task_name)\n",
    "        metrics_per_assay[set_name][task_name] = MetricsPerAssay().compute_all_chunked_acc_per_assay(  # type: ignore\n",
    "            all_preds=task_df,\n",
    "            categories=[task_name],\n",
    "            no_epiatlas=False,\n",
    "            merge_assays=True,\n",
    "            column_templates={\n",
    "                \"True\": \"True class\",\n",
    "                \"Predicted\": \"Predicted class\",\n",
    "                \"Max pred\": \"Max pred\",\n",
    "            },\n",
    "            verbose=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dfs = []\n",
    "for set_name, task_dict in metrics_per_assay.items():\n",
    "    concat_results = []\n",
    "    for task_name, task_metrics in task_dict.items():\n",
    "        metrics_df = MetricsPerAssay().create_metrics_dataframe(\n",
    "            input_metrics=task_metrics,\n",
    "            chunked=True,\n",
    "        )\n",
    "        concat_results.append(metrics_df)\n",
    "    new_df = pd.concat(concat_results, axis=0)\n",
    "    new_df.insert(loc=0, column=\"feature_set\", value=set_name)\n",
    "    metrics_dfs.append(new_df)\n",
    "\n",
    "full_metrics_df = pd.concat(metrics_dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = paper_dir / \"tables\" / \"dfreeze_v2\"\n",
    "full_metrics_df.to_csv(\n",
    "    output_dir / \"all_EpiATLAS_metrics_per_assay_various_feature_sets.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supp Fig 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 1A,1B - All classifiers metrics on EpiAtlas - Assay and Sample Ontology - 100kb resolution\n",
    "\n",
    "Fig 1A,1B: data points are included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_models_split_metrics(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    label_category: str,\n",
    "    logdir: Path | None = None,\n",
    "    filename: str = \"fig1_all_classifiers_metrics\",\n",
    ") -> None:\n",
    "    \"\"\"Render to box plots the metrics per classifier/models and split, each in its own subplot.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A dictionary containing metric scores for each classifier and split.\n",
    "        label_category: The label category for the classification task.\n",
    "        name: The name of the figure.\n",
    "        logdir: The directory to save the figure to. If None, the figure is only displayed.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the figure and saves it to the logdir if provided.\n",
    "    \"\"\"\n",
    "    metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "    classifier_names = list(next(iter(split_metrics.values())).keys())\n",
    "    classifier_names = [\"NN\", \"LR\", \"LGBM\", \"LinearSVC\", \"RF\"]\n",
    "\n",
    "    # Create subplots, one row for each metric\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(metrics),\n",
    "        subplot_titles=metrics,\n",
    "        horizontal_spacing=0.075,\n",
    "    )\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        for classifier in classifier_names:\n",
    "            values = [split_metrics[split][classifier][metric] for split in split_metrics]\n",
    "            if classifier == \"NN\":\n",
    "                classifier = \"MLP\"\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=values,\n",
    "                    name=classifier,\n",
    "                    line=dict(color=\"black\", width=1.5),\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",  # or \"outliers\" to show only outliers\n",
    "                    pointpos=-1.4,\n",
    "                    showlegend=False,\n",
    "                    width=0.5,\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in zip(split_metrics, values)\n",
    "                    ],\n",
    "                ),\n",
    "                row=1,\n",
    "                col=i + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{label_category} classification - Metric distribution for 10fold cross-validation\",\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode=\"group\",\n",
    "    )\n",
    "\n",
    "    # Adjust y-axis\n",
    "    if label_category == ASSAY:\n",
    "        range_acc = [0.95, 1.001]\n",
    "        range_AUC = [0.992, 1.0001]\n",
    "    elif label_category == CELL_TYPE:\n",
    "        range_acc = [0.81, 1]\n",
    "        range_AUC = [0.96, 1]\n",
    "    else:\n",
    "        range_acc = [0.6, 1.001]\n",
    "        range_AUC = [0.9, 1.0001]\n",
    "\n",
    "    fig.update_layout(yaxis=dict(range=range_acc))\n",
    "    fig.update_layout(yaxis2=dict(range=range_acc))\n",
    "    fig.update_layout(yaxis3=dict(range=range_AUC))\n",
    "    fig.update_layout(yaxis4=dict(range=range_AUC))\n",
    "\n",
    "    # Save figure\n",
    "    if logdir:\n",
    "        fig.write_image(logdir / f\"{filename}.svg\")\n",
    "        fig.write_image(logdir / f\"{filename}.png\")\n",
    "        fig.write_html(logdir / f\"{filename}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_assays = False\n",
    "\n",
    "for label_category in [ASSAY, CELL_TYPE]:\n",
    "    all_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "        results_dir=data_dir_100kb, label_category=label_category\n",
    "    )\n",
    "\n",
    "    if merge_assays and label_category == ASSAY:\n",
    "        for split_name, split_dfs in all_split_dfs.items():\n",
    "            for classifier_type, df in split_dfs.items():\n",
    "                split_dfs[classifier_type] = merge_similar_assays(df)\n",
    "\n",
    "    split_metrics = split_results_handler.compute_split_metrics(all_split_dfs)\n",
    "\n",
    "    plot_multiple_models_split_metrics(\n",
    "        split_metrics,\n",
    "        label_category=label_category,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Going forward, all results are for MLP classifiers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 1C - Metrics for zeroed blacklist values and winsorized files - 100kb resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blklst_graphs(\n",
    "    feature_set_metrics_dict: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n",
    "    logdir: Path | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Create boxplots for blacklisted related feature sets.\n",
    "\n",
    "    Args:\n",
    "        feature_set_metrics_dict (Dict[str, Dict[str, Dict[str, Dict[str, float]]]]): The dictionary containing all metrics for all blklst related feature sets.\n",
    "            format: {feature_set: {task_name: {split_name: metric_dict}}}\n",
    "        logdir (Path, Optional): The directory to save the figure to. If None, the figure is only displayed.\n",
    "    \"\"\"\n",
    "    # Assume names exist in all feature sets\n",
    "    task_names = list(feature_set_metrics_dict.values())[0].keys()\n",
    "\n",
    "    traces_names_dict = {\n",
    "        \"hg38_100kb_all_none\": \"observed\",\n",
    "        \"hg38_100kb_all_none_0blklst\": \"0blklst\",\n",
    "        \"hg38_100kb_all_none_0blklst_winsorized\": \"0blklst_winsorized\",\n",
    "    }\n",
    "\n",
    "    for task_name in task_names:\n",
    "        category_fig = make_subplots(\n",
    "            rows=1,\n",
    "            cols=2,\n",
    "            shared_yaxes=False,\n",
    "            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n",
    "            x_title=\"Feature set\",\n",
    "            y_title=\"Metric value\",\n",
    "            horizontal_spacing=0.1,\n",
    "        )\n",
    "        for feature_set_name, tasks_dicts in feature_set_metrics_dict.items():\n",
    "            task_dict = tasks_dicts[task_name]\n",
    "            trace_name = traces_names_dict[feature_set_name]\n",
    "\n",
    "            # Accuracy\n",
    "            metric = \"Accuracy\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "            metric = \"F1_macro\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=2,\n",
    "            )\n",
    "\n",
    "        category_fig.update_xaxes(\n",
    "            categoryorder=\"array\",\n",
    "            categoryarray=list(traces_names_dict.values()),\n",
    "        )\n",
    "        category_fig.update_yaxes(range=[0.85, 1.001])\n",
    "\n",
    "        task_name = task_name.replace(\"_1l_3000n-10fold\", \"\")\n",
    "        category_fig.update_layout(\n",
    "            title=f\"MLP performance<br>{task_name}\",\n",
    "        )\n",
    "\n",
    "        width = 500\n",
    "        height = width * 1.5\n",
    "        category_fig.update_layout(\n",
    "            autosize=False,\n",
    "            width=width,\n",
    "            height=height,\n",
    "        )\n",
    "        # Save figure\n",
    "        if logdir:\n",
    "            base_name = f\"metrics_{task_name}\"\n",
    "            category_fig.write_html(logdir / f\"{base_name}.html\")\n",
    "            category_fig.write_image(logdir / f\"{base_name}.svg\")\n",
    "            category_fig.write_image(logdir / f\"{base_name}.png\")\n",
    "\n",
    "        category_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_sets = [\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_100kb_all_none_0blklst\",\n",
    "    \"hg38_100kb_all_none_0blklst_winsorized\",\n",
    "]\n",
    "\n",
    "results_folder_blklst = base_data_dir / \"training_results\" / \"2023-01-epiatlas-freeze\"\n",
    "if not results_folder_blklst.exists():\n",
    "    raise FileNotFoundError(f\"Folder '{results_folder_blklst}' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10-fold oversampling runs\n",
    "# expected result shape: {feature_set: {task_name: {split_name: metrics_dict}}}\n",
    "all_metrics: Dict[\n",
    "    str, Dict[str, Dict[str, Dict[str, float]]]\n",
    "] = split_results_handler.obtain_all_feature_set_data(\n",
    "    return_type=\"metrics\",\n",
    "    parent_folder=results_folder_blklst,\n",
    "    merge_assays=False,\n",
    "    include_categories=[ASSAY, CELL_TYPE],\n",
    "    include_sets=include_sets,\n",
    "    oversampled_only=False,\n",
    "    verbose=False,\n",
    ")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_blklst_graphs(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 1D - Accuracy per assay + confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_performance_per_assay_across_categories(\n",
    "    all_split_results: Dict[str, Dict[str, pd.DataFrame]],\n",
    "    title_end: str = \"\",\n",
    "    exclude_categories: List[str] | None = None,\n",
    "    y_range: None | List[float] = None,\n",
    "    logdir: Path | None = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"Create a box plot of assay accuracy for each classifier.\n",
    "\n",
    "    all_split_results (Dict[str, Dict[str, pd.DataFrame]]): The dictionary containing all split results for each classifier.\n",
    "    title_end (str, optional): The title to append to the figure title.\n",
    "    exclude_categories (List[str], optional): The categories to exclude from the figure.\n",
    "    y_range (None | List[float], optional): The y-axis range for the figure.\n",
    "    logdir (Path, optional): The directory to save the figure to. If None, the figure is only displayed.\n",
    "    verbose (bool, optional): Whether to print progress information.\n",
    "    \"\"\"\n",
    "    all_split_results = copy.deepcopy(all_split_results)\n",
    "\n",
    "    # Exclude some categories\n",
    "    classifier_names = list(all_split_results.keys())\n",
    "    if exclude_categories is not None:\n",
    "        for category in exclude_categories:\n",
    "            classifier_names = [c for c in classifier_names if category not in c]\n",
    "\n",
    "    metadata_df = MetadataHandler(paper_dir).load_metadata_df(\"v2-encode\")\n",
    "\n",
    "    # One graph per metadata category\n",
    "    for task_name in classifier_names:\n",
    "        if verbose:\n",
    "            print(f\"Processing {task_name}\")\n",
    "        split_results = all_split_results[task_name]\n",
    "        if ASSAY in task_name:\n",
    "            for split_name in split_results:\n",
    "                try:\n",
    "                    split_results[split_name] = merge_similar_assays(\n",
    "                        split_results[split_name]\n",
    "                    )\n",
    "                except ValueError as e:\n",
    "                    print(f\"Skipping {task_name} assay merging: {e}\")\n",
    "                    break\n",
    "\n",
    "        assay_acc_df = split_results_handler.compute_acc_per_assay(\n",
    "            split_results, metadata_df\n",
    "        )\n",
    "\n",
    "        fig = go.Figure()\n",
    "        for assay in ASSAY_ORDER:\n",
    "            try:\n",
    "                assay_accuracies = assay_acc_df[assay]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=assay_accuracies.values,\n",
    "                    name=assay,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=True,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=assay_colors[assay],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in assay_accuracies.items()\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        yrange = [assay_acc_df.min(), 1.001]  # type: ignore\n",
    "        if y_range is not None:\n",
    "            yrange = y_range\n",
    "\n",
    "        fig.update_yaxes(range=yrange)\n",
    "\n",
    "        title_text = f\"NN classification - {task_name}\"\n",
    "        if title_end:\n",
    "            title_text += f\" - {title_end}\"\n",
    "        fig.update_layout(\n",
    "            title_text=title_text,\n",
    "            yaxis_title=\"Accuracy\",\n",
    "            xaxis_title=\"Assay\",\n",
    "            width=1000,\n",
    "            height=700,\n",
    "        )\n",
    "\n",
    "        # Save figure\n",
    "        if logdir:\n",
    "            filename = \"NN_assay_performance_\" + task_name\n",
    "            fig.write_image(logdir / f\"{filename}.svg\")\n",
    "            fig.write_image(logdir / f\"{filename}.png\")\n",
    "            fig.write_html(logdir / f\"{filename}.html\")\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    min_pred_score: float = 0,\n",
    "    majority: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Create a confusion matrix for the given DataFrame and save it to the logdir.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the results.\n",
    "        logdir (Path): The directory path for saving the figures.\n",
    "        name (str): The name for the saved figures.\n",
    "        min_pred_score (float): The minimum prediction score to consider.\n",
    "        majority (bool): Whether to use majority vote (uuid-wise) for the predicted class.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    classes = sorted(df[\"True class\"].unique())\n",
    "    if \"Max pred\" not in df.columns:\n",
    "        df[\"Max pred\"] = df[classes].max(axis=1)  # type: ignore\n",
    "    filtered_df = df[df[\"Max pred\"] > min_pred_score]\n",
    "\n",
    "    if majority:\n",
    "        # Majority vote for predicted class\n",
    "        groupby_uuid = filtered_df.groupby([\"uuid\", \"True class\", \"Predicted class\"])[\n",
    "            \"Max pred\"\n",
    "        ].aggregate([\"size\", \"mean\"])\n",
    "\n",
    "        if groupby_uuid[\"size\"].max() > 3:\n",
    "            raise ValueError(\"More than three predictions for the same uuid.\")\n",
    "\n",
    "        groupby_uuid = groupby_uuid.reset_index().sort_values(\n",
    "            [\"uuid\", \"True class\", \"size\"], ascending=[True, True, False]\n",
    "        )\n",
    "        groupby_uuid = groupby_uuid.drop_duplicates(\n",
    "            subset=[\"uuid\", \"True class\"], keep=\"first\"\n",
    "        )\n",
    "        filtered_df = groupby_uuid\n",
    "\n",
    "    confusion_mat = sk_cm(\n",
    "        filtered_df[\"True class\"], filtered_df[\"Predicted class\"], labels=classes\n",
    "    )\n",
    "\n",
    "    mat_writer = ConfusionMatrixWriter(labels=classes, confusion_matrix=confusion_mat)\n",
    "    files = mat_writer.to_all_formats(logdir, name=f\"{name}_n{len(filtered_df)}\")\n",
    "    print(f\"Saved confusion matrix to {logdir}:\")\n",
    "    for file in files:\n",
    "        print(Path(file).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "    results_dir=data_dir_100kb, label_category=ASSAY, only_NN=True\n",
    ")\n",
    "concat_assay_df = split_results_handler.concatenate_split_results(assay_split_dfs)[\"NN\"]\n",
    "\n",
    "df_with_meta = metadata_handler.join_metadata(concat_assay_df, metadata_v2)  # type: ignore\n",
    "if \"Predicted class\" not in df_with_meta.columns:\n",
    "    raise ValueError(\"`Predicted class` not in DataFrame\")\n",
    "\n",
    "classifier_name = \"MLP\"\n",
    "min_pred_score = 0\n",
    "majority = False\n",
    "\n",
    "name = f\"{classifier_name}_pred>{min_pred_score}\"\n",
    "\n",
    "logdir = base_fig_dir / \"fig1_EpiAtlas_assay\" / \"fig1_supp_D-assay_c11_confusion_matrices\"\n",
    "if majority:\n",
    "    logdir = logdir / \"per_uuid\"\n",
    "else:\n",
    "    logdir = logdir / \"per_file\"\n",
    "logdir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_confusion_matrix(\n",
    "    df=df_with_meta,\n",
    "    min_pred_score=min_pred_score,\n",
    "    logdir=logdir,\n",
    "    name=name,\n",
    "    majority=majority,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_per_task: Dict[str, Dict[str, pd.DataFrame]] = {ASSAY: split_results_handler.invert_metrics_dict(assay_split_dfs)[\"NN\"]}  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_performance_per_assay_across_categories(all_split_results=results_per_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 1E,1F,1G - Distribution of average prediction scores per assay\n",
    "\n",
    "- E: Assay training 10-fold validation\n",
    "- F: Assay complete training, predictions on imputed data\n",
    "- G: Sample ontology 10-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(\n",
    "    results_df: pd.DataFrame,\n",
    "    label_category: str,\n",
    "    logdir: Path | None = None,\n",
    "    name: str = \"roc_curve\",\n",
    "    title: str | None = None,\n",
    "    colors_dict: Dict | None = None,  # Optional specific colors\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates and plots ROC curves for multi-class classification results using Plotly.\n",
    "\n",
    "    Calculates and plots individual class ROC curves, micro-average, and macro-average ROC curves.\n",
    "\n",
    "    Args:\n",
    "        results_df (pd.DataFrame): DataFrame with true labels and prediction probabilities for each class.\n",
    "                                   Must contain the `label_category` column (e.g., 'True class')\n",
    "                                   and probability columns named after each class.\n",
    "        label_category (str): The column name containing the true labels (e.g., 'True class', ASSAY, CELL_TYPE).\n",
    "        merge_assay_pairs (bool): Whether to merge similar assays based on ASSAY_MERGE_DICT if\n",
    "                                  `label_category` is ASSAY.\n",
    "        logdir (Path | None): Directory to save the figure. If None, only displays the figure.\n",
    "        name (str): Base name for saved files (e.g., \"supp_fig1e\").\n",
    "        title (str | None): Title suffix for the plot. If None, a default title based on label_category is used.\n",
    "        colors_dict (Dict | None): Optional dictionary mapping class names to colors. If None or a class\n",
    "                                   is missing, default Plotly colors are used.\n",
    "    \"\"\"\n",
    "    df = results_df.copy()\n",
    "    true_label_col = \"True class\"  # Assuming 'True class' holds the ground truth labels\n",
    "\n",
    "    if true_label_col not in df.columns:\n",
    "        raise ValueError(f\"True label column '{true_label_col}' not found in DataFrame.\")\n",
    "\n",
    "    classes = sorted(df[true_label_col].unique())\n",
    "    print(f\"Using classes: {classes}\")\n",
    "\n",
    "    n_classes = len(classes)\n",
    "    if n_classes < 2:\n",
    "        print(\n",
    "            f\"Warning: Only {n_classes} class found after processing. Cannot generate ROC curve.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Check if probability columns exist for all determined classes\n",
    "    missing_cols = [c for c in classes if c not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing probability columns for classes: {missing_cols}\")\n",
    "\n",
    "    # Binarize the true labels against the final set of classes\n",
    "    try:\n",
    "        y_true = label_binarize(df[true_label_col], classes=classes)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\n",
    "            f\"Error binarizing labels for classes {classes}. Check if all labels in '{true_label_col}' are included in 'classes'.\"\n",
    "        ) from e\n",
    "\n",
    "    if n_classes == 2 and y_true.shape[1] == 1:\n",
    "        # Adjust for binary case where label_binarize might return one column\n",
    "        y_true = np.hstack((1 - y_true, y_true))\n",
    "    elif y_true.shape[1] != n_classes:\n",
    "        raise ValueError(\n",
    "            f\"Binarized labels shape {y_true.shape} does not match number of classes {n_classes}\"\n",
    "        )\n",
    "\n",
    "    # Get the predicted probabilities for each class\n",
    "    # Ensure columns are in the same order as 'classes'\n",
    "    y_score = df[classes].values\n",
    "\n",
    "    # --- Compute ROC curve and ROC area for each class ---\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i, class_name in enumerate(classes):\n",
    "        try:\n",
    "            fpr[class_name], tpr[class_name], _ = roc_curve(\n",
    "                y_true=y_true[:, i], y_score=y_score[:, i]  # type: ignore\n",
    "            )\n",
    "            roc_auc[class_name] = auc(fpr[class_name], tpr[class_name])\n",
    "        except ValueError as e:\n",
    "            print(f\"Could not compute ROC for class {class_name}. Error: {e}\")\n",
    "            fpr[class_name], tpr[class_name], roc_auc[class_name] = (\n",
    "                np.array([0, 1]),\n",
    "                np.array([0, 1]),\n",
    "                0.5,\n",
    "            )  # Default bad ROC\n",
    "\n",
    "    # --- Compute micro-average ROC curve and ROC area ---\n",
    "    try:\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not compute micro-average ROC. Error: {e}\")\n",
    "        fpr[\"micro\"], tpr[\"micro\"], roc_auc[\"micro\"] = (\n",
    "            np.array([0, 1]),\n",
    "            np.array([0, 1]),\n",
    "            0.5,\n",
    "        )\n",
    "\n",
    "    # --- Compute macro-average ROC curve and ROC area ---\n",
    "    try:\n",
    "        # Aggregate all false positive rates\n",
    "        all_fpr = np.unique(\n",
    "            np.concatenate(\n",
    "                [fpr[class_name] for class_name in classes if class_name in fpr]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Interpolate all ROC curves at these points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        valid_classes_count = 0\n",
    "        for class_name in classes:\n",
    "            if class_name in fpr and class_name in tpr:\n",
    "                mean_tpr += np.interp(all_fpr, fpr[class_name], tpr[class_name])\n",
    "                valid_classes_count += 1\n",
    "\n",
    "        # Average it and compute AUC\n",
    "        if valid_classes_count > 0:\n",
    "            mean_tpr /= valid_classes_count\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "            roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "        else:\n",
    "            raise ValueError(\"No valid classes found for macro averaging.\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not compute macro-average ROC. Error: {e}\")\n",
    "        fpr[\"macro\"], tpr[\"macro\"], roc_auc[\"macro\"] = (\n",
    "            np.array([0, 1]),\n",
    "            np.array([0, 1]),\n",
    "            0.5,\n",
    "        )\n",
    "\n",
    "    # --- Plot all ROC curves ---\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot diagonal line for reference\n",
    "    fig.add_shape(\n",
    "        type=\"line\", line=dict(dash=\"dash\", color=\"grey\", width=1), x0=0, x1=1, y0=0, y1=1\n",
    "    )\n",
    "\n",
    "    # Define colors for plotting\n",
    "    color_cycle = px.colors.qualitative.Plotly  # Default cycle\n",
    "    plot_colors = {}\n",
    "    for i, cls_name in enumerate(classes):\n",
    "        if colors_dict and cls_name in colors_dict:\n",
    "            plot_colors[cls_name] = colors_dict[cls_name]\n",
    "        else:\n",
    "            plot_colors[cls_name] = color_cycle[i % len(color_cycle)]\n",
    "\n",
    "    # Plot Micro-average ROC curve first (often plotted thicker/dashed)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=fpr[\"micro\"],\n",
    "            y=tpr[\"micro\"],\n",
    "            mode=\"lines\",\n",
    "            name=f'Micro-average ROC (AUC = {roc_auc[\"micro\"]:.5f})',\n",
    "            line=dict(color=\"deeppink\", width=3, dash=\"dash\"),\n",
    "            hoverinfo=\"skip\",  # Less important for hover usually\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot Macro-average ROC curve\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=fpr[\"macro\"],\n",
    "            y=tpr[\"macro\"],\n",
    "            mode=\"lines\",\n",
    "            name=f'Macro-average ROC (AUC = {roc_auc[\"macro\"]:.5f})',\n",
    "            line=dict(color=\"navy\", width=3, dash=\"dash\"),\n",
    "            hoverinfo=\"skip\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Plot individual class ROC curves\n",
    "    for class_name in classes:\n",
    "        if class_name not in fpr or class_name not in tpr or class_name not in roc_auc:\n",
    "            continue  # Skip if calculation failed\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=fpr[class_name],\n",
    "                y=tpr[class_name],\n",
    "                mode=\"lines\",\n",
    "                name=f\"{class_name} (AUC = {roc_auc[class_name]:.5f})\",\n",
    "                line=dict(width=1.5, color=plot_colors.get(class_name)),\n",
    "                hovertemplate=f\"<b>{class_name}</b><br>FPR=%{{x:.5f}}<br>TPR=%{{y:.5f}}<extra></extra>\",  # Show class name and values on hover\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # --- Update layout ---\n",
    "    base_title = f\"ROC Curves - {label_category}\"\n",
    "    plot_title = f\"{base_title} - {title}\" if title else base_title\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=plot_title,\n",
    "        xaxis_title=\"False Positive Rate (1 - Specificity)\",\n",
    "        yaxis_title=\"True Positive Rate (Sensitivity)\",\n",
    "        xaxis=dict(range=[0.0, 1.0], constrain=\"domain\"),  # Ensure axes range 0-1\n",
    "        yaxis=dict(\n",
    "            range=[0.0, 1.05], scaleanchor=\"x\", scaleratio=1, constrain=\"domain\"\n",
    "        ),  # Make it square-ish, slight top margin\n",
    "        width=800,\n",
    "        height=750,\n",
    "        hovermode=\"closest\",\n",
    "        legend=dict(\n",
    "            traceorder=\"reversed\",  # Show averages first in legend\n",
    "            title=\"Classes & Averages\",\n",
    "            font=dict(size=9),\n",
    "            itemsizing=\"constant\",\n",
    "            # Optional: Position legend if needed, e.g., bottom right\n",
    "            # yanchor=\"bottom\", y=0.01,\n",
    "            # xanchor=\"right\", x=0.99\n",
    "        ),\n",
    "        margin=dict(l=60, r=30, t=80, b=60),  # Adjust margins for labels/title\n",
    "    )\n",
    "\n",
    "    # --- Save figure if logdir is provided ---\n",
    "    if logdir:\n",
    "        logdir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n",
    "        filename_base = f\"{name}_{label_category}_roc\"\n",
    "        filepath_base = logdir / filename_base\n",
    "\n",
    "        fig.write_html(f\"{filepath_base}.html\")\n",
    "        fig.write_image(f\"{filepath_base}.svg\", width=800, height=750)\n",
    "        fig.write_image(f\"{filepath_base}.png\", width=800, height=750, scale=2)\n",
    "\n",
    "        print(f\"Saved ROC curve plots for {label_category} to {logdir}\")\n",
    "        print(f\" -> {filename_base}.html / .svg / .png\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_scores_distribution(\n",
    "    results_df: pd.DataFrame,\n",
    "    merge_assay_pairs: bool = True,\n",
    "    logdir: Path | None = None,\n",
    "    name: str = \"prediction_score_distribution\",\n",
    "    group_by_column: str = \"True class\",\n",
    "    min_y: float = 0.7,\n",
    "    use_aggregate_vote: bool = True,\n",
    "    title: str | None = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with violin plots and associated scatter plots for each group.\n",
    "    Supports both aggregated and non-aggregated data visualization with enhanced styling.\n",
    "\n",
    "    Args:\n",
    "        results_df (pd.DataFrame): DataFrame containing prediction results and metadata\n",
    "        merge_assay_pairs (bool): Whether to merge similar assays (mrna/rna, wgbs-pbat/wgbs-standard)\n",
    "        logdir (Path | None): Directory to save figures. If None, only displays the figure\n",
    "        name (str): Base name for saved files\n",
    "        group_by_column (str): Column name to use for grouping traces\n",
    "        merge_similar_assays (bool): Whether to merge similar assays (mrna/rna, wgbs-pbat/wgbs-standard)\n",
    "        min_y (float): Minimum y-axis value\n",
    "        use_aggregate_vote (bool): If True, aggregate by EpiRR. If False, use individual predictions\n",
    "        title (str | None): Additional title text to append\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    jitter_amplitude = 0.05  # Scatter plot jittering\n",
    "    scatter_offset = 0.2  # Scatter plots offset\n",
    "\n",
    "    if merge_assay_pairs:\n",
    "        try:\n",
    "            results_df = merge_similar_assays(results_df)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping assay merging: {e}\")\n",
    "\n",
    "    # Group ordering\n",
    "    if group_by_column == ASSAY and merge_assay_pairs:\n",
    "        group_labels = ASSAY_ORDER\n",
    "    else:\n",
    "        group_labels = sorted(set(results_df[group_by_column].unique()))\n",
    "    group_index = {label: i for i, label in enumerate(group_labels)}\n",
    "\n",
    "    # Colors for each group\n",
    "    if group_by_column == ASSAY:\n",
    "        colors = assay_colors\n",
    "    else:\n",
    "        grey = \"rgba(237, 231, 225, 1)\"\n",
    "        colors = {label: grey for label in group_labels}\n",
    "\n",
    "    for label in group_labels:\n",
    "        sub_df = results_df[results_df[group_by_column] == label]\n",
    "\n",
    "        if use_aggregate_vote:\n",
    "            # Aggregate by EpiRR with majority voting\n",
    "            groupby = sub_df.groupby([\"EpiRR\", \"Predicted class\", \"True class\"])[\n",
    "                \"Max pred\"\n",
    "            ].aggregate([\"size\", \"mean\"])\n",
    "            groupby = groupby.reset_index().sort_values(\n",
    "                [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "            )\n",
    "            groupby = groupby.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "            assert groupby[\"EpiRR\"].is_unique\n",
    "            mean_pred = groupby[\"mean\"]\n",
    "\n",
    "            # Compare predicted class against true class for matches\n",
    "            matches = groupby[\"Predicted class\"] == groupby[\"True class\"]\n",
    "            match_pred = mean_pred[matches]\n",
    "            mismatch_pred = mean_pred[~matches]\n",
    "\n",
    "            hover_template = [\n",
    "                f\"EpiRR: {row[1]['EpiRR']}, Expected: {row[1]['True class']}, Pred: {row[1]['Predicted class']}, \"\n",
    "                f\"Mean pred: {row[1]['mean']:.3f}, n={row[1]['size']}\"\n",
    "                for row in groupby.iterrows()\n",
    "            ]\n",
    "        else:\n",
    "            # Use individual predictions\n",
    "            mean_pred = sub_df[\"Max pred\"]\n",
    "            matches = sub_df[\"Predicted class\"] == sub_df[\"True class\"]\n",
    "            match_pred = mean_pred[matches]\n",
    "            mismatch_pred = mean_pred[~matches]\n",
    "\n",
    "            hover_template = [\n",
    "                f\"ID: {row['md5sum']}, Expected: {row['True class']}, Pred: {row['Predicted class']}, \"\n",
    "                f\"Pred: {row['Max pred']:.3f}\"\n",
    "                for _, row in sub_df.iterrows()\n",
    "            ]\n",
    "\n",
    "        # Add violin plot\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[group_index[label]] * len(mean_pred),\n",
    "                y=mean_pred,\n",
    "                name=label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=False,\n",
    "                fillcolor=colors[label],\n",
    "                line_color=\"white\",\n",
    "                line=dict(width=0.5),\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add jittered scatter plots for matches and mismatches\n",
    "        np.random.seed(42)\n",
    "        jitter = np.random.uniform(-jitter_amplitude, jitter_amplitude, len(mean_pred))\n",
    "        x_positions = (\n",
    "            np.array([group_index[label]] * len(mean_pred)) + jitter - scatter_offset\n",
    "        )\n",
    "\n",
    "        # Plot matches (black points)\n",
    "        if len(match_pred) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=x_positions[matches],\n",
    "                    y=match_pred,\n",
    "                    mode=\"markers\",\n",
    "                    name=f\"Match {label}\",\n",
    "                    marker=dict(color=\"black\", size=1),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[hover_template[i] for i, m in enumerate(matches) if m],\n",
    "                    showlegend=False,\n",
    "                    legendgroup=\"match\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Plot mismatches (red points)\n",
    "        if len(mismatch_pred) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=x_positions[~matches],\n",
    "                    y=mismatch_pred,\n",
    "                    mode=\"markers\",\n",
    "                    name=f\"Mismatch {label}\",\n",
    "                    marker=dict(color=\"red\", size=3),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[hover_template[i] for i, m in enumerate(matches) if not m],\n",
    "                    showlegend=False,\n",
    "                    legendgroup=\"mismatch\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Add legend entries\n",
    "    for legend_entry in [(\"Match\", \"black\", 10), (\"Mismatch\", \"red\", 10)]:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[None],\n",
    "                y=[None],\n",
    "                mode=\"markers\",\n",
    "                name=legend_entry[0],\n",
    "                marker=dict(color=legend_entry[1], size=legend_entry[2]),\n",
    "                showlegend=True,\n",
    "                legendgroup=legend_entry[0].lower(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    title_text = \"Prediction Score Distribution\"\n",
    "    if use_aggregate_vote:\n",
    "        title_text += \" (EpiRR majority vote)\"\n",
    "    if title:\n",
    "        title_text += f\" - {title}\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title_text,\n",
    "        yaxis_title=\"Prediction Score\"\n",
    "        if not use_aggregate_vote\n",
    "        else \"Avg. Prediction Score (majority class)\",\n",
    "        xaxis_title=group_by_column,\n",
    "        yaxis_range=[min_y, 1.001],\n",
    "        xaxis=dict(\n",
    "            tickvals=list(group_index.values()),\n",
    "            ticktext=list(group_index.keys()),\n",
    "        ),\n",
    "        legend=dict(\n",
    "            title_text=\"Legend\",\n",
    "            itemsizing=\"constant\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Save figure if logdir is provided\n",
    "    if logdir:\n",
    "        filename = f\"{name}_epirr\" if use_aggregate_vote else name\n",
    "        fig.write_html(logdir / f\"{filename}.html\")\n",
    "        fig.write_image(logdir / f\"{filename}.svg\")\n",
    "        fig.write_image(logdir / f\"{filename}.png\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supp fig 1E: ASSAY - Prediction scores for 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_e_data_dir = data_dir_100kb / f\"{ASSAY}_1l_3000n\" / \"11c\" / \"10fold-oversampling\"\n",
    "if not fig_e_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {fig_e_data_dir} does not exist.\")\n",
    "\n",
    "dfs = split_results_handler.read_split_results(fig_e_data_dir)\n",
    "concat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\n",
    "concat_df = split_results_handler.add_max_pred(concat_df)\n",
    "concat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_scores_distribution(\n",
    "    results_df=concat_df_w_meta,\n",
    "    group_by_column=ASSAY,\n",
    "    merge_assay_pairs=True,\n",
    "    min_y=0.4,\n",
    "    title=\"11 classes assay training - Prediction scores for 10-fold cross-validation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"ROC_curves\"\n",
    "\n",
    "df = merge_similar_assays(concat_df_w_meta.copy())\n",
    "N_error = (df[\"True class\"] != df[\"Predicted class\"]).sum()\n",
    "print(f\"Only {N_error}/{len(df)} errors. ({N_error/len(df):.3%} error rate)\")\n",
    "\n",
    "plot_roc_curves(\n",
    "    results_df=merge_similar_assays(concat_df_w_meta.copy()),\n",
    "    label_category=ASSAY,\n",
    "    # logdir=logdir,\n",
    "    # name=\"aggregate\", # File name prefix\n",
    "    title=\"Aggregated 10fold\",  # Title suffix\n",
    "    colors_dict=assay_colors,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supp fig 1F: ASSAY -  Classifer=split0 from previous training. Predicting on imputed data (all pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputation_dir = base_data_dir / \"training_results\" / \"imputation\"\n",
    "if not imputation_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {imputation_dir} does not exist.\")\n",
    "\n",
    "fig_f_pred_dir = (\n",
    "    imputation_dir\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / f\"{ASSAY}_1l_3000n\"\n",
    "    / \"chip-seq-only\"\n",
    "    / \"10fold-oversampling\"\n",
    "    / \"split0\"\n",
    "    / \"predict_imputed\"\n",
    ")\n",
    "if not fig_f_pred_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {fig_f_pred_dir} does not exist.\")\n",
    "\n",
    "df_pred = pd.read_csv(\n",
    "    fig_f_pred_dir / \"split0_test_prediction_100kb_all_none_chip-seq_imputed.csv\",\n",
    "    index_col=0,\n",
    ")\n",
    "df_pred = split_results_handler.add_max_pred(df_pred)\n",
    "df_pred[\"EpiRR\"] = df_pred.index\n",
    "df_pred[ASSAY] = df_pred[\"True class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_scores_distribution(\n",
    "    results_df=df_pred,\n",
    "    group_by_column=ASSAY,\n",
    "    merge_assay_pairs=True,\n",
    "    min_y=0.9,\n",
    "    title=\"split0 assay classifier, predicting on imputed data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supp fig 1G: Sample Ontology - Prediction scores for 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_g_data_dir = data_dir_100kb / f\"{CELL_TYPE}_1l_3000n\" / \"10fold-oversampling\"\n",
    "if not fig_g_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {fig_g_data_dir} does not exist.\")\n",
    "\n",
    "dfs = split_results_handler.read_split_results(fig_g_data_dir)\n",
    "concat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\n",
    "concat_df = split_results_handler.add_max_pred(concat_df)\n",
    "concat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\n",
    "concat_df_w_meta[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_scores_distribution(\n",
    "    results_df=concat_df_w_meta,\n",
    "    group_by_column=ASSAY,\n",
    "    min_y=0,\n",
    "    title=\"Sample Ontology training - Prediction scores for 10-fold cross-validation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"ROC_curves\"\n",
    "plot_roc_curves(\n",
    "    results_df=concat_df_w_meta.copy(),\n",
    "    label_category=CELL_TYPE,\n",
    "    # logdir=logdir,\n",
    "    # name=\"aggregate\", # File name prefix\n",
    "    title=\"Aggregated 10fold\",  # Title suffix\n",
    "    colors_dict=cell_type_colors,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
