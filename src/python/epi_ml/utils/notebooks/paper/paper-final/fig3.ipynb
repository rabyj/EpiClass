{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Figure core creation: Fig3\n",
    "\n",
    "Formatting of the figures may not be identical to the paper, but they contain the same data points.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, too-many-branches, consider-using-f-string, duplicate-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import skops.io as skio\n",
    "import upsetplot\n",
    "from IPython.display import display\n",
    "\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_ORDER,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANCER = \"harmonized_sample_cancer_high\"\n",
    "CORE_ASSAYS = ASSAY_ORDER[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General paths setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "paper_dir = base_dir\n",
    "if not paper_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {paper_dir} does not exist.\")\n",
    "\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "tables_dir = base_dir / \"tables\"\n",
    "\n",
    "base_metadata_dir = base_data_dir / \"metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map\n",
    "sex_colors = IHECColorMap.sex_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()\n",
    "\n",
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata_v2 = metadata_handler.load_metadata(\"v2\")\n",
    "metadata_v2_df = metadata_handler.load_metadata_df(\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pred_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"predictions\"\n",
    "if not base_pred_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_pred_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChIP-Atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions from following models and more:\n",
    "| Metadata category|   Nb classes |     Experiment Key (comet.com)       | Training Files |\n",
    "|------------------|--------------|--------------------------------------|----------|\n",
    "| assay_epiclass   |     7      | 69488630801b4a05a53b5d9e572f0aaa       | 16788    |\n",
    "| assay_epiclass   |     11      | 0f8e5eb996114868a17057bebe64f87c      | 20922    |\n",
    "| harmonized_donor_sex | 3       | 4b908b83e0ec45c3ab991e65aa27af0c | 18299    |\n",
    "| harmonized_donor_life_stage | 5    | 91214ed0b1664395b1826dc69a495ed4 | 15372    |\n",
    "| harmonized_sample_cancer_high | 2    | 15da476b92f140eab818ece369248f4c | 20922    |\n",
    "| harmonized_biomaterial_type | 4 | f42b6f4e147c4f1bbe378f3eed415099 | 20922 |\n",
    "\n",
    "\n",
    "Classes:\n",
    "\n",
    "- assay 7c: 6 h3k* histone marks + input\n",
    "- assay 11c: assay7c + rna_seq + mrna_seq + wgbs_standard + wgbs_pbat\n",
    "- harmonized_donor_sex: male, female, mixed\n",
    "- harmonized_donor_life_stage: adult, child, newborn, fetal, embryonic\n",
    "- harmonized_sample_cancer_high (modification of harmonized_sample_disease_high): cancer, non-cancer (healthy/None+disease)\n",
    "- harmonized_biomaterial_type (biomat): cell line, primary cell, primary cell culture, primary tissue\n",
    "\n",
    "Training metadata: Approximately `IHEC_sample_metadata_harmonization.v1.1.extended.csv`.\n",
    "See `data/metadata/epiatlas/README.md` for metadata details, and `training_metadata_vs_official_v1.1.json` for exact difference of our training data and v1.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recount3 predictions\n",
    "\n",
    "Same classifiers as with ChIP-Atlas data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE predictions\n",
    "\n",
    "Same models as ChIP-Atlas predictions.\n",
    "Additonally, the `harmonized_sample_ontology_intermediate` model was used on a subset of files with known EpiATLAS biospecimens.\n",
    "\n",
    "| Metadata category|   Nb classes |     Experiment Key (comet.com)      | Training Files |\n",
    "|-------------------------------------------|-------------|---------------------------------------|----------|\n",
    "| harmonized_sample_ontology_intermediate   |     16      | bb66b72ae83645d587e50b34aebb39c3      | 16379    |\n",
    "\n",
    "Metadata for ENCODE predictions created using:  \n",
    "FILE + EXPERIMENT + BIOSAMPLE accessions, starting from filenames.  \n",
    "See `src/python/epi_ml/utils/notebooks/paper/encode_metadata_creation.ipynb`  \n",
    "Final metadata file: `encode_full_metadata_2025-02_no_revoked.freeze1.csv(.xz)`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 3A - ENCODE dataset core/non-core predictions (5 tasks)\n",
    "\n",
    "See `src/python/epi_ml/utils/notebooks/paper/encode_pred_analysis.ipynb` at section `All 5 tasks metrics summary`  \n",
    "\n",
    "Results were manually merged into Supplementary Table X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 3B - ChIP-Atlas assay (7classes) prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_preds_dir = tables_dir / \"dfreeze_v2\" / \"predictions\"\n",
    "ca_preds_path = (\n",
    "    ca_preds_dir / \"ChIP-Atlas_predictions_20240606_merge_metadata_freeze1.csv.xz\"\n",
    ")\n",
    "ca_preds_df = pd.read_csv(ca_preds_path, sep=\",\", low_memory=False, compression=\"xz\")\n",
    "\n",
    "print(f\"ChIP-Atlas: {ca_preds_df.shape[0]} total files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing ChIP-Atlas experiment overlap with EpiATLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ChIP-Atlas: Initial {ca_preds_df.shape[0]} files\")\n",
    "no_epiatlas_df = ca_preds_df[ca_preds_df[\"is_EpiAtlas_EpiRR\"] == \"0\"]\n",
    "\n",
    "diff = ca_preds_df.shape[0] - no_epiatlas_df.shape[0]\n",
    "print(f\"ChIP-Atlas: {diff} files with EpiAtlas EpiRR removed\")\n",
    "print(f\"ChIP-Atlas: {no_epiatlas_df.shape[0]} files without EpiAtlas EpiRR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoring non-core consensus files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_core_labels = [\"non-core\", \"CTCF\"]\n",
    "ca_core7_df = no_epiatlas_df[\n",
    "    ~no_epiatlas_df[\"target_majority_consensus\"].isin((non_core_labels))\n",
    "]\n",
    "\n",
    "diff = no_epiatlas_df.shape[0] - ca_core7_df.shape[0]\n",
    "print(f\"ChIP-Atlas: {diff} files with non-core consensus removed\")\n",
    "print(f\"ChIP-Atlas: {ca_core7_df.shape[0]} files with core consensus\")\n",
    "\n",
    "display(ca_core7_df[\"target_majority_consensus\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-confidence predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_N = ca_core7_df.shape[0]\n",
    "high_confidence_pred_df = ca_core7_df[ca_core7_df[\"Max_pred_assay7\"] >= 0.6]\n",
    "\n",
    "high_confidence_N = high_confidence_pred_df.shape[0]\n",
    "N_percent = high_confidence_N / total_N\n",
    "display(\n",
    "    f\"ChIP-Atlas: {high_confidence_N}/{total_N} files ({high_confidence_N/total_N:.2%}) with high confidence assay prediction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match between manual target consensus and MLP prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_N = high_confidence_pred_df.shape[0]\n",
    "\n",
    "match_rule = (\n",
    "    high_confidence_pred_df[\"target_majority_consensus\"]\n",
    "    == high_confidence_pred_df[\"Predicted_class_assay7\"]\n",
    ")\n",
    "match_df = high_confidence_pred_df[match_rule]\n",
    "mismatch_df = high_confidence_pred_df[~match_rule]\n",
    "\n",
    "agreement_N = match_df.shape[0]\n",
    "\n",
    "print(\n",
    "    f\"ChIP-Atlas: {agreement_N}/{total_N} files ({agreement_N / total_N:.2%}) with agreement between consensus and predicted assay\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mismatch breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mismatch = mismatch_df.shape[0]\n",
    "input_rule = mismatch_df[\"Predicted_class_assay7\"] == \"input\"\n",
    "input_pred_N = input_rule.sum()\n",
    "\n",
    "print(\n",
    "    f\"ChIP-Atlas: {input_pred_N}/{total_mismatch} files ({input_pred_N / total_mismatch:.2%}) with mismatch predicted as input\"\n",
    ")\n",
    "print(\n",
    "    f\"ChIP-Atlas: {total_mismatch-input_pred_N}/{total_mismatch} files ({(total_mismatch-input_pred_N) / total_mismatch:.2%}) potential mislabels\"\n",
    ")\n",
    "display(mismatch_df[~input_rule][\"core7_DBs_consensus\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 3C-J\n",
    "\n",
    "See Supp Fig 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supp Fig 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 3C-J + Supp Fig 8A,D - Public databases: Metrics for know labels\n",
    "\n",
    "ChIP-Atlas, ENCODE and recount3 metrics are generated in their respective file, using `MetricsPerAssay` class (from `src/python/epi_ml/utils/notebooks/paper/metrics_per_assay.py`).\n",
    "\n",
    "- ChIP-Atlas: `src/python/epi_ml/utils/notebooks/paper/c-a_pred_analysis.ipynb` (section `Summary metrics by assay`)\n",
    "- ENCODE: `src/python/epi_ml/utils/notebooks/paper/encode_pred_analysis.ipynb` (section `All 5 tasks metrics summary`, run `Setup` section beforehand)\n",
    "- recount3: `src/python/epi_ml/utils/notebooks/paper/recount3_pred_analysis.ipynb`\n",
    "\n",
    "Results were manually merged into Supplementary Table 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 8B - ENCODE non-core predictions with assay category mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_meta_dir = base_data_dir / \"metadata\" / \"encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_preds_dir = base_pred_dir / \"encode\"\n",
    "encode_preds_path = (\n",
    "    encode_preds_dir / \"complete_encode_predictions_augmented_2025-02_metadata.csv.gz\"\n",
    ")\n",
    "encode_preds_df = pd.read_csv(\n",
    "    encode_preds_path, sep=\",\", low_memory=False, compression=\"gzip\"\n",
    ")\n",
    "print(f\"ENCODE: {encode_preds_df.shape[0]} total files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_preds_df = encode_preds_df[encode_preds_df[\"in_epiatlas\"].astype(str) == \"False\"]\n",
    "print(f\"ENCODE: {encode_preds_df.shape[0]} total files with no EpiAtlas overlap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_core_categories_path = (\n",
    "    encode_meta_dir / \"non-core_encode_assay_category_2024-08-29.csv\"\n",
    ")\n",
    "\n",
    "non_core_categories_df = pd.read_csv(non_core_categories_path, sep=\",\", low_memory=False)\n",
    "non_core_map = non_core_categories_df.set_index(\"target\").to_dict()[\"Assay category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_non_core_df = encode_preds_df[\n",
    "    encode_preds_df[ASSAY].isin([\"non-core\", \"ctcf\"])\n",
    "].copy()\n",
    "\n",
    "# Map assays to categories\n",
    "encode_non_core_df[\"assay_category\"] = (\n",
    "    encode_non_core_df[\"assay\"].str.lower().replace(non_core_map)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_categories_order = [\n",
    "    \"trx_reg\",\n",
    "    \"heterochrom\",\n",
    "    \"polycomb\",\n",
    "    \"splicing\",\n",
    "    \"insulator\",\n",
    "    \"other/mixed\",\n",
    "    \"not_looked\",\n",
    "]\n",
    "\n",
    "assay_epiclass_order = [\n",
    "    \"h3k27ac\",\n",
    "    \"h3k4me3\",\n",
    "    \"h3k4me1\",\n",
    "    \"h3k9me3\",\n",
    "    \"h3k27me3\",\n",
    "    \"h3k36me3\",\n",
    "    \"input\",\n",
    "]\n",
    "assay_epiclass_order = {assay: i for i, assay in enumerate(assay_epiclass_order)}\n",
    "pred_col = f\"Predicted class ({ASSAY}_7c)\"\n",
    "max_pred_col = f\"Max pred ({ASSAY}_7c)\"\n",
    "\n",
    "for min_pred in [0, 0.6]:\n",
    "    sub_df = encode_non_core_df[encode_non_core_df[max_pred_col] >= min_pred]\n",
    "    groupby = (\n",
    "        sub_df.groupby([\"assay_category\", pred_col])\n",
    "        .size()\n",
    "        .reset_index(name=\"Count\")\n",
    "        .sort_values([\"assay_category\", \"Count\"], ascending=[True, False])\n",
    "    )\n",
    "    groupby[\"Percentage\"] = groupby.groupby(\"assay_category\")[\"Count\"].transform(\n",
    "        lambda x: (x / x.sum()) * 100\n",
    "    )\n",
    "\n",
    "    # Add order for plotting\n",
    "    groupby[\"assay_order\"] = groupby[pred_col].map(assay_epiclass_order)\n",
    "    groupby = groupby.sort_values(\n",
    "        [\"assay_category\", \"assay_order\"], ascending=[False, True]\n",
    "    )\n",
    "\n",
    "    # Main plot\n",
    "    fig = px.bar(\n",
    "        groupby,\n",
    "        x=\"assay_category\",\n",
    "        y=\"Percentage\",\n",
    "        color=pred_col,\n",
    "        barmode=\"stack\",\n",
    "        category_orders={\"assay_category\": assay_categories_order},\n",
    "        color_discrete_map=assay_colors,\n",
    "        title=f\"core7 predictions for non-core assays, predScore >= {min_pred:.2f}\",\n",
    "        labels={\"Percentage\": \"Percentage (%)\", \"assay_category\": \"Assay Category\"},\n",
    "    )\n",
    "\n",
    "    # Modify x-axis labels\n",
    "    total_counts = groupby.groupby(\"assay_category\")[\"Count\"].sum()\n",
    "    ticktext = [\n",
    "        f\"{assay_category} (N={total_counts[assay_category]})\"\n",
    "        for assay_category in assay_categories_order\n",
    "    ]\n",
    "    fig.update_xaxes(tickvals=assay_categories_order, ticktext=ticktext)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 8C,F,I - PCAs\n",
    "\n",
    "PCAs were computed via `src/python/epi_ml/utils/compute_pca.py`, using `IncrementalPCA` from `scikit-learn`.  \n",
    "Graphing was done using code similar to `src/python/epi_ml/utils/notebooks/paper/pca_plot.ipynb`.  \n",
    "\n",
    "Full PCA values and filenames are specified in Supp File X/Supp Table Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check subfig naming convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA tables, which include filenames\n",
    "\n",
    "pca_dir = base_data_dir / \"pca\"\n",
    "chip_dir = pca_dir / \"chip_3projects\"\n",
    "rna_dir = pca_dir / \"rna_enc_epi_recount3\"\n",
    "encode_dir = pca_dir / \"epiatlas_encode\"\n",
    "\n",
    "for folder in [chip_dir, rna_dir, encode_dir]:\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"Folder {folder} does not exist.\")\n",
    "\n",
    "    results_path = list(folder.glob(\"X_IPCA_*.skops\"))\n",
    "    if len(results_path) != 1:\n",
    "        raise ValueError(\n",
    "            f\"Expected 1 results file in {folder}, found {len(results_path)}\"\n",
    "        )\n",
    "    results_path = results_path[0]\n",
    "\n",
    "    data = skio.load(results_path)\n",
    "    print(folder.name, data[\"X_ipca\"].shape)\n",
    "\n",
    "    filenames = pd.DataFrame(data[\"file_names\"], columns=[\"filename\"])\n",
    "    filenames.to_csv(\n",
    "        folder / results_path.name.replace(\".skops\", \"_filenames.list\"),\n",
    "        index=False,\n",
    "        header=False,\n",
    "    )\n",
    "\n",
    "    results = data[\"X_ipca\"]\n",
    "\n",
    "    results_df = pd.DataFrame(\n",
    "        results, columns=[f\"PC{i+1}\" for i in range(results.shape[1])]\n",
    "    )\n",
    "    results_df = pd.concat([filenames, results_df], axis=1)\n",
    "\n",
    "    new_name = results_path.name.replace(\".skops\", \".csv.xz\")\n",
    "    new_name = new_name.replace(\"X_IPCA\", f\"{folder.name}_X_IPCA\")\n",
    "\n",
    "    results_df.to_csv(\n",
    "        folder / new_name,\n",
    "        index=True,\n",
    "        header=True,\n",
    "        compression=\"xz\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 5E - UpsetPlot of assay labels provided in 4 DBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ca_core7_df.columns:\n",
    "    if \"consensus\" in col:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_col = \"core7_DBs_consensus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_4DB_consensus_description(\n",
    "    ca_core_df: pd.DataFrame, db_cols: List[str]\n",
    ") -> pd.Series:\n",
    "    \"\"\"Create a description of the 4DB assay consensus labels.\n",
    "\n",
    "    Treat \"Unclassified\" from Chip-Atlas as absent samples for the target consensus evaluation.\n",
    "\n",
    "    The consensus description is based on the following rules:\n",
    "    - \"Identical\" if all labels are the same\n",
    "    - \"Different\" if at least one label is different\n",
    "    - \"1 source\" if only one DB has a label\n",
    "    - \"Ignored - Potential non-core\" if any label is not in the core assays\n",
    "\n",
    "    Args:\n",
    "        ca_core_df: ChIP-Atlas core7 DataFrame\n",
    "\n",
    "    Returns:\n",
    "        Series with the target consensus description\n",
    "    \"\"\"\n",
    "    id_db_target = []\n",
    "    tmp_df = ca_core_df.loc[:, db_cols].copy()\n",
    "    tmp_df[\"C-A\"].replace(\"unclassified\", \"----\", inplace=True)\n",
    "\n",
    "    for labels in tmp_df.values:\n",
    "        missing_N = sum(label == \"----\" for label in labels)\n",
    "        db_labels = set(labels)\n",
    "\n",
    "        try:\n",
    "            db_labels.remove(\"----\")\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if any(label not in CORE_ASSAYS + [\"ctrl\"] for label in db_labels):\n",
    "            id_db_target.append(\"Ignored - Potential non-core\")\n",
    "        elif missing_N == 3:\n",
    "            id_db_target.append(\"1 source\")\n",
    "        elif len(db_labels) == 1:\n",
    "            id_db_target.append(\"Identical\")\n",
    "        else:\n",
    "            id_db_target.append(\"Different\")\n",
    "\n",
    "    return pd.Series(id_db_target, index=ca_core_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_db_upsetplot(\n",
    "    df: pd.DataFrame, consensus_col: str, db_cols: List[str], title: str\n",
    ") -> upsetplot.UpSet:\n",
    "    \"\"\"Make an upsetplot of the sample presence in the different databases.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Create a new DataFrame with boolean columns for each database\n",
    "    upset_df = pd.DataFrame()\n",
    "    for col in db_cols:\n",
    "        upset_df[col] = df[col] != \"----\"\n",
    "    upset_df[consensus_col] = df[consensus_col]\n",
    "\n",
    "    # Set the index for the UpSet plot\n",
    "    upset_df = upset_df.set_index(db_cols)\n",
    "\n",
    "    # Create the UpSet plot\n",
    "    upset = upsetplot.UpSet(\n",
    "        upset_df,\n",
    "        intersection_plot_elements=0,  # disable the default bar chart\n",
    "        sort_by=\"cardinality\",\n",
    "        show_counts=True,  # type: ignore\n",
    "        orientation=\"horizontal\",\n",
    "    )\n",
    "\n",
    "    # Add stacked bars\n",
    "    upset.add_stacked_bars(by=consensus_col, elements=15)\n",
    "\n",
    "    # Plot and set title\n",
    "    axes = upset.plot()\n",
    "    plt.suptitle(title)\n",
    "    axes[\"totals\"].set_title(\"Total\")\n",
    "    plt.legend(loc=\"center left\")\n",
    "    plt.show()\n",
    "    return upset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_COLS = [\"GEO_target_mod\", \"C-A_target\", \"Cistrome_target\", \"NGS_target_mod\"]\n",
    "title = \"ChIP-Atlas core 7 samples presence in used DBs\\nTarget Consensus - No EpiAtlas overlap\"\n",
    "upset = make_db_upsetplot(\n",
    "    df=ca_core7_df, consensus_col=consensus_col, db_cols=DB_COLS, title=title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 8G - ChIP-Atlas mislabeled datasets\n",
    "\n",
    "Genome browser screenshots, using coordinates and samples specified in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supp Fig 8H - Effect of EpiATLAS imputation on assay (7 classes) training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_metadata_path = (\n",
    "    base_metadata_dir\n",
    "    / \"epiatlas\"\n",
    "    / \"imputed\"\n",
    "    / \"hg38_epiatlas_imputed_pval_chip_2024-02.json\"\n",
    ")\n",
    "metadata_imputed: pd.DataFrame = metadata_handler.load_any_metadata(imputed_metadata_path, as_dataframe=True)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_metadata_path = base_metadata_dir / \"chip_atlas\" / \"CA.full_info_metadata.freeze1.tsv\"\n",
    "metadata_ca = pd.read_csv(ca_metadata_path, sep=\"\\t\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "pred_dfs = {}  # Gather all 4 cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather predictions from classifier training on observed core6 data (all pval, no input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_dir = (\n",
    "    data_dir\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / \"assay_epiclass_1l_3000n\"\n",
    "    / \"chip-seq-only\"\n",
    "    / \"complete_no_valid_oversample\"\n",
    ")\n",
    "observed_inf_imputed_path = next((observed_dir / \"predict_imputed\").glob(\"*.csv\"))\n",
    "observed_inf_CA_path = next((observed_dir / \"predict_C-A\").glob(\"*.csv\"))\n",
    "\n",
    "basename = \"observed_core6_pval_inf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(observed_inf_imputed_path, header=0, index_col=0, low_memory=False)\n",
    "df = pd.merge(df, metadata_imputed, left_index=True, right_on=\"md5sum\")\n",
    "df[\"True class\"] = df[ASSAY]\n",
    "\n",
    "print(f\"Imputed: {df.shape[0]} total files\")\n",
    "pred_dfs[f\"{basename}_imputed\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(observed_inf_CA_path, header=0, index_col=0, low_memory=False)\n",
    "df = pd.merge(df, metadata_ca, left_index=True, right_on=\"ID\")\n",
    "df[\"True class\"] = df[\"expected_assay\"]\n",
    "print(f\"CA: {df.shape[0]} total files\")\n",
    "\n",
    "display(df[\"expected_assay\"].value_counts(dropna=False))\n",
    "pred_dfs[f\"{basename}_C-A\"] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather predictions from classifier training on imputed data (all pval, no input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_dir = (\n",
    "    data_dir\n",
    "    / \"hg38_100kb_all_none_imputed\"\n",
    "    / \"assay_epiclass_1l_3000n\"\n",
    "    / \"chip-seq-only\"\n",
    "    / \"complete_no_valid_oversample\"\n",
    ")\n",
    "imputed_inf_observed_path = next(\n",
    "    (imputed_dir / \"predict_epiatlas_pval_chip-seq\").glob(\"*.csv\")\n",
    ")\n",
    "imputed_inf_CA_path = next((imputed_dir / \"predict_C-A\").glob(\"*.csv\"))\n",
    "\n",
    "basename = \"imputed_core6_pval_inf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(imputed_inf_observed_path, header=0, index_col=0, low_memory=False)\n",
    "df = pd.merge(df, metadata_v2_df, left_index=True, right_on=\"md5sum\")\n",
    "df[\"True class\"] = df[ASSAY]\n",
    "\n",
    "print(f\"EpiATLAS pval ChIP: {df.shape[0]} total files\")\n",
    "pred_dfs[f\"{basename}_obs_core6_pval\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(imputed_inf_CA_path, header=0, index_col=0, low_memory=False)\n",
    "df = pd.merge(df, metadata_ca, left_index=True, right_on=\"ID\")\n",
    "df[\"True class\"] = df[\"expected_assay\"]\n",
    "\n",
    "print(f\"CA: {df.shape[0]} total files\")\n",
    "pred_dfs[f\"{basename}_C-A\"] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy per assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_fig_dir = base_fig_dir / \"fig3\" / \"imputation_effect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core6_assays = ASSAY_ORDER[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for name, df in pred_dfs.items():\n",
    "    if \"Max pred\" not in df.columns:\n",
    "        df[\"Max pred\"] = df[core6_assays].max(axis=1)\n",
    "\n",
    "    task_name = f\"train_{name}\"\n",
    "\n",
    "    for label in core6_assays:\n",
    "        assay_df = df[df[\"True class\"] == label]\n",
    "\n",
    "        for min_pred in [\"0.0\", \"0.6\", \"0.9\"]:\n",
    "            sub_df = assay_df[assay_df[\"Max pred\"] > float(min_pred)]\n",
    "            acc = (sub_df[\"True class\"] == sub_df[\"Predicted class\"]).mean()\n",
    "            rows.append([task_name, label, min_pred, acc, len(sub_df)])\n",
    "\n",
    "df_acc_per_assay = pd.DataFrame(\n",
    "    rows, columns=[\"task_name\", \"assay\", \"min_predScore\", \"acc\", \"nb_samples\"]\n",
    ")\n",
    "\n",
    "df_acc_per_assay.to_csv(\n",
    "    this_fig_dir / \"imputation_impact_acc_per_assay.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_predScore_color_map = {\"0.0\": \"blue\", \"0.6\": \"orange\", \"0.9\": \"red\"}\n",
    "\n",
    "df_acc_per_assay[\"scatter_name\"] = (\n",
    "    df_acc_per_assay[\"task_name\"]\n",
    "    .replace(\"train_\", \"\", regex=True)\n",
    "    .replace(\"imputed\", \"imp\", regex=True)\n",
    "    .replace(\"observed\", \"obs\", regex=True)\n",
    ")\n",
    "df_acc_per_assay[\"inf_target\"] = df_acc_per_assay[\"scatter_name\"].str.split(\"_\").str[-1]\n",
    "\n",
    "df_acc_per_assay = df_acc_per_assay.sort_values(\n",
    "    [\"assay\", \"min_predScore\", \"scatter_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_type = \"all\"\n",
    "# graph_type = \"no_C-A\"\n",
    "# graph_type = \"only_C-A\"\n",
    "# graph_type = \"only_C-A+core7\"\n",
    "\n",
    "if graph_type == \"all\":\n",
    "    minY = 0.7\n",
    "    maxY = 1.005\n",
    "elif graph_type == \"no_C-A\":\n",
    "    minY = 0.97\n",
    "    maxY = 1.001\n",
    "elif graph_type == \"only_C-A\":\n",
    "    minY = 0.7\n",
    "    maxY = 1.005\n",
    "else:\n",
    "    raise ValueError(f\"Unknown graph_type: {graph_type}\")\n",
    "\n",
    "graph_df = df_acc_per_assay.copy()\n",
    "graph_df = graph_df.sort_values([\"inf_target\", \"scatter_name\"])\n",
    "if graph_type == \"no_C-A\":\n",
    "    graph_df = graph_df[graph_df[\"inf_target\"] != \"C-A\"]\n",
    "elif \"only_C-A\" in graph_type:\n",
    "    graph_df = graph_df[graph_df[\"inf_target\"] == \"C-A\"]\n",
    "\n",
    "if graph_type != \"only_C-A+core7\":\n",
    "    graph_df = graph_df[~graph_df[\"scatter_name\"].str.contains(\"core7\")]\n",
    "\n",
    "\n",
    "# Prepare boxplot data\n",
    "tick_group = [\n",
    "    \"obs_core6_pval_inf_imp\",\n",
    "    \"imp_core6_pval_inf_obs_core6_pval\",\n",
    "    \"obs_core6_pval_inf_C-A\",\n",
    "    \"imp_core6_pval_inf_C-A\",\n",
    "]\n",
    "scatter_name_to_position = {name: i for i, name in enumerate(tick_group)}\n",
    "\n",
    "min_pred_values = [\"0.0\", \"0.6\", \"0.9\"]\n",
    "offset = [-0.25, 0, 0.25]  # Offset for each min_pred within a tick group\n",
    "\n",
    "# Define jitter magnitude (like 0.05 for left/right spacing)\n",
    "jitter = 0.05\n",
    "jitter_offsets = [-jitter, 0, jitter]\n",
    "\n",
    "fig = go.Figure()\n",
    "for name in tick_group:\n",
    "    group = graph_df[graph_df[\"scatter_name\"] == name]\n",
    "\n",
    "    for i, min_pred in enumerate(min_pred_values):\n",
    "        df_subset = group[group[\"min_predScore\"] == min_pred]\n",
    "\n",
    "        x_position = scatter_name_to_position[name] + offset[i]\n",
    "        x_positions = [x_position] * len(df_subset)\n",
    "        y_values = df_subset[\"acc\"]\n",
    "        hover_texts = [\n",
    "            f\"{row['assay']}<br>Samples: {row['nb_samples']}\"\n",
    "            for _, row in df_subset.iterrows()\n",
    "        ]\n",
    "        colors = [assay_colors[assay] for assay in df_subset[\"assay\"]]\n",
    "\n",
    "        # Add box plot without points\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                x=x_positions,\n",
    "                y=y_values,\n",
    "                name=f\"{name} - Min Pred Score: {min_pred}\",\n",
    "                line=dict(\n",
    "                    color=min_predScore_color_map[min_pred],\n",
    "                ),\n",
    "                boxpoints=\"all\",\n",
    "                marker=dict(\n",
    "                    opacity=0, size=1e-5\n",
    "                ),  # hide points, so whiskers don't go to min/max\n",
    "                boxmean=True,\n",
    "                showlegend=False,\n",
    "                hoverinfo=\"none\",\n",
    "            )\n",
    "        )\n",
    "        # Add scatter plot for individual points\n",
    "        x_jittered = [\n",
    "            x + jitter_offsets[i % len(jitter_offsets)] for i, x in enumerate(x_positions)\n",
    "        ]\n",
    "\n",
    "        # sort x/y together\n",
    "        x_jittered, y_values, hover_texts = zip(\n",
    "            *sorted(zip(x_jittered, y_values, hover_texts))\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_jittered,\n",
    "                y=y_values,\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=colors, size=8, line=dict(color=\"Black\", width=1)),\n",
    "                name=f\"{name} - Min Pred Score: {min_pred}\",\n",
    "                showlegend=False,\n",
    "                text=hover_texts,\n",
    "                hoverinfo=\"text+y\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Update x-axis tick labels\n",
    "ticktext = []\n",
    "for tick in tick_group:\n",
    "    train, inf = tick.split(\"_inf_\")\n",
    "    ticktext.append(f\"<b>{train}</b> \\u2192 <b>{inf}</b>\")\n",
    "\n",
    "fig.update_xaxes(tickmode=\"array\", ticktext=ticktext, tickvals=list(range(len(ticktext))))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Accuracy per Task (6 core assays)\",\n",
    "    xaxis_title=\"Task (training data \\u2192 inference data)\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    showlegend=True,\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    yaxis=dict(tickformat=\".2%\", range=[minY, maxY]),\n",
    ")\n",
    "\n",
    "# Add a legend for minPred colors\n",
    "for val, color in min_predScore_color_map.items():\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=10, color=color, symbol=\"square\"),\n",
    "            name=f\"Min Pred Score: {val}\",\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add a legend for assay colors\n",
    "for assay in sorted(core6_assays):\n",
    "    color = assay_colors[assay]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=10, color=color),\n",
    "            name=assay,\n",
    "            legendgroup=\"assays\",\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add legend for obs and imp\n",
    "fig.add_annotation(\n",
    "    x=1.2,\n",
    "    y=0.2,\n",
    "    yref=\"paper\",\n",
    "    xref=\"paper\",\n",
    "    text=\"obs = observed<br>imp = imputed\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=14),\n",
    ")\n",
    "\n",
    "if save_fig:\n",
    "    name = \"imputation_impact_acc_per_assay\"\n",
    "    fig.write_image(this_fig_dir / f\"{name}.png\")\n",
    "    fig.write_image(this_fig_dir / f\"{name}.svg\")\n",
    "    fig.write_html(this_fig_dir / f\"{name}.html\")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
