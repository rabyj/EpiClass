{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create figures destined for the paper.\"\"\"\n",
    "\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, too-many-branches, duplicate-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display  # pylint: disable=unused-import\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_data_dir} does not exist.\")\n",
    "\n",
    "base_results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "if not base_results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_results_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flagship assay/ct accuracy\n",
    "\n",
    "cell type classifier:  \n",
    "\n",
    "  for each assay, have a violin plot for accuracy per cell type (16 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"flagship\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_results_cell_type = (\n",
    "    base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    ")\n",
    "if not path_results_cell_type.exists():\n",
    "    raise FileNotFoundError(f\"{path_results_cell_type} does not exist.\")\n",
    "\n",
    "# for path in path_results_cell_type.glob(\"*.csv\"):\n",
    "#     print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load split results into one combined dataframe\n",
    "ct_split_dfs = split_results_handler.gather_split_results_across_categories(\n",
    "    path_results_cell_type\n",
    ")[\"harmonized_sample_ontology_intermediate_1l_3000n_10fold-oversampling\"]\n",
    "ct_full_df = pd.concat(ct_split_dfs.values(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata and join with split results\n",
    "metadata_2 = metadata_handler.load_metadata(\"v2\")\n",
    "ct_full_df = metadata_handler.join_metadata(ct_full_df, metadata_2)\n",
    "ct_full_df.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### violin version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_flagship_ct(\n",
    "    cell_type_df: pd.DataFrame,\n",
    "    logdir: Path | None = None,\n",
    "    name: str | None = None,\n",
    "    normal_ct: bool = True,\n",
    "    task_name: str = CELL_TYPE,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create a figure showing the cell type classifier performance on different assays.\n",
    "    Handles cases where not all cell types have samples in all assays.\n",
    "\n",
    "    Args:\n",
    "        cell_type_df (pd.DataFrame): DataFrame containing the cell type prediction results.\n",
    "        logdir (Path): The directory path for saving the figure.\n",
    "        name (str): The name for the saved figure.\n",
    "        normal_ct (bool): Whether to use predefined colors for normal cell types.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    # Assuming all classifiers have the same assays for simplicity\n",
    "    assay_labels = ASSAY_ORDER\n",
    "    num_assays = len(assay_labels)\n",
    "\n",
    "    ct_labels = set(\n",
    "        sorted(cell_type_df[\"True class\"].unique())\n",
    "        + sorted(cell_type_df[\"Predicted class\"].unique())\n",
    "    )\n",
    "    if normal_ct:\n",
    "        if len(ct_labels) != 16:\n",
    "            raise AssertionError(f\"Expected 16 cell type labels, got {len(ct_labels)}\")\n",
    "\n",
    "    scatter_offset = 0.1  # Scatter plot jittering\n",
    "\n",
    "    # Calculate the size of the grid\n",
    "    grid_size = int(np.ceil(np.sqrt(num_assays)))\n",
    "    rows, cols = grid_size, grid_size\n",
    "    # print(f\"Grid size: {grid_size}x{grid_size} ({num_assays} assays)\")\n",
    "\n",
    "    # Compute assay acc values beforehand\n",
    "    assay_acc_dict = {}\n",
    "    subclass_sizes = {}\n",
    "\n",
    "    # Track number of assays per cell type\n",
    "    ct_assay_counts = {ct: 0 for ct in ct_labels}\n",
    "\n",
    "    for idx, assay_label in enumerate(assay_labels):\n",
    "        assay_df = cell_type_df[cell_type_df[ASSAY] == assay_label]\n",
    "\n",
    "        # Get total samples per cell type\n",
    "        subclass_sizes[assay_label] = assay_df.groupby([\"True class\"]).agg(\"size\")\n",
    "\n",
    "        # Get correct predictions per cell type\n",
    "        correct_predictions = (\n",
    "            assay_df[assay_df[\"True class\"] == assay_df[\"Predicted class\"]]\n",
    "            .groupby(\"True class\")\n",
    "            .size()\n",
    "        )\n",
    "\n",
    "        # Calculate accuracies for all cell types\n",
    "        ct_accuracies = {\n",
    "            ct_label: (\n",
    "                correct_predictions.get(ct_label, 0)\n",
    "                / subclass_sizes[assay_label].get(ct_label, np.nan)\n",
    "                if ct_label in subclass_sizes[assay_label]\n",
    "                else np.nan\n",
    "            )\n",
    "            for ct_label in sorted(ct_labels)\n",
    "        }\n",
    "\n",
    "        # Update assay counts for cell types present in this assay\n",
    "        for ct_label in ct_labels:\n",
    "            if not np.isnan(ct_accuracies[ct_label]):\n",
    "                ct_assay_counts[ct_label] += 1\n",
    "\n",
    "        assay_acc_dict[assay_label] = ct_accuracies\n",
    "\n",
    "    # Create subplots with a square grid\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=ASSAY_ORDER,\n",
    "        shared_yaxes=\"all\",  # type: ignore\n",
    "        horizontal_spacing=0,\n",
    "        vertical_spacing=0.02,\n",
    "        y_title=\"Cell type subclass accuracy\",\n",
    "    )\n",
    "\n",
    "    for idx, assay_label in enumerate(ASSAY_ORDER):\n",
    "        row, col = divmod(idx, grid_size)\n",
    "\n",
    "        # Filter out NaN values for violin plot\n",
    "        acc_values = [v for v in assay_acc_dict[assay_label].values() if not np.isnan(v)]\n",
    "        valid_ct_labels = [\n",
    "            ct\n",
    "            for ct in sorted(ct_labels)\n",
    "            if not np.isnan(assay_acc_dict[assay_label][ct])\n",
    "        ]\n",
    "\n",
    "        if acc_values:  # Only create violin plot if there are valid values\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    x=[idx] * len(acc_values),\n",
    "                    y=acc_values,\n",
    "                    name=assay_label,\n",
    "                    spanmode=\"hard\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=False,\n",
    "                    fillcolor=assay_colors[assay_label],\n",
    "                    line_color=\"white\",\n",
    "                    line=dict(width=0.8),\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=row + 1,\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "        fig.update_xaxes(showticklabels=False)\n",
    "\n",
    "        # Scatter plot data preparation\n",
    "        if valid_ct_labels:\n",
    "            jittered_x_positions = (\n",
    "                np.random.uniform(\n",
    "                    -scatter_offset, scatter_offset, size=len(valid_ct_labels)\n",
    "                )\n",
    "                + idx\n",
    "                - 0.35\n",
    "            )\n",
    "            valid_colors = [\n",
    "                cell_type_colors[ct_label]\n",
    "                if normal_ct\n",
    "                else px.colors.qualitative.Dark24[list(ct_labels).index(ct_label)]\n",
    "                for ct_label in valid_ct_labels\n",
    "            ]\n",
    "\n",
    "            # Create hover text safely using .get()\n",
    "            hover_texts = [\n",
    "                f\"{ct_label} ({assay_acc_dict[assay_label][ct_label]:.3f}, \"\n",
    "                f\"n={subclass_sizes[assay_label].get(ct_label, 0)})\"\n",
    "                for ct_label in valid_ct_labels\n",
    "            ]\n",
    "\n",
    "            scatter_marker_size = 10\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=jittered_x_positions,\n",
    "                    y=[assay_acc_dict[assay_label][ct] for ct in valid_ct_labels],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=scatter_marker_size, color=valid_colors),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hover_texts,\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=row + 1,\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "    # Add a dummy scatter plot for legend\n",
    "    for ct_label in sorted(ct_labels):\n",
    "        color = (\n",
    "            cell_type_colors[ct_label]\n",
    "            if normal_ct\n",
    "            else px.colors.qualitative.Dark24[list(ct_labels).index(ct_label)]\n",
    "        )\n",
    "\n",
    "        legend_name = f\"{ct_label} ({ct_assay_counts[ct_label]} assays)\"\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[None],\n",
    "                y=[None],\n",
    "                mode=\"markers\",\n",
    "                name=legend_name,\n",
    "                marker=dict(color=color, size=scatter_marker_size),\n",
    "                showlegend=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(range=[0, 1.01])\n",
    "\n",
    "    title_text = f\"{task_name.replace('_', ' ')} classifier: Accuracy per assay\"\n",
    "    if name and \"minPredScore\" in name:\n",
    "        title_text += f\" (minPredScore={name.split('minPredScore', 1)[1]})\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title_text,\n",
    "        height=1500,\n",
    "        width=1500,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    if logdir:\n",
    "        name = name if name else \"ct_assay_accuracy_violin\"\n",
    "        fig.write_image(logdir / f\"{name}.svg\")\n",
    "        fig.write_image(logdir / f\"{name}.png\")\n",
    "        fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"flagship\" / \"ct_assay_accuracy\"\n",
    "# fig_flagship_ct(ct_full_df, logdir=fig_dir, name=\"ct_assay_accuracy_violin_10kb\")\n",
    "fig_flagship_ct(ct_full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "all_split_results = split_results_handler.general_split_metrics(\n",
    "    results_dir=gen_data_dir,\n",
    "    merge_assays=True,\n",
    "    return_type=\"split_results\",\n",
    "    include_categories=[CELL_TYPE, \"cell_type_martin\", \"cell_type_PE\"],\n",
    "    exclude_names=[\"16ct\", \"27ct\", \"7c\", \"chip-seq-only\"],\n",
    ")\n",
    "all_concat_results = split_results_handler.concatenate_split_results(\n",
    "    all_split_results,  # type: ignore\n",
    "    concat_first_level=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ct_meta_path = base_data_dir / \"metadata\" / \"Martin_class_v3_041224.tsv\"\n",
    "new_ct_meta_df = pd.read_csv(\n",
    "    new_ct_meta_path,\n",
    "    sep=\"\\t\",\n",
    "    names=[\"epirr_id_without_version\", \"cell_type_martin\", \"cell_type_PE\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = (\n",
    "    base_fig_dir\n",
    "    / \"flagship\"\n",
    "    / \"ct_assay_accuracy\"\n",
    "    / \"other_cell_type_groupings\"\n",
    "    / \"violin\"\n",
    ")\n",
    "if not fig_dir.exists():\n",
    "    fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for task_name, df in all_concat_results.items():\n",
    "    this_logdir = fig_dir / str(task_name)\n",
    "    if not this_logdir.exists():\n",
    "        this_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "    full_df = metadata_handler.join_metadata(df, metadata_2)  # type: ignore\n",
    "    full_df = full_df.merge(new_ct_meta_df, on=\"epirr_id_without_version\")\n",
    "    full_df.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n",
    "\n",
    "    if \"Max pred\" not in full_df.columns:\n",
    "        if any(label not in full_df.columns for label in [\"Predicted class\", \"split\"]):\n",
    "            raise ValueError(\n",
    "                \"Cannot find predicted class or split column, cannot compute max pred.\"\n",
    "            )\n",
    "        full_df = full_df.copy(deep=True)\n",
    "        idx1: int = full_df.columns.get_loc(\"Predicted class\")  # type: ignore\n",
    "        idx2: int = full_df.columns.get_loc(\"split\")  # type: ignore\n",
    "        full_df[\"Max pred\"] = full_df[full_df.columns[idx1 + 1 : idx2]].max(axis=1)\n",
    "\n",
    "    # for min_pred_score in [0, 0.6, 0.8]:\n",
    "    for min_pred_score in [0]:\n",
    "        full_df = full_df[full_df[\"Max pred\"] >= min_pred_score]\n",
    "        fig_flagship_ct(\n",
    "            full_df,\n",
    "            normal_ct=False,\n",
    "            task_name=task_name,  # type: ignore\n",
    "            logdir=this_logdir,\n",
    "            name=f\"{task_name}_ct_assay_accuracy_violin_minPredScore{min_pred_score:.2f}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boxplot version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_flagship_ct_boxplot(\n",
    "    cell_type_df: pd.DataFrame, name: str | None = None, logdir: Path | None = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates a boxplot for cell type classification accuracy across different assays.\n",
    "\n",
    "    This function creates a single figure with boxplots for each assay, displaying the accuracy\n",
    "    of cell type classification. Each boxplot represents the distribution of accuracy for one assay\n",
    "    across different cell types.\n",
    "\n",
    "    Args:\n",
    "        cell_type_df: DataFrame containing the cell type prediction results.\n",
    "        logdir: The directory path for saving the figure.\n",
    "        name: The name for the saved figure.\n",
    "    \"\"\"\n",
    "    # Assuming all classifiers have the same assays for simplicity\n",
    "    assay_labels = sorted(ASSAY_ORDER)\n",
    "    ct_labels = sorted(cell_type_df[\"True class\"].unique())\n",
    "\n",
    "    if len(ct_labels) != 16:\n",
    "        raise AssertionError(f\"Expected 16 cell type labels, got {len(ct_labels)}\")\n",
    "\n",
    "    assay_acc_dict = defaultdict(list)\n",
    "    for assay_label in assay_labels:\n",
    "        assay_df = cell_type_df[cell_type_df[ASSAY] == assay_label]\n",
    "\n",
    "        # cell type subclass accuracy\n",
    "        subclass_size = assay_df.groupby([\"True class\"]).agg(\"size\")\n",
    "        pred_confusion_matrix = assay_df.groupby([\"True class\", \"Predicted class\"]).agg(\n",
    "            \"size\"\n",
    "        )\n",
    "        for ct_label in sorted(ct_labels):\n",
    "            acc = pred_confusion_matrix[ct_label][ct_label] / subclass_size[ct_label]\n",
    "            assay_acc_dict[assay_label].append(acc)\n",
    "\n",
    "    # assay_sorted_by_mean_acc = sorted(\n",
    "    #     assay_acc_dict, key=lambda x: np.mean(assay_acc_dict[x]), reverse=True\n",
    "    # )\n",
    "\n",
    "    # Create the boxplot\n",
    "    fig = go.Figure()\n",
    "    for assay_label in ASSAY_ORDER:\n",
    "        # Select accuracies corresponding to the current assay\n",
    "        assay_accuracies = assay_acc_dict[assay_label]\n",
    "        assert len(assay_accuracies) == 16\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=assay_accuracies,\n",
    "                name=assay_label,\n",
    "                boxpoints=\"outliers\",\n",
    "                boxmean=True,\n",
    "                fillcolor=assay_colors[assay_label],\n",
    "                line_color=\"black\",\n",
    "                showlegend=False,\n",
    "                marker=dict(size=2),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(range=[0.34, 1.01])\n",
    "\n",
    "    title_text = f\"{CELL_TYPE.replace('_', ' ').title()} classifier: Accuracy per assay\"\n",
    "    fig.update_layout(\n",
    "        title=title_text,\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        xaxis_title=\"Assay\",\n",
    "        height=600,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    # Save and display the figure\n",
    "    if logdir:\n",
    "        this_name = name if name else \"ct_assay_accuracy_boxplot\"\n",
    "        fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "        fig.write_image(logdir / f\"{this_name}.png\")\n",
    "        fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_flagship_ct_boxplot(ct_full_df, logdir=fig_dir, name=\"ct_assay_accuracy_boxplot_10kb\")\n",
    "fig_flagship_ct_boxplot(ct_full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subclass (assay, ct, life_stage) accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_labels = sorted(ct_full_df[ASSAY].unique())\n",
    "ct_labels = sorted(ct_full_df[CELL_TYPE].unique())\n",
    "life_stages = ct_full_df[\"harmonized_donor_life_stage\"].unique().tolist()\n",
    "life_stages.remove(\"unknown\")\n",
    "\n",
    "acc_list = []\n",
    "for assay_label in assay_labels:\n",
    "    assay_df = ct_full_df[ct_full_df[ASSAY] == assay_label]\n",
    "    for ct in ct_labels:\n",
    "        ct_df = assay_df[assay_df[CELL_TYPE] == ct]\n",
    "        for life_stage in life_stages:\n",
    "            life_stage_df = ct_df[ct_df[\"harmonized_donor_life_stage\"] == life_stage]\n",
    "            acc = life_stage_df[\"Predicted class\"].eq(life_stage_df[\"True class\"]).mean()\n",
    "            size = len(life_stage_df)\n",
    "            acc_list.append((assay_label, ct, life_stage, size, acc))\n",
    "\n",
    "acc_df = pd.DataFrame(\n",
    "    acc_list, columns=[\"Assay\", \"Cell Type\", \"Life Stage\", \"Size\", \"Accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_df.to_csv(base_fig_dir / \"flagship\" / \"assay_ct_life_stage_accuracy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input classification - Hdf5 values at top SHAP features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparer le signal de input dans les différents cell-types (e.g. boxplot des 40 régions \"input & t cell\" vs les 9 régions \"input & lymphocyte of b lineage\" dans ces 2 CT (donc 4 boxplots) ou même ajouter un autre CT externe comme muscle comme ctrl neg (6 boxplots))\n",
    "\n",
    "all input, check 40 and 9 regions in\n",
    "- t cell\n",
    "- b cell\n",
    "- muscle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flagship_supp_shap_input(\n",
    "    paper_dir: Path, base_results_dir: Path, logdir: Path | None = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot hdf5 values of different (input, cell type) pairs for top SHAP values\n",
    "    of T cell and B cell. (only ct which produced common top shap values features)\n",
    "\n",
    "    Args:\n",
    "        paper_dir (Path): The directory containing the paper data.\n",
    "        logdir (Path, optional): The directory to save the figure. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    # Load relevant metadata\n",
    "    ct_labels = [\"T cell\", \"lymphocyte of B lineage\", \"neutrophil\", \"muscle organ\"]\n",
    "    metadata_2 = MetadataHandler(paper_dir).load_metadata(\"v2\")\n",
    "    metadata_2.select_category_subsets(ASSAY, [\"input\"])\n",
    "    metadata_2.select_category_subsets(CELL_TYPE, ct_labels)\n",
    "    md5_per_ct = metadata_2.md5_per_class(CELL_TYPE)\n",
    "\n",
    "    # Load feature bins\n",
    "    task_dir = (\n",
    "        base_results_dir\n",
    "        / \"hg38_100kb_all_none\"\n",
    "        / f\"{CELL_TYPE}_1l_3000n\"\n",
    "        / \"10fold-oversampling\"\n",
    "    )\n",
    "    if not task_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {task_dir} does not exist.\")\n",
    "    hdf5_val_dir = task_dir / \"global_shap_analysis\" / \"top303\" / \"input\"\n",
    "    if not hdf5_val_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {hdf5_val_dir} does not exist.\")\n",
    "\n",
    "    feature_filepath = hdf5_val_dir / \"features_n8.json\"\n",
    "    with open(feature_filepath, \"r\", encoding=\"utf8\") as f:\n",
    "        features: Dict[str, List[int]] = json.load(f)\n",
    "\n",
    "    # Load feature values\n",
    "    hdf5_val_path = hdf5_val_dir / \"hdf5_values_100kb_all_none_input_4ct_features_n8.csv\"\n",
    "    df = pd.read_csv(hdf5_val_path, index_col=0, header=0)\n",
    "\n",
    "    df_ct_dict = {}\n",
    "    for ct in ct_labels:\n",
    "        md5s = md5_per_ct[ct]\n",
    "        df_ct_dict[ct] = df.loc[md5s]\n",
    "\n",
    "    # Make two groups of boxplots, four boxplot per group (one per cell type)\n",
    "    # Each boxplot will take the values of the columns in the features dict\n",
    "    fig = go.Figure()\n",
    "    for ct_label, df_ct in df_ct_dict.items():\n",
    "        for i, (cell_type, top_shap_bins) in enumerate(features.items()):\n",
    "            top_shap_bins = [str(b) for b in top_shap_bins]\n",
    "            mean_bin_values_per_md5 = df_ct[top_shap_bins].mean(axis=1)\n",
    "\n",
    "            hovertext = [\n",
    "                f\"{md5}. {val:02f}\"\n",
    "                for md5, val in zip(df_ct.index, mean_bin_values_per_md5)\n",
    "            ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    x=[f\"Top SHAP '{cell_type}' bins\"] * len(mean_bin_values_per_md5),\n",
    "                    y=mean_bin_values_per_md5,\n",
    "                    name=f\"{ct_label} (n={len(hovertext)})\",\n",
    "                    points=\"all\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    spanmode=\"hard\",\n",
    "                    fillcolor=cell_type_colors[ct_label],\n",
    "                    line_color=\"black\",\n",
    "                    showlegend=i == 0,\n",
    "                    marker=dict(size=2),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                    legendgroup=ct_label,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Input files bin z-score values for top SHAP features\",\n",
    "        yaxis_title=\"Mean z-score across bins\",\n",
    "        xaxis_title=\"Top SHAP features groups\",\n",
    "        boxmode=\"group\",\n",
    "        violinmode=\"group\",\n",
    "        height=1000,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    if logdir:\n",
    "        name = \"input_feature_values_top_shap_bins_per_md5\"\n",
    "        fig.write_image(logdir / f\"{name}.svg\")\n",
    "        fig.write_image(logdir / f\"{name}.png\")\n",
    "        fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagship_supp_shap_input(paper_dir, base_results_dir=base_results_dir)\n",
    "# flagship_supp_shap_input(paper_dir, logdir=base_fig_dir / \"flagship\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell type classification - Training per assay results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with unique assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir_1 = (\n",
    "    base_results_dir\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / \"harmonized_sample_ontology_intermediate_1l_3000n\"\n",
    "    / \"10fold-oversampling-unique_assay\"\n",
    ")\n",
    "results_dir_2 = (\n",
    "    base_results_dir\n",
    "    / \"hg38_cpg_topvar_200bp_10kb_coord_n30k/harmonized_sample_ontology_intermediate_1l_3000n/10fold-oversampling_wgbs-only\"\n",
    ")\n",
    "\n",
    "assay_results = {}\n",
    "for assay_folder in results_dir_1.glob(\"*_only\"):\n",
    "    assay_results[assay_folder.name] = split_results_handler.read_split_results(\n",
    "        assay_folder\n",
    "    )\n",
    "\n",
    "assay_results[\n",
    "    \"wgbs_only-cpg_topvar_200bp_n30321\"\n",
    "] = split_results_handler.read_split_results(results_dir_2)\n",
    "\n",
    "assay_metrics = split_results_handler.compute_split_metrics(\n",
    "    assay_results, concat_first_level=True\n",
    ")\n",
    "assay_metrics = split_results_handler.invert_metrics_dict(assay_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_metrics[\"rna_seq_only\"] = assay_metrics.pop(\"rna_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed assays training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir_3 = (\n",
    "    base_results_dir\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / \"harmonized_sample_ontology_intermediate_1l_3000n\"\n",
    "    / \"10fold-oversampling\"\n",
    ")\n",
    "\n",
    "metadata_2_df = MetadataHandler(paper_dir).load_metadata_df(\"v2\")\n",
    "\n",
    "split_results = split_results_handler.read_split_results(result_dir_3)\n",
    "acc_per_assay = split_results_handler.compute_acc_per_assay(split_results, metadata_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to save format as compute_split_metrics outputs.\n",
    "for col in acc_per_assay.columns:\n",
    "    split_accs = acc_per_assay[col].to_dict()\n",
    "    for split, acc in list(split_accs.items()):\n",
    "        split_accs[split] = {\"Accuracy\": acc}\n",
    "    assay_metrics[col] = split_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_performance_per_assay(\n",
    "    assay_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    name: str | None = None,\n",
    "    logdir: Path | None = None,\n",
    "    title_end: str = \"\",\n",
    "    y_range: None | List[float] = None,\n",
    "):\n",
    "    \"\"\"Create a box plot of assay accuracy for each classifier.\"\"\"\n",
    "    fig = go.Figure()\n",
    "    for assay in ASSAY_ORDER:\n",
    "        try:\n",
    "            metrics_unique_assay = assay_metrics[f\"{assay}_only\"]\n",
    "            unique_assay_acc = {\n",
    "                split: metrics_unique_assay[split][\"Accuracy\"]\n",
    "                for split in metrics_unique_assay\n",
    "            }\n",
    "\n",
    "            metrics_assay_mixed = assay_metrics[f\"{assay}\"]\n",
    "            mixed_assay_acc = {\n",
    "                split: metrics_assay_mixed[split][\"Accuracy\"]\n",
    "                for split in metrics_assay_mixed\n",
    "            }\n",
    "        except KeyError:\n",
    "            print(f\"KeyError. Skipping '{assay}'\")\n",
    "            continue\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=list(unique_assay_acc.values()),\n",
    "                name=f\"{assay}_only\",\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                showlegend=True,\n",
    "                marker=dict(size=3, color=\"black\"),\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                fillcolor=assay_colors[assay],\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"{split}: {value:.4f}\" for split, value in unique_assay_acc.items()\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=list(mixed_assay_acc.values()),\n",
    "                name=f\"{assay}_mixed\",\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                showlegend=True,\n",
    "                marker=dict(size=3, color=\"black\"),\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                fillcolor=assay_colors[assay],\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"{split}: {value:.4f}\" for split, value in mixed_assay_acc.items()\n",
    "                ],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    name_other = \"wgbs_only-cpg_topvar_200bp_n30321\"\n",
    "    wgbs_extra = assay_metrics[name_other]\n",
    "    acc = {split: wgbs_extra[split][\"Accuracy\"] for split in wgbs_extra}\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=list(acc.values()),\n",
    "            name=\"wgbs_cpg*\",\n",
    "            boxmean=True,\n",
    "            boxpoints=\"all\",\n",
    "            showlegend=True,\n",
    "            marker=dict(size=3, color=\"black\"),\n",
    "            line=dict(width=1, color=\"black\"),\n",
    "            fillcolor=assay_colors[\"wgbs\"],\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=[f\"{split}: {value:.4f}\" for split, value in acc.items()],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if y_range is not None:\n",
    "        fig.update_yaxes(range=y_range)\n",
    "\n",
    "    title_text = \"NN classification (100kb_all_none) - Sample ontology - Training per assay VS mixed\"\n",
    "    if title_end:\n",
    "        title_text += f\" - {title_end}\"\n",
    "    fig.update_layout(\n",
    "        title_text=title_text,\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        xaxis_title=\"Assay\",\n",
    "        width=1000,\n",
    "        height=700,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    if logdir:\n",
    "        name = name if name else \"acc_training_per_assay\"\n",
    "        fig.write_image(logdir / f\"{name}.svg\")\n",
    "        fig.write_image(logdir / f\"{name}.png\")\n",
    "        fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"flagship\" / \"cell_type_unique_assay_training\"\n",
    "for y_min in [0.4, 0.5]:\n",
    "    NN_performance_per_assay(\n",
    "        assay_metrics=assay_metrics,\n",
    "        name=f\"cell_type_training_per_assay_Y_{y_min:.2f}\",\n",
    "        logdir=logdir,\n",
    "        y_range=[y_min, 1.001],\n",
    "    )\n",
    "# NN_performance_per_assay(\n",
    "#     assay_metrics=assay_metrics,\n",
    "#     y_range=[y_min, 1.001],\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
