{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create supplementary prediction files destined for the paper.\n",
    "\n",
    "Must include all data predictions used to create paper figures.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    SplitResultsHandler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "table_dir = paper_dir / \"tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### assay_epiclass + sample ontology for all 5 model types - 100kb_all_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df_for_save(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Prepare DataFrame for saving to CSV.\"\"\"\n",
    "    df.insert(0, \"Expected class\", df.pop(\"True class\"))\n",
    "    df.set_index(\"md5sum\", inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_100kb = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "logdir = table_dir / \"dfreeze_v2\" / \"100kb_all_none\"\n",
    "if not logdir.exists():\n",
    "    logdir.mkdir(parents=True)\n",
    "\n",
    "split_md5sums = []\n",
    "for category in [ASSAY, CELL_TYPE]:\n",
    "    all_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "        results_dir=data_dir_100kb,\n",
    "        label_category=category,\n",
    "        only_NN=False,\n",
    "    )\n",
    "\n",
    "    # Sanity check, same shape, same input files for each method\n",
    "    for split_dict in all_split_dfs.values():\n",
    "        ref_dict = split_dict[\"NN\"]\n",
    "        ref_md5sums = sorted(ref_dict.index.values.tolist())\n",
    "        ref_shape = ref_dict.shape\n",
    "        for method, df in split_dict.items():\n",
    "            if not ref_md5sums == sorted(df.index.values.tolist()):\n",
    "                raise ValueError(\"MD5sums do not match\")\n",
    "            if ref_shape != df.shape:\n",
    "                raise ValueError(\"Shapes do not match\")\n",
    "\n",
    "    all_split_dfs_concat = split_results_handler.concatenate_split_results(all_split_dfs)\n",
    "\n",
    "    # Save to file\n",
    "    for method, df in all_split_dfs_concat.items():\n",
    "        df = prepare_df_for_save(df)\n",
    "\n",
    "        if method == \"NN\":\n",
    "            method = \"MLP\"\n",
    "\n",
    "        filename = f\"10fold_predictions_{category}_{method}.csv\"\n",
    "        # df.to_csv(logdir / filename, index=True, sep=\",\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other MLP results - 100kb_all_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"paired_end\",\n",
    "    \"harmonized_sample_cancer_high\",\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    \"harmonized_biomaterial_type\",\n",
    "    \"project\",\n",
    "]\n",
    "\n",
    "# Select 10-fold oversampling runs\n",
    "all_split_dfs = split_results_handler.general_split_metrics(\n",
    "    results_dir=data_dir_100kb,\n",
    "    merge_assays=False,\n",
    "    include_categories=categories,\n",
    "    exclude_names=[\"reg\", \"no-mixed\", \"chip\"],\n",
    "    return_type=\"split_results\",\n",
    "    oversampled_only=True,\n",
    "    verbose=False,\n",
    ")\n",
    "all_split_dfs_concat = split_results_handler.concatenate_split_results(all_split_dfs, concat_first_level=True)  # type: ignore\n",
    "\n",
    "# Save to file\n",
    "for category, df in all_split_dfs_concat.items():\n",
    "    df = prepare_df_for_save(df)\n",
    "\n",
    "    filename = f\"10fold_predictions_{category}_MLP.csv\"\n",
    "    # df.to_csv(logdir / filename, index=True, sep=\",\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for other feature sets (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_splits_identity(\n",
    "    all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]],\n",
    "    task_names: List[str],\n",
    "    verbose: bool | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Verify that the splits are identical between feature sets for each task.\n",
    "\n",
    "    all_results: {feature_set: {task_name: {split_name: results_dataframe}}}\n",
    "    task_names: list of task names to verify\n",
    "    verbose: print additional information\n",
    "    \"\"\"\n",
    "    # Sanity check : MD5sums and shapes should match between reference and other feature sets, for each split\n",
    "    for task_name in task_names:\n",
    "        if verbose:\n",
    "            print(f\"Verifying task '{task_name}'\")\n",
    "        # Select a reference feature set and use its splits as the baseline for comparison\n",
    "        reference_feature_set = \"hg38_100kb_all_none\"\n",
    "        reference_splits = all_results[reference_feature_set][task_name]\n",
    "\n",
    "        # Create reference MD5sums and shapes for each split in the reference feature set\n",
    "        reference_md5sums = {\n",
    "            split_name: sorted(df.index.tolist())\n",
    "            for split_name, df in reference_splits.items()\n",
    "        }\n",
    "        reference_shapes = {\n",
    "            split_name: df.shape for split_name, df in reference_splits.items()\n",
    "        }\n",
    "\n",
    "        # Iterate over each feature set and compare its splits against the reference\n",
    "        for feature_set_name, tasks_dict in all_results.items():\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Verifying feature set '{feature_set_name}' against reference feature set '{reference_feature_set}'\"\n",
    "                )\n",
    "            for split_name, df in tasks_dict[task_name].items():\n",
    "                if reference_shapes[split_name] != df.shape:\n",
    "                    print(\n",
    "                        f\"WARNING: Shape mismatch in task '{task_name}', split '{split_name}', \"\n",
    "                        f\"between reference feature set '{reference_feature_set}' and feature set '{feature_set_name}'\",\n",
    "                        file=sys.stderr,\n",
    "                    )\n",
    "                if reference_md5sums[split_name] != sorted(df.index.tolist()):\n",
    "                    print(\n",
    "                        f\"WARNING: MD5sums mismatch in task '{task_name}', split '{split_name}', \"\n",
    "                        f\"between reference feature set '{reference_feature_set}' and feature set '{feature_set_name}'\",\n",
    "                        file=sys.stderr,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ASSAY, CELL_TYPE]\n",
    "include_sets = [\n",
    "    \"hg38_10mb_all_none_1mb_coord\",\n",
    "    \"hg38_100kb_random_n316_none\",\n",
    "    \"hg38_1mb_all_none\",\n",
    "    \"hg38_100kb_random_n3044_none\",\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_gene_regions_100kb_coord_n19864\",\n",
    "    \"hg38_10kb_random_n30321_none\",\n",
    "    \"hg38_regulatory_regions_n30321\",\n",
    "    \"hg38_1kb_random_n30321_none\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n",
    "    \"hg38_10kb_all_none\",\n",
    "    \"hg38_regulatory_regions_n303114\",\n",
    "    \"hg38_1kb_random_n303114_none\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n300k\",\n",
    "]\n",
    "exclude_names = [\"7c\", \"chip-seq-only\", \"27ct\", \"16ct\"]\n",
    "\n",
    "# Select 10-fold oversampling runs\n",
    "# expected result shape: {feature_set: {task_name: {split_name: results_dataframe}}}\n",
    "all_results: Dict[\n",
    "    str, Dict[str, Dict[str, pd.DataFrame]]\n",
    "] = split_results_handler.obtain_all_feature_set_data(\n",
    "    return_type=\"split_results\",\n",
    "    parent_folder=data_dir_100kb.parent,\n",
    "    merge_assays=False,\n",
    "    include_categories=categories,\n",
    "    include_sets=include_sets,\n",
    "    exclude_names=exclude_names,\n",
    "    verbose=False,\n",
    ")  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_set_name in all_results.keys():\n",
    "    try:\n",
    "        all_results[feature_set_name][ASSAY] = all_results[feature_set_name][\"assay_epiclass_11c\"]  # type: ignore\n",
    "        del all_results[feature_set_name][\"assay_epiclass_11c\"]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_splits_identity(all_results, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = table_dir / \"dfreeze_v2\" / \"other_feature_sets\"\n",
    "logdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for feature_set_name, tasks_dict in all_results.items():\n",
    "    if feature_set_name == \"hg38_100kb_all_none\":\n",
    "        continue\n",
    "    all_split_dfs_concat = split_results_handler.concatenate_split_results(\n",
    "        tasks_dict, concat_first_level=True\n",
    "    )\n",
    "    for task_name, df in all_split_dfs_concat.items():\n",
    "        df = prepare_df_for_save(df)\n",
    "\n",
    "        filename = f\"{feature_set_name}_10fold_predictions_{task_name}.csv\"\n",
    "        # df.to_csv(logdir / filename, index=True, sep=\",\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winsorized files and/or blacklist zeroed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [ASSAY, CELL_TYPE, SEX, \"harmonized_biomaterial_type\"]\n",
    "include_sets = [\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_100kb_all_none_0blklst\",\n",
    "    \"hg38_100kb_all_none_0blklst_winsorized\",\n",
    "]\n",
    "\n",
    "results_folder = base_data_dir / \"training_results\" / \"2023-01-epiatlas-freeze\"\n",
    "if not results_folder.exists():\n",
    "    raise FileNotFoundError(f\"Folder '{results_folder}' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10-fold oversampling runs\n",
    "# expected result shape: {feature_set: {task_name: {split_name: results_dataframe}}}\n",
    "all_results: Dict[\n",
    "    str, Dict[str, Dict[str, pd.DataFrame]]\n",
    "] = split_results_handler.obtain_all_feature_set_data(\n",
    "    return_type=\"split_results\",\n",
    "    parent_folder=results_folder,\n",
    "    merge_assays=False,\n",
    "    include_categories=categories,\n",
    "    include_sets=include_sets,\n",
    "    oversampled_only=False,\n",
    "    verbose=False,\n",
    ")  # type: ignore\n",
    "\n",
    "all_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_collected = list(all_results[\"hg38_100kb_all_none\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_splits_identity(all_results, tasks_collected, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
