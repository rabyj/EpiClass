{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to quantify bias present in metadata\n",
    "Q: Can you identify certain labels by using other metadata\n",
    "e.g. find cell type using project+assay+other\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, pointless-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    create_mislabel_corrector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata_df = metadata_handler.load_metadata_df(\"v2\")\n",
    "metadata = metadata_handler.load_metadata(\"v2\")\n",
    "\n",
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate bias in input samples classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect observed average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "\n",
    "exclusion = [\"cancer\", \"random\", \"track\", \"disease\", \"second\", \"end\"]\n",
    "exclude_names = [\"chip\", \"no-mixed\", \"ct\", \"7c\"]\n",
    "\n",
    "all_split_results = split_results_handler.general_split_metrics(\n",
    "    results_dir=results_dir,\n",
    "    exclude_categories=exclusion,\n",
    "    exclude_names=exclude_names,\n",
    "    merge_assays=True,\n",
    "    mislabel_corrections=create_mislabel_corrector(paper_dir),\n",
    "    return_type=\"split_results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_split_results: Dict[str, pd.DataFrame] = split_results_handler.concatenate_split_results(all_split_results, concat_first_level=True)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_name, df in list(concat_split_results.items()):\n",
    "    new_df = metadata_handler.join_metadata(df, metadata)\n",
    "    concat_split_results[cat_name] = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_input_acc = {}\n",
    "for cat_name, df in list(concat_split_results.items()):\n",
    "    # filtered_df = df[df[ASSAY] == \"input\"]\n",
    "    filtered_df = df\n",
    "    acc = (filtered_df[\"True class\"] == filtered_df[\"Predicted class\"]).sum() / len(\n",
    "        filtered_df\n",
    "    )\n",
    "    avg_input_acc[cat_name] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(avg_input_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_input_acc[SEX] = avg_input_acc[\"harmonized_donor_sex_w-mixed\"]\n",
    "concat_split_results[SEX] = concat_split_results[\"harmonized_donor_sex_w-mixed\"]\n",
    "\n",
    "avg_input_acc[ASSAY] = avg_input_acc[\"assay_epiclass_11c\"]\n",
    "concat_split_results[ASSAY] = concat_split_results[\"assay_epiclass_11c\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute max bias accuracy using metadata as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_input_bias_categories(target_category: str) -> List[List[str]]:\n",
    "    \"\"\"Define bias categories used for bias analysis.\n",
    "\n",
    "    Args:\n",
    "        target_category (str): Classification target category. Is excluded from input lists.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: List of bias categories.\n",
    "    \"\"\"\n",
    "    bias_categories_1 = [ASSAY, \"project\", \"harmonized_biomaterial_type\", CELL_TYPE]\n",
    "    bias_categories_2 = [\n",
    "        ASSAY,\n",
    "        \"project\",\n",
    "        \"harmonized_biomaterial_type\",\n",
    "        CELL_TYPE,\n",
    "        LIFE_STAGE,\n",
    "    ]\n",
    "    bias_categories_3 = [ASSAY, \"project\", \"harmonized_biomaterial_type\", CELL_TYPE, SEX]\n",
    "    bias_categories_4 = [\n",
    "        ASSAY,\n",
    "        \"project\",\n",
    "        \"harmonized_biomaterial_type\",\n",
    "        CELL_TYPE,\n",
    "        SEX,\n",
    "        LIFE_STAGE,\n",
    "    ]\n",
    "\n",
    "    all_bias_categories = [\n",
    "        bias_categories_1,\n",
    "        bias_categories_2,\n",
    "        bias_categories_3,\n",
    "        bias_categories_4,\n",
    "    ]\n",
    "    for bias_categories in all_bias_categories:\n",
    "        try:\n",
    "            bias_categories.remove(target_category)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return all_bias_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models() -> List:\n",
    "    \"\"\"Create models for bias analysis.\"\"\"\n",
    "    lr_model_1 = LogisticRegression(\n",
    "        solver=\"lbfgs\", max_iter=1000, multi_class=\"multinomial\", random_state=42\n",
    "    )\n",
    "    lr_model_2 = LogisticRegression(\n",
    "        solver=\"lbfgs\", max_iter=1000, multi_class=\"ovr\", random_state=42\n",
    "    )\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "    svm_model_rbf = SVC(kernel=\"rbf\", random_state=42)\n",
    "    return [lr_model_1, lr_model_2, rf_model, svm_model, svm_model_rbf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_samples(\n",
    "    metadata_df: pd.DataFrame, target_category: str, verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Filter samples based on the output category to match the original training set.\"\"\"\n",
    "    df = metadata_df.copy(deep=True)\n",
    "\n",
    "    if \"md5sum\" not in df.columns:\n",
    "        df[\"md5sum\"] = df.index\n",
    "\n",
    "    df = df[df[\"md5sum\"].isin(concat_split_results[target_category][\"md5sum\"])]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Metadata shape:\", metadata_df.shape)\n",
    "        print(\"Filtered shape:\", df.shape)\n",
    "        display(df[target_category].value_counts())\n",
    "\n",
    "    return df  # type: ignore\n",
    "\n",
    "\n",
    "def find_max_bias(\n",
    "    metadata_df: pd.DataFrame, target_category: str, verbose: bool = True\n",
    ") -> Dict[Tuple[str, ...], float]:\n",
    "    \"\"\"Find the bias categories that provide the highest accuracy for the target category.\"\"\"\n",
    "\n",
    "    filtered_df = filter_samples(metadata_df, target_category)\n",
    "\n",
    "    max_bias_dict = {}\n",
    "    for bias_categories in define_input_bias_categories(target_category):\n",
    "        print(f\"Using bias categories: {bias_categories}\")\n",
    "        X = filtered_df[bias_categories]\n",
    "        y = filtered_df[target_category]\n",
    "\n",
    "        # one-hot encode the data\n",
    "        X_encoded = OneHotEncoder().fit_transform(X).toarray()  # type: ignore\n",
    "        y_encoded = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        max_acc = 0\n",
    "        for model in create_models():\n",
    "            scores = cross_val_score(\n",
    "                model, X_encoded, y_encoded, cv=10, scoring=\"accuracy\", n_jobs=-1\n",
    "            )\n",
    "            if verbose:\n",
    "                print(f\"Model: {model}\")\n",
    "                print(f\"Accuracy: {np.mean(scores):.2f} (+/- {np.std(scores):.2f})\")\n",
    "            if np.mean(scores) > max_acc:\n",
    "                max_acc = np.mean(scores)\n",
    "                max_bias_dict[tuple(bias_categories)] = max_acc\n",
    "\n",
    "    return max_bias_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_max_bias(\n",
    "    metadata_df: pd.DataFrame, target_categories: List[str], verbose: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Compute the max metadata bias for all target categories.\"\"\"\n",
    "    final_results: Dict[str, Any] = {}\n",
    "    for target_category in target_categories:\n",
    "        if verbose:\n",
    "            print(f\"Target category: {target_category}\")\n",
    "\n",
    "        max_bias_dict = find_max_bias(metadata_df, target_category)\n",
    "        max_bias_cats, max_bias_acc = max(max_bias_dict.items(), key=lambda x: x[1])\n",
    "        if verbose:\n",
    "            print(f\"Max bias categories: {max_bias_cats}\")\n",
    "            print(f\"Max bias acc: {max_bias_acc}\")\n",
    "\n",
    "        MLP_acc = avg_input_acc[target_category]\n",
    "\n",
    "        acc_to_compare = [\n",
    "            acc for cat, acc in avg_input_acc.items() if cat in max_bias_cats\n",
    "        ]\n",
    "        avg_MLP_acc = np.mean(acc_to_compare)\n",
    "        max_acc_with_bias = max_bias_acc * avg_MLP_acc\n",
    "\n",
    "        if verbose:\n",
    "            print(\"CLASSIFICATION ACCURACY\")\n",
    "            print(f\"Average {target_category} observed acc: {MLP_acc:.1%}\")\n",
    "            print(f\"Average MLP acc on bias categories: {avg_MLP_acc:.1%}\")\n",
    "            print(\n",
    "                f\"Max avg acc with bias from ({max_bias_cats}): {max_acc_with_bias:.1%}\"\n",
    "            )\n",
    "            print(f\"Not accounted for: {MLP_acc - max_acc_with_bias:.1%}\\n\")\n",
    "\n",
    "        final_results[target_category] = {\n",
    "            \"max_bias_cats\": max_bias_cats,\n",
    "            \"max_bias_acc\": max_bias_acc,\n",
    "            \"MLP_acc\": MLP_acc,\n",
    "            \"bias_avg_MLP_acc\": avg_MLP_acc,\n",
    "            \"max_bias_acc_corrected\": max_acc_with_bias,\n",
    "            \"acc_diff\": MLP_acc - max_acc_with_bias,\n",
    "        }\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_categories = [\"project\", \"harmonized_biomaterial_type\", CELL_TYPE, SEX, LIFE_STAGE]\n",
    "final_results = compute_all_max_bias(metadata_df, target_categories)\n",
    "\n",
    "final_results_df = pd.DataFrame.from_dict(final_results, orient=\"index\")\n",
    "final_results_df.to_csv(\"metadata_bias_analysis_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
