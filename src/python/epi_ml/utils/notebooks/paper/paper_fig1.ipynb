{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create figures destined for the paper.\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    CELL_TYPE,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    merge_similar_assays,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_fig_dir = base_fig_dir / \"fig1_EpiAtlas_assay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata_v2 = metadata_handler.load_metadata(\"v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare assay predictions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "    results_dir=base_data_dir / \"dfreeze_v2\",\n",
    "    label_category=ASSAY,\n",
    "    only_NN=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dfs = split_results_handler.concatenate_split_results(all_split_dfs)\n",
    "merged_dfs = {classifier: merge_similar_assays(df) for classifier, df in full_dfs.items()}\n",
    "assays = merged_dfs[next(iter(merged_dfs))][\"True class\"].unique()\n",
    "\n",
    "# Add Max pred\n",
    "for classifier, df in merged_dfs.items():\n",
    "    df[\"Max pred\"] = df[assays].max(axis=1)\n",
    "\n",
    "# Join metadata\n",
    "for classifier, df in merged_dfs.items():\n",
    "    merged_dfs[classifier] = metadata_handler.join_metadata(df, metadata_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction score per assay\n",
    "\n",
    "Average distribution of prediction scores per assay \n",
    "violin plot. One point per UUID, track types averaged (combine 2xRNA and 2xWGBS)\n",
    "points with 3 colors: \n",
    "- black for pred same class\n",
    "- red for pred different class/mislabel\n",
    "- orange bad qual (IHEC flag, was removed in later stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig1_a(\n",
    "    NN_results: pd.DataFrame, logdir: Path, name: str, merge_assay_pairs: bool\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with violin plots and associated scatter plots for each class.\n",
    "    Red scatter points, indicating a mismatch, appear on top and have a larger size.\n",
    "\n",
    "    Args:\n",
    "        NN_results (pd.DataFrame): The DataFrame containing the neural network results, including metadata.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "        name (str): The name of the figure.\n",
    "        merge_assay_pairs (bool): Whether to merge similar assays (mrna/rna, wgbs-pbat/wgbs-standard)\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    y_range = [0.42, 1.01]  # Y-axis range for the plot\n",
    "\n",
    "    # Combine similar assays\n",
    "    if merge_assay_pairs:\n",
    "        NN_results = merge_similar_assays(NN_results)\n",
    "\n",
    "    # Adjustments for replacement and class ordering\n",
    "    class_labels = NN_results[\"True class\"].unique()\n",
    "    class_labels_sorted = sorted(class_labels)\n",
    "    class_index = {label: i for i, label in enumerate(class_labels_sorted)}\n",
    "\n",
    "    scatter_offset = 0.05  # Scatter plot jittering\n",
    "\n",
    "    for label in class_labels_sorted:\n",
    "        df = NN_results[NN_results[\"True class\"] == label]\n",
    "\n",
    "        # Majority vote, mean prediction score\n",
    "        groupby_epirr = df.groupby([\"EpiRR\", \"Predicted class\"])[\"Max pred\"].aggregate(\n",
    "            [\"size\", \"mean\"]\n",
    "        )\n",
    "\n",
    "        groupby_epirr = groupby_epirr.reset_index().sort_values(\n",
    "            [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "        )\n",
    "        groupby_epirr = groupby_epirr.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "        assert groupby_epirr[\"EpiRR\"].is_unique\n",
    "\n",
    "        mean_pred = groupby_epirr[\"mean\"]\n",
    "\n",
    "        # Add violin plot with integer x positions\n",
    "        line_color = \"white\"\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[class_index[label]] * len(mean_pred),\n",
    "                y=mean_pred,\n",
    "                name=label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=\"all\",\n",
    "                fillcolor=assay_colors[label],\n",
    "                line_color=line_color,\n",
    "                line=dict(width=0.8),\n",
    "                marker=dict(size=1, color=assay_colors[label]),\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # # Prepare data for scatter plots\n",
    "        # jittered_x_positions = np.random.uniform(-scatter_offset, scatter_offset, size=len(mean_pred)) + class_index[label] - 0.25  # type: ignore\n",
    "\n",
    "        # match_pred = [\n",
    "        #     mean_pred.iloc[i]\n",
    "        #     for i, row in enumerate(groupby_epirr.iterrows())\n",
    "        #     if row[1][\"Predicted class\"] == label\n",
    "        # ]\n",
    "        # mismatch_pred = [\n",
    "        #     mean_pred.iloc[i]\n",
    "        #     for i, row in enumerate(groupby_epirr.iterrows())\n",
    "        #     if row[1][\"Predicted class\"] != label\n",
    "        # ]\n",
    "\n",
    "        # match_x_positions = [\n",
    "        #     jittered_x_positions[i]\n",
    "        #     for i, row in enumerate(groupby_epirr.iterrows())\n",
    "        #     if row[1][\"Predicted class\"] == label\n",
    "        # ]\n",
    "        # mismatch_x_positions = [\n",
    "        #     jittered_x_positions[i]\n",
    "        #     for i, row in enumerate(groupby_epirr.iterrows())\n",
    "        #     if row[1][\"Predicted class\"] != label\n",
    "        # ]\n",
    "\n",
    "        # # Add scatter plots for matches in black\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         x=match_x_positions,\n",
    "        #         y=match_pred,\n",
    "        #         mode=\"markers\",\n",
    "        #         name=f\"Match {label}\",\n",
    "        #         marker=dict(\n",
    "        #             color=\"black\",\n",
    "        #             size=1,  # Standard size for matches\n",
    "        #         ),\n",
    "        #         hoverinfo=\"text\",\n",
    "        #         hovertext=[\n",
    "        #             f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.2f}\"\n",
    "        #             for row in groupby_epirr.iterrows()\n",
    "        #             if row[1][\"Predicted class\"] == label\n",
    "        #         ],\n",
    "        #         showlegend=False,\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        # # Add scatter plots for mismatches in red, with larger size\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         x=mismatch_x_positions,\n",
    "        #         y=mismatch_pred,\n",
    "        #         mode=\"markers\",\n",
    "        #         name=f\"Mismatch {label}\",\n",
    "        #         marker=dict(\n",
    "        #             color=\"red\",\n",
    "        #             size=3,  # Larger size for mismatches\n",
    "        #         ),\n",
    "        #         hoverinfo=\"text\",\n",
    "        #         hovertext=[\n",
    "        #             f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.3f}\"\n",
    "        #             for row in groupby_epirr.iterrows()\n",
    "        #             if row[1][\"Predicted class\"] != label\n",
    "        #         ],\n",
    "        #         showlegend=False,\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "    # Update layout to improve visualization\n",
    "    fig.update_layout(\n",
    "        title_text=\"Prediction score distribution per assay class\",\n",
    "        yaxis_title=\"Average prediction score (majority class)\",\n",
    "        xaxis_title=\"Expected class label\",\n",
    "    )\n",
    "    fig.update_yaxes(range=y_range)\n",
    "    fig.update_xaxes(tickvals=list(class_index.values()), ticktext=class_labels_sorted)\n",
    "\n",
    "    # # Add a dummy scatter plot for legend - black points\n",
    "    # fig.add_trace(\n",
    "    #     go.Scatter(\n",
    "    #         x=[None],\n",
    "    #         y=[None],\n",
    "    #         mode=\"markers\",\n",
    "    #         name=\"Match\",\n",
    "    #         marker=dict(color=\"black\", size=10),\n",
    "    #         showlegend=True,\n",
    "    #         legendgroup=\"match\",\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # # Add a dummy scatter plot for legend - red points\n",
    "    # fig.add_trace(\n",
    "    #     go.Scatter(\n",
    "    #         x=[None],\n",
    "    #         y=[None],\n",
    "    #         mode=\"markers\",\n",
    "    #         name=\"Mismatch\",\n",
    "    #         marker=dict(color=\"red\", size=10),\n",
    "    #         showlegend=True,\n",
    "    #         legendgroup=\"mismatch\",\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # # Update the layout to adjust the legend\n",
    "    # fig.update_layout(\n",
    "    #     legend=dict(\n",
    "    #         title_text=\"Legend\",\n",
    "    #         itemsizing=\"constant\",\n",
    "    #         orientation=\"h\",\n",
    "    #         yanchor=\"bottom\",\n",
    "    #         y=1.02,\n",
    "    #         xanchor=\"right\",\n",
    "    #         x=1,\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_results = merged_dfs[\"NN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig1--pred_score_per_assay\"\n",
    "# fig1_a(NN_results, logdir=logdir, name=\"pred_score_per_assay\", merge_assay_pairs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of classification algorithm (boxplot)\n",
    "\n",
    "Violin plot (10 folds) of overall accuracy for each model (NN, LR, RF, LGBM, SV).  \n",
    "For each split, 4 box plot per model:\n",
    "  - Acc\n",
    "  - F1\n",
    "  - AUROC (OvR, both micro/macro)\n",
    "\n",
    "Source files:  \n",
    "epiatlas-dfreeze-v2.1/hg38_100kb_all_none (oversampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_split_metrics(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    label_category: str,\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    ") -> None:\n",
    "    \"\"\"Render to box plots the metrics per classifier and split, each in its own subplot.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A dictionary containing metric scores for each classifier and split.\n",
    "    \"\"\"\n",
    "    metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "    classifiers = list(next(iter(split_metrics.values())).keys())\n",
    "\n",
    "    # Create subplots, one row for each metric\n",
    "    fig = make_subplots(rows=1, cols=len(metrics), subplot_titles=metrics)\n",
    "\n",
    "    colors = {\n",
    "        classifier: px.colors.qualitative.Plotly[i]\n",
    "        for i, classifier in enumerate(classifiers)\n",
    "    }\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        for classifier in classifiers:\n",
    "            values = [split_metrics[split][classifier][metric] for split in split_metrics]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=values,\n",
    "                    name=classifier,\n",
    "                    marker_color=colors[classifier],\n",
    "                    line=dict(color=\"black\", width=1),\n",
    "                    marker=dict(size=2),\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",  # or \"outliers\" to show only outliers\n",
    "                    pointpos=-1.4,\n",
    "                    showlegend=False,\n",
    "                    width=0.5,\n",
    "                    hoverinfo=\"text\",\n",
    "                    hovertext=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in zip(split_metrics, values)\n",
    "                    ],\n",
    "                ),\n",
    "                row=1,\n",
    "                col=i + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{label_category} classification - Metric distribution for 10fold cross-validation\",\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode=\"group\",\n",
    "    )\n",
    "\n",
    "    # Adjust y-axis\n",
    "    if label_category == ASSAY:\n",
    "        range_acc = [0.955, 1.001]\n",
    "        range_AUC = [0.992, 1.0001]\n",
    "    elif label_category == CELL_TYPE:\n",
    "        range_acc = [0.81, 1]\n",
    "        range_AUC = [0.96, 1]\n",
    "    else:\n",
    "        range_acc = [0.6, 1.001]\n",
    "        range_AUC = [0.9, 1.0001]\n",
    "\n",
    "    fig.update_layout(yaxis=dict(range=range_acc))\n",
    "    fig.update_layout(yaxis2=dict(range=range_acc))\n",
    "    fig.update_layout(yaxis3=dict(range=range_AUC))\n",
    "    fig.update_layout(yaxis4=dict(range=range_AUC))\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_category in [ASSAY, CELL_TYPE]:\n",
    "    path = base_data_dir / \"dfreeze_v2\"\n",
    "    all_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "        results_dir=path, label_category=label_category\n",
    "    )\n",
    "    split_metrics = split_results_handler.compute_split_metrics(all_split_dfs)\n",
    "\n",
    "    # plot_split_metrics(\n",
    "    #     split_metrics,\n",
    "    #     label_category=label_category,\n",
    "    #     logdir=base_fig_dir,\n",
    "    #     name=f\"{label_category}_10fold_metrics_all_classifiers\",\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction score per assay across classifiers\n",
    "\n",
    "Per model, compute score distribution per assay (1 violin per assay). No SVM. Agree black, red disagree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig1_supp_B(df_dict: Dict[str, pd.DataFrame], logdir: Path, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with subplots for each assay, each containing violin plots for different classifiers\n",
    "    and associated scatter plots for matches (in black) and mismatches (in red).\n",
    "\n",
    "    Args:\n",
    "        df_dict (Dict[str, pd.DataFrame]): Dictionary with the DataFrame containing the results for each classifier.\n",
    "        logdir (Path): The directory path for saving the figures.\n",
    "        name (str): The name for the saved figures.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    # Ignore LinearSVC and RandomForest for this figure\n",
    "    if \"LinearSVC\" in df_dict:\n",
    "        del df_dict[\"LinearSVC\"]\n",
    "    if \"RF\" in df_dict:\n",
    "        del df_dict[\"RF\"]\n",
    "\n",
    "    # Assuming all classifiers have the same assays for simplicity\n",
    "    first_key = next(iter(df_dict))\n",
    "    class_labels = df_dict[first_key][\"True class\"].unique()\n",
    "    class_labels_sorted = sorted(class_labels)\n",
    "    num_assays = len(class_labels_sorted)\n",
    "\n",
    "    classifiers = list(df_dict.keys())\n",
    "    classifier_index = {name: i for i, name in enumerate(classifiers)}\n",
    "    num_classifiers = len(classifiers)\n",
    "\n",
    "    scatter_offset = 0.05  # Scatter plot jittering\n",
    "\n",
    "    # Calculate the size of the grid\n",
    "    grid_size = int(np.ceil(np.sqrt(num_assays)))\n",
    "    rows, cols = grid_size, grid_size\n",
    "\n",
    "    # Create subplots with a square grid\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=class_labels_sorted,\n",
    "        shared_yaxes=\"all\",  # type: ignore\n",
    "        horizontal_spacing=0.05,\n",
    "        vertical_spacing=0.05,\n",
    "        y_title=\"Average prediction score\",\n",
    "    )\n",
    "    for idx, label in enumerate(class_labels_sorted):\n",
    "        row, col = divmod(idx, grid_size)\n",
    "        for classifier_name, classifier_df in df_dict.items():\n",
    "            df = classifier_df[classifier_df[\"True class\"] == label]\n",
    "\n",
    "            # Majority vote, mean prediction score\n",
    "            groupby_epirr = df.groupby([\"EpiRR\", \"Predicted class\"])[\n",
    "                \"Max pred\"\n",
    "            ].aggregate([\"size\", \"mean\"])\n",
    "            groupby_epirr = groupby_epirr.reset_index().sort_values(\n",
    "                [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "            )\n",
    "            groupby_epirr = groupby_epirr.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "            assert groupby_epirr[\"EpiRR\"].is_unique\n",
    "\n",
    "            mean_pred = groupby_epirr[\"mean\"]\n",
    "            classifier_pos = classifier_index[classifier_name]\n",
    "\n",
    "            # Add violin plot with integer x positions\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    x=classifier_pos * np.ones(len(mean_pred)),\n",
    "                    y=mean_pred,\n",
    "                    name=label,\n",
    "                    spanmode=\"hard\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=False,\n",
    "                    fillcolor=\"grey\",\n",
    "                    line_color=\"black\",\n",
    "                    line=dict(width=0.8),\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=row + 1,  # Plotly rows are 1-indexed\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "            # Prepare data for scatter plots\n",
    "            jittered_x_positions = np.random.uniform(-scatter_offset, scatter_offset, size=len(mean_pred)) + classifier_pos - 0.3  # type: ignore\n",
    "\n",
    "            match_pred = [\n",
    "                mean_pred.iloc[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] == label\n",
    "            ]\n",
    "            mismatch_pred = [\n",
    "                mean_pred.iloc[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] != label\n",
    "            ]\n",
    "\n",
    "            match_x_positions = [\n",
    "                jittered_x_positions[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] == label\n",
    "            ]\n",
    "            mismatch_x_positions = [\n",
    "                jittered_x_positions[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] != label\n",
    "            ]\n",
    "\n",
    "            # Add scatter plots for matches in black\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=match_x_positions,\n",
    "                    y=match_pred,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(color=\"black\", size=1),\n",
    "                    showlegend=False,\n",
    "                    name=f\"Match {classifier_name}\",\n",
    "                ),\n",
    "                row=row + 1,  # Plotly rows are 1-indexed\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "            # Add scatter plots for mismatches in red\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=mismatch_x_positions,\n",
    "                    y=mismatch_pred,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(color=\"red\", size=3),\n",
    "                    showlegend=False,\n",
    "                    name=f\"Mismatch {classifier_name}\",\n",
    "                ),\n",
    "                row=row + 1,  # Plotly rows are 1-indexed\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - black points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Match\",\n",
    "            marker=dict(color=\"black\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"match\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - red points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Mismatch\",\n",
    "            marker=dict(color=\"red\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"mismatch\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update the layout to adjust the legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            title_text=\"Legend\",\n",
    "            itemsizing=\"constant\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.025,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout to improve visualization, adjust if needed for better appearance with multiple classifiers\n",
    "    fig.update_layout(\n",
    "        title_text=\"Prediction score distribution per assay across classifiers\",\n",
    "        height=1500,  # Adjust the height as necessary\n",
    "        width=1500,  # Adjust the width based on the number of assays\n",
    "    )\n",
    "\n",
    "    # fig.update_layout(yaxis2=dict(range=[0.9, 1.01]))\n",
    "\n",
    "    # Adjust tick names\n",
    "    # Assuming equal spacing between each classifier on the x-axis\n",
    "    tickvals = list(\n",
    "        range(0, num_classifiers + 1)\n",
    "    )  # Generate tick values (1-indexed for Plotly)\n",
    "    ticktext = classifiers  # Use classifier names as tick labels\n",
    "    for i, j in itertools.product(range(rows), range(cols)):\n",
    "        fig.update_xaxes(tickvals=tickvals, ticktext=ticktext, row=i + 1, col=j + 1)\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_data_dir / \"fig1--pred_score_per_assay_across_classifiers\"\n",
    "# fig1_supp_B(merged_dfs, logdir=base_fig_dir, name=\"fig1_supp_B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusions matrices across classification algorithms\n",
    "\n",
    "For each classifier type\n",
    "\n",
    "Confusion matrix (1point=1 uuid) for observed datasets with average scores>0.9\n",
    "- Goal: Represent global predictions/mislabels. 11c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix input problem, file VS epirr incoherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    min_pred_score: float,\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    majority: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Create a confusion matrix for the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the neural network results.\n",
    "        min_pred_score (float): The minimum prediction score to consider.\n",
    "        logdir (Path): The directory path for saving the figures.\n",
    "        name (str): The name for the saved figures.\n",
    "        majority (bool): Whether to use majority vote (uuid-wise) for the predicted class.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    classes = sorted(df[\"True class\"].unique())\n",
    "    if \"Max pred\" not in df.columns:\n",
    "        df[\"Max pred\"] = df[classes].max(axis=1)  # type: ignore\n",
    "    filtered_df = df[df[\"Max pred\"] > min_pred_score]\n",
    "\n",
    "    if majority:\n",
    "        # Majority vote for predicted class\n",
    "        groupby_uuid = df.groupby([\"uuid\", \"True class\", \"Predicted class\"])[\n",
    "            \"Max pred\"\n",
    "        ].aggregate([\"size\", \"mean\"])\n",
    "        groupby_uuid = groupby_uuid.reset_index().sort_values(\n",
    "            [\"uuid\", \"True class\", \"size\"], ascending=[True, True, False]\n",
    "        )\n",
    "        groupby_uuid = groupby_uuid.drop_duplicates(\n",
    "            subset=[\"uuid\", \"True class\"], keep=\"first\"\n",
    "        )\n",
    "        filtered_df = groupby_uuid\n",
    "\n",
    "    confusion_mat = sk_cm(\n",
    "        filtered_df[\"True class\"], filtered_df[\"Predicted class\"], labels=classes\n",
    "    )\n",
    "\n",
    "    mat_writer = ConfusionMatrixWriter(labels=classes, confusion_matrix=confusion_mat)\n",
    "    mat_writer.to_all_formats(logdir, name=f\"{name}_n{len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pred_score = 0.9\n",
    "majority = True\n",
    "\n",
    "for classifier_name, df in full_dfs.items():\n",
    "    df_with_meta = metadata_handler.join_metadata(df, metadata_v2)\n",
    "    assert \"Predicted class\" in df_with_meta.columns\n",
    "\n",
    "    name = f\"{classifier_name}_pred>{min_pred_score}\"\n",
    "    if classifier_name == \"LinearSVC\":\n",
    "        name = f\"{classifier_name}\"\n",
    "\n",
    "    logdir = base_fig_dir / \"fig1_supp_F-assay_c11_confusion_matrices\"\n",
    "    if majority:\n",
    "        logdir = logdir / \"per_uuid\"\n",
    "    else:\n",
    "        logdir = logdir / \"per_file\"\n",
    "\n",
    "# create_confusion_matrix(\n",
    "#     df=df_with_meta,\n",
    "#     min_pred_score=min_pred_score,\n",
    "#     logdir=logdir,\n",
    "#     name=name,\n",
    "#     majority=majority,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputed prediction score\n",
    "\n",
    "Inference on imputed data: Violin plot with pred score per assay (like Fig1A)  \n",
    "VS  \n",
    "Inference on observed data, from imputed   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"fig1--imputed_pred_score\"\n",
    "data_dir = base_data_dir / \"imputation\"\n",
    "\n",
    "# Load data\n",
    "normal_inf_imputed_path = next(\n",
    "    (data_dir / \"hg38_100kb_all_none/assay_epiclass_1l_3000n\").glob(\"**/*.csv\")\n",
    ")\n",
    "normal_inf_imputed_df = pd.read_csv(\n",
    "    normal_inf_imputed_path, header=0, index_col=0, low_memory=False\n",
    ")\n",
    "\n",
    "imputed_inf_normal_path = next(\n",
    "    (data_dir / \"hg38_100kb_all_none_imputed/assay_epiclass_1l_3000n\").rglob(\"**/*.csv\")\n",
    ")\n",
    "imputed_inf_normal_df = pd.read_csv(\n",
    "    imputed_inf_normal_path, header=0, index_col=0, low_memory=False\n",
    ")\n",
    "\n",
    "# Plot\n",
    "assay_labels = normal_inf_imputed_df[\"True class\"].unique()\n",
    "for name, df in zip(\n",
    "    [\"train_normal_inf_imputed\", \"train_imputed_inf_normal\"],\n",
    "    [normal_inf_imputed_df, imputed_inf_normal_df],\n",
    "):\n",
    "    df[\"EpiRR\"] = list(df.index)\n",
    "    df[ASSAY] = df[\"True class\"]\n",
    "    df[\"Max pred\"] = df[assay_labels].max(axis=1)\n",
    "    # fig1_a(\n",
    "    #     df, logdir=fig_dir, name=f\"fig1_supp_D-{name}_n{len(df)}\", merge_assay_pairs=False\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN - Accuracy per assay (boxplot 10fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig1_acc_per_assay(\n",
    "    all_split_dfs: Dict[str, Dict[str, pd.DataFrame]], logdir: Path, name: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with a boxplot for the accuracy of each assay over all splits.\n",
    "\n",
    "    Args:\n",
    "        NN_results (pd.DataFrame): The DataFrame containing the neural network results.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "        name (str): The name of the figure.\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Compute accuracy per assay per split\n",
    "    assay_accuracies = defaultdict(dict)\n",
    "    for split_name, classifier_dict in all_split_dfs.items():\n",
    "        df = classifier_dict[\"NN\"]\n",
    "        df = merge_similar_assays(df)\n",
    "        pred_groupby = df.groupby([\"True class\"])[\"Predicted class\"].value_counts(\n",
    "            normalize=True\n",
    "        )\n",
    "        assay_acc = pred_groupby.unstack().fillna(0).max(axis=1)\n",
    "        assay_accuracies[split_name] = assay_acc.to_dict()\n",
    "\n",
    "    # invert the dictionary\n",
    "    assay_accuracies_inv = defaultdict(dict)\n",
    "    for split_name, assay_acc in assay_accuracies.items():\n",
    "        for assay, acc in assay_acc.items():\n",
    "            assay_accuracies_inv[assay][split_name] = acc\n",
    "\n",
    "    class_labels_sorted = sorted(assay_accuracies_inv.keys())\n",
    "\n",
    "    for label in class_labels_sorted:\n",
    "        hovertext = [\n",
    "            f\"{split_name}: {acc:.4f}\"\n",
    "            for split_name, acc in assay_accuracies_inv[label].items()\n",
    "        ]\n",
    "        line_color = \"black\"\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=label,\n",
    "                y=list(assay_accuracies_inv[label].values()),\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                fillcolor=assay_colors[label],\n",
    "                line_color=line_color,\n",
    "                marker=dict(size=3, color=assay_colors[label]),\n",
    "                showlegend=True,\n",
    "                hoverinfo=\"text\",\n",
    "                hovertext=hovertext,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout to improve visualization\n",
    "    fig.update_layout(\n",
    "        title_text=\"Neural network assay classification 10-fold accuracy\",\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        xaxis_title=\"Assay experiment\",\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "    results_dir=base_data_dir / \"dfreeze_v2\",\n",
    "    label_category=ASSAY,\n",
    "    only_NN=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig1--acc_per_assay\"\n",
    "name = \"fig1--acc_per_assay\"\n",
    "fig1_acc_per_assay(all_split_dfs, logdir=logdir, name=name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
