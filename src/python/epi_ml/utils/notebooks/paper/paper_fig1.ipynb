{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create figures destined for the paper.\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    merge_similar_assays,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_fig_dir = base_fig_dir / \"fig1_EpiAtlas_assay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata_v2 = metadata_handler.load_metadata(\"v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare assay predictions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_100kb = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "all_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "    results_dir=data_dir_100kb,\n",
    "    label_category=ASSAY,\n",
    "    only_NN=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dfs = split_results_handler.concatenate_split_results(all_split_dfs)\n",
    "merged_dfs = {classifier: merge_similar_assays(df) for classifier, df in full_dfs.items()}\n",
    "assays = merged_dfs[next(iter(merged_dfs))][\"True class\"].unique()\n",
    "\n",
    "# Add Max pred\n",
    "for classifier, df in merged_dfs.items():\n",
    "    df[\"Max pred\"] = df[assays].max(axis=1)\n",
    "\n",
    "# Join metadata\n",
    "for classifier, df in merged_dfs.items():\n",
    "    merged_dfs[classifier] = metadata_handler.join_metadata(df, metadata_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction score per assay\n",
    "\n",
    "Average distribution of prediction scores per assay \n",
    "violin plot. One point per UUID, track types averaged (combine 2xRNA and 2xWGBS)\n",
    "points with 3 colors: \n",
    "- black for pred same class\n",
    "- red for pred different class/mislabel\n",
    "- orange bad qual (IHEC flag, was removed in later stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig1_a(\n",
    "    NN_results: pd.DataFrame,\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    merge_assay_pairs: bool,\n",
    "    min_y: float = 0.7,\n",
    "    title: str | None = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with violin plots and associated scatter plots for each class.\n",
    "    Red scatter points, indicating a mismatch, appear on top and have a larger size.\n",
    "\n",
    "    Args:\n",
    "        NN_results (pd.DataFrame): The DataFrame containing the neural network results, including metadata.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "        name (str): The name of the figure.\n",
    "        merge_assay_pairs (bool): Whether to merge similar assays (mrna/rna, wgbs-pbat/wgbs-standard)\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    # Combine similar assays\n",
    "    if merge_assay_pairs:\n",
    "        NN_results = merge_similar_assays(NN_results)\n",
    "\n",
    "    # Adjustments for replacement and class ordering\n",
    "    class_index = {label: i for i, label in enumerate(ASSAY_ORDER)}\n",
    "\n",
    "    scatter_offset = 0.05  # Scatter plot jittering\n",
    "\n",
    "    for label in ASSAY_ORDER:\n",
    "        df = NN_results[NN_results[\"True class\"] == label]\n",
    "\n",
    "        # Majority vote, mean prediction score\n",
    "        groupby_epirr = df.groupby([\"EpiRR\", \"Predicted class\"])[\"Max pred\"].aggregate(\n",
    "            [\"size\", \"mean\"]\n",
    "        )\n",
    "\n",
    "        groupby_epirr = groupby_epirr.reset_index().sort_values(\n",
    "            [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "        )\n",
    "        groupby_epirr = groupby_epirr.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "        assert groupby_epirr[\"EpiRR\"].is_unique\n",
    "\n",
    "        mean_pred = groupby_epirr[\"mean\"]\n",
    "\n",
    "        line_color = \"white\"\n",
    "        # fig.add_trace(\n",
    "        #     go.Violin(\n",
    "        #         y=mean_pred,\n",
    "        #         name=label,\n",
    "        #         spanmode=\"hard\",\n",
    "        #         box_visible=True,\n",
    "        #         meanline_visible=True,\n",
    "        #         points=\"all\",\n",
    "        #         pointpos=0,\n",
    "        #         fillcolor=assay_colors[label],\n",
    "        #         line_color=line_color,\n",
    "        #         line=dict(width=0.5),\n",
    "        #         marker=dict(size=2, color=\"black\"),\n",
    "        #         showlegend=False,\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[class_index[label]] * len(mean_pred),\n",
    "                y=mean_pred,\n",
    "                name=label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=False,\n",
    "                fillcolor=assay_colors[label],\n",
    "                line_color=line_color,\n",
    "                line=dict(width=0.5),\n",
    "                marker=dict(size=2, color=\"black\"),\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Prepare data for scatter plots\n",
    "        jittered_x_positions = np.random.uniform(-scatter_offset, scatter_offset, size=len(mean_pred)) + class_index[label] - 0.25  # type: ignore\n",
    "\n",
    "        match_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] == label\n",
    "        ]\n",
    "        mismatch_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] != label\n",
    "        ]\n",
    "\n",
    "        match_x_positions = [\n",
    "            jittered_x_positions[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] == label\n",
    "        ]\n",
    "        mismatch_x_positions = [\n",
    "            jittered_x_positions[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] != label\n",
    "        ]\n",
    "\n",
    "        # Add scatter plots for matches in black\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=match_x_positions,\n",
    "                y=match_pred,\n",
    "                mode=\"markers\",\n",
    "                name=f\"Match {label}\",\n",
    "                marker=dict(\n",
    "                    color=\"black\",\n",
    "                    size=1,  # Standard size for matches\n",
    "                ),\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.2f}\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] == label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "                legendgroup=\"match\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add scatter plots for mismatches in red, with larger size\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=mismatch_x_positions,\n",
    "                y=mismatch_pred,\n",
    "                mode=\"markers\",\n",
    "                name=f\"Mismatch {label}\",\n",
    "                marker=dict(\n",
    "                    color=\"red\",\n",
    "                    size=3,  # Larger size for mismatches\n",
    "                ),\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.3f}\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] != label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "                legendgroup=\"mismatch\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout to improve visualization\n",
    "    fig.update_yaxes(range=[min_y, 1.001])\n",
    "    fig.update_xaxes(tickvals=np.arange(len(ASSAY_ORDER)), ticktext=ASSAY_ORDER)\n",
    "\n",
    "    title_text = \"Prediction score distribution per assay class\"\n",
    "    if title is not None:\n",
    "        title_text += f\" - {title}\"\n",
    "    fig.update_layout(\n",
    "        title_text=title_text,\n",
    "        yaxis_title=\"Avg. prediction score (majority class)\",\n",
    "        xaxis_title=\"Expected class label\",\n",
    "    )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - black points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Match\",\n",
    "            marker=dict(color=\"black\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"match\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - red points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Mismatch\",\n",
    "            marker=dict(color=\"red\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"mismatch\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update the layout to adjust the legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            title_text=\"Legend\",\n",
    "            itemsizing=\"constant\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_results = merged_dfs[\"NN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig1--pred_score_per_assay\"\n",
    "# fig1_a(NN_results, logdir=logdir, name=\"pred_score_per_assay_internal\", merge_assay_pairs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of classification algorithm (boxplot)\n",
    "\n",
    "Violin plot (10 folds) of overall accuracy for each model (NN, LR, RF, LGBM, SV).  \n",
    "For each split, 4 box plot per model:\n",
    "  - Acc\n",
    "  - F1\n",
    "  - AUROC (OvR, both micro/macro)\n",
    "\n",
    "Source files:  \n",
    "epiatlas-dfreeze-v2.1/hg38_100kb_all_none (oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_CLASSIFIERS = [\"NN\", \"LR\", \"LGBM\", \"LinearSVC\", \"RF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_split_metrics_one_algo(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    label_category: str,\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    classifier_name: str = \"NN\",\n",
    ") -> None:\n",
    "    \"\"\"Render to box plots the metrics per classifier and split, each in its own subplot.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A dictionary containing metric scores for each classifier and split.\n",
    "    \"\"\"\n",
    "    if classifier_name not in ALL_CLASSIFIERS:\n",
    "        raise ValueError(f\"Invalid classifier name: {classifier_name}\")\n",
    "    metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "\n",
    "    # Create subplots, one row for each metric\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Accuracy/F1\", \"AUC micro/macro\"])\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [split_metrics[split][classifier][metric] for split in split_metrics]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=values,\n",
    "                name=metric,\n",
    "                line=dict(color=\"black\", width=1.5),\n",
    "                marker=dict(size=3, color=\"black\"),\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",  # or \"outliers\" to show only outliers\n",
    "                pointpos=-1.4,\n",
    "                showlegend=False,\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"{split}: {value:.4f}\" for split, value in zip(split_metrics, values)\n",
    "                ],\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1 if i < 2 else 2,\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{label_category} {classifier_name} classification - Metric distribution for 10fold cross-validation\",\n",
    "        yaxis_title=\"Value\",\n",
    "    )\n",
    "\n",
    "    # Adjust y-axis\n",
    "    if label_category == ASSAY:\n",
    "        # range_acc = [0.95, 1]\n",
    "        # range_AUC = [0.998, 1]\n",
    "        range_acc = [0.98, 1.001]\n",
    "        range_AUC = [0.996, 1.0001]\n",
    "    elif label_category == CELL_TYPE:\n",
    "        range_acc = [0.93, 1]\n",
    "        range_AUC = [0.996, 1]\n",
    "    else:\n",
    "        range_acc = [0.6, 1.001]\n",
    "        range_AUC = [0.9, 1.0001]\n",
    "\n",
    "    fig.update_layout(yaxis=dict(range=range_acc))\n",
    "    fig.update_layout(yaxis2=dict(range=range_AUC))\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_split_metrics(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    label_category: str,\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    ") -> None:\n",
    "    \"\"\"Render to box plots the metrics per classifier and split, each in its own subplot.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A dictionary containing metric scores for each classifier and split.\n",
    "    \"\"\"\n",
    "    metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "    classifier_names = list(next(iter(split_metrics.values())).keys())\n",
    "    classifier_names = [\"NN\", \"LR\", \"LGBM\", \"LinearSVC\", \"RF\"]\n",
    "\n",
    "    # # Sort classifiers by accuracy\n",
    "    # mean_acc = {}\n",
    "    # for classifier in classifier_names:\n",
    "    #     mean_acc[classifier] = np.mean(\n",
    "    #         [split_metrics[split][classifier][\"Accuracy\"] for split in split_metrics]\n",
    "    #     )\n",
    "    # classifier_names = sorted(classifier_names, key=lambda x: mean_acc[x], reverse=True)\n",
    "\n",
    "    # Create subplots, one row for each metric\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(metrics),\n",
    "        subplot_titles=metrics,\n",
    "        horizontal_spacing=0.075,\n",
    "    )\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        for classifier in classifier_names:\n",
    "            values = [split_metrics[split][classifier][metric] for split in split_metrics]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=values,\n",
    "                    name=classifier,\n",
    "                    line=dict(color=\"black\", width=1.5),\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",  # or \"outliers\" to show only outliers\n",
    "                    pointpos=-1.4,\n",
    "                    showlegend=False,\n",
    "                    width=0.5,\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in zip(split_metrics, values)\n",
    "                    ],\n",
    "                ),\n",
    "                row=1,\n",
    "                col=i + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{label_category} classification - Metric distribution for 10fold cross-validation\",\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode=\"group\",\n",
    "    )\n",
    "\n",
    "    # Adjust y-axis\n",
    "    if label_category == ASSAY:\n",
    "        # range_acc = [0.955, 1.001]\n",
    "        # range_AUC = [0.992, 1.0001]\n",
    "        range_acc = [0.98, 1.001]\n",
    "        range_AUC = [0.996, 1.0001]\n",
    "    elif label_category == CELL_TYPE:\n",
    "        range_acc = [0.81, 1]\n",
    "        range_AUC = [0.96, 1]\n",
    "    else:\n",
    "        range_acc = [0.6, 1.001]\n",
    "        range_AUC = [0.9, 1.0001]\n",
    "\n",
    "    fig.update_layout(yaxis=dict(range=range_acc))\n",
    "    fig.update_layout(yaxis2=dict(range=range_acc))\n",
    "    fig.update_layout(yaxis3=dict(range=range_AUC))\n",
    "    fig.update_layout(yaxis4=dict(range=range_AUC))\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_100kb = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "fig_dir = base_fig_dir / \"fig1--boxplot_perf_all_algorithms\"\n",
    "merge_assays = True\n",
    "\n",
    "# fig dir path\n",
    "assay_fig_dir = fig_dir / \"assay_computed\"\n",
    "if merge_assays:\n",
    "    assay_fig_dir = Path(f\"{assay_fig_dir}_9c\")\n",
    "else:\n",
    "    assay_fig_dir = Path(f\"{assay_fig_dir}_11c\")\n",
    "\n",
    "for label_category in [ASSAY, CELL_TYPE]:\n",
    "    all_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "        results_dir=data_dir_100kb, label_category=label_category\n",
    "    )\n",
    "\n",
    "    this_fig_dir = assay_fig_dir if label_category == ASSAY else fig_dir\n",
    "\n",
    "    name_base = f\"{label_category}_10fold_metrics\"\n",
    "    if merge_assays and label_category == ASSAY:\n",
    "        name_base += \"_merged_assays\"\n",
    "\n",
    "        for split_name, split_dfs in all_split_dfs.items():\n",
    "            for classifier_type, df in split_dfs.items():\n",
    "                all_split_dfs[split_name][classifier_type] = merge_similar_assays(df)\n",
    "\n",
    "    # split_metrics = split_results_handler.compute_split_metrics(all_split_dfs)\n",
    "\n",
    "    # plot_split_metrics(\n",
    "    #     split_metrics,\n",
    "    #     label_category=label_category,\n",
    "    #     logdir=this_fig_dir,\n",
    "    #     name=f\"{name_base}_all_classifiers_y0.98\",\n",
    "    # )\n",
    "\n",
    "    # plot_split_metrics_one_algo(\n",
    "    #     split_metrics,\n",
    "    #     label_category=label_category,\n",
    "    #     logdir=this_fig_dir,\n",
    "    #     name=f\"{name_base}_NN_y0.98\",\n",
    "    # )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction score per assay across classifiers\n",
    "\n",
    "Per model, compute score distribution per assay (1 violin per assay). No SVM. Agree black, red disagree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig1_supp_B(df_dict: Dict[str, pd.DataFrame], logdir: Path, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with subplots for each assay, each containing violin plots for different classifiers\n",
    "    and associated scatter plots for matches (in black) and mismatches (in red).\n",
    "\n",
    "    Args:\n",
    "        df_dict (Dict[str, pd.DataFrame]): Dictionary with the DataFrame containing the results for each classifier.\n",
    "        logdir (Path): The directory path for saving the figures.\n",
    "        name (str): The name for the saved figures.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    class_labels_sorted = ASSAY_ORDER\n",
    "    num_assays = len(class_labels_sorted)\n",
    "\n",
    "    classifiers = list(df_dict.keys())\n",
    "    try:\n",
    "        classifiers.remove(\"LinearSVC\")\n",
    "        classifiers.remove(\"RF\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "    classifier_index = {name: i for i, name in enumerate(classifiers)}\n",
    "    num_classifiers = len(classifiers)\n",
    "\n",
    "    scatter_offset = 0.05  # Scatter plot jittering\n",
    "\n",
    "    # Calculate the size of the grid\n",
    "    grid_size = int(np.ceil(np.sqrt(num_assays)))\n",
    "    rows, cols = grid_size, grid_size\n",
    "\n",
    "    # Create subplots with a square grid\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=class_labels_sorted,\n",
    "        shared_yaxes=\"all\",  # type: ignore\n",
    "        horizontal_spacing=0.05,\n",
    "        vertical_spacing=0.05,\n",
    "        y_title=\"Average prediction score\",\n",
    "    )\n",
    "    for idx, label in enumerate(class_labels_sorted):\n",
    "        row, col = divmod(idx, grid_size)\n",
    "        for classifier_name in classifiers:\n",
    "            df = df_dict[classifier_name]\n",
    "            df = df[df[\"True class\"] == label]\n",
    "\n",
    "            # Majority vote, mean prediction score\n",
    "            groupby_epirr = df.groupby([\"EpiRR\", \"Predicted class\"])[\n",
    "                \"Max pred\"\n",
    "            ].aggregate([\"size\", \"mean\"])\n",
    "            groupby_epirr = groupby_epirr.reset_index().sort_values(\n",
    "                [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "            )\n",
    "            groupby_epirr = groupby_epirr.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "            assert groupby_epirr[\"EpiRR\"].is_unique\n",
    "\n",
    "            mean_pred = groupby_epirr[\"mean\"]\n",
    "            classifier_pos = classifier_index[classifier_name]\n",
    "\n",
    "            # Add violin plot with integer x positions\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    x=classifier_pos * np.ones(len(mean_pred)),\n",
    "                    y=mean_pred,\n",
    "                    name=label,\n",
    "                    spanmode=\"hard\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=False,\n",
    "                    fillcolor=\"grey\",\n",
    "                    line_color=\"black\",\n",
    "                    line=dict(width=0.8),\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=row + 1,  # Plotly rows are 1-indexed\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "            # Prepare data for scatter plots\n",
    "            jittered_x_positions = np.random.uniform(-scatter_offset, scatter_offset, size=len(mean_pred)) + classifier_pos - 0.3  # type: ignore\n",
    "\n",
    "            match_pred = [\n",
    "                mean_pred.iloc[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] == label\n",
    "            ]\n",
    "            mismatch_pred = [\n",
    "                mean_pred.iloc[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] != label\n",
    "            ]\n",
    "\n",
    "            match_x_positions = [\n",
    "                jittered_x_positions[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] == label\n",
    "            ]\n",
    "            mismatch_x_positions = [\n",
    "                jittered_x_positions[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] != label\n",
    "            ]\n",
    "\n",
    "            # Add scatter plots for matches in black\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=match_x_positions,\n",
    "                    y=match_pred,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(color=\"black\", size=1),\n",
    "                    showlegend=False,\n",
    "                    name=f\"Match {classifier_name}\",\n",
    "                    legendgroup=\"match\",\n",
    "                ),\n",
    "                row=row + 1,  # Plotly rows are 1-indexed\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "            # Add scatter plots for mismatches in red\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=mismatch_x_positions,\n",
    "                    y=mismatch_pred,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(color=\"red\", size=3),\n",
    "                    showlegend=False,\n",
    "                    name=f\"Mismatch {classifier_name}\",\n",
    "                    legendgroup=\"mismatch\",\n",
    "                ),\n",
    "                row=row + 1,  # Plotly rows are 1-indexed\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - black points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Match\",\n",
    "            marker=dict(color=\"black\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"match\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - red points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Mismatch\",\n",
    "            marker=dict(color=\"red\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"mismatch\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update the layout to adjust the legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            title_text=\"Legend\",\n",
    "            itemsizing=\"constant\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.025,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout to improve visualization, adjust if needed for better appearance with multiple classifiers\n",
    "    fig.update_layout(\n",
    "        title_text=\"Prediction score distribution per assay across classifiers\",\n",
    "        height=1500,  # Adjust the height as necessary\n",
    "        width=1500,  # Adjust the width based on the number of assays\n",
    "    )\n",
    "\n",
    "    # fig.update_layout(yaxis2=dict(range=[0.9, 1.01]))\n",
    "\n",
    "    # Adjust tick names\n",
    "    # Assuming equal spacing between each classifier on the x-axis\n",
    "    tickvals = list(\n",
    "        range(0, num_classifiers + 1)\n",
    "    )  # Generate tick values (1-indexed for Plotly)\n",
    "    ticktext = classifiers  # Use classifier names as tick labels\n",
    "    for i, j in itertools.product(range(rows), range(cols)):\n",
    "        fig.update_xaxes(tickvals=tickvals, ticktext=ticktext, row=i + 1, col=j + 1)\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(merged_dfs['NN']))\n",
    "# for metadata_category in [\"EpiRR\", \"uuid\"]:\n",
    "#     print(f\"{metadata_category}: {len(merged_dfs['NN'][metadata_category].unique())}\")\n",
    "\n",
    "# display(merged_dfs[\"NN\"][ASSAY].value_counts())\n",
    "# display(merged_dfs[\"NN\"][\"track_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig1--pred_score_per_assay_across_classifiers\"\n",
    "# fig1_supp_B(merged_dfs, logdir=logdir, name=\"fig1_supp_B_3classifiers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig1_supp_B_2(df_dict: Dict[str, pd.DataFrame], logdir: Path, name: str) -> None:\n",
    "    \"\"\"\n",
    "    pred_score_per_assay_across_classifiers: 1 graph per classifier\n",
    "\n",
    "    Args:\n",
    "        df_dict (Dict[str, pd.DataFrame]): Dictionary with the DataFrame containing the results for each classifier.\n",
    "        logdir (Path): The directory path for saving the figures.\n",
    "        name (str): The name for the saved figures.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    for classifier_name, classifier_df in df_dict.items():\n",
    "        fig = go.Figure()\n",
    "        for assay in ASSAY_ORDER:\n",
    "            assay_df = classifier_df[classifier_df[\"True class\"] == assay]\n",
    "\n",
    "            # Majority vote, mean prediction score\n",
    "            groupby_epirr = assay_df.groupby([\"EpiRR\", \"Predicted class\"])[\n",
    "                \"Max pred\"\n",
    "            ].aggregate([\"size\", \"mean\"])\n",
    "            groupby_epirr = groupby_epirr.reset_index().sort_values(\n",
    "                [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "            )\n",
    "            groupby_epirr = groupby_epirr.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "            assert groupby_epirr[\"EpiRR\"].is_unique\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    y=groupby_epirr[\"mean\"],\n",
    "                    name=assay,\n",
    "                    spanmode=\"hard\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=\"all\",\n",
    "                    pointpos=0,\n",
    "                    fillcolor=assay_colors[assay],\n",
    "                    line_color=\"white\",\n",
    "                    line=dict(width=0.5),\n",
    "                    marker=dict(size=2, color=\"black\"),\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        fig.update_yaxes(range=[0.7, 1.001])\n",
    "        fig.update_layout(\n",
    "            title_text=f\"{classifier_name} - Prediction score distribution per assay\",\n",
    "        )\n",
    "\n",
    "        # Save figure\n",
    "        this_name = f\"{name}_{classifier_name}\"\n",
    "        fig.write_html(logdir / f\"{this_name}.html\")\n",
    "        fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "        fig.write_image(logdir / f\"{this_name}.png\")\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig1--pred_score_per_assay_across_classifiers\"\n",
    "# fig1_supp_B_2(merged_dfs, logdir=logdir, name=\"fig1_supp_B_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusions matrices across classification algorithms\n",
    "\n",
    "For each classifier type\n",
    "\n",
    "Confusion matrix (1point=1 uuid) for observed datasets with average scores>0.9\n",
    "- Goal: Represent global predictions/mislabels. 11c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    min_pred_score: float,\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    majority: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Create a confusion matrix for the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the neural network results.\n",
    "        min_pred_score (float): The minimum prediction score to consider.\n",
    "        logdir (Path): The directory path for saving the figures.\n",
    "        name (str): The name for the saved figures.\n",
    "        majority (bool): Whether to use majority vote (uuid-wise) for the predicted class.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    classes = sorted(df[\"True class\"].unique())\n",
    "    if \"Max pred\" not in df.columns:\n",
    "        df[\"Max pred\"] = df[classes].max(axis=1)  # type: ignore\n",
    "    filtered_df = df[df[\"Max pred\"] > min_pred_score]\n",
    "\n",
    "    if majority:\n",
    "        # Majority vote for predicted class\n",
    "        groupby_uuid = filtered_df.groupby([\"uuid\", \"True class\", \"Predicted class\"])[\n",
    "            \"Max pred\"\n",
    "        ].aggregate([\"size\", \"mean\"])\n",
    "\n",
    "        if groupby_uuid[\"size\"].max() > 3:\n",
    "            raise ValueError(\"More than three predictions for the same uuid\")\n",
    "\n",
    "        groupby_uuid = groupby_uuid.reset_index().sort_values(\n",
    "            [\"uuid\", \"True class\", \"size\"], ascending=[True, True, False]\n",
    "        )\n",
    "        groupby_uuid = groupby_uuid.drop_duplicates(\n",
    "            subset=[\"uuid\", \"True class\"], keep=\"first\"\n",
    "        )\n",
    "        filtered_df = groupby_uuid\n",
    "\n",
    "    confusion_mat = sk_cm(\n",
    "        filtered_df[\"True class\"], filtered_df[\"Predicted class\"], labels=classes\n",
    "    )\n",
    "\n",
    "    mat_writer = ConfusionMatrixWriter(labels=classes, confusion_matrix=confusion_mat)\n",
    "    mat_writer.to_all_formats(logdir, name=f\"{name}_n{len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_pred_score, majority in itertools.product([0, 0.9], [True, False]):\n",
    "    for classifier_name, df in full_dfs.items():\n",
    "        # df_with_meta = metadata_handler.join_metadata(df, metadata_v2)\n",
    "        # assert \"Predicted class\" in df_with_meta.columns\n",
    "        # # if classifier_name != \"NN\":\n",
    "        # #     continue\n",
    "\n",
    "        # name = f\"{classifier_name}_pred>{min_pred_score}\"\n",
    "        # if classifier_name == \"LinearSVC\":\n",
    "        #     name = f\"{classifier_name}\"\n",
    "\n",
    "        # logdir = base_fig_dir / \"fig1_supp_F-assay_c11_confusion_matrices\"\n",
    "        # if majority:\n",
    "        #     logdir = logdir / \"per_uuid\"\n",
    "        # else:\n",
    "        #     logdir = logdir / \"per_file\"\n",
    "\n",
    "        # create_confusion_matrix(\n",
    "        #     df=df_with_meta,\n",
    "        #     min_pred_score=min_pred_score,\n",
    "        #     logdir=logdir,\n",
    "        #     name=name,\n",
    "        #     majority=majority,\n",
    "        # )\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputed prediction score & accuracy\n",
    "\n",
    "Inference on imputed data: Violin plot with pred score per assay (like Fig1A)  \n",
    "VS  \n",
    "Inference on observed data, from imputed   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_folder = base_fig_dir / \"fig1--imputation_impact\" / \"complete_no_valid_oversample\"\n",
    "if not section_folder.exists():\n",
    "    raise FileNotFoundError(f\"Folder {section_folder} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_metadata_path = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"predictions\"\n",
    "    / \"C-A\"\n",
    "    / \"assay_epiclass\"\n",
    "    / \"CA_metadata_4DB+all_pred_subset.20240606_mod2.tsv\"\n",
    ")\n",
    "ca_metadata = pd.read_csv(ca_metadata_path, sep=\"\\t\")\n",
    "ca_id_col = ca_metadata.columns[0]\n",
    "ca_target = ca_metadata[[ca_id_col, \"manual_target_consensus\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = base_data_dir / \"training_results\" / \"imputation\"\n",
    "\n",
    "# Load data\n",
    "observed_dir = (\n",
    "    data_dir\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / \"assay_epiclass_1l_3000n\"\n",
    "    / \"chip-seq-only\"\n",
    "    / \"complete_no_valid_oversample\"\n",
    ")\n",
    "observed_inf_imputed_path = next((observed_dir / \"predict_imputed\").glob(\"*.csv\"))\n",
    "observed_inf_CA_path = next((observed_dir / \"predict_C-A\").glob(\"*.csv\"))\n",
    "\n",
    "observed_inf_imputed_df = pd.read_csv(\n",
    "    observed_inf_imputed_path, header=0, index_col=0, low_memory=False\n",
    ")\n",
    "observed_inf_CA_df = pd.read_csv(\n",
    "    observed_inf_CA_path, header=0, index_col=0, low_memory=False\n",
    ")\n",
    "\n",
    "imputed_dir = (\n",
    "    data_dir\n",
    "    / \"hg38_100kb_all_none_imputed\"\n",
    "    / \"assay_epiclass_1l_3000n\"\n",
    "    / \"chip-seq-only\"\n",
    "    / \"complete_no_valid_oversample\"\n",
    ")\n",
    "imputed_inf_observed_path = next(\n",
    "    (imputed_dir / \"predict_epiatlas_pval_chip-seq\").glob(\"*.csv\")\n",
    ")\n",
    "imputed_inf_CA_path = next((imputed_dir / \"predict_C-A\").glob(\"*.csv\"))\n",
    "\n",
    "imputed_inf_observed_df = pd.read_csv(\n",
    "    imputed_inf_observed_path, header=0, index_col=0, low_memory=False\n",
    ")\n",
    "imputed_inf_CA_df = pd.read_csv(\n",
    "    imputed_inf_CA_path, header=0, index_col=0, low_memory=False\n",
    ")\n",
    "\n",
    "imputed_inf_CA_df = imputed_inf_CA_df.merge(\n",
    "    ca_target, left_index=True, right_on=ca_id_col\n",
    ")\n",
    "observed_inf_CA_df = observed_inf_CA_df.merge(\n",
    "    ca_target, left_index=True, right_on=ca_id_col\n",
    ")\n",
    "for df in [imputed_inf_CA_df, observed_inf_CA_df]:\n",
    "    df[\"True class\"] = df[\"manual_target_consensus\"]\n",
    "    df.drop(\"manual_target_consensus\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_labels = observed_inf_imputed_df[\"True class\"].unique()\n",
    "var_names = [\n",
    "    \"observed_inf_imputed\",\n",
    "    \"observed_inf_C-A\",\n",
    "    \"imputed_inf_observed\",\n",
    "    \"imputed_inf_C-A\",\n",
    "]\n",
    "var_list = [\n",
    "    observed_inf_imputed_df,\n",
    "    observed_inf_CA_df,\n",
    "    imputed_inf_observed_df,\n",
    "    imputed_inf_CA_df,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get C-A core7 all track types preds\n",
    "wanted_cols = [\"True class\", \"Predicted class\", \"Max pred\"]\n",
    "for old_name, new_name in zip(\n",
    "    [\"manual_target_consensus\", \"Predicted_class_assay7\", \"Max_pred_assay7\"], wanted_cols\n",
    "):\n",
    "    ca_metadata.rename(columns={old_name: new_name}, inplace=True)\n",
    "\n",
    "ca_df = ca_metadata[ca_metadata[\"True class\"].isin(assay_labels)][\n",
    "    [\"Experimental-id\"] + wanted_cols\n",
    "]\n",
    "\n",
    "# Exclude predictions as 'input'\n",
    "ca_df = ca_df[ca_df[\"Predicted class\"] != \"input\"]\n",
    "name = \"obs_core7_inf_C-A\"\n",
    "\n",
    "var_names.append(name)\n",
    "var_list.append(ca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_fig_dir = section_folder / \"pred_scores\"\n",
    "if not this_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Folder {this_fig_dir} does not exist\")\n",
    "\n",
    "for name, df in zip(var_names, var_list):\n",
    "    print(df.shape)\n",
    "    df[\"EpiRR\"] = list(df.index)\n",
    "    df[ASSAY] = df[\"True class\"]\n",
    "    if \"Max pred\" not in df.columns:\n",
    "        df[\"Max pred\"] = df[assay_labels].max(axis=1)\n",
    "\n",
    "    name = f\"train_{name}\"\n",
    "    # print(f\"Graphing {name}\")\n",
    "\n",
    "    # min_y = 0.7\n",
    "    # fig1_a(\n",
    "    #     df, logdir=this_fig_dir, name=f\"imputation_impact-{name}_n{len(df)}_minY{min_y:.1f}\", merge_assay_pairs=False, min_y=min_y, title=name\n",
    "    # )\n",
    "\n",
    "    # min_y = 0.9\n",
    "    # fig1_a(\n",
    "    #     df, logdir=this_fig_dir, name=f\"imputation_impact-{name}_n{len(df)}_minY{min_y:.1f}\", merge_assay_pairs=False, min_y=min_y, title=name\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_fig_dir = section_folder / \"acc_per_assay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acc_per_assay = {}\n",
    "for name, df in zip(var_names, var_list):\n",
    "    if \"Max pred\" not in df.columns:\n",
    "        df[\"Max pred\"] = df[assay_labels].max(axis=1)\n",
    "\n",
    "    name = f\"train_{name}\"\n",
    "    # {assay: [(min_pred, acc, nb_samples), ...], ...}\n",
    "    acc_per_assay: Dict[str, List[Tuple[str, float, int]]] = {}\n",
    "    for label in assay_labels:\n",
    "        acc_per_assay[label] = []\n",
    "        assay_df = df[df[\"True class\"] == label]\n",
    "        for min_pred in [\"0.0\", \"0.6\", \"0.9\"]:\n",
    "            sub_df = assay_df[assay_df[\"Max pred\"] > float(min_pred)]\n",
    "            acc = (sub_df[\"True class\"] == sub_df[\"Predicted class\"]).mean()\n",
    "            acc_per_assay[label].append((min_pred, acc, len(sub_df)))\n",
    "\n",
    "    all_acc_per_assay[name] = acc_per_assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc per assay to table\n",
    "# cols = [classifier+task, assay, min_pred, acc, nb_samples]\n",
    "rows = []\n",
    "for name, acc_per_assay in all_acc_per_assay.items():\n",
    "    for assay, values in acc_per_assay.items():\n",
    "        for min_pred, acc, nb_samples in values:\n",
    "            rows.append([name, assay, min_pred, acc, nb_samples])\n",
    "df_acc_per_assay = pd.DataFrame(\n",
    "    rows, columns=[\"task_name\", \"assay\", \"min_predScore\", \"acc\", \"nb_samples\"]\n",
    ")\n",
    "df_acc_per_assay.to_csv(\n",
    "    this_fig_dir / \"imputation_impact_acc_per_assay.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_predScore_color_map = {\"0.0\": \"blue\", \"0.6\": \"orange\", \"0.9\": \"red\"}\n",
    "\n",
    "df_acc_per_assay[\"scatter_name\"] = (\n",
    "    df_acc_per_assay[\"task_name\"]\n",
    "    .replace(\"train_\", \"\", regex=True)\n",
    "    .replace(\"imputed\", \"imp\", regex=True)\n",
    "    .replace(\"observed\", \"obs\", regex=True)\n",
    ")\n",
    "df_acc_per_assay[\"inf_target\"] = df_acc_per_assay[\"scatter_name\"].str.split(\"_\").str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc_per_assay = df_acc_per_assay.sort_values(\n",
    "    [\"assay\", \"min_predScore\", \"scatter_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_type = \"no_C-A\"\n",
    "# graph_type = \"all\"\n",
    "# graph_type = \"only_C-A\"\n",
    "graph_type = \"only_C-A+core7\"\n",
    "\n",
    "if graph_type == \"no_C-A\":\n",
    "    minY = 0.97\n",
    "    maxY = 1.001\n",
    "    trace_per_assay = 2\n",
    "if graph_type == \"all\":\n",
    "    minY = 0.7\n",
    "    maxY = 1.005\n",
    "    trace_per_assay = 4\n",
    "if graph_type == \"only_C-A\":\n",
    "    minY = 0.7\n",
    "    maxY = 1.005\n",
    "    trace_per_assay = 2\n",
    "if graph_type == \"only_C-A+core7\":\n",
    "    minY = 0.7\n",
    "    maxY = 1.005\n",
    "    trace_per_assay = 3\n",
    "\n",
    "\n",
    "graph_df = df_acc_per_assay.copy()\n",
    "\n",
    "if graph_type == \"no_C-A\":\n",
    "    graph_df = graph_df[graph_df[\"inf_target\"] != \"C-A\"]\n",
    "elif \"only_C-A\" in graph_type:\n",
    "    graph_df = graph_df[graph_df[\"inf_target\"] == \"C-A\"]\n",
    "elif graph_type == \"all\":\n",
    "    pass\n",
    "\n",
    "if graph_type != \"only_C-A+core7\":\n",
    "    graph_df = graph_df[~graph_df[\"scatter_name\"].str.contains(\"core7\")]\n",
    "\n",
    "# Calculate average over assays\n",
    "avg_df = graph_df.groupby([\"min_predScore\", \"scatter_name\"])[\"acc\"].mean().reset_index()\n",
    "avg_df[\"assay\"] = \"Average\"\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for min_pred in [\"0.0\", \"0.6\", \"0.9\"]:\n",
    "    df_subset = graph_df[graph_df[\"min_predScore\"] == min_pred]\n",
    "    avg_subset = avg_df[avg_df[\"min_predScore\"] == min_pred]\n",
    "\n",
    "    # Add average over assay trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[\"Average - \" + name for name in avg_subset[\"scatter_name\"]],\n",
    "            y=avg_subset[\"acc\"],\n",
    "            mode=\"markers\",\n",
    "            name=f\"Avg Min Pred Score: {min_pred}\",\n",
    "            marker=dict(\n",
    "                color=min_predScore_color_map[min_pred],\n",
    "                size=9,\n",
    "                symbol=\"star\",\n",
    "            ),\n",
    "            hoverinfo=\"y+x\",\n",
    "            showlegend=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add individual assay traces\n",
    "    hovertext = list(\n",
    "        zip(df_subset[\"assay\"], df_subset[\"nb_samples\"].apply(lambda x: f\"Samples: {x}\"))\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_subset[\"assay\"] + \" - \" + df_subset[\"scatter_name\"],\n",
    "            y=df_subset[\"acc\"],\n",
    "            mode=\"markers\",\n",
    "            name=f\"Min Pred Score: {min_pred}\",\n",
    "            marker=dict(\n",
    "                color=min_predScore_color_map[min_pred],\n",
    "                size=9,\n",
    "            ),\n",
    "            text=hovertext,\n",
    "            hoverinfo=\"text+y+x\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Modify x-axis tick labels\n",
    "ticktext = []\n",
    "tick_group = list(df_subset[\"scatter_name\"].unique())\n",
    "for i, tick in enumerate(tick_group):\n",
    "    train, inf = tick.split(\"_inf_\")\n",
    "    tick_group[i] = f\"<b>{train}</b> \\u2192 <b>{inf}</b>\"\n",
    "\n",
    "for i in range(len(assay_labels)):\n",
    "    ticktext.extend(tick_group)\n",
    "\n",
    "fig.update_xaxes(tickmode=\"array\", ticktext=ticktext, tickvals=list(range(len(ticktext))))\n",
    "\n",
    "# Add assay labels on top + vertical lines between assay groups\n",
    "fig.add_annotation(\n",
    "    x=len(tick_group) / 2 - 0.5,\n",
    "    y=1.05,\n",
    "    yref=\"paper\",\n",
    "    text=\"Average\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=14),\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=len(tick_group) - 0.5, line_width=2, line_dash=\"solid\", line_color=\"black\"\n",
    ")\n",
    "\n",
    "for i, label in enumerate(sorted(assay_labels)):\n",
    "    fig.add_annotation(\n",
    "        x=(i + 1) * len(tick_group) + len(tick_group) / 2 - 0.5,\n",
    "        y=1.05,\n",
    "        yref=\"paper\",\n",
    "        text=label,\n",
    "        showarrow=False,\n",
    "        font=dict(size=14),\n",
    "    )\n",
    "    fig.add_vline(\n",
    "        x=(i + 1) * len(tick_group) - 0.5,\n",
    "        line_width=1,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"black\",\n",
    "    )\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=1.15,\n",
    "    y=0.6,\n",
    "    yref=\"paper\",\n",
    "    xref=\"paper\",\n",
    "    text=\"obs = observed<br>imp = imputed\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=14),\n",
    ")\n",
    "\n",
    "# titles + yaxis range\n",
    "fig.update_layout(\n",
    "    title=\"Accuracy per Assay and Task\",\n",
    "    xaxis_title=\"Assay - Task (training data \\u2192 inference data)\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    xaxis_tickangle=-45,\n",
    "    showlegend=True,\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    yaxis=dict(tickformat=\".2%\", range=[minY, maxY]),\n",
    ")\n",
    "\n",
    "fig.add_hline(y=1, line_width=1, line_color=\"black\")\n",
    "\n",
    "# Show/Write the plot\n",
    "print(f\"Graphing {graph_type}\")\n",
    "figname = f\"imputation_impact_{graph_type}_acc_per_assay_minY{minY:.2f}\"\n",
    "fig.write_html(this_fig_dir / f\"{figname}.html\")\n",
    "fig.write_image(this_fig_dir / f\"{figname}.png\")\n",
    "fig.write_image(this_fig_dir / f\"{figname}.svg\")\n",
    "fig.show()\n",
    "\n",
    "del fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph accuracy boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_fig_dir = section_folder / \"acc_per_assay\" / \"boxplot\" / \"assay_colored\"\n",
    "if not this_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Folder {this_fig_dir} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_type = \"no_C-A\"\n",
    "# graph_type = \"all\"\n",
    "# graph_type = \"only_C-A\"\n",
    "graph_type = \"only_C-A+core7\"\n",
    "\n",
    "if graph_type == \"no_C-A\":\n",
    "    minY = 0.97\n",
    "    maxY = 1.001\n",
    "if graph_type == \"all\":\n",
    "    minY = 0.7\n",
    "    maxY = 1.005\n",
    "if graph_type == \"only_C-A\":\n",
    "    minY = 0.7\n",
    "    maxY = 1.005\n",
    "\n",
    "graph_df = df_acc_per_assay.copy()\n",
    "graph_df = graph_df.sort_values([\"inf_target\", \"scatter_name\"])\n",
    "if graph_type == \"no_C-A\":\n",
    "    graph_df = graph_df[graph_df[\"inf_target\"] != \"C-A\"]\n",
    "elif \"only_C-A\" in graph_type:\n",
    "    graph_df = graph_df[graph_df[\"inf_target\"] == \"C-A\"]\n",
    "\n",
    "if graph_type != \"only_C-A+core7\":\n",
    "    graph_df = graph_df[~graph_df[\"scatter_name\"].str.contains(\"core7\")]\n",
    "\n",
    "\n",
    "# Prepare boxplot data\n",
    "tick_group = graph_df[\"scatter_name\"].unique()\n",
    "scatter_name_to_position = {name: i for i, name in enumerate(tick_group)}\n",
    "\n",
    "min_pred_values = [\"0.0\", \"0.6\", \"0.9\"]\n",
    "offset = [-0.25, 0, 0.25]  # Offset for each min_pred within a tick group\n",
    "\n",
    "fig = go.Figure()\n",
    "for name in tick_group:\n",
    "    group = graph_df[graph_df[\"scatter_name\"] == name]\n",
    "\n",
    "    for i, min_pred in enumerate(min_pred_values):\n",
    "        df_subset = group[group[\"min_predScore\"] == min_pred]\n",
    "\n",
    "        x_position = scatter_name_to_position[name] + offset[i]\n",
    "        x_positions = [x_position] * len(df_subset)\n",
    "        y_values = df_subset[\"acc\"]\n",
    "        hover_texts = [\n",
    "            f\"{row['assay']}<br>Samples: {row['nb_samples']}\"\n",
    "            for _, row in df_subset.iterrows()\n",
    "        ]\n",
    "        colors = [assay_colors[assay] for assay in df_subset[\"assay\"]]\n",
    "\n",
    "        # Add box plot without points\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                x=x_positions,\n",
    "                y=y_values,\n",
    "                name=f\"{name} - Min Pred Score: {min_pred}\",\n",
    "                line=dict(\n",
    "                    color=min_predScore_color_map[min_pred],\n",
    "                ),\n",
    "                boxpoints=False,\n",
    "                boxmean=True,\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "        # Add scatter plot for individual points\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[x + np.random.uniform(-0.01, 0.01) for x in x_positions],\n",
    "                y=y_values,\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=colors, size=8, line=dict(color=\"Black\", width=1)),\n",
    "                name=f\"{name} - Min Pred Score: {min_pred}\",\n",
    "                showlegend=False,\n",
    "                text=hover_texts,\n",
    "                hoverinfo=\"text+y\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Update x-axis tick labels\n",
    "ticktext = []\n",
    "for tick in tick_group:\n",
    "    train, inf = tick.split(\"_inf_\")\n",
    "    ticktext.append(f\"<b>{train}</b> \\u2192 <b>{inf}</b>\")\n",
    "\n",
    "fig.update_xaxes(tickmode=\"array\", ticktext=ticktext, tickvals=list(range(len(ticktext))))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Accuracy per Task (6 core assays)\",\n",
    "    xaxis_title=\"Task (training data \\u2192 inference data)\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    showlegend=True,\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    yaxis=dict(tickformat=\".2%\", range=[minY, maxY]),\n",
    ")\n",
    "\n",
    "# Add a legend for minPred colors\n",
    "for val, color in min_predScore_color_map.items():\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=10, color=color, symbol=\"square\"),\n",
    "            name=f\"Min Pred Score: {val}\",\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add a legend for assay colors\n",
    "for assay in sorted(assay_labels):\n",
    "    color = assay_colors[assay]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=10, color=color),\n",
    "            name=assay,\n",
    "            legendgroup=\"assays\",\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Add legend for obs and imp\n",
    "fig.add_annotation(\n",
    "    x=1.2,\n",
    "    y=0.3,\n",
    "    yref=\"paper\",\n",
    "    xref=\"paper\",\n",
    "    text=\"obs = observed<br>imp = imputed\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=14),\n",
    ")\n",
    "\n",
    "# Show/Write the plot\n",
    "figname = f\"imputation_impact_{graph_type}_boxplot_minY{minY:.2f}\"\n",
    "fig.write_html(this_fig_dir / f\"{figname}.html\")\n",
    "fig.write_image(this_fig_dir / f\"{figname}.png\")\n",
    "fig.write_image(this_fig_dir / f\"{figname}.svg\")\n",
    "fig.show()\n",
    "\n",
    "del fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence threshold VS samples conserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_fig_dir = section_folder / \"samples_conserved\"\n",
    "if not this_fig_dir.exists():\n",
    "    raise ValueError(f\"Directory {this_fig_dir} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_left_dict = {}\n",
    "\n",
    "for name, df in zip(var_names, var_list):\n",
    "    df[ASSAY] = df[\"True class\"]\n",
    "    pred_scores = sorted(df[\"Max pred\"])\n",
    "\n",
    "    nb_samples = len(df)\n",
    "    x_vals = np.linspace(0, 1, 200)\n",
    "    y_vals = np.zeros_like(x_vals)\n",
    "    y_vals_acc = np.zeros_like(x_vals)\n",
    "    for i, min_pred in enumerate(x_vals):\n",
    "        sub_df = df[df[\"Max pred\"] >= min_pred]\n",
    "\n",
    "        samples_left_ratio = len(sub_df) / nb_samples\n",
    "        y_vals[i] = samples_left_ratio\n",
    "\n",
    "        acc = (sub_df[\"True class\"] == sub_df[\"Predicted class\"]).mean()\n",
    "        y_vals_acc[i] = acc\n",
    "\n",
    "    samples_left_dict[name] = (x_vals, y_vals, y_vals_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_min_pred_score_metrics = {}\n",
    "\n",
    "for name, df in zip(var_names, var_list):\n",
    "    fixed_min_pred_score_metrics[name] = {}\n",
    "    for assay in assay_labels:\n",
    "        sub_df = df[df[\"True class\"] == assay]\n",
    "        nb_samples = len(sub_df)\n",
    "\n",
    "        x_vals = np.linspace(0, 1, 200)\n",
    "        y_vals = np.zeros_like(x_vals)\n",
    "        y_vals_acc = np.zeros_like(x_vals)\n",
    "        for i, min_pred in enumerate(x_vals):\n",
    "            sub_df = sub_df[sub_df[\"Max pred\"] >= min_pred]\n",
    "\n",
    "            samples_left_ratio = len(sub_df) / nb_samples\n",
    "            y_vals[i] = samples_left_ratio\n",
    "\n",
    "            acc = (sub_df[\"True class\"] == sub_df[\"Predicted class\"]).mean()\n",
    "            y_vals_acc[i] = acc\n",
    "\n",
    "        fixed_min_pred_score_metrics[name][assay] = (x_vals, y_vals, y_vals_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_sample_nb_metrics = {}\n",
    "for name, df in zip(var_names, var_list):\n",
    "    fixed_sample_nb_metrics[name] = {}\n",
    "    for assay in assay_labels:\n",
    "        sub_df = df[df[\"True class\"] == assay]\n",
    "        nb_samples = len(sub_df)\n",
    "\n",
    "        # Define the fixed ratios of samples to keep\n",
    "        fixed_ratios = np.linspace(0, 1, 200)\n",
    "\n",
    "        min_pred_scores = []\n",
    "        accuracies = []\n",
    "\n",
    "        for ratio in fixed_ratios:\n",
    "            samples_to_keep = int(nb_samples * ratio)\n",
    "            if samples_to_keep == 0:\n",
    "                samples_to_keep = 1\n",
    "\n",
    "            # Sort by prediction score in descending order\n",
    "            sorted_df = sub_df.sort_values(\"Max pred\", ascending=False)\n",
    "\n",
    "            # Keep top samples\n",
    "            kept_df = sorted_df.head(samples_to_keep)\n",
    "\n",
    "            # Compute min prediction score for kept samples\n",
    "            min_pred_score = kept_df[\"Max pred\"].min()\n",
    "            min_pred_scores.append(min_pred_score)\n",
    "\n",
    "            # Compute accuracy for kept samples\n",
    "            acc = (kept_df[\"True class\"] == kept_df[\"Predicted class\"]).mean()\n",
    "            accuracies.append(acc)\n",
    "\n",
    "        fixed_sample_nb_metrics[name][assay] = (fixed_ratios, min_pred_scores, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "for i, (name, (x_vals, y_vals, y_vals_acc)) in enumerate(samples_left_dict.items()):\n",
    "    if not name.endswith(\"C-A\"):\n",
    "        continue\n",
    "\n",
    "    name = (\n",
    "        name.replace(\"_inf_C-A\", \"\")\n",
    "        .replace(\"observed\", \"train_observed\")\n",
    "        .replace(\"imputed\", \"train_imputed\")\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_vals,\n",
    "            y=y_vals_acc,\n",
    "            name=f\"{name}: Acc\",\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=colors[i]),\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_vals,\n",
    "            y=y_vals,\n",
    "            name=f\"{name}: % left\",\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=colors[i], dash=\"dash\"),\n",
    "            yaxis=\"y2\",\n",
    "            showlegend=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Accuracy and samples conserved for C-A predictions.\",\n",
    "    xaxis_title=\"Minimum prediction score\",\n",
    "    yaxis=dict(title=\"Accuracy (%)\", tickformat=\".2%\"),\n",
    "    yaxis2=dict(title=\"Subset Size (%)\", overlaying=\"y\", side=\"right\", tickformat=\".2%\"),\n",
    "    legend=dict(orientation=\"v\", x=1.08, y=1),\n",
    "    showlegend=True,\n",
    "    height=600,\n",
    "    width=1000,\n",
    ")\n",
    "\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "fig.update_xaxes(range=[0, 1])\n",
    "\n",
    "# Show/Write the plot\n",
    "figname = \"C-A_predScore_samples_left_w_core7\"\n",
    "fig.write_html(this_fig_dir / f\"{figname}.html\")\n",
    "fig.write_image(this_fig_dir / f\"{figname}.png\")\n",
    "fig.write_image(this_fig_dir / f\"{figname}.svg\")\n",
    "fig.show()\n",
    "\n",
    "del fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "\n",
    "for assay in assay_labels:\n",
    "    for name in var_names:\n",
    "        if not name.endswith(\"C-A\"):\n",
    "            continue\n",
    "\n",
    "        if \"obs\" in name:\n",
    "            marker = \"circle\"\n",
    "        elif \"imp\" in name:\n",
    "            marker = \"x\"\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid name: {name}\")\n",
    "\n",
    "        x_vals, y_vals, y_vals_acc = fixed_min_pred_score_metrics[name][assay]\n",
    "\n",
    "        color = assay_colors[assay]\n",
    "        name = (\n",
    "            name.replace(\"_inf_C-A\", \"\")\n",
    "            .replace(\"observed\", \"train_observed\")\n",
    "            .replace(\"imputed\", \"train_imputed\")\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_vals,\n",
    "                y=y_vals_acc,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=color),\n",
    "                showlegend=False,\n",
    "                name=f\"{assay},{name},Acc\",\n",
    "                legendgroup=f\"{assay}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_vals[::10],\n",
    "                y=y_vals_acc[::10],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    color=color, symbol=marker, line_color=\"black\", line_width=1.5, size=8\n",
    "                ),\n",
    "                showlegend=False,\n",
    "                name=f\"{assay},{name},Acc\",\n",
    "                legendgroup=f\"{assay}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_vals,\n",
    "                y=y_vals,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=color, dash=\"dash\"),\n",
    "                yaxis=\"y2\",\n",
    "                showlegend=False,\n",
    "                name=f\"{assay},{name},% samples\",\n",
    "                legendgroup=f\"{assay}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_vals[::10],\n",
    "                y=y_vals[::10],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    color=color, symbol=marker, line_color=\"black\", line_width=1.5, size=8\n",
    "                ),\n",
    "                yaxis=\"y2\",\n",
    "                showlegend=False,\n",
    "                name=f\"{assay},{name},% samples\",\n",
    "                legendgroup=f\"{assay}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[None],\n",
    "                y=[None],\n",
    "                mode=\"lines+markers\",\n",
    "                marker=dict(\n",
    "                    color=color, symbol=marker, line_color=\"black\", line_width=1.5, size=8\n",
    "                ),\n",
    "                name=f\"{name} - {assay}\",\n",
    "                showlegend=True,\n",
    "                legendgroup=f\"{assay}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Accuracy and samples conserved for C-A predictions.\",\n",
    "        xaxis_title=\"Minimum prediction score\",\n",
    "        yaxis=dict(title=\"Accuracy (%)\", tickformat=\".2%\"),\n",
    "        yaxis2=dict(\n",
    "            title=\"Subset Size (%)\", overlaying=\"y\", side=\"right\", tickformat=\".2%\"\n",
    "        ),\n",
    "        legend=dict(orientation=\"v\", x=1.15, y=1),\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        showlegend=True,\n",
    "    )\n",
    "\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "fig.update_xaxes(range=[0, 1])\n",
    "\n",
    "# fig.add_vline(x=0.603, line_dash=\"dash\", line_color=\"black\")\n",
    "# fig.add_hline(y=87.96/100, line_dash=\"dash\", line_color=\"black\", yref=\"y2\")\n",
    "\n",
    "# fig.add_vline(x=0.7186, line_dash=\"dash\", line_color=\"black\")\n",
    "\n",
    "# Show/Write the plot\n",
    "figname = \"C-A_predScore_samples_left_per_assay\"\n",
    "fig.write_html(this_fig_dir / f\"{figname}.html\")\n",
    "fig.write_image(this_fig_dir / f\"{figname}.png\")\n",
    "fig.write_image(this_fig_dir / f\"{figname}.svg\")\n",
    "fig.show()\n",
    "\n",
    "del fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "for assay in assay_labels:\n",
    "    for name in var_names:\n",
    "        if not name.endswith(\"C-A\"):\n",
    "            continue\n",
    "        if \"obs\" in name:\n",
    "            marker = \"circle\"\n",
    "        elif \"imp\" in name:\n",
    "            marker = \"x\"\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid name: {name}\")\n",
    "\n",
    "        fixed_ratios, min_pred_scores, accuracies = fixed_sample_nb_metrics[name][assay]\n",
    "        color = assay_colors[assay]\n",
    "        name = (\n",
    "            name.replace(\"_inf_C-A\", \"\")\n",
    "            .replace(\"observed\", \"train_observed\")\n",
    "            .replace(\"imputed\", \"train_imputed\")\n",
    "        )\n",
    "\n",
    "        # Accuracy trace\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=fixed_ratios,\n",
    "                y=accuracies,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=color),\n",
    "                showlegend=False,\n",
    "                name=f\"{assay},{name},Acc\",\n",
    "                legendgroup=f\"{assay}\",\n",
    "            )\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=fixed_ratios[::10],\n",
    "                y=accuracies[::10],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    color=color, symbol=marker, line_color=\"black\", line_width=1.5, size=8\n",
    "                ),\n",
    "                showlegend=False,\n",
    "                name=f\"{assay},{name},Acc\",\n",
    "                legendgroup=f\"{assay}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Minimum prediction score trace\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=fixed_ratios,\n",
    "                y=min_pred_scores,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=color, dash=\"dash\"),\n",
    "                yaxis=\"y2\",\n",
    "                showlegend=False,\n",
    "                name=f\"{assay},{name},Min pred score\",\n",
    "                legendgroup=f\"{assay}\",\n",
    "            )\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=fixed_ratios[::10],\n",
    "                y=min_pred_scores[::10],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    color=color, symbol=marker, line_color=\"black\", line_width=1.5, size=8\n",
    "                ),\n",
    "                yaxis=\"y2\",\n",
    "                showlegend=False,\n",
    "                name=f\"{assay},{name},Min pred score\",\n",
    "                legendgroup=f\"{assay}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Legend trace\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[None],\n",
    "                y=[None],\n",
    "                mode=\"lines+markers\",\n",
    "                marker=dict(\n",
    "                    color=color, symbol=marker, line_color=\"black\", line_width=1.5, size=8\n",
    "                ),\n",
    "                name=f\"{name} - {assay}\",\n",
    "                showlegend=True,\n",
    "                legendgroup=f\"{assay}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Accuracy and minimum prediction score for C-A predictions.\",\n",
    "    xaxis_title=\"Ratio of samples kept\",\n",
    "    yaxis=dict(title=\"Accuracy (%)\", tickformat=\".2%\"),\n",
    "    yaxis2=dict(\n",
    "        title=\"Minimum prediction score\", overlaying=\"y\", side=\"right\", tickformat=\".3f\"\n",
    "    ),\n",
    "    legend=dict(orientation=\"v\", x=1.15, y=1),\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    showlegend=True,\n",
    ")\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "fig.update_xaxes(range=[0, 1])\n",
    "\n",
    "# Show/Write the plot\n",
    "# figname = \"C-A_sampleRatio_minPredScore_per_assay\"\n",
    "# fig.write_html(this_fig_dir / f\"{figname}.html\")\n",
    "# fig.write_image(this_fig_dir / f\"{figname}.png\")\n",
    "# fig.write_image(this_fig_dir / f\"{figname}.svg\")\n",
    "fig.show()\n",
    "del fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN - Accuracy per assay (boxplot 10fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig1_acc_per_assay(\n",
    "    all_split_dfs: Dict[str, Dict[str, pd.DataFrame]], logdir: Path, name: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with a boxplot for the accuracy of each assay over all splits.\n",
    "\n",
    "    Args:\n",
    "        NN_results (pd.DataFrame): The DataFrame containing the neural network results.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "        name (str): The name of the figure.\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Compute accuracy per assay per split\n",
    "    assay_accuracies = defaultdict(dict)\n",
    "    for split_name, classifier_dict in all_split_dfs.items():\n",
    "        df = classifier_dict[\"NN\"]\n",
    "        df = merge_similar_assays(df)\n",
    "        pred_groupby = df.groupby([\"True class\"])[\"Predicted class\"].value_counts(\n",
    "            normalize=True\n",
    "        )\n",
    "        assay_acc = pred_groupby.unstack().fillna(0).max(axis=1)\n",
    "        assay_accuracies[split_name] = assay_acc.to_dict()\n",
    "\n",
    "    # invert the dictionary\n",
    "    assay_accuracies_inv = defaultdict(dict)\n",
    "    for split_name, assay_acc in assay_accuracies.items():\n",
    "        for assay, acc in assay_acc.items():\n",
    "            assay_accuracies_inv[assay][split_name] = acc\n",
    "\n",
    "    class_labels_sorted = ASSAY_ORDER\n",
    "\n",
    "    for label in class_labels_sorted:\n",
    "        hovertext = [\n",
    "            f\"{split_name}: {acc:.4f}\"\n",
    "            for split_name, acc in assay_accuracies_inv[label].items()\n",
    "        ]\n",
    "        line_color = \"black\"\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=label,\n",
    "                y=list(assay_accuracies_inv[label].values()),\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                fillcolor=assay_colors[label],\n",
    "                line_color=line_color,\n",
    "                marker=dict(size=3, color=assay_colors[label]),\n",
    "                showlegend=True,\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(range=[0.985, 1.001])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Neural network assay classification 10-fold accuracy\",\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        xaxis_title=\"Assay experiment\",\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "    results_dir=base_data_dir / \"dfreeze_v2\" / \"hg38_100kb_all_none\",\n",
    "    label_category=ASSAY,\n",
    "    only_NN=True,\n",
    ")\n",
    "\n",
    "logdir = base_fig_dir / \"fig1--acc_per_assay\"\n",
    "name = \"fig1--acc_per_assay\"\n",
    "fig1_acc_per_assay(all_split_dfs, logdir=logdir, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_split_dfs = split_results_handler.gather_split_results_across_methods(\n",
    "    results_dir=base_data_dir / \"dfreeze_v2\" / \"hg38_10kb_all_none\",\n",
    "    label_category=ASSAY,\n",
    "    only_NN=True,\n",
    ")\n",
    "\n",
    "logdir = paper_dir / \"figures\" / \"fig2_EpiAtlas_other\" / \"fig2--10kb\"\n",
    "if not logdir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {logdir} does not exist\")\n",
    "\n",
    "name = f\"fig2--{ASSAY}_acc_per_assay_10kb\"\n",
    "fig1_acc_per_assay(all_split_dfs, logdir=logdir, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
