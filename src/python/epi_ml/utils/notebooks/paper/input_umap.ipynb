{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute UMAP embedding for some input+wgbs data in epiatlas and chip-atlas datasets.\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "from importlib import metadata\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import umap\n",
    "from umap.umap_ import nearest_neighbors\n",
    "\n",
    "from epi_ml.core.hdf5_loader import Hdf5Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_basedir = Path(\"/lustre06/project/6007017/rabyj/epilap/input\")\n",
    "# input_basedir = Path(\"/home/local/USHERBROOKE/rabj2301/mounts/narval-mount/project-rabyj/epilap/input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromsize_path = input_basedir / \"chromsizes\" / \"hg38.noy.chrom.sizes\"\n",
    "hdf5_loader = Hdf5Loader(chrom_file=chromsize_path, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make temporary file list out of two filelists\n",
    "hdf5_input_dir = Path(os.environ.get(\"SLURM_TMPDIR\", \"/tmp\"))\n",
    "\n",
    "hdf5_lists_dir = input_basedir / \"hdf5_list\"\n",
    "epiatlas_filename_list_path = (\n",
    "    hdf5_lists_dir / \"hg38_epiatlas-freeze-v2/100kb_all_none_input_wgbs.list\"\n",
    ")\n",
    "chip_atlas_filename_list_path = hdf5_lists_dir / \"C-A_100kb_all_none_input.list\"\n",
    "\n",
    "for path in [epiatlas_filename_list_path, chip_atlas_filename_list_path]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Could not find {path}\")\n",
    "\n",
    "epiatlas_files = hdf5_loader.read_list(epiatlas_filename_list_path)\n",
    "chip_atlas_files = hdf5_loader.read_list(chip_atlas_filename_list_path)\n",
    "\n",
    "epiatlas_filepaths = [\n",
    "    hdf5_input_dir / \"epiatlas_dfreeze_100kb_all_none\" / name\n",
    "    for name in epiatlas_files.values()\n",
    "]\n",
    "chip_atlas_filepaths = [\n",
    "    hdf5_input_dir / \"100kb_all_none\" / name for name in chip_atlas_files.values()\n",
    "]\n",
    "all_paths = epiatlas_filepaths + chip_atlas_filepaths\n",
    "\n",
    "hdf5_paths_list_path = hdf5_input_dir / \"hdf5_paths.list\"\n",
    "with open(hdf5_paths_list_path, \"w\", encoding=\"utf8\") as f:\n",
    "    f.writelines([str(path) + \"\\n\" for path in all_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untar data from both tars into local node tmpdir, and create list of files that takes into account different folder structure for each\n",
    "chip_atlas_tar_path = Path(\n",
    "    \"/lustre07/scratch/rabyj/other_data/C-A/hdf5/100kb_all_none.tar\"\n",
    ")\n",
    "epiatlas_tar_path = Path(\n",
    "    \"/lustre06/project/6007515/ihec_share/local_ihec_data/epiatlas/hg38/hdf5/epiatlas_dfreeze_100kb_all_none.tar\"\n",
    ")\n",
    "\n",
    "for path in [chip_atlas_tar_path, epiatlas_tar_path]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Could not find {path}\")\n",
    "\n",
    "for path in [chip_atlas_tar_path, epiatlas_tar_path]:\n",
    "    subprocess.run([\"tar\", \"-xf\", str(path), \"-C\", str(hdf5_input_dir)], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf5_paths_list_path = \"/home/local/USHERBROOKE/rabj2301/Projects/epiclass/input/hdf5_list/100kb_all_none_50samples.list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant files\n",
    "hdf5_dict = hdf5_loader.load_hdf5s(\n",
    "    data_file=hdf5_paths_list_path,\n",
    "    strict=True,\n",
    ").signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_params = {\n",
    "    \"standard\": {\n",
    "        \"n_neighbors\": 15,\n",
    "        \"min_dist\": 0.1,\n",
    "        \"n_components\": 3,\n",
    "        \"metric\": \"precomputed\",\n",
    "        \"low_memory\": False,\n",
    "    },\n",
    "    \"cluster\": {\n",
    "        \"n_neighbors\": 30,\n",
    "        \"min_dist\": 0,\n",
    "        \"n_components\": 10,\n",
    "        \"metric\": \"precomputed\",\n",
    "        \"low_memory\": False,\n",
    "    },\n",
    "    \"densmap\": {\n",
    "        \"n_neighbors\": 30,\n",
    "        \"min_dist\": 0.1,\n",
    "        \"n_components\": 3,\n",
    "        \"metric\": \"precomputed\",\n",
    "        \"low_memory\": False,\n",
    "        \"densmap\": True,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    output_dir = chip_atlas_tar_path.parent / \"umap\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "except NameError:\n",
    "    output_dir = Path.home()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(list(hdf5_dict.values()), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "precomputed_knn = nearest_neighbors(\n",
    "    X=data,\n",
    "    n_neighbors=30,\n",
    "    metric=\"correlation\",\n",
    "    random_state=42,\n",
    "    low_memory=False,\n",
    "    metric_kwds=None,\n",
    "    angular=None,\n",
    ")\n",
    "with open(output_dir / \"precomputed_knn.pkl\", \"wb\") as f:\n",
    "    pickle.dump(precomputed_knn, f)\n",
    "\n",
    "# Save requirements so pickle is never lost in the future\n",
    "dists = metadata.distributions()\n",
    "with open(output_dir / \"pickle_requirements.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    for dist in dists:\n",
    "        name = dist.metadata[\"Name\"]\n",
    "        version = dist.version\n",
    "        f.write(f\"{name}=={version}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = list(hdf5_dict.keys())\n",
    "for name, params in embedding_params.items():\n",
    "    embedding = umap.UMAP(\n",
    "        **params, random_state=42, precomputed_knn=precomputed_knn\n",
    "    ).fit_transform(X=data)\n",
    "    with open(output_dir / f\"embedding_{name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"ids\": file_names, \"embedding\": embedding, \"params\": params}, f)\n",
    "        print(f\"Saved embedding_{name}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
