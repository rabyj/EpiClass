{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code related to SHAP analyses for flagship paper + supplementary figures.\"\"\"\n",
    "\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, duplicate-code, unreachable, unused-argument, use-dict-literal, unused-import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How relevant files were downloaded\n",
    "\n",
    "~~~bash\n",
    "base_path=\"/home/rabyj/scratch/epilap-logs/epiatlas-dfreeze-v2.1/hg38_100kb_all_none/harmonized_sample_ontology_intermediate_1l_3000n/10fold-oversampling\"\n",
    "rsync --info=progress2 -aR narval:${base_path}/./split*/*.md5 .\n",
    "\n",
    "# rsync --info=progress2 -aR --exclude \"*.npz\" --exclude \"analysis_n*_f80.00/\" narval:${base_path}/./split*/shap .\n",
    "rsync --info=progress2 -aR narval:${base_path}/./split*/shap/*background*.npz .\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import tarfile\n",
    "from collections import Counter, defaultdict\n",
    "from math import floor\n",
    "from pathlib import Path\n",
    "from typing import Collection, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.special import softmax\n",
    "\n",
    "from epi_ml.core import metadata\n",
    "from epi_ml.core.data_source import EpiDataSource\n",
    "from epi_ml.core.epiatlas_treatment import EpiAtlasFoldFactory, EpiAtlasMetadata\n",
    "from epi_ml.utils import modify_metadata\n",
    "from epi_ml.utils.bed_utils import bed_ranges_to_bins, read_bed_to_ranges\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    CANCER,\n",
    "    CELL_TYPE,\n",
    "    SEX,\n",
    "    MetadataHandler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "base_fig_dir = paper_dir / \"figures\"\n",
    "base_data_dir = (\n",
    "    base_dir / \"data\" / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    ")\n",
    "if not base_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_data_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromsize_path = paper_dir / \"data\" / \"chromsizes\" / \"hg38.noy.chrom.sizes\"\n",
    "chroms = EpiDataSource.load_external_chrom_file(chromsize_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata_df = metadata_handler.load_metadata_df(\"v2\")\n",
    "print(metadata_df.shape)\n",
    "print(metadata_df.index.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background selection details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_training_vs_background(\n",
    "    parent_dir: Path,\n",
    "    label_category: str,\n",
    "    dataset_handler: EpiAtlasFoldFactory,\n",
    "    output_dir: Path,\n",
    "):\n",
    "    \"\"\"Create a table that compares the training data to the SHAP background data for each split.\n",
    "\n",
    "    Args:\n",
    "        parent_dir: The directory containing each split/fold directory.\n",
    "        label_category: The category to compare the training and background data on.\n",
    "        datasource: The data source to use for the comparison.\n",
    "    \"\"\"\n",
    "    my_metadata = dataset_handler.epiatlas_dataset.metadata\n",
    "    # Get md5s from dataset handler (with oversampling info)\n",
    "    oversample_md5_training_composition = defaultdict(list)\n",
    "    for i, my_data in enumerate(dataset_handler.yield_split(oversample=True)):\n",
    "        split = f\"split{i}\"\n",
    "        full_training_md5s = list(my_data.train.ids)\n",
    "        oversample_md5_training_composition[split] = full_training_md5s\n",
    "\n",
    "    # Get md5s saved to file (no oversampling info)\n",
    "    split_md5_composition = defaultdict(dict)\n",
    "    metadata_split_dfs = defaultdict(dict)\n",
    "    for split_dir in parent_dir.glob(\"split*\"):\n",
    "        if not split_dir.is_dir():\n",
    "            continue\n",
    "        split = split_dir.name\n",
    "\n",
    "        training_data_path = list(split_dir.glob(\"*training*.md5\"))\n",
    "        if len(training_data_path) != 1:\n",
    "            raise ValueError(f\"Multiple training data files found in {split_dir}\")\n",
    "        training_data_path = training_data_path[0]\n",
    "\n",
    "        background_data_path = list(split_dir.glob(\"shap/*background*.npz\"))\n",
    "        if len(background_data_path) != 1:\n",
    "            raise ValueError(f\"Multiple background data files found in {split_dir}\")\n",
    "        background_data_path = background_data_path[0]\n",
    "\n",
    "        training_md5s = set(\n",
    "            pd.read_csv(training_data_path, index_col=False, header=None)[0].values\n",
    "        )\n",
    "        background_data = np.load(background_data_path)\n",
    "        background_md5s = set(background_data[\"background_md5s\"])\n",
    "\n",
    "        # Sanity check: background data is a subset of training data\n",
    "        diff = background_md5s - training_md5s\n",
    "        if diff:\n",
    "            raise ValueError(\n",
    "                f\"Background data is not a subset of the training data in {split_dir}: {len(diff)} md5s unique to background data.\"\n",
    "            )\n",
    "\n",
    "        # Sanity check: training data set == oversampled training data set\n",
    "        full_training_md5s = oversample_md5_training_composition[split]\n",
    "        if training_md5s != set(full_training_md5s):\n",
    "            raise ValueError(\n",
    "                f\"Training data (unique={len(training_md5s)}) in {split_dir} does not match oversampled training data (unique={len(full_training_md5s)}).\"\n",
    "            )\n",
    "\n",
    "        split_md5_composition[split] = {\n",
    "            \"training\": list(full_training_md5s),\n",
    "            \"background\": list(background_md5s),\n",
    "        }\n",
    "        print(\n",
    "            f\"Split {split} has {len(full_training_md5s)} training md5s (unique={len(set(full_training_md5s))}) and {len(background_md5s)} background md5s.\"\n",
    "        )\n",
    "\n",
    "        # for diff ratio, do ((ratio background - ratio training))*100 -> positive if background has more\n",
    "        # do assay / cell type / output class ratios\n",
    "        # table should have full number + ratios + diff ratio for each metadata category\n",
    "        # do for each split, save to a file, then average accross splits\n",
    "        background_md5s = split_md5_composition[split][\"background\"]\n",
    "        training_md5s = split_md5_composition[split][\"training\"]\n",
    "\n",
    "        for category in set([ASSAY, CELL_TYPE, label_category]):\n",
    "            background_composition = Counter(\n",
    "                my_metadata[md5][category] for md5 in background_md5s\n",
    "            )\n",
    "            training_composition = Counter(\n",
    "                my_metadata[md5][category] for md5 in training_md5s\n",
    "            )\n",
    "            pre_oversampling_composition = Counter(\n",
    "                my_metadata[md5][category] for md5 in set(training_md5s)\n",
    "            )\n",
    "\n",
    "            background_ratios = {\n",
    "                key: value / len(background_md5s)\n",
    "                for key, value in background_composition.items()\n",
    "            }\n",
    "            training_ratios = {\n",
    "                key: value / len(training_md5s)\n",
    "                for key, value in training_composition.items()\n",
    "            }\n",
    "\n",
    "            diff_ratios = {\n",
    "                key: (background_ratios[key] - training_ratios[key]) * 100\n",
    "                for key in set(background_ratios.keys()) | set(training_ratios.keys())\n",
    "            }\n",
    "            metadata_split_dfs[label_category][split] = pd.DataFrame(\n",
    "                {\n",
    "                    \"Background size\": background_composition,\n",
    "                    \"Pre-oversampling training size\": pre_oversampling_composition,\n",
    "                    \"Training size\": training_composition,\n",
    "                    \"Background Ratio\": background_ratios,\n",
    "                    \"Training Ratio\": training_ratios,\n",
    "                    \"(Bg ratio - Tr ratio)*100\": diff_ratios,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    full_split_sizes = pd.DataFrame(\n",
    "        {\n",
    "            \"Training size\": {\n",
    "                split: len(set(split_md5_composition[split][\"training\"]))\n",
    "                for split in oversample_md5_training_composition.keys()\n",
    "            },\n",
    "            \"Background size\": {\n",
    "                split: len(set(split_md5_composition[split][\"background\"]))\n",
    "                for split in oversample_md5_training_composition.keys()\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    full_split_sizes.loc[\"all\", :] = full_split_sizes.mean(axis=0)\n",
    "    # full_split_sizes.loc[:, \"Ratio\"] = (\n",
    "    #     full_split_sizes[\"Background size\"] / full_split_sizes[\"Training size\"]\n",
    "    # )\n",
    "\n",
    "    # average par class label\n",
    "    for category, split_dfs in metadata_split_dfs.items():\n",
    "        mean_df = pd.concat(split_dfs.values()).groupby(level=0).mean()\n",
    "        mean_df = mean_df.sort_values(\"Background Ratio\", ascending=False)\n",
    "        mean_df.rename(\n",
    "            columns={\n",
    "                \"Background size\": \"Avg. Background size\",\n",
    "                \"Pre-oversampling training size\": \"Avg. Pre-oversampling training size\",\n",
    "                \"Training size\": \"Avg. Training size\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        mean_df.loc[\"all\", \"Avg. Background size\"] = full_split_sizes.loc[\n",
    "            \"all\", \"Background size\"\n",
    "        ]\n",
    "        mean_df.loc[\"all\", \"Avg. Pre-oversampling training size\"] = full_split_sizes.loc[\n",
    "            \"all\", \"Training size\"\n",
    "        ]\n",
    "\n",
    "        mean_df.to_csv(\n",
    "            output_dir / f\"shap_metadata_composition_{category}.csv\", index_label=category\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version v2-encode has sample_cancer_high as a label category\n",
    "# attempt to compare epiatlas_training.py metadata pre-processing.\n",
    "# for folder, metadata_version in zip([cell_type_dir, cancer_dir], [\"v2\", \"v2-encode\"]):\n",
    "for task, metadata_version in zip([SEX, CELL_TYPE, CANCER], [\"v2\", \"v2\", \"v2-encode\"]):\n",
    "    print(f\"Processing {task} with metadata version {metadata_version}\")\n",
    "    label_category = task\n",
    "\n",
    "    folder = base_data_dir / f\"{task}_1l_3000n\" / \"10fold-oversampling\"\n",
    "    if not folder.exists():\n",
    "        raise ValueError(f\"Folder {folder} does not exist.\")\n",
    "\n",
    "    # special hdf5 filepath with no real path\n",
    "    metadata_filename = Path(metadata_handler.version_names[metadata_version])\n",
    "    metadata_path = paper_dir / \"data\" / \"metadata\" / \"epiatlas\" / metadata_filename\n",
    "    md5_filepath = metadata_path.parent / f\"{metadata_filename.stem}.md5\"\n",
    "\n",
    "    # epiatlas training treatment\n",
    "    my_datasource = EpiDataSource(\n",
    "        hdf5=md5_filepath,\n",
    "        chromsize=chromsize_path,\n",
    "        metadata=metadata_path,\n",
    "    )\n",
    "\n",
    "    my_metadata = metadata.UUIDMetadata(my_datasource.metadata_file)\n",
    "\n",
    "    my_metadata.remove_category_subsets(\n",
    "        label_category=label_category, labels=[\"\", \"unknown\"]\n",
    "    )\n",
    "    my_metadata.remove_category_subsets(\n",
    "        label_category=\"track_type\", labels=[\"Unique.raw\"]\n",
    "    )\n",
    "    my_metadata.remove_missing_labels(label_category)\n",
    "\n",
    "    if label_category in set(\n",
    "        [\n",
    "            \"harmonized_sample_ontology_intermediate\",\n",
    "            \"harm_sample_ontology_intermediate\",\n",
    "            \"cell_type\",\n",
    "        ]\n",
    "    ):\n",
    "        categories = set(my_metadata.get_categories())\n",
    "        if \"assay_epiclass\" in categories:\n",
    "            assay_cat = \"assay_epiclass\"\n",
    "        elif \"assay\" in categories:\n",
    "            assay_cat = \"assay\"\n",
    "        else:\n",
    "            raise ValueError(\"Cannot find assay category for class pairs.\")\n",
    "        my_metadata = modify_metadata.filter_by_pairs(\n",
    "            my_metadata,\n",
    "            assay_cat=assay_cat,\n",
    "            cat2=label_category,\n",
    "            nb_pairs=9,\n",
    "            min_per_pair=10,\n",
    "        )\n",
    "\n",
    "    label_list = metadata.env_filtering(my_metadata, label_category)\n",
    "\n",
    "    # ratio comparison\n",
    "    print(f\"Creating dataset for {label_category}.\")\n",
    "    full_dataset = EpiAtlasMetadata(\n",
    "        datasource=my_datasource,\n",
    "        metadata=my_metadata,\n",
    "        label_category=label_category,\n",
    "        label_list=label_list,\n",
    "        min_class_size=10,\n",
    "        force_filter=True,\n",
    "    )\n",
    "    print(\"Creating dataset handler.\")\n",
    "    dataset_handler = EpiAtlasFoldFactory(\n",
    "        epiatlas_dataset=full_dataset,\n",
    "        n_fold=10,\n",
    "        test_ratio=0,\n",
    "    )\n",
    "\n",
    "    output_dir = folder / \"shap_analysis\"\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    print(\"Entering compare_all_training_vs_background\")\n",
    "    compare_all_training_vs_background(\n",
    "        parent_dir=folder,\n",
    "        label_category=label_category,\n",
    "        dataset_handler=dataset_handler,\n",
    "        output_dir=output_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP values ranking\n",
    "\n",
    "SHAP rank significance analysis (sample ontology)\n",
    "\n",
    "- Gather rank of all bins for sample output class (20k x 30k matrix)\n",
    "- For each % of highest values starting from top to 10%, compute mean SHAP value (20k x 10 matrix)\n",
    "- Create a graph that represents the ratio between 0-1% and 1-2% chunks, etc up until 10%, for each sample. 1 violin or boxplot per chunk (10 plots), 20k points per plot.\n",
    "\n",
    "Then, with flagship cell GO, get rank distribution of features \"unique\" in group1 VS group2 (t-cell, neutrophil: k27ac vs k27me3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute SHAP rankings / 1% chunks means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(shap_dir: Path) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"Load evaluation and background data from specified directory.\n",
    "\n",
    "    Args:\n",
    "        shap_dir (Path): Directory containing the SHAP data files.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing evaluation results and background data.\n",
    "    \"\"\"\n",
    "    # Find all npz files once\n",
    "    npz_files = list(shap_dir.glob(\"*.npz\"))\n",
    "    eval_files = [f for f in npz_files if \"evaluation\" in f.name]\n",
    "    background_files = [f for f in npz_files if \"background\" in f.name]\n",
    "\n",
    "    if len(eval_files) != 1:\n",
    "        raise ValueError(\n",
    "            f\"Expected one evaluation file, found {len(eval_files)} in {shap_dir}\"\n",
    "        )\n",
    "    if len(background_files) != 1:\n",
    "        raise ValueError(\n",
    "            f\"Expected one background file, found {len(background_files)} in {shap_dir}\"\n",
    "        )\n",
    "\n",
    "    eval_results = dict(np.load(eval_files[0], allow_pickle=True))\n",
    "    background_data = dict(np.load(background_files[0], allow_pickle=True))\n",
    "\n",
    "    return eval_results, background_data\n",
    "\n",
    "\n",
    "def compute_mean_shap_values(\n",
    "    shap_matrices: List[np.ndarray],\n",
    "    md5_indices: List[str],\n",
    "    classes: Dict[str, int],\n",
    "    metadata_df: pd.DataFrame,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"Rank amplitude of shap values and computes the mean SHAP values for specific segments of feature importance rankings\n",
    "    for each sample, and aggregates this information along with their softmax transformations.\n",
    "\n",
    "    This function processes SHAP values for given classes, extracts the relevant SHAP values\n",
    "    for each sample using its MD5 index, ranks these values, and calculates the mean of these values\n",
    "    in the top 10% segments. Additionally, it applies a softmax transformation to the SHAP values\n",
    "    and computes the mean for these as well, allowing for comparison between raw and transformed importance.\n",
    "\n",
    "    Args:\n",
    "        shap_matrices (List[np.ndarray]): SHAP values for all classes.\n",
    "        md5s (List[str]): Indices corresponding to MD5 hashes.\n",
    "        classes (dict): Mapping of class names to indices.\n",
    "        metadata_df (pd.DataFrame): DataFrame containing metadata.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict]: Dictionary of SHAP details for each sample ({md5: details}).\n",
    "    \"\"\"\n",
    "    input_size = len(shap_matrices[0][0])\n",
    "    chunks_10perc_idx = [\n",
    "        (floor(input_size / 100) * i, floor(input_size / 100) * (i + 1))\n",
    "        for i in range(10)\n",
    "    ]\n",
    "    shap_details = {}\n",
    "\n",
    "    for md5_idx, md5 in enumerate(md5_indices):\n",
    "        cell_type: str = metadata_df.loc[md5][CELL_TYPE]  # type: ignore\n",
    "        class_idx = classes[cell_type]\n",
    "        sample_shaps = shap_matrices[class_idx][md5_idx]\n",
    "\n",
    "        sample_shaps = np.abs(sample_shaps)  # magnitude only\n",
    "        softmax_shaps = softmax(sample_shaps)\n",
    "        ranks = np.argsort(sample_shaps)[::-1]  # descending order, greatest to smallest\n",
    "\n",
    "        sorted_shap_vals = sample_shaps[ranks]\n",
    "        sorted_softmax_shap_vals = softmax_shaps[ranks]\n",
    "\n",
    "        mean_10perc_vals = [\n",
    "            np.mean(sorted_shap_vals[idx1:idx2], dtype=np.float64)\n",
    "            for idx1, idx2 in chunks_10perc_idx\n",
    "        ]\n",
    "        mean_10perc_vals_softmax = [\n",
    "            np.mean(sorted_softmax_shap_vals[idx1:idx2], dtype=np.float64)\n",
    "            for idx1, idx2 in chunks_10perc_idx\n",
    "        ]\n",
    "\n",
    "        shap_details[md5] = {\n",
    "            \"ranks\": ranks,\n",
    "            \"mean_10perc_vals\": mean_10perc_vals,\n",
    "            \"mean_10perc_vals_softmax\": mean_10perc_vals_softmax,\n",
    "        }\n",
    "\n",
    "    return shap_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shap_tables(shap_details: Dict[str, Dict]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Create tables containing SHAP details for each sample.\n",
    "\n",
    "    Args:\n",
    "        shap_details (Dict[str, Dict]): Dictionary of SHAP details for each sample\n",
    "            format: {md5: {\"ranks\": vals, \"mean_10perc_vals\": vals, \"mean_10perc_vals_softmax\": vals}}\n",
    "\n",
    "    Returns:\n",
    "       Dict[str, pd.DataFrame]: List of DataFrames containing each shap details category.\n",
    "    \"\"\"\n",
    "    ranks = pd.DataFrame.from_dict(\n",
    "        data={md5: details[\"ranks\"] for md5, details in shap_details.items()},\n",
    "        orient=\"index\",\n",
    "        dtype=\"int32\",\n",
    "    )\n",
    "\n",
    "    mean_10perc_vals = pd.DataFrame.from_dict(\n",
    "        data={md5: details[\"mean_10perc_vals\"] for md5, details in shap_details.items()},\n",
    "        orient=\"index\",\n",
    "        dtype=\"float64\",\n",
    "    )\n",
    "    mean_10perc_vals.columns = [f\"mean(top {i}% to {i+1}%)\" for i in range(10)]\n",
    "\n",
    "    mean_10perc_vals_softmax = pd.DataFrame.from_dict(\n",
    "        data={\n",
    "            md5: details[\"mean_10perc_vals_softmax\"]\n",
    "            for md5, details in shap_details.items()\n",
    "        },\n",
    "        orient=\"index\",\n",
    "        dtype=\"float64\",\n",
    "    )\n",
    "    mean_10perc_vals_softmax.columns = mean_10perc_vals.columns\n",
    "\n",
    "    return {\n",
    "        \"ranks\": ranks,  # actually sorting indices, not ranks\n",
    "        \"mean_10perc_vals\": mean_10perc_vals,\n",
    "        \"mean_10perc_vals_softmax\": mean_10perc_vals_softmax,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_shap_tables(\n",
    "    tables: Dict[str, pd.DataFrame], output_dir: Path, verbose: bool = True\n",
    "):\n",
    "    \"\"\"Save SHAP tables to specified directory.\n",
    "\n",
    "    Args:\n",
    "        tables (Dict[str, pd.DataFrame]): Dictionary of tables to save.\n",
    "        output_dir (Path): Directory to save the tables.\n",
    "    \"\"\"\n",
    "    for name, table in tables.items():\n",
    "        if name == \"ranks\":\n",
    "            output_path = output_dir / \"shap_abs_ranks.npz\"\n",
    "            np.savez_compressed(\n",
    "                output_path,\n",
    "                **{\n",
    "                    \"index\": table.index.values,\n",
    "                    \"columns\": table.columns.values,\n",
    "                    \"values\": table.values,\n",
    "                },\n",
    "            )\n",
    "            if verbose:\n",
    "                print(f\"Saved SHAP ranks to {output_path}\")\n",
    "        else:\n",
    "            output_path = output_dir / f\"shap_table_{name}.csv\"\n",
    "            table.to_csv(output_path)\n",
    "            if verbose:\n",
    "                print(f\"Saved SHAP details to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_name = \"split0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap_details = defaultdict(dict)\n",
    "# for shap_dir in cell_type_dir.glob(f\"{split_name}/shap\"):\n",
    "#     if not shap_dir.is_dir():\n",
    "#         continue\n",
    "\n",
    "#     try:\n",
    "#         eval_results, background_data = load_data(shap_dir)\n",
    "#     except ValueError as e:\n",
    "#         print(e)\n",
    "#         continue\n",
    "\n",
    "#     classes = {class_name: int(idx) for idx, class_name in background_data[\"classes\"]}\n",
    "\n",
    "#     shap_matrices = eval_results[\"shap_values\"]\n",
    "\n",
    "#     shap_details.update(\n",
    "#         compute_mean_shap_values(\n",
    "#             shap_matrices, eval_results[\"evaluation_md5s\"], classes, metadata_df\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "# del shap_matrices, eval_results, background_data\n",
    "\n",
    "\n",
    "# shap_tables = create_shap_tables(shap_details)\n",
    "\n",
    "\n",
    "# output_dir = cell_type_dir / \"global_shap_analysis\"\n",
    "# if not output_dir.exists():\n",
    "#     raise ValueError(f\"Output directory {output_dir} does not exist.\")\n",
    "\n",
    "# output_dir = output_dir / f\"{split_name}_details\"\n",
    "# output_dir.mkdir(exist_ok=True)\n",
    "# save_shap_tables(shap_tables, output_dir)\n",
    "\n",
    "# print(f\"Processed {len(shap_details)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph computed means ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_shap_10perc(shap_table_mean_path: Path, output_dir: Path):\n",
    "    \"\"\"Plot the variation of mean SHAP values accross 1% segments (top 10)\"\"\"\n",
    "    df = pd.read_csv(shap_table_mean_path, index_col=0)\n",
    "    fig = go.Figure()\n",
    "    for idx, col in enumerate(df.columns):\n",
    "        try:\n",
    "            ratio = df.iloc[:, idx] / df.iloc[:, idx + 1]\n",
    "        except IndexError:\n",
    "            continue\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                y=ratio,\n",
    "                name=f\"Ratio {col} / {df.columns[idx+1]}\",\n",
    "                points=\"all\",\n",
    "                marker=dict(size=1),\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[f\"{md5}: {diff}\" for md5, diff in zip(df.index, ratio)],\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                spanmode=\"hard\",\n",
    "            )\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title=\"Ratio of mean SHAP values between 1% segments\",\n",
    "        xaxis_title=\"1% segment\",\n",
    "        yaxis_title=\"Ratio\",\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    output = output_dir / shap_table_mean_path.stem\n",
    "\n",
    "    fig.write_html(f\"{output}.html\")\n",
    "    fig.write_image(f\"{output}.png\")\n",
    "    fig.write_image(f\"{output}.svg\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_output_dir = base_fig_dir / \"flagship\" / \"features_SHAP_ranks\"\n",
    "\n",
    "# mean_vals_path = (\n",
    "#     cell_type_dir / \"global_shap_analysis\" / \"shap_details_mean_10perc_vals.csv\"\n",
    "# )\n",
    "# if not mean_vals_path.exists():\n",
    "#     raise FileNotFoundError(mean_vals_path)\n",
    "# graph_shap_10perc(mean_vals_path, fig_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beds_to_bins(\n",
    "    feature_beds_tar: Path, chroms: List[Tuple[str, int]]\n",
    ") -> Dict[str, List[int]]:\n",
    "    \"\"\"Extracts and processes .bed files from a tar.gz archive, targeting specific cells and markers.\n",
    "\n",
    "    Args:\n",
    "        feature_beds_tar (Path): The path to the tar.gz file containing .bed files.\n",
    "        chroms (List[Tuple[str, int]]): List of tuples containing chromosome names and their lengths.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[int]]: A dictionary with keys as feature names and values as lists of bins derived from the .bed ranges.\n",
    "    \"\"\"\n",
    "    feature_bins = {}\n",
    "\n",
    "    def is_target_cell(name: str) -> bool:\n",
    "        \"\"\"Check if the file name indicates a target cell type.\"\"\"\n",
    "        return \"T_cell\" in name or \"neutrophil\" in name\n",
    "\n",
    "    def has_target_marker(name: str) -> bool:\n",
    "        \"\"\"Check if the file name includes target histone markers.\"\"\"\n",
    "        return any(marker in name for marker in [\"h3k27ac\", \"h3k27me3\", \"h3k9me3\"])\n",
    "\n",
    "    with tarfile.open(feature_beds_tar, \"r:gz\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if (\n",
    "                member.name.endswith(\".bed\")\n",
    "                and is_target_cell(member.name)\n",
    "                and has_target_marker(member.name)\n",
    "            ):\n",
    "                with tar.extractfile(member) as f:  # type: ignore\n",
    "                    bed_ranges = read_bed_to_ranges(f)\n",
    "                    bed_bins = bed_ranges_to_bins(\n",
    "                        bed_ranges, chroms=chroms, resolution=100 * 1000\n",
    "                    )\n",
    "                    name = Path(member.name).stem.replace(\"_feature\", \"\")\n",
    "                    feature_bins[name] = bed_bins\n",
    "\n",
    "    return feature_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph rank of important relevant features from previous analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"flagship\" / \"features_SHAP_ranks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_beds_tar = cell_type_dir / \"global_shap_analysis\" / \"select_beds_top303.tar.gz\"\n",
    "\n",
    "# feature_bins_dict = beds_to_bins(\n",
    "#     feature_beds_tar=feature_beds_tar,\n",
    "#     chroms=chroms,\n",
    "# )\n",
    "\n",
    "# feature_bins_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Loading rank values.\")\n",
    "# output_dir = cell_type_dir / \"global_shap_analysis\"\n",
    "# rank_file = output_dir / \"shap_abs_ranks.npz\"\n",
    "# print(rank_file)\n",
    "\n",
    "# rank_data = {}\n",
    "# with np.load(rank_file, allow_pickle=True) as f:\n",
    "#     rank_data[\"md5s\"] = f[\"index\"]\n",
    "#     rank_data[\"feature_index\"] = f[\"columns\"]\n",
    "#     rank_data[\"values\"] = f[\"values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_ranks_sample_ontology(\n",
    "    rank_data: Dict[str, np.ndarray],\n",
    "    feature_bins: Collection[int],\n",
    "    metadata_df: pd.DataFrame,\n",
    "    output_dir: Path,\n",
    "    title_name: str,\n",
    "):\n",
    "    \"\"\"Graph the rank of specific sample ontology features, from perspective of different classes.\"\"\"\n",
    "    # metadata groups: t-cell/neutrophil with h3k27ac/h3k27me3\n",
    "    cell_types = [\"T cell\", \"neutrophil\"]\n",
    "    assays = [\"h3k27ac\", \"h3k27me3\"]\n",
    "    md5s_dict = {\n",
    "        f\"{cell}_{assay}_md5s\": list(\n",
    "            metadata_df[(metadata_df[CELL_TYPE] == cell) & (metadata_df[ASSAY] == assay)][\n",
    "                \"md5sum\"\n",
    "            ].values\n",
    "        )\n",
    "        for cell in cell_types\n",
    "        for assay in assays\n",
    "    }\n",
    "\n",
    "    # get md5s for each of the four metadata groups, and then get the ranks for each of the shared bins.\n",
    "    # fig1: one boxplot group per metadata group, with the sample ranks for each shared bin\n",
    "    # fig2: 1 violin plot with all ranks for each metadata group\n",
    "    fig1 = go.Figure()\n",
    "    fig2 = go.Figure()\n",
    "    for name, md5s in md5s_dict.items():\n",
    "        avail_md5s = np.isin(rank_data[\"md5s\"], md5s)\n",
    "        shared_ranks = rank_data[\"values\"][avail_md5s]\n",
    "\n",
    "        nb_samples = shared_ranks.shape[0]\n",
    "\n",
    "        # Each bin is a different boxplot within the group\n",
    "        all_ranks = []\n",
    "        for bin_index in sorted(feature_bins):\n",
    "            # need to do this since the ranks are actually sorting indices\n",
    "            feature_ranks = [np.where(ranks == bin_index)[0][0] for ranks in shared_ranks]\n",
    "            all_ranks.extend(feature_ranks)\n",
    "\n",
    "            fig1.add_trace(\n",
    "                go.Box(\n",
    "                    x=[f\"{name.replace('_md5s', '')} (f={len(feature_ranks)})\"]\n",
    "                    * len(feature_ranks),\n",
    "                    y=feature_ranks,\n",
    "                    name=f\"{name}_bin_{bin_index}\",\n",
    "                    line=dict(color=\"black\", width=0.1),\n",
    "                    showlegend=False,\n",
    "                    boxpoints=False,\n",
    "                    hovertext=[f\"bin={bin_index},Rank={rank}\" for rank in feature_ranks],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        fig2.add_trace(\n",
    "            go.Violin(\n",
    "                y=all_ranks,\n",
    "                name=f\"{name.replace('_md5s', '')} (f={nb_samples})\",\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                showlegend=True,\n",
    "                fillcolor=\"grey\",\n",
    "                line=dict(color=\"black\", width=1),\n",
    "                points=False,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    fig1.update_layout(yaxis=dict(range=[-5, 20000], autorange=False))\n",
    "    fig2.update_layout(yaxis=dict(range=[-5, 20000], autorange=False))\n",
    "\n",
    "    for y_val in [303, 909]:\n",
    "        fig1.add_hline(\n",
    "            y=y_val,\n",
    "            line_width=1,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"black\",\n",
    "            annotation_text=f\"Top {y_val}\",\n",
    "        )\n",
    "        fig2.add_hline(\n",
    "            y=y_val,\n",
    "            line_width=1,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"black\",\n",
    "            annotation_text=f\"Top {y_val}\",\n",
    "        )\n",
    "\n",
    "    fig1.update_layout(\n",
    "        title=f\"{title_name} features ranks (n={len(feature_bins)} bins) - top303 abs(SHAP),80% samples,8/10 fold\",\n",
    "        xaxis_title=f\"Sample group ({len(feature_bins)} features, f files)\",\n",
    "        yaxis_title=\"Rank\",\n",
    "        boxmode=\"group\",\n",
    "        width=1000,\n",
    "        height=1000,\n",
    "    )\n",
    "    fig2.update_layout(\n",
    "        title=f\"{title_name} features ranks (n={len(feature_bins)} bins mixed) - top303 abs(SHAP),80% samples,8/10 fold\",\n",
    "        xaxis_title=\"Sample group (f files)\",\n",
    "        yaxis_title=\"Rank\",\n",
    "        width=1000,\n",
    "        height=1000,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    output = output_dir / f\"feature_ranks_{title_name}\"\n",
    "    fig1.write_html(f\"{output}.html\")\n",
    "    fig1.write_image(f\"{output}.png\")\n",
    "    fig1.write_image(f\"{output}.svg\")\n",
    "\n",
    "    fig1.show()\n",
    "\n",
    "    output = output_dir / f\"all_feature_ranks_{title_name}\"\n",
    "    fig2.write_html(f\"{output}.html\")\n",
    "    fig2.write_image(f\"{output}.png\")\n",
    "    fig2.write_image(f\"{output}.svg\")\n",
    "\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared_k27ac_k27me3_bins = set(feature_bins_dict[\"h3k27ac_T_cells\"]) & set(\n",
    "#     feature_bins_dict[\"h3k27me3_T_cells\"]\n",
    "# )\n",
    "# shared_k27ac_k9me3_bins = set(feature_bins_dict[\"h3k27ac_T_cells\"]) & set(\n",
    "#     feature_bins_dict[\"h3k9me3_T_cells\"]\n",
    "# )\n",
    "\n",
    "# unique_k27ac_v_k27me3_bins = set(feature_bins_dict[\"h3k27ac_T_cells\"]) - set(\n",
    "#     feature_bins_dict[\"h3k27me3_T_cells\"]\n",
    "# )\n",
    "# unique_k27ac_v_k9me3_bins = set(feature_bins_dict[\"h3k27ac_T_cells\"]) - set(\n",
    "#     feature_bins_dict[\"h3k9me3_T_cells\"]\n",
    "# )\n",
    "\n",
    "# unique_k27me3_v_k27ac_bins = set(feature_bins_dict[\"h3k27me3_T_cells\"]) - set(\n",
    "#     feature_bins_dict[\"h3k27ac_T_cells\"]\n",
    "# )\n",
    "# unique_k9me3_v_k27ac_bins = set(feature_bins_dict[\"h3k9me3_T_cells\"]) - set(\n",
    "#     feature_bins_dict[\"h3k27ac_T_cells\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_output_dir = fig_dir / \"k27ac_k27me3\"\n",
    "\n",
    "# for var_name in [\n",
    "#     \"shared_k27ac_k27me3_bins\",\n",
    "#     \"unique_k27ac_v_k27me3_bins\",\n",
    "#     \"unique_k27me3_v_k27ac_bins\",\n",
    "# ]:\n",
    "#     graph_ranks_sample_ontology(\n",
    "#         rank_data=rank_data,\n",
    "#         feature_bins=globals()[var_name],\n",
    "#         metadata_df=metadata_df,\n",
    "#         output_dir=fig_output_dir,\n",
    "#         title_name=var_name,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_output_dir = fig_dir / \"k27ac_k9me3\"\n",
    "\n",
    "# for var_name in [\n",
    "#     \"shared_k27ac_k9me3_bins\",\n",
    "#     \"unique_k27ac_v_k9me3_bins\",\n",
    "#     \"unique_k9me3_v_k27ac_bins\",\n",
    "# ]:\n",
    "#     graph_ranks_sample_ontology(\n",
    "#         rank_data=rank_data,\n",
    "#         feature_bins=globals()[var_name],\n",
    "#         metadata_df=metadata_df,\n",
    "#         output_dir=fig_output_dir,\n",
    "#         title_name=var_name,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChromHMM regulatory features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_shap_folder = (\n",
    "    Path.home()\n",
    "    / \"scratch/epiclass/join_important_features/hg38_regulatory_regions_n30321_100kb_coord/harmonized_sample_ontology_intermediate_1l_3000n/10fold-oversampling/global_shap_analysis/top303\"\n",
    ")\n",
    "\n",
    "chrom_hmm_folder = Path.home() / \"Projects/epiclass/output/paper/data/ChromHMM\"\n",
    "\n",
    "for path in [base_shap_folder, chrom_hmm_folder]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "\n",
    "corr_path = chrom_hmm_folder / \"StackedChromHMM_hg38_EnhancerMaxK27acCorrelations.txt.gz\"\n",
    "other_info_path = (\n",
    "    chrom_hmm_folder\n",
    "    / \"StackedChromHMM_hg38_Xie_AllInteractionsBackgroundCorr_02PearAdj_JointCorrelations_gABCOnly_RemPromPCorr_REMLabels.txt.gz\"\n",
    ")\n",
    "\n",
    "# cross-reference both ChromHMM files to find negative corr values that are part of 30k.\n",
    "\n",
    "# Do a basic most important features analysis, take core6 pval + other marks, do an upset plot of most important features if we ignore cell types\n",
    "\n",
    "# compare important features with negative correlation values from ChromHMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge ChromHMM files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(chrom_hmm_folder / corr_path, \"rt\") as f:\n",
    "    corr_df = pd.read_csv(f, sep=\"\\t\", header=0)\n",
    "\n",
    "with gzip.open(chrom_hmm_folder / other_info_path, \"rt\") as f:\n",
    "    other_info_df = pd.read_csv(f, sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr_df.shape, other_info_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_regions = list(corr_df.columns[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corr_df = corr_df.merge(other_info_df, left_on=bed_regions, right_on=[0, 1, 2])\n",
    "full_corr_df = full_corr_df.drop(columns=[0, 1, 2])\n",
    "full_corr_df[\"tag\"] = full_corr_df[3].astype(str)\n",
    "full_corr_df = full_corr_df.drop(columns=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corr_df[\"max(abs(spear_r))\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_corr_df = full_corr_df.loc[range(0, 30321), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_tag_counts = full_corr_df[\"tag\"].value_counts()\n",
    "baseline_tag_dist = baseline_tag_counts / baseline_tag_counts.sum() * 100\n",
    "display(baseline_tag_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_tag_counts = used_corr_df[\"tag\"].value_counts()\n",
    "used_tag_dist = used_tag_counts / used_tag_counts.sum() * 100\n",
    "display(used_tag_dist)\n",
    "display(used_tag_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect important reg NN features and count tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: List[Dict] = []\n",
    "for folder in sorted(base_shap_folder.iterdir()):\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    important_features_path = folder / \"features_n8_all.json\"\n",
    "    if not important_features_path.exists():\n",
    "        continue\n",
    "\n",
    "    with open(important_features_path, \"r\", encoding=\"utf8\") as f:\n",
    "        important_features = json.load(f)\n",
    "\n",
    "    features_details = full_corr_df.iloc[important_features, :]\n",
    "\n",
    "    tag_count = features_details[\"tag\"].value_counts()\n",
    "    tag_dist = tag_count / tag_count.sum() * 100\n",
    "    dist_diff = tag_dist - used_tag_dist\n",
    "    fold_change = tag_dist / used_tag_dist\n",
    "\n",
    "    # Store results\n",
    "    folder_data = {\"subsampling\": folder.name}\n",
    "    for tag in tag_count.index:\n",
    "        folder_data[f\"{tag}_count\"] = tag_count[tag]\n",
    "        folder_data[f\"{tag}_perc\"] = tag_dist[tag]\n",
    "        folder_data[f\"{tag}_perc_diff\"] = dist_diff[tag]\n",
    "        folder_data[f\"{tag}_fc\"] = fold_change[tag]\n",
    "\n",
    "    data.append(folder_data)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data).fillna(0)\n",
    "df = df.set_index(\"subsampling\")\n",
    "df.to_csv(\n",
    "    base_shap_folder / \"important_features_vs_chromHMM_tags.csv\", header=True, sep=\",\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
