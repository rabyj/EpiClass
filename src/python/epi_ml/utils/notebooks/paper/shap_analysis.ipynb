{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Code related to SHAP analyses for flagship paper + supplementary figures.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Code related to SHAP analyses for flagship paper + supplementary figures.\"\"\"\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, duplicate-code, unreachable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How relevant files were downloaded\n",
    "\n",
    "~~~bash\n",
    "base_path=\"/home/rabyj/scratch/epilap-logs/epiatlas-dfreeze-v2.1/hg38_100kb_all_none/harmonized_sample_ontology_intermediate_1l_3000n/10fold-oversampling\"\n",
    "rsync --info=progress2 -aR narval:${base_path}/./split*/*.md5 .\n",
    "\n",
    "# rsync --info=progress2 -aR --exclude \"*.npz\" --exclude \"analysis_n*_f80.00/\" narval:${base_path}/./split*/shap .\n",
    "rsync --info=progress2 -aR narval:${base_path}/./split*/shap/*background*.npz .\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import floor\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import ASSAY, CELL_TYPE, MetadataHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANCER = \"harmonized_sample_cancer_high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = (\n",
    "    base_dir / \"data\" / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    ")\n",
    "paper_dir = base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata = metadata_handler.load_metadata(\"v2\")\n",
    "metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n",
    "metadata_df.set_index(\"md5sum\", inplace=True)\n",
    "del metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_dir = base_data_dir / f\"{CELL_TYPE}_1l_3000n\" / \"10fold-oversampling\"\n",
    "cancer_dir = base_data_dir / f\"{CANCER}_1l_3000n\" / \"10fold-oversampling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in [cell_type_dir, cancer_dir]:\n",
    "    assert folder.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background selection details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_training_vs_background(parent_dir: Path):\n",
    "    \"\"\"Create a table that compares the training data to the SHAP background data for each split.\n",
    "\n",
    "    Args:\n",
    "        parent_dir: The directory containing each split/fold directory.\n",
    "    \"\"\"\n",
    "    split_composition_dfs = {}\n",
    "    for split_dir in parent_dir.glob(\"split*\"):\n",
    "        if not split_dir.is_dir():\n",
    "            continue\n",
    "        split = split_dir.name\n",
    "\n",
    "        training_data_path = list(split_dir.glob(\"*training*.md5\"))\n",
    "        if len(training_data_path) != 1:\n",
    "            raise ValueError(f\"Multiple training data files found in {split_dir}\")\n",
    "        training_data_path = training_data_path[0]\n",
    "\n",
    "        background_data_path = list(split_dir.glob(\"shap/*background*.npz\"))\n",
    "        if len(background_data_path) != 1:\n",
    "            raise ValueError(f\"Multiple background data files found in {split_dir}\")\n",
    "        background_data_path = background_data_path[0]\n",
    "\n",
    "        training_md5s = set(\n",
    "            pd.read_csv(training_data_path, index_col=False, header=None)[0].values\n",
    "        )\n",
    "        background_data = np.load(background_data_path)\n",
    "        background_md5s = set(background_data[\"background_md5s\"])\n",
    "\n",
    "        # print(f\"Split: {split}\")\n",
    "        # print(f\"Training data: {len(training_md5s)}\")\n",
    "        # print(f\"Background data: {len(background_md5s)}\")\n",
    "\n",
    "        # Sanity check: background data is a subset of training data\n",
    "        diff = background_md5s - training_md5s\n",
    "        if diff:\n",
    "            raise ValueError(\n",
    "                f\"Background data is not a subset of the training data in {split_dir}: {len(diff)} md5s unique to background data.\"\n",
    "            )\n",
    "\n",
    "        # for diff ratio, do ((ratio background - ration training))*100 -> positive if background has more\n",
    "        # do assay / cell type / output class ratios\n",
    "        # table should have full number + ratios + diff ratio for each metadata category\n",
    "        # do for each split, save to a file, then average accross splits\n",
    "\n",
    "        raise NotImplementedError(\"Finish this function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for folder in [cell_type_dir, cancer_dir]:\n",
    "#     compare_all_training_vs_background(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP values ranking\n",
    "\n",
    "SHAP rank significance analysis (sample ontology)\n",
    "\n",
    "- Gather rank of all bins for sample output class (20k x 30k matrix)\n",
    "- For each % of highest values starting from top to 10%, compute mean SHAP value (20k x 10 matrix)\n",
    "- Create a graph that represents the ratio between 0-1% and 1-2% chunks, etc up until 10%, for each sample. 1 violin or boxplot per chunk (10 plots), 20k points per plot.\n",
    "\n",
    "Then, with flagship cell GO, get rank distribution of features \"unique\" in group1 VS group2 (t-cell, neutrophil: k27ac vs k27me3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(shap_dir: Path):\n",
    "    \"\"\"Load evaluation and background data from specified directory.\n",
    "\n",
    "    Args:\n",
    "        shap_dir (Path): Directory containing the SHAP data files.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing evaluation results and background data.\n",
    "    \"\"\"\n",
    "    # Find all npz files once\n",
    "    npz_files = list(shap_dir.glob(\"*.npz\"))\n",
    "    eval_files = [f for f in npz_files if \"evaluation\" in f.name]\n",
    "    background_files = [f for f in npz_files if \"background\" in f.name]\n",
    "\n",
    "    if len(eval_files) != 1:\n",
    "        raise ValueError(\n",
    "            f\"Expected one evaluation file, found {len(eval_files)} in {shap_dir}\"\n",
    "        )\n",
    "    if len(background_files) != 1:\n",
    "        raise ValueError(\n",
    "            f\"Expected one background file, found {len(background_files)} in {shap_dir}\"\n",
    "        )\n",
    "\n",
    "    eval_results = np.load(eval_files[0], allow_pickle=True)\n",
    "    background_data = np.load(background_files[0], allow_pickle=True)\n",
    "\n",
    "    return eval_results, background_data\n",
    "\n",
    "\n",
    "def compute_mean_shap_values(\n",
    "    shap_matrices: List[np.ndarray],\n",
    "    md5_indices: List[str],\n",
    "    classes: Dict[str, int],\n",
    "    metadata_df: pd.DataFrame,\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"Rank amplitude of shap values and computes the mean SHAP values for specific segments of feature importance rankings\n",
    "    for each sample, and aggregates this information along with their softmax transformations.\n",
    "\n",
    "    This function processes SHAP values for given classes, extracts the relevant SHAP values\n",
    "    for each sample using its MD5 index, ranks these values, and calculates the mean of these values\n",
    "    in the top 10% segments. Additionally, it applies a softmax transformation to the SHAP values\n",
    "    and computes the mean for these as well, allowing for comparison between raw and transformed importance.\n",
    "\n",
    "    Args:\n",
    "        shap_matrices (List[np.ndarray]): SHAP values for all classes.\n",
    "        md5s (List[str]): Indices corresponding to MD5 hashes.\n",
    "        classes (dict): Mapping of class names to indices.\n",
    "        metadata_df (pd.DataFrame): DataFrame containing metadata.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict]: Dictionary of SHAP details for each sample ({md5: details}).\n",
    "    \"\"\"\n",
    "    input_size = len(shap_matrices[0][0])\n",
    "    chunks_10perc_idx = [\n",
    "        (floor(input_size / 100) * i, floor(input_size / 100) * (i + 1))\n",
    "        for i in range(10)\n",
    "    ]\n",
    "    shap_details = {}\n",
    "\n",
    "    for md5_idx, md5 in enumerate(md5_indices):\n",
    "        cell_type: str = metadata_df.loc[md5][CELL_TYPE]  # type: ignore\n",
    "        class_idx = classes[cell_type]\n",
    "        sample_shaps = shap_matrices[class_idx][md5_idx]\n",
    "\n",
    "        sample_shaps = np.abs(sample_shaps)  # magnitude only\n",
    "        softmax_shaps = softmax(sample_shaps)\n",
    "        ranks = np.argsort(sample_shaps)[::-1]  # descending order, greatest to smallest\n",
    "\n",
    "        sorted_shap_vals = sample_shaps[ranks]\n",
    "        sorted_softmax_shap_vals = softmax_shaps[ranks]\n",
    "\n",
    "        mean_10perc_vals = [\n",
    "            np.mean(sorted_shap_vals[idx1:idx2], dtype=np.float64)\n",
    "            for idx1, idx2 in chunks_10perc_idx\n",
    "        ]\n",
    "        mean_10perc_vals_softmax = [\n",
    "            np.mean(sorted_softmax_shap_vals[idx1:idx2], dtype=np.float64)\n",
    "            for idx1, idx2 in chunks_10perc_idx\n",
    "        ]\n",
    "\n",
    "        shap_details[md5] = {\n",
    "            \"ranks\": ranks,\n",
    "            \"mean_10perc_vals\": mean_10perc_vals,\n",
    "            \"mean_10perc_vals_softmax\": mean_10perc_vals_softmax,\n",
    "        }\n",
    "\n",
    "    return shap_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shap_tables(shap_details: Dict[str, Dict]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Create tables containing SHAP details for each sample.\n",
    "\n",
    "    Args:\n",
    "        shap_details (Dict[str, Dict]): Dictionary of SHAP details for each sample\n",
    "            format: {md5: {\"ranks\": vals, \"mean_10perc_vals\": vals, \"mean_10perc_vals_softmax\": vals}}\n",
    "\n",
    "    Returns:\n",
    "       Dict[str, pd.DataFrame]: List of DataFrames containing each shap details category.\n",
    "    \"\"\"\n",
    "    ranks = pd.DataFrame.from_dict(\n",
    "        data={md5: details[\"ranks\"] for md5, details in shap_details.items()},\n",
    "        orient=\"index\",\n",
    "        dtype=\"int32\",\n",
    "    )\n",
    "\n",
    "    mean_10perc_vals = pd.DataFrame.from_dict(\n",
    "        data={md5: details[\"mean_10perc_vals\"] for md5, details in shap_details.items()},\n",
    "        orient=\"index\",\n",
    "        dtype=\"float64\",\n",
    "    )\n",
    "    mean_10perc_vals.columns = [f\"mean(top {i}% to {i+1}%)\" for i in range(10)]\n",
    "\n",
    "    mean_10perc_vals_softmax = pd.DataFrame.from_dict(\n",
    "        data={\n",
    "            md5: details[\"mean_10perc_vals_softmax\"]\n",
    "            for md5, details in shap_details.items()\n",
    "        },\n",
    "        orient=\"index\",\n",
    "        dtype=\"float64\",\n",
    "    )\n",
    "    mean_10perc_vals_softmax.columns = mean_10perc_vals.columns\n",
    "\n",
    "    return {\n",
    "        \"ranks\": ranks,\n",
    "        \"mean_10perc_vals\": mean_10perc_vals,\n",
    "        \"mean_10perc_vals_softmax\": mean_10perc_vals_softmax,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_shap_tables(\n",
    "    tables: Dict[str, pd.DataFrame], output_dir: Path, verbose: bool = True\n",
    "):\n",
    "    \"\"\"Save SHAP tables to specified directory.\n",
    "\n",
    "    Args:\n",
    "        tables (Dict[str, pd.DataFrame]): Dictionary of tables to save.\n",
    "        output_dir (Path): Directory to save the tables.\n",
    "    \"\"\"\n",
    "    for name, table in tables.items():\n",
    "        if name == \"ranks\":\n",
    "            output_path = output_dir / \"shap_abs_ranks.npz\"\n",
    "            np.savez_compressed(\n",
    "                output_path,\n",
    "                **{\n",
    "                    \"index\": table.index.values,\n",
    "                    \"columns\": table.columns.values,\n",
    "                    \"values\": table.values,\n",
    "                },\n",
    "            )\n",
    "            if verbose:\n",
    "                print(f\"Saved SHAP ranks to {output_path}\")\n",
    "        else:\n",
    "            output_path = output_dir / f\"shap_details_{name}.csv\"\n",
    "            table.to_csv(output_path)\n",
    "            if verbose:\n",
    "                print(f\"Saved SHAP details to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_details = defaultdict(dict)\n",
    "for shap_dir in cell_type_dir.glob(\"split*/shap\"):\n",
    "    if not shap_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        eval_results, background_data = load_data(shap_dir)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "    classes = {class_name: int(idx) for idx, class_name in background_data[\"classes\"]}\n",
    "\n",
    "    shap_matrices = eval_results[\"shap_values\"]\n",
    "\n",
    "    shap_details.update(\n",
    "        compute_mean_shap_values(\n",
    "            shap_matrices, eval_results[\"evaluation_md5s\"], classes, metadata_df\n",
    "        )\n",
    "    )\n",
    "\n",
    "shap_tables = create_shap_tables(shap_details)\n",
    "\n",
    "save_shap_tables(shap_tables, cell_type_dir)\n",
    "\n",
    "print(f\"Processed {len(shap_details)} samples.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
