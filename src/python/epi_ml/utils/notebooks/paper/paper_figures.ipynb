{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create figures destined for the paper.\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix as sk_cm,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.core.metadata import Metadata\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    CELL_TYPE,\n",
    "    IHECColorMap,\n",
    "    merge_similar_assays,\n",
    "    return_metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1.A\n",
    "\n",
    "Average distribution of prediction scores per assay \n",
    "violin plot. One point per UUID, track types averaged (combine 2xRNA and 2xWGBS)\n",
    "points with 3 colors: \n",
    "- black for pred same class\n",
    "- red for pred different class/mislabel\n",
    "- orange bad qual (IHEC flag, was removed in later stages)\n",
    "\n",
    "Graph version with color saturation gradient using max_pred/input score ratio\n",
    "\n",
    "Using [EpiClass_EA-21606_Assay11_100kb](https://drive.google.com/drive/folders/1SzyTFCVk2Cyw7NXW08sSYB_k49y-1KoJ) : EA_NN--full-10fold-validation_prediction_augmented-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in NN_results.columns:\n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig1_a(\n",
    "    NN_results: pd.DataFrame, logdir: Path, name: str, merge_assay_pairs: bool\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with violin plots and associated scatter plots for each class.\n",
    "    Red scatter points, indicating a mismatch, appear on top and have a larger size.\n",
    "\n",
    "    Args:\n",
    "        NN_results (pd.DataFrame): The DataFrame containing the neural network results.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "        name (str): The name of the figure.\n",
    "        merge_assay_pairs (bool): Whether to merge similar assays (mrna/rna, wgbs-pbat/wgbs-standard)\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Combine similar assays\n",
    "    if merge_assay_pairs:\n",
    "        NN_results = merge_similar_assays(NN_results)\n",
    "\n",
    "    # Adjustments for replacement and class ordering\n",
    "    class_labels = NN_results[\"True class\"].unique()\n",
    "    class_labels_sorted = sorted(class_labels)\n",
    "    class_index = {label: i for i, label in enumerate(class_labels_sorted)}\n",
    "\n",
    "    scatter_offset = 0.05  # Scatter plot jittering\n",
    "\n",
    "    for label in class_labels_sorted:\n",
    "        df = NN_results[NN_results[ASSAY] == label]\n",
    "\n",
    "        # Majority vote, mean prediction score\n",
    "        groupby_epirr = df.groupby([\"EpiRR\", \"Predicted class\"])[\"Max pred\"].aggregate(\n",
    "            [\"size\", \"mean\"]\n",
    "        )\n",
    "\n",
    "        groupby_epirr = groupby_epirr.reset_index().sort_values(\n",
    "            [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "        )\n",
    "        groupby_epirr = groupby_epirr.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "        assert groupby_epirr[\"EpiRR\"].is_unique\n",
    "\n",
    "        mean_pred = groupby_epirr[\"mean\"]\n",
    "\n",
    "        # Add violin plot with integer x positions\n",
    "        line_color = \"white\"\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[class_index[label]] * len(mean_pred),\n",
    "                y=mean_pred,\n",
    "                name=label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=False,\n",
    "                fillcolor=assay_colors[label],\n",
    "                line_color=line_color,\n",
    "                line=dict(width=0.8),\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Prepare data for scatter plots\n",
    "        jittered_x_positions = np.random.uniform(-scatter_offset, scatter_offset, size=len(mean_pred)) + class_index[label] - 0.25  # type: ignore\n",
    "\n",
    "        match_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] == label\n",
    "        ]\n",
    "        mismatch_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] != label\n",
    "        ]\n",
    "\n",
    "        match_x_positions = [\n",
    "            jittered_x_positions[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] == label\n",
    "        ]\n",
    "        mismatch_x_positions = [\n",
    "            jittered_x_positions[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] != label\n",
    "        ]\n",
    "\n",
    "        # Add scatter plots for matches in black\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=match_x_positions,\n",
    "                y=match_pred,\n",
    "                mode=\"markers\",\n",
    "                name=f\"Match {label}\",\n",
    "                marker=dict(\n",
    "                    color=\"black\",\n",
    "                    size=1,  # Standard size for matches\n",
    "                ),\n",
    "                hoverinfo=\"text\",\n",
    "                hovertext=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.2f}\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] == label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add scatter plots for mismatches in red, with larger size\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=mismatch_x_positions,\n",
    "                y=mismatch_pred,\n",
    "                mode=\"markers\",\n",
    "                name=f\"Mismatch {label}\",\n",
    "                marker=dict(\n",
    "                    color=\"red\",\n",
    "                    size=3,  # Larger size for mismatches\n",
    "                ),\n",
    "                hoverinfo=\"text\",\n",
    "                hovertext=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.3f}\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] != label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout to improve visualization\n",
    "    fig.update_layout(\n",
    "        title_text=\"Prediction score distribution per assay class\",\n",
    "        yaxis_title=\"Average prediction score (majority class)\",\n",
    "        xaxis_title=\"Expected class label\",\n",
    "    )\n",
    "    fig.update_yaxes(range=[0.25, 1.01])\n",
    "    fig.update_xaxes(tickvals=list(class_index.values()), ticktext=class_labels_sorted)\n",
    "\n",
    "    # Add a dummy scatter plot for legend - black points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Match\",\n",
    "            marker=dict(color=\"black\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"match\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - red points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Mismatch\",\n",
    "            marker=dict(color=\"red\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"mismatch\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update the layout to adjust the legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            title_text=\"Legend\",\n",
    "            itemsizing=\"constant\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_results_path = (\n",
    "    base_data_dir\n",
    "    / \"EpiClass_EA-21606_Assay11_100kb\"\n",
    "    / \"NN\"\n",
    "    / \"full-10fold-validation_prediction_augmented-all.csv\"\n",
    ")\n",
    "NN_results = pd.read_csv(NN_results_path, header=0, index_col=\"md5sum\", low_memory=False)\n",
    "\n",
    "# fig1_a(NN_results, logdir=base_fig_dir, name=\"fig1_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1.supp.A\n",
    "\n",
    "Violin plot (10 folds) of overall accuracy for each model (NN, LR, RF, LGBM, SV).  \n",
    "For each split, 4 box plot per model:\n",
    "  - Acc\n",
    "  - F1\n",
    "  - AUROC (OvR, both micro/macro)\n",
    "\n",
    "\n",
    "Source files:\n",
    "~~~bash\n",
    "cd ~/mounts/narval-mount/projects/rrg-jacquesp-ab/rabyj/epiclass-project/output/epiclass-logs/2023-01-epiatlas-freeze/  \n",
    "find assay_epiclass* -type f -name *validation_prediction.csv -print0 | rsync -av --files-from=- --from0 ./ ~/Projects/epiclass/output/paper/data/EpiClass_EA-21606_Assay11_100kb/all_splits/\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_results_path = (\n",
    "    base_data_dir / \"EpiClass_EA-21606_Assay11_100kb\" / \"all-predictions-merged.csv\"\n",
    ")\n",
    "merged_results = pd.read_csv(\n",
    "    NN_results_path, header=0, index_col=\"md5sum\", low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying that results are from metadata v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that merged results and nn results have the same EpiRRs\n",
    "assert (\n",
    "    len(set(merged_results[\"epirr_id\"]) & set(NN_results[\"epirr_id\"]))\n",
    "    == NN_results[\"epirr_id\"].nunique()\n",
    ")\n",
    "assert len(set(merged_results.index) & set(NN_results.index)) == len(\n",
    "    set(NN_results.index)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(merged_results[\"epirr_id\"] == \"IHECRE00003355.2\") == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del merged_results  # using separate split results for this figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_split_results(\n",
    "    results_dir: Path, label_category: str, only_NN: bool = False\n",
    ") -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Gather split results for each classifier.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, pd.DataFrame]]: {split_name:{classifier_name: results_df}}\n",
    "    \"\"\"\n",
    "    all_split_dfs = {}\n",
    "    for split in [f\"split{i}\" for i in range(10)]:\n",
    "        # Get the csv paths\n",
    "        if label_category == ASSAY:\n",
    "            second_dir_end = \"\"\n",
    "        elif label_category == CELL_TYPE:\n",
    "            second_dir_end = \"-dfreeze-v2\"\n",
    "\n",
    "        NN_csv_path = (\n",
    "            results_dir\n",
    "            / f\"{label_category}_1l_3000n\"\n",
    "            / f\"10fold{second_dir_end}\"\n",
    "            / split\n",
    "            / \"validation_prediction.csv\"\n",
    "        )\n",
    "        other_csv_root = (\n",
    "            results_dir / f\"{label_category}\" / f\"predict-10fold{second_dir_end}\"\n",
    "        )\n",
    "\n",
    "        if not only_NN:\n",
    "            if not other_csv_root.exists():\n",
    "                raise FileNotFoundError(f\"Could not find {other_csv_root}\")\n",
    "            other_csv_paths = other_csv_root.glob(\n",
    "                f\"*/*_{split}_validation_prediction.csv\"\n",
    "            )\n",
    "\n",
    "            other_csv_paths = list(other_csv_paths)\n",
    "            if len(other_csv_paths) != 4:\n",
    "                raise AssertionError(\n",
    "                    f\"Expected 4 other_csv_paths, got {len(other_csv_paths)}\"\n",
    "                )\n",
    "\n",
    "        # Load the dataframes\n",
    "        dfs = {}\n",
    "        dfs[\"NN\"] = pd.read_csv(NN_csv_path, header=0, index_col=0, low_memory=False)\n",
    "\n",
    "        if not only_NN:\n",
    "            for path in other_csv_paths:\n",
    "                name = path.name.split(\"_\", maxsplit=1)[0]\n",
    "                dfs[name] = pd.read_csv(path, header=0, index_col=0, low_memory=False)\n",
    "\n",
    "        # Verify that all dataframes have the same md5sums\n",
    "        md5s = {}\n",
    "        for key, df in dfs.items():\n",
    "            md5s[key] = set(df.index)\n",
    "\n",
    "        base_md5s = md5s[\"NN\"]\n",
    "        if not base_md5s.intersection(*list(md5s.values())) == base_md5s:\n",
    "            raise AssertionError(\"Not all dataframes have the same md5sums\")\n",
    "\n",
    "        all_split_dfs[split] = dfs\n",
    "\n",
    "    return all_split_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_split_metrics(\n",
    "    all_split_dfs: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> Dict[str, Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"Compute desired metrics for each split and classifier.\"\"\"\n",
    "    split_metrics = {}\n",
    "    for split in [f\"split{i}\" for i in range(10)]:\n",
    "        dfs = all_split_dfs[split]\n",
    "\n",
    "        # Compute metrics for the split\n",
    "        metrics = {}\n",
    "        for key, df in dfs.items():\n",
    "            # One-hot encode true and predicted classes\n",
    "            classes_order = df.columns[2:]\n",
    "            onehot_true = (\n",
    "                pd.get_dummies(df[\"True class\"], dtype=int)\n",
    "                .reindex(columns=classes_order, fill_value=0)\n",
    "                .values\n",
    "            )\n",
    "            pred_probs = df[\n",
    "                classes_order\n",
    "            ].values  # Ensure this aligns with your model's output format\n",
    "\n",
    "            ravel_true = np.argmax(onehot_true, axis=1)\n",
    "            ravel_pred = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "            metrics[key] = {\n",
    "                \"Accuracy\": accuracy_score(ravel_true, ravel_pred),\n",
    "                \"F1_macro\": f1_score(ravel_true, ravel_pred, average=\"macro\"),\n",
    "                \"AUC_micro\": roc_auc_score(\n",
    "                    onehot_true, pred_probs, multi_class=\"ovr\", average=\"micro\"\n",
    "                ),\n",
    "                \"AUC_macro\": roc_auc_score(\n",
    "                    onehot_true, pred_probs, multi_class=\"ovr\", average=\"macro\"\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            split_metrics[split] = metrics\n",
    "\n",
    "    return split_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_split_metrics(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    label_category: str,\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    ") -> None:\n",
    "    \"\"\"Render to box plots the metrics per classifier and split, each in its own subplot.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A dictionary containing metric scores for each classifier and split.\n",
    "    \"\"\"\n",
    "    metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "    classifiers = list(next(iter(split_metrics.values())).keys())\n",
    "\n",
    "    # Create subplots, one row for each metric\n",
    "    fig = make_subplots(rows=1, cols=len(metrics), subplot_titles=metrics)\n",
    "\n",
    "    colors = {\n",
    "        classifier: px.colors.qualitative.Plotly[i]\n",
    "        for i, classifier in enumerate(classifiers)\n",
    "    }\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        for classifier in classifiers:\n",
    "            values = [split_metrics[split][classifier][metric] for split in split_metrics]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=values,\n",
    "                    name=classifier,\n",
    "                    marker_color=colors[classifier],\n",
    "                    line=dict(color=\"black\", width=1),\n",
    "                    marker=dict(size=2),\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",  # or \"outliers\" to show only outliers\n",
    "                    pointpos=-1.4,\n",
    "                    showlegend=False,\n",
    "                    width=0.5,\n",
    "                    hoverinfo=\"text\",\n",
    "                    hovertext=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in zip(split_metrics, values)\n",
    "                    ],\n",
    "                ),\n",
    "                row=1,\n",
    "                col=i + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{label_category} classification - Metric distribution for 10fold cross-validation\",\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode=\"group\",\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_results_assay = base_data_dir / \"EpiClass_EA-21606_Assay11_100kb\" / \"all_splits\"\n",
    "path_results_cell_type = base_data_dir / CELL_TYPE / \"all_splits\"\n",
    "\n",
    "for label_category, path in zip(\n",
    "    [ASSAY, CELL_TYPE], [path_results_assay, path_results_cell_type]\n",
    "):\n",
    "    all_split_dfs = gather_split_results(results_dir=path, label_category=label_category)\n",
    "    split_metrics = compute_split_metrics(all_split_dfs)\n",
    "    dfreeze_version = \"dfreeze-v2\" if label_category == CELL_TYPE else \"dfreeze-v1\"\n",
    "    plot_split_metrics(\n",
    "        split_metrics,\n",
    "        label_category=label_category,\n",
    "        logdir=base_fig_dir,\n",
    "        name=f\"{label_category}_10fold_metrics_all_classifiers_{dfreeze_version}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1.supp.B\n",
    "\n",
    "Per model, compute score distribution per assay (1 violin per assay). No SVM. Agree black, red disagree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig1_supp_B(df_dict: Dict[str, pd.DataFrame], logdir: Path, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with subplots for each assay, each containing violin plots for different classifiers\n",
    "    and associated scatter plots for matches (in black) and mismatches (in red).\n",
    "\n",
    "    Args:\n",
    "        df_dict (Dict[str, pd.DataFrame]): Dictionary with the DataFrame containing the results for each classifier.\n",
    "        logdir (Path): The directory path for saving the figures.\n",
    "        name (str): The name for the saved figures.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    # Ignore LinearSVC and RandomForest for this figure\n",
    "    if \"LinearSVC\" in df_dict:\n",
    "        del df_dict[\"LinearSVC\"]\n",
    "    if \"RF\" in df_dict:\n",
    "        del df_dict[\"RF\"]\n",
    "\n",
    "    # Assuming all classifiers have the same assays for simplicity\n",
    "    first_key = next(iter(df_dict))\n",
    "    class_labels = df_dict[first_key][\"True class\"].unique()\n",
    "    class_labels_sorted = sorted(class_labels)\n",
    "    num_assays = len(class_labels_sorted)\n",
    "\n",
    "    classifiers = list(df_dict.keys())\n",
    "    classifier_index = {name: i for i, name in enumerate(classifiers)}\n",
    "    num_classifiers = len(classifiers)\n",
    "\n",
    "    scatter_offset = 0.05  # Scatter plot jittering\n",
    "\n",
    "    # Calculate the size of the grid\n",
    "    grid_size = int(np.ceil(np.sqrt(num_assays)))\n",
    "    rows, cols = grid_size, grid_size\n",
    "\n",
    "    # Create subplots with a square grid\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=class_labels_sorted,\n",
    "        shared_yaxes=\"all\",  # type: ignore\n",
    "        horizontal_spacing=0.05,\n",
    "        vertical_spacing=0.05,\n",
    "        y_title=\"Average prediction score\",\n",
    "    )\n",
    "    for idx, label in enumerate(class_labels_sorted):\n",
    "        row, col = divmod(idx, grid_size)\n",
    "        for classifier_name, classifier_df in df_dict.items():\n",
    "            df = classifier_df[classifier_df[\"True class\"] == label]\n",
    "\n",
    "            # Majority vote, mean prediction score\n",
    "            groupby_epirr = df.groupby([\"EpiRR\", \"Predicted class\"])[\n",
    "                \"Max pred\"\n",
    "            ].aggregate([\"size\", \"mean\"])\n",
    "            groupby_epirr = groupby_epirr.reset_index().sort_values(\n",
    "                [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "            )\n",
    "            groupby_epirr = groupby_epirr.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "            assert groupby_epirr[\"EpiRR\"].is_unique\n",
    "\n",
    "            mean_pred = groupby_epirr[\"mean\"]\n",
    "            classifier_pos = classifier_index[classifier_name]\n",
    "\n",
    "            # Add violin plot with integer x positions\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    x=classifier_pos * np.ones(len(mean_pred)),\n",
    "                    y=mean_pred,\n",
    "                    name=label,\n",
    "                    spanmode=\"hard\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=False,\n",
    "                    fillcolor=\"grey\",\n",
    "                    line_color=\"black\",\n",
    "                    line=dict(width=0.8),\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=row + 1,  # Plotly rows are 1-indexed\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "            # Prepare data for scatter plots\n",
    "            jittered_x_positions = np.random.uniform(-scatter_offset, scatter_offset, size=len(mean_pred)) + classifier_pos - 0.3  # type: ignore\n",
    "\n",
    "            match_pred = [\n",
    "                mean_pred.iloc[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] == label\n",
    "            ]\n",
    "            mismatch_pred = [\n",
    "                mean_pred.iloc[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] != label\n",
    "            ]\n",
    "\n",
    "            match_x_positions = [\n",
    "                jittered_x_positions[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] == label\n",
    "            ]\n",
    "            mismatch_x_positions = [\n",
    "                jittered_x_positions[i]\n",
    "                for i, row in enumerate(groupby_epirr.iterrows())\n",
    "                if row[1][\"Predicted class\"] != label\n",
    "            ]\n",
    "\n",
    "            # Add scatter plots for matches in black\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=match_x_positions,\n",
    "                    y=match_pred,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(color=\"black\", size=1),\n",
    "                    showlegend=False,\n",
    "                    name=f\"Match {classifier_name}\",\n",
    "                ),\n",
    "                row=row + 1,  # Plotly rows are 1-indexed\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "            # Add scatter plots for mismatches in red\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=mismatch_x_positions,\n",
    "                    y=mismatch_pred,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(color=\"red\", size=3),\n",
    "                    showlegend=False,\n",
    "                    name=f\"Mismatch {classifier_name}\",\n",
    "                ),\n",
    "                row=row + 1,  # Plotly rows are 1-indexed\n",
    "                col=col + 1,\n",
    "            )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - black points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Match\",\n",
    "            marker=dict(color=\"black\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"match\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - red points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Mismatch\",\n",
    "            marker=dict(color=\"red\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"mismatch\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update the layout to adjust the legend\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            title_text=\"Legend\",\n",
    "            itemsizing=\"constant\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.025,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout to improve visualization, adjust if needed for better appearance with multiple classifiers\n",
    "    fig.update_layout(\n",
    "        title_text=\"Prediction score distribution per assay across classifiers\",\n",
    "        height=1500,  # Adjust the height as necessary\n",
    "        width=1500,  # Adjust the width based on the number of assays\n",
    "    )\n",
    "\n",
    "    fig.update_layout(yaxis2=dict(range=[0.9, 1.01]))\n",
    "\n",
    "    # Adjust tick names\n",
    "    # Assuming equal spacing between each classifier on the x-axis\n",
    "    tickvals = list(\n",
    "        range(0, num_classifiers + 1)\n",
    "    )  # Generate tick values (1-indexed for Plotly)\n",
    "    ticktext = classifiers  # Use classifier names as tick labels\n",
    "    for i, j in itertools.product(range(rows), range(cols)):\n",
    "        fig.update_xaxes(tickvals=tickvals, ticktext=ticktext, row=i + 1, col=j + 1)\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}_min0.9.svg\")\n",
    "    fig.write_image(logdir / f\"{name}_min0.9.png\")\n",
    "    fig.write_html(logdir / f\"{name}_min0.9.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_split_results(\n",
    "    split_dfs: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Concatenate split results for each different classifier.\n",
    "\n",
    "    Args:\n",
    "        split_dfs (Dict[str, Dict[str, pd.DataFrame]]): {split_name:{classifier_name: results_df}}\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: {classifier_name: concatenated_df}\n",
    "    \"\"\"\n",
    "    to_concat_dfs = defaultdict(list)\n",
    "    for dfs in split_dfs.values():\n",
    "        for classifier, df in dfs.items():\n",
    "            to_concat_dfs[classifier].append(df)\n",
    "\n",
    "    concatenated_dfs = {\n",
    "        classifier: pd.concat(dfs, axis=0) for classifier, dfs in to_concat_dfs.items()\n",
    "    }\n",
    "\n",
    "    # Verify index is still md5sum\n",
    "    for df in concatenated_dfs.values():\n",
    "        if not isinstance(df.index[0], str):\n",
    "            raise AssertionError(\"Index is not md5sum\")\n",
    "\n",
    "    return concatenated_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_metadata(df: pd.DataFrame, metadata: Metadata) -> pd.DataFrame:\n",
    "    \"\"\"Join the metadata to the results dataframe.\"\"\"\n",
    "    metadata_df = pd.DataFrame(metadata.datasets)\n",
    "    metadata_df.set_index(\"md5sum\", inplace=True)\n",
    "\n",
    "    diff_set = set(df.index) - set(metadata_df.index)\n",
    "    if diff_set:\n",
    "        err_df = pd.DataFrame(diff_set, columns=[\"md5sum\"])\n",
    "        err_df.to_csv(base_data_dir / \"join_missing_md5sums.csv\", index=False)\n",
    "        raise AssertionError(\n",
    "            f\"{len(diff_set)} md5sums in the results dataframe are not present in the metadata dataframe. Saved error md5sums to join_missing_md5sums.csv.\"\n",
    "        )\n",
    "\n",
    "    merged_df = df.merge(metadata_df, how=\"left\", left_index=True, right_index=True)\n",
    "    if len(merged_df) != len(df):\n",
    "        raise AssertionError(\n",
    "            \"Merged dataframe has different length than original dataframe\"\n",
    "        )\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_split_dfs = gather_split_results(path_results_assay, ASSAY)\n",
    "# full_dfs = concatenate_split_results(all_split_dfs)\n",
    "# merged_dfs = {classifier: merge_similar_assays(df) for classifier, df in full_dfs.items()}\n",
    "# assays = merged_dfs[next(iter(merged_dfs))][\"True class\"].unique()\n",
    "\n",
    "# # Add Max pred\n",
    "# for classifier, df in merged_dfs.items():\n",
    "#     df[\"Max pred\"] = df[assays].max(axis=1)\n",
    "\n",
    "# # Join metadata\n",
    "# metadata_path = (\n",
    "#     base_data_dir / \"metadata\" / \"hg38_2023-epiatlas_dfreeze_formatted_JR.json\"\n",
    "# )\n",
    "# metadata_dfreeze1 = Metadata(metadata_path)\n",
    "# metadata_dfreeze1_df = pd.DataFrame(metadata_dfreeze1.datasets)\n",
    "\n",
    "# for classifier, df in merged_dfs.items():\n",
    "#     merged_dfs[classifier] = df.merge(\n",
    "#         metadata_dfreeze1_df, how=\"left\", left_index=True, right_on=\"md5sum\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig1_supp_B(merged_dfs, logdir=base_fig_dir, name=\"fig1_supp_B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1.supp.F\n",
    "\n",
    "For each classifier type\n",
    "\n",
    "Confusion matrix (1point=1 uuid) for observed datasets with average scores>0.9\n",
    "- Goal: Represent global predictions/mislabels. 11c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    min_pred_score: float,\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    majority: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Create a confusion matrix for the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the neural network results.\n",
    "        min_pred_score (float): The minimum prediction score to consider.\n",
    "        logdir (Path): The directory path for saving the figures.\n",
    "        name (str): The name for the saved figures.\n",
    "        majority (bool): Whether to use majority vote (uuid-wise) for the predicted class.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    classes = sorted(df[\"True class\"].unique())\n",
    "    if \"Max pred\" not in df.columns:\n",
    "        df[\"Max pred\"] = df[classes].max(axis=1)\n",
    "    filtered_df = df[df[\"Max pred\"] > min_pred_score]\n",
    "\n",
    "    if majority:\n",
    "        # Majority vote for predicted class\n",
    "        groupby_uuid = df.groupby([\"uuid\", \"True class\", \"Predicted class\"])[\n",
    "            \"Max pred\"\n",
    "        ].aggregate([\"size\", \"mean\"])\n",
    "        groupby_uuid = groupby_uuid.reset_index().sort_values(\n",
    "            [\"uuid\", \"True class\", \"size\"], ascending=[True, True, False]\n",
    "        )\n",
    "        groupby_uuid = groupby_uuid.drop_duplicates(\n",
    "            subset=[\"uuid\", \"True class\"], keep=\"first\"\n",
    "        )\n",
    "        filtered_df = groupby_uuid\n",
    "\n",
    "    confusion_mat = sk_cm(\n",
    "        filtered_df[\"True class\"], filtered_df[\"Predicted class\"], labels=classes\n",
    "    )\n",
    "\n",
    "    mat_writer = ConfusionMatrixWriter(labels=classes, confusion_matrix=confusion_mat)\n",
    "    mat_writer.to_all_formats(logdir, name=f\"{name}_n{len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_pred_score = 0.9\n",
    "# majority = True\n",
    "\n",
    "# for classifier_name, df in full_dfs.items():\n",
    "#     df_with_meta = df.merge(\n",
    "#         metadata_dfreeze1_df, how=\"left\", left_index=True, right_on=\"md5sum\"\n",
    "#     )\n",
    "#     assert \"Predicted class\" in df_with_meta.columns\n",
    "\n",
    "#     name = f\"{classifier_name}_pred>{min_pred_score}\"\n",
    "#     if classifier_name == \"LinearSVC\":\n",
    "#         name = f\"{classifier_name}\"\n",
    "\n",
    "#     logdir = base_fig_dir / \"fig1_supp_F-assay_c11_confusion_matrices\"\n",
    "#     if majority:\n",
    "#         logdir = logdir / \"per_uuid\"\n",
    "#     else:\n",
    "#         logdir = logdir / \"per_file\"\n",
    "\n",
    "# create_confusion_matrix(\n",
    "#     df=df_with_meta,\n",
    "#     min_pred_score=min_pred_score,\n",
    "#     logdir=logdir,\n",
    "#     name=name,\n",
    "#     majority=majority\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1.supp.D\n",
    "\n",
    "Inference on imputed data: Violin plot with pred score per assay (like Fig1A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"fig1_supp_D\"\n",
    "this_data_dir = base_data_dir / \"imputation\"\n",
    "\n",
    "# Load data\n",
    "normal_inf_imputed_path = next(\n",
    "    (this_data_dir / \"hg38_100kb_all_none/assay_epiclass_1l_3000n\").glob(\"**/*.csv\")\n",
    ")\n",
    "normal_inf_imputed_df = pd.read_csv(\n",
    "    normal_inf_imputed_path, header=0, index_col=0, low_memory=False\n",
    ")\n",
    "\n",
    "imputed_inf_normal_path = next(\n",
    "    (this_data_dir / \"hg38_100kb_all_none_imputed/assay_epiclass_1l_3000n\").rglob(\n",
    "        \"**/*.csv\"\n",
    "    )\n",
    ")\n",
    "imputed_inf_normal_df = pd.read_csv(\n",
    "    imputed_inf_normal_path, header=0, index_col=0, low_memory=False\n",
    ")\n",
    "\n",
    "assay_labels = normal_inf_imputed_df[\"True class\"].unique()\n",
    "for name, df in zip(\n",
    "    [\"train_normal_inf_imputed\", \"train_imputed_inf_normal\"],\n",
    "    [normal_inf_imputed_df, imputed_inf_normal_df],\n",
    "):\n",
    "    df[\"EpiRR\"] = list(df.index)\n",
    "    df[ASSAY] = df[\"True class\"]\n",
    "    df[\"Max pred\"] = df[assay_labels].max(axis=1)\n",
    "    # fig1_a(\n",
    "    #     df, logdir=fig_dir, name=f\"fig1_supp_D-{name}_n{len(df)}\", merge_assay_pairs=False\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flagship paper figure\n",
    "\n",
    "cell type classifier:  \n",
    "\n",
    "  for each assay, have a violin plot for accuracy per cell type (16 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_split_dfs = gather_split_results(path_results_cell_type, CELL_TYPE)\n",
    "ct_full_df = concatenate_split_results(ct_split_dfs)[\"NN\"]\n",
    "metadata_2 = return_metadata(\"v2\", paper_dir)\n",
    "ct_full_df = join_metadata(ct_full_df, metadata_2)\n",
    "ct_full_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### violin version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_flagship_ct(cell_type_df: pd.DataFrame, logdir: Path, name: str) -> None:\n",
    "    \"\"\"\n",
    "    [FILL HERE]\n",
    "\n",
    "    Args:\n",
    "        cell_type_df (pd.DataFrame): DataFrame containing the cell type prediction results.\n",
    "        logdir (Path): The directory path for saving the figure.\n",
    "        name (str): The name for the saved figure.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "\n",
    "    # Assuming all classifiers have the same assays for simplicity\n",
    "    assay_labels = sorted(cell_type_df[ASSAY].unique())\n",
    "    num_assays = len(assay_labels)\n",
    "\n",
    "    ct_labels = sorted(cell_type_df[\"True class\"].unique())\n",
    "    if len(ct_labels) != 16:\n",
    "        raise AssertionError(f\"Expected 16 cell type labels, got {len(ct_labels)}\")\n",
    "    ct_colors = [cell_type_colors[ct_label] for ct_label in ct_labels]\n",
    "\n",
    "    scatter_offset = 0.1  # Scatter plot jittering\n",
    "\n",
    "    # Calculate the size of the grid\n",
    "    grid_size = int(np.ceil(np.sqrt(num_assays)))\n",
    "    rows, cols = grid_size, grid_size\n",
    "\n",
    "    # Compute assay acc values beforehand, to be able to sort the assays by mean acc\n",
    "    assay_acc_dict = defaultdict(dict)\n",
    "    subclass_sizes = defaultdict(dict)\n",
    "    for idx, assay_label in enumerate(assay_labels):\n",
    "        assay_df = cell_type_df[cell_type_df[ASSAY] == assay_label]\n",
    "\n",
    "        # cell type subclass accuracy\n",
    "        subclass_sizes[assay_label] = assay_df.groupby([\"True class\"]).agg(\"size\")\n",
    "        subclass_groupby_acc = assay_df.groupby([\"True class\", \"Predicted class\"]).agg(\n",
    "            \"size\"\n",
    "        )\n",
    "        ct_accuracies = {}\n",
    "        for ct_label in sorted(ct_labels):\n",
    "            acc = float(subclass_groupby_acc[ct_label][ct_label]) / float(\n",
    "                subclass_sizes[assay_label][ct_label]\n",
    "            )\n",
    "            ct_accuracies[ct_label] = acc\n",
    "\n",
    "        assay_acc_dict[assay_label] = ct_accuracies\n",
    "\n",
    "    assay_sorted_by_mean_acc = sorted(\n",
    "        assay_acc_dict,\n",
    "        key=lambda x: np.mean(list(assay_acc_dict[x].values())),\n",
    "        reverse=True,\n",
    "    )\n",
    "\n",
    "    # Create subplots with a square grid\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=assay_sorted_by_mean_acc,\n",
    "        shared_yaxes=\"all\",  # type: ignore\n",
    "        horizontal_spacing=0,\n",
    "        vertical_spacing=0.02,\n",
    "        y_title=\"Cell type subclass accuracy\",\n",
    "    )\n",
    "\n",
    "    for idx, assay_label in enumerate(assay_sorted_by_mean_acc):\n",
    "        row, col = divmod(idx, grid_size)\n",
    "\n",
    "        acc_values = list(assay_acc_dict[assay_label].values())\n",
    "        # Add violin plot with integer x positions\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[idx] * len(acc_values),\n",
    "                y=acc_values,\n",
    "                name=assay_label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=False,\n",
    "                fillcolor=assay_colors[assay_label],\n",
    "                line_color=\"white\",\n",
    "                line=dict(width=0.8),\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=row + 1,  # Plotly rows are 1-indexed\n",
    "            col=col + 1,\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(showticklabels=False)\n",
    "\n",
    "        # Prepare data for scatter plots\n",
    "        jittered_x_positions = np.random.uniform(-scatter_offset, scatter_offset, size=len(acc_values)) + idx - 0.4  # type: ignore\n",
    "\n",
    "        scatter_marker_size = 10\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=jittered_x_positions,\n",
    "                y=acc_values,\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=scatter_marker_size, color=ct_colors),\n",
    "                hoverinfo=\"text\",\n",
    "                hovertext=[\n",
    "                    f\"{ct_label} ({assay_acc_dict[assay_label][ct_label]:.3f}, n={subclass_sizes[assay_label][ct_label]})\"\n",
    "                    for ct_label in assay_acc_dict[assay_label]\n",
    "                ],\n",
    "                showlegend=False,\n",
    "            ),\n",
    "            row=row + 1,  # Plotly rows are 1-indexed\n",
    "            col=col + 1,\n",
    "        )\n",
    "\n",
    "    # Add a dummy scatter plot for legend\n",
    "    for ct_label in ct_labels:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[None],\n",
    "                y=[None],\n",
    "                mode=\"markers\",\n",
    "                name=ct_label,\n",
    "                marker=dict(color=cell_type_colors[ct_label], size=scatter_marker_size),\n",
    "                showlegend=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    title_text = f\"{CELL_TYPE.replace('_', ' ').title()} classifier: Accuracy per assay\"\n",
    "    fig.update_layout(\n",
    "        title=title_text,\n",
    "        height=1500,\n",
    "        width=1500,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"flagship\"\n",
    "fig_flagship_ct(ct_full_df, logdir=fig_dir, name=\"ct_assay_accuracy_violin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### boxplot version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_flagship_ct_boxplot(cell_type_df: pd.DataFrame, logdir: Path, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Generates a boxplot for cell type classification accuracy across different assays.\n",
    "\n",
    "    This function creates a single figure with boxplots for each assay, displaying the accuracy\n",
    "    of cell type classification. Each boxplot represents the distribution of accuracy for one assay\n",
    "    across different cell types.\n",
    "\n",
    "    Args:\n",
    "        cell_type_df: DataFrame containing the cell type prediction results.\n",
    "        logdir: The directory path for saving the figure.\n",
    "        name: The name for the saved figure.\n",
    "    \"\"\"\n",
    "    # Assuming all classifiers have the same assays for simplicity\n",
    "    assay_labels = sorted(cell_type_df[ASSAY].unique())\n",
    "    ct_labels = sorted(cell_type_df[\"True class\"].unique())\n",
    "\n",
    "    if len(ct_labels) != 16:\n",
    "        raise AssertionError(f\"Expected 16 cell type labels, got {len(ct_labels)}\")\n",
    "\n",
    "    assay_acc_dict = defaultdict(list)\n",
    "    for assay_label in assay_labels:\n",
    "        assay_df = cell_type_df[cell_type_df[ASSAY] == assay_label]\n",
    "\n",
    "        # cell type subclass accuracy\n",
    "        subclass_size = assay_df.groupby([\"True class\"]).agg(\"size\")\n",
    "        subclass_groupby_acc = assay_df.groupby([\"True class\", \"Predicted class\"]).agg(\n",
    "            \"size\"\n",
    "        )\n",
    "        for ct_label in sorted(ct_labels):\n",
    "            acc = subclass_groupby_acc[ct_label][ct_label] / subclass_size[ct_label]\n",
    "            assay_acc_dict[assay_label].append(acc)\n",
    "\n",
    "    assay_sorted_by_mean_acc = sorted(\n",
    "        assay_acc_dict, key=lambda x: np.mean(assay_acc_dict[x]), reverse=True\n",
    "    )\n",
    "\n",
    "    # Create the boxplot\n",
    "    fig = go.Figure()\n",
    "    for assay_label in assay_sorted_by_mean_acc:\n",
    "        # Select accuracies corresponding to the current assay\n",
    "        assay_accuracies = assay_acc_dict[assay_label]\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=assay_accuracies,\n",
    "                name=assay_label,\n",
    "                boxpoints=\"outliers\",\n",
    "                boxmean=True,\n",
    "                fillcolor=assay_colors[assay_label],\n",
    "                line_color=\"black\",\n",
    "                showlegend=False,\n",
    "                marker=dict(size=2),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    title_text = f\"{CELL_TYPE.replace('_', ' ').title()} classifier: Accuracy per assay\"\n",
    "    fig.update_layout(\n",
    "        title=title_text,\n",
    "        yaxis_title=\"Accuracy\",\n",
    "        xaxis_title=\"Assay\",\n",
    "        height=600,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    # Save and display the figure\n",
    "    if not logdir.exists():\n",
    "        raise FileNotFoundError(f\"Could not find {logdir}\")\n",
    "\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"flagship\"\n",
    "fig_flagship_ct_boxplot(ct_full_df, logdir=fig_dir, name=\"ct_assay_accuracy_boxplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata 3rd factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell type classification: check (input, ct) pairs for enrichment in any metadata category\n",
    "    - use:\n",
    "        - harmonized_donor_life_stage\n",
    "        - harmonized_donor_sex\n",
    "        - harmonized_sample_cancer_high\n",
    "        - harmonized_biomaterial\n",
    "        - paired_end\n",
    "        - project\n",
    "    - find a 3rd factor metric, e.g. if any pair (assay, ct) subclass is very different from global dist, it can use that info as 3rd factor, and we're looking at assay specifically\n",
    "        - one score per pair, pearson w accuracy vector (one vector per assay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metadata_distribution(\n",
    "    df: pd.DataFrame, columns: List[str]\n",
    ") -> Dict[str, pd.Series]:\n",
    "    \"\"\"\n",
    "    Calculates the percentage of metadata labels within specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: A pandas DataFrame containing the data.\n",
    "        columns: A list of column names to analyze.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are column names and values are Series objects containing\n",
    "        the percentage of each unique label in the respective column.\n",
    "    \"\"\"\n",
    "    distribution = {}\n",
    "    nb_samples = len(df)\n",
    "    for column in columns:\n",
    "        # Count the occurrences of each unique value in the column\n",
    "        value_counts = df[column].value_counts(dropna=False)\n",
    "        # Calculate the percentages\n",
    "        percentages = (value_counts / nb_samples) * 100\n",
    "        # Store the results in the dictionary\n",
    "        distribution[column] = percentages\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_label_ratios(\n",
    "    target_distribution: Dict[str, pd.Series],\n",
    "    comparison_distributions: List[Dict[str, pd.Series]],\n",
    "    labels: List[str],\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compares label ratios of a target distribution against multiple comparison distributions,\n",
    "    calculating the difference in percentage points for each label within each metadata category.\n",
    "\n",
    "    Args:\n",
    "        target_distribution: A dictionary of Series representing the target distribution for comparison.\n",
    "        comparison_distributions: A list of dictionaries of Series, where each dictionary\n",
    "                                  represents a distribution (e.g., assay, cell type, global) for comparison.\n",
    "        labels: A list of labels corresponding to each distribution in `comparison_distributions`,\n",
    "                used for labeling the columns in the result.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of DataFrames, where each DataFrame shows the difference in percentage points\n",
    "        for each label in a metadata category between the target distribution and each of the\n",
    "        comparison distributions.\n",
    "    \"\"\"\n",
    "    comparison_results = {}\n",
    "    for category, target_series in target_distribution.items():\n",
    "        # Initialize a DataFrame to store comparison results for this category\n",
    "        comparison_df = pd.DataFrame()\n",
    "\n",
    "        for label, comparison_distribution in zip(labels, comparison_distributions):\n",
    "            # Ensure the comparison distribution series for this category exists and align target with comparison\n",
    "            comparison_series = comparison_distribution.get(\n",
    "                category, pd.Series(dtype=\"float64\")\n",
    "            )\n",
    "            aligned_target, aligned_comparison = target_series.align(\n",
    "                comparison_series, fill_value=0\n",
    "            )\n",
    "\n",
    "            # Calculate difference in percentage points\n",
    "            difference = aligned_target - aligned_comparison\n",
    "\n",
    "            # Store the results in the comparison DataFrame\n",
    "            comparison_df[f\"Difference_vs_{label}\"] = difference\n",
    "\n",
    "        comparison_results[category] = comparison_df\n",
    "\n",
    "    return comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_categories = [\n",
    "    \"harmonized_donor_life_stage\",\n",
    "    \"harmonized_donor_sex\",\n",
    "    \"harmonized_sample_disease_high\",\n",
    "    \"harmonized_biomaterial_type\",\n",
    "    \"paired_end\",\n",
    "    \"project\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_third_factor_correlation(\n",
    "    ct_full_df: pd.DataFrame,\n",
    "    metadata_categories: List[str],\n",
    "    save_full_details: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the correlation between third factor influence and cell type classification accuracy for each assay.\n",
    "\n",
    "    This function operates on classification results to evaluate how a third factor, represented by metadata category distributions,\n",
    "    correlates with the accuracy of cell type classifications across assays. It involves comparing metadata distributions\n",
    "    within assay and cell type groups to a global distribution, and then correlating these comparisons with classification\n",
    "    accuracies.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with epigenomics data, including assays, cell types, and metadata for classification.\n",
    "        metadata_categories (List[str]): A list of metadata categories to analyze.\n",
    "    \"\"\"\n",
    "    global_dist = calculate_metadata_distribution(ct_full_df, metadata_categories)\n",
    "    subclass_distributions = {}\n",
    "    comparison_results = (\n",
    "        {}\n",
    "    )  # Initialize a dict to hold comparison results for each subgroup\n",
    "\n",
    "    for group in ct_full_df.groupby(ASSAY):\n",
    "        label = group[0]\n",
    "        sub_df = group[1]\n",
    "        subclass_distributions[label] = calculate_metadata_distribution(\n",
    "            sub_df, metadata_categories\n",
    "        )\n",
    "\n",
    "    for group in ct_full_df.groupby(CELL_TYPE):\n",
    "        label = group[0]\n",
    "        sub_df = group[1]\n",
    "        subclass_distributions[label] = calculate_metadata_distribution(\n",
    "            sub_df, metadata_categories\n",
    "        )\n",
    "\n",
    "    # Loop through each group and compare to global\n",
    "    for group in ct_full_df.groupby([ASSAY, CELL_TYPE]):\n",
    "        assay, cell_type = group[0]\n",
    "        sub_df = group[1]\n",
    "        pair_subclass_dist = calculate_metadata_distribution(sub_df, metadata_categories)\n",
    "        subclass_distributions[(assay, cell_type)] = pair_subclass_dist\n",
    "\n",
    "        assay_dist = subclass_distributions[assay]\n",
    "        cell_type_dist = subclass_distributions[cell_type]\n",
    "\n",
    "        comparisons_dists = [assay_dist, cell_type_dist, global_dist]\n",
    "        comparison_labels = [assay, cell_type, \"global\"]\n",
    "\n",
    "        comparison_results[(assay, cell_type)] = compare_label_ratios(\n",
    "            target_distribution=pair_subclass_dist,\n",
    "            comparison_distributions=comparisons_dists,\n",
    "            labels=comparison_labels,\n",
    "        )\n",
    "\n",
    "    pair_dfs = {}\n",
    "    pairs_3rd_factor = {}\n",
    "    for (assay, cell_type), comparisons in comparison_results.items():\n",
    "        # Initialize an empty list to collect DataFrames for concatenation\n",
    "        dfs_to_concat = []\n",
    "\n",
    "        for category, df_comparison in comparisons.items():\n",
    "            df_comparison.columns = [\n",
    "                \"Difference vs Assay\",\n",
    "                \"Difference vs Cell Type\",\n",
    "                \"Difference vs Global\",\n",
    "            ]\n",
    "            # Add identifiers for the assay, cell type, and category\n",
    "            df_comparison[\"Assay\"] = assay\n",
    "            df_comparison[\"Cell Type\"] = cell_type\n",
    "            df_comparison[\"Category\"] = category\n",
    "\n",
    "            subclass_dist = subclass_distributions[(assay, cell_type)][category]\n",
    "            df_comparison[\"(assay, ct) subclass %\"] = subclass_dist\n",
    "\n",
    "            # Collect the DataFrame\n",
    "            dfs_to_concat.append(df_comparison.reset_index())\n",
    "\n",
    "        # Concatenate all DataFrames along rows\n",
    "        final_df = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "        final_df.fillna(0, inplace=True)\n",
    "\n",
    "        new_columns = final_df.columns.tolist()\n",
    "        new_first = [\"index\", \"Category\", \"(assay, ct) subclass %\"]\n",
    "        for label in new_first:\n",
    "            new_columns.remove(label)\n",
    "        new_columns = new_first + new_columns\n",
    "        final_df = final_df[new_columns]\n",
    "\n",
    "        pair_dfs[(assay, cell_type)] = final_df\n",
    "\n",
    "        # val_3rd_factor = (final_df[\"Difference vs Assay\"] - ).abs().sum()\n",
    "        # val_3rd_factor= final_df[\"Difference vs Global\"].abs().max()\n",
    "        val_3rd_factor = final_df[\"Difference vs Global\"].min()\n",
    "        pairs_3rd_factor[(assay, cell_type)] = val_3rd_factor\n",
    "\n",
    "    # Subclass accuracy per assay\n",
    "    assay_labels = sorted(ct_full_df[ASSAY].unique())\n",
    "    ct_labels = sorted(ct_full_df[CELL_TYPE].unique())\n",
    "    assay_accuracies = {}\n",
    "    for assay_label in assay_labels:\n",
    "        assay_df = ct_full_df[ct_full_df[ASSAY] == assay_label]\n",
    "\n",
    "        # cell type subclass accuracy\n",
    "        subclass_size = assay_df.groupby([\"True class\"]).agg(\"size\")\n",
    "        subclass_groupby_acc = assay_df.groupby([\"True class\", \"Predicted class\"]).agg(\n",
    "            \"size\"\n",
    "        )\n",
    "        accuracies = {}\n",
    "        for ct_label in sorted(ct_labels):\n",
    "            acc_label = subclass_groupby_acc[ct_label][ct_label] / subclass_size[ct_label]\n",
    "            accuracies[ct_label] = acc_label\n",
    "\n",
    "        assay_accuracies[assay_label] = accuracies\n",
    "\n",
    "    # Concatenate all DataFrames along rows\n",
    "    if save_full_details:\n",
    "        all_pairs_df = pd.concat(pair_dfs, axis=0, ignore_index=True)\n",
    "        for assay in assay_labels:\n",
    "            for ct in ct_labels:\n",
    "                all_pairs_df.loc[\n",
    "                    (all_pairs_df[\"Assay\"] == assay) & (all_pairs_df[\"Cell Type\"] == ct),\n",
    "                    \"Accuracy\",\n",
    "                ] = assay_accuracies[assay][ct]\n",
    "\n",
    "        all_pairs_df.columns = [\n",
    "            \"Label\" if x == \"index\" else x for x in all_pairs_df.columns\n",
    "        ]\n",
    "        file_path = base_fig_dir / \"flagship\" / \"metadata_comparison_all.csv\"\n",
    "        all_pairs_df.to_csv(file_path, index=False)\n",
    "\n",
    "    pearson_3rd_factor = {}\n",
    "    for assay, acc_dict in assay_accuracies.items():\n",
    "        acc_vector = {ct: acc_dict[ct] for ct in ct_labels}\n",
    "        acc_vector = pd.Series(acc_vector)\n",
    "\n",
    "        diff_metric = {ct: pairs_3rd_factor[(assay, ct)] for ct in ct_labels}\n",
    "        diff_metric = pd.Series(diff_metric)\n",
    "        pearson = acc_vector.corr(diff_metric, method=\"pearson\")\n",
    "        pearson_3rd_factor[assay] = pearson\n",
    "\n",
    "    return pearson_3rd_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_third_factor_correlation(ct_full_df, metadata_categories, save_full_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_series = []\n",
    "for metadata_category in metadata_categories:\n",
    "    pearson_dict = compute_third_factor_correlation(ct_full_df, [metadata_category])\n",
    "    pearson_df = pd.DataFrame(pearson_dict, index=[metadata_category])\n",
    "    pearson_series.append(pearson_df)\n",
    "\n",
    "full_pearson_df = pd.concat(pearson_series, axis=0)\n",
    "\n",
    "# Add  max for each row and column\n",
    "full_pearson_df[\"Max\"] = full_pearson_df.abs().max(axis=1)\n",
    "max_row = full_pearson_df.abs().max(axis=0)\n",
    "max_row.name = \"Max\"\n",
    "full_pearson_df = full_pearson_df.append(max_row)\n",
    "\n",
    "full_pearson_df.to_csv(\n",
    "    base_fig_dir / \"flagship\" / \"3rd_factor_min_diff_pearson_correlation.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_labels = sorted(ct_full_df[ASSAY].unique())\n",
    "ct_labels = sorted(ct_full_df[CELL_TYPE].unique())\n",
    "life_stages = ct_full_df[\"harmonized_donor_life_stage\"].unique().tolist()\n",
    "life_stages.remove(\"unknown\")\n",
    "\n",
    "acc_list = []\n",
    "for assay_label in assay_labels:\n",
    "    assay_df = ct_full_df[ct_full_df[ASSAY] == assay_label]\n",
    "    for ct in ct_labels:\n",
    "        ct_df = assay_df[assay_df[CELL_TYPE] == ct]\n",
    "        for life_stage in life_stages:\n",
    "            life_stage_df = ct_df[ct_df[\"harmonized_donor_life_stage\"] == life_stage]\n",
    "            acc = life_stage_df[\"Predicted class\"].eq(life_stage_df[\"True class\"]).mean()\n",
    "            size = len(life_stage_df)\n",
    "            acc_list.append((assay_label, ct, life_stage, size, acc))\n",
    "\n",
    "acc_df = pd.DataFrame(\n",
    "    acc_list, columns=[\"Assay\", \"Cell Type\", \"Life Stage\", \"Size\", \"Accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df.to_csv(base_fig_dir / \"flagship\" / \"assay_ct_life_stage_accuracy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input classification - Hdf5 values at top SHAP features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparer le signal de input dans les diffrents cell-types (e.g. boxplot des 40 rgions \"input & t cell\" vs les 9 rgions \"input & lymphocyte of b lineage\" dans ces 2 CT (donc 4 boxplots) ou mme ajouter un autre CT externe comme muscle comme ctrl neg (6 boxplots))\n",
    "\n",
    "all input, check 40 and 9 regions in\n",
    "- t cell\n",
    "- b cell\n",
    "- muscle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flagship_supp_shap_input(paper_dir: Path) -> None:\n",
    "    \"\"\"\n",
    "    Plot hdf5 values of different (input, cell type) pairs for top SHAP values\n",
    "    of T cell and B cell. (only ct which produced common top shap values features)\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    # Load relevant metadata\n",
    "    ct_labels = [\"T cell\", \"lymphocyte of B lineage\", \"neutrophil\", \"muscle organ\"]\n",
    "    metadata_2 = return_metadata(\"v2\", paper_dir)\n",
    "    metadata_2.select_category_subsets(ASSAY, [\"input\"])\n",
    "    metadata_2.select_category_subsets(CELL_TYPE, ct_labels)\n",
    "    md5_per_ct = metadata_2.md5_per_class(CELL_TYPE)\n",
    "\n",
    "    # Load feature bins\n",
    "    hdf5_val_dir = (\n",
    "        base_data_dir\n",
    "        / \"harmonized_sample_ontology_intermediate/all_splits/harmonized_sample_ontology_intermediate_1l_3000n/10fold-dfreeze-v2/global_shap_analysis/top303/input\"\n",
    "    )\n",
    "    feature_filepath = hdf5_val_dir / \"features_n8.json\"\n",
    "    with open(feature_filepath, \"r\", encoding=\"utf8\") as f:\n",
    "        features: Dict[str, List[int]] = json.load(f)\n",
    "\n",
    "    # Load feature values\n",
    "    hdf5_val_path = hdf5_val_dir / \"hdf5_values_100kb_all_none_input_4ct_features_n8.csv\"\n",
    "    df = pd.read_csv(hdf5_val_path, index_col=0, header=0)\n",
    "\n",
    "    df_ct_dict = {}\n",
    "    for ct in ct_labels:\n",
    "        md5s = md5_per_ct[ct]\n",
    "        df_ct_dict[ct] = df.loc[md5s]\n",
    "\n",
    "    # Make two groups of boxplots, four boxplot per group (one per cell type)\n",
    "    # Each boxplot will take the values of the columns in the features dict\n",
    "    fig = go.Figure()\n",
    "    for ct_label, df_ct in df_ct_dict.items():\n",
    "        for i, (cell_type, top_shap_bins) in enumerate(features.items()):\n",
    "            top_shap_bins = [str(b) for b in top_shap_bins]\n",
    "            mean_bin_values_per_md5 = df_ct[top_shap_bins].mean(axis=1)\n",
    "\n",
    "            hovertext = [\n",
    "                f\"{md5}. {val:02f}\"\n",
    "                for md5, val in zip(df_ct.index, mean_bin_values_per_md5)\n",
    "            ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    x=[f\"Top SHAP '{cell_type}' bins\"] * len(mean_bin_values_per_md5),\n",
    "                    y=mean_bin_values_per_md5,\n",
    "                    name=f\"{ct_label} (n={len(hovertext)})\",\n",
    "                    points=\"all\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    spanmode=\"hard\",\n",
    "                    fillcolor=cell_type_colors[ct_label],\n",
    "                    line_color=\"black\",\n",
    "                    showlegend=i == 0,\n",
    "                    marker=dict(size=2),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                    legendgroup=ct_label,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Input files bin z-score values for top SHAP features\",\n",
    "        yaxis_title=\"Mean z-score across bins\",\n",
    "        xaxis_title=\"Top SHAP features groups\",\n",
    "        boxmode=\"group\",\n",
    "        violinmode=\"group\",\n",
    "        height=1000,\n",
    "        width=1000,\n",
    "    )\n",
    "\n",
    "    logdir = base_fig_dir / \"flagship\"\n",
    "    name = \"input_feature_values_top_shap_bins_per_md5\"\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagship_supp_shap_input(paper_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
