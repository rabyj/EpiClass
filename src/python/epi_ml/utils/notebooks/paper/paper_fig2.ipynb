{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create figures (fig2) destined for the paper.\n",
    "\n",
    "Please use dfreeze v2 for these.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    merge_similar_assays,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map\n",
    "sex_colors = IHECColorMap.sex_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata_v2 = metadata_handler.load_metadata(\"v2\")\n",
    "metadata_v2_df = pd.DataFrame.from_records(list(metadata_v2.datasets))\n",
    "metadata_v2_df.set_index(\"md5sum\", inplace=True)\n",
    "metadata_v2_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_metadata_path = (\n",
    "    base_data_dir / \"all_results_cometml_filtered_oversampling-fixed.csv\"\n",
    ")\n",
    "RUN_METADATA = pd.read_csv(parameters_metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig 2 - EpiClass results on EpiAtlas other metadata\n",
    "\n",
    "For following figures, use v1.1 of sample metadata (called v2.1 internally), i.e. dfreeze 2\n",
    "\n",
    "A) Histogram of performance (accuracy and F1 scores) for each category  \n",
    "\n",
    "B) Violin plot of average z-score on chrY per sex, black dots for pred same class and red for pred different class.  \n",
    "- Do the split male female violin per assay (only FC, merge 2xwgbs and 2xrna, no rna unique_raw). \n",
    "- Use scatter for points on each side, agree same color as violin, disagree other.\n",
    "- Point labels: uuid, epirr\n",
    "\n",
    "C) ---  \n",
    "D) ---  \n",
    "E) --- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network performance across metadata categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if oversampling is uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_oversampling(parent_dir: Path):\n",
    "    \"\"\"Check for oversampling status in the results.\n",
    "\n",
    "    Returns a ValeError if not all experiments have oversampling.\n",
    "    \"\"\"\n",
    "    # Identify experiments\n",
    "    exp_key_line = \"The current experiment key is\"\n",
    "    exp_keys_dict = defaultdict(list)\n",
    "    folders = list(parent_dir.iterdir())\n",
    "    for folder in folders:\n",
    "        for stdout_file in folder.glob(\"output_job*.o\"):\n",
    "            with open(stdout_file, \"r\", encoding=\"utf8\") as f:\n",
    "                lines = [l.rstrip() for l in f if exp_key_line in l]\n",
    "            exp_keys = [l.split(exp_key_line)[1].strip() for l in lines]\n",
    "            exp_keys_dict[folder.name].extend(exp_keys)\n",
    "\n",
    "    # Filter metadata to only include experiments in the results\n",
    "    all_exp_keys = set()\n",
    "    for exp_keys in exp_keys_dict.values():\n",
    "        all_exp_keys.update(exp_keys)\n",
    "\n",
    "    df = RUN_METADATA[RUN_METADATA[\"experimentKey\"].isin(all_exp_keys)]\n",
    "\n",
    "    # Check oversampling values, ignore nan\n",
    "    df = df[df[\"hparams/oversampling\"].notna()]\n",
    "    if not (df[\"hparams/oversampling\"] == \"true\").all():\n",
    "        df[\"general_name\"] = df[\"Name\"].str.replace(r\"[_-]?split\\d+$\", \"\", regex=True)\n",
    "        err_df = df.groupby([\"general_name\", \"hparams/oversampling\"]).agg(\"size\")\n",
    "        logging.warning(\n",
    "            \"Not all experiments have oversampling:\\n%s\",\n",
    "            err_df,\n",
    "        )\n",
    "\n",
    "    print(f\"Checked {len(folders)} folders and {len(df)} experiments.\")\n",
    "    if len(folders) * 10 != len(df):\n",
    "        logging.warning(\"Could not read oversampling value of all visited experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_dir = (\n",
    "    base_data_dir\n",
    "    / \"dfreeze_v2\"\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / \"assay_epiclass_1l_3000n\"\n",
    "    / \"11c\"\n",
    ")\n",
    "ct_dir = (\n",
    "    base_data_dir\n",
    "    / \"dfreeze_v2\"\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / \"harmonized_sample_ontology_intermediate_1l_3000n\"\n",
    ")\n",
    "# check_for_oversampling(assay_dir,s ASSAY)\n",
    "# check_for_oversampling(ct_dir, CELL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2_results_dir = base_data_dir / \"dfreeze_v2\"\n",
    "# check_for_oversampling_global(base_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of performance (accuracy and F1 scores) for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mislabel_corrector():\n",
    "    \"\"\"Obtain information necessary to correct sex and life_stage mislabels.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: {md5sum: EpiRR_no-v}\n",
    "        Dict[str, Dict[str, str]]: {label_category: {EpiRR_no-v: corrected_label}}\n",
    "    \"\"\"\n",
    "    epirr_no_v = \"EpiRR_no-v\"\n",
    "    # Associate epirrs to md5sums\n",
    "    metadata = MetadataHandler(paper_dir).load_metadata(\"v2\")\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n",
    "    md5sum_to_epirr = metadata_df.set_index(\"md5sum\")[epirr_no_v].to_dict()\n",
    "\n",
    "    # Load mislabels\n",
    "    epirr_to_corrections = {}\n",
    "    metadata_dir = base_data_dir / \"metadata\"\n",
    "\n",
    "    sex_mislabeled = pd.read_csv(metadata_dir / \"official_Sex_mislabeled.csv\")\n",
    "    epirr_to_corrections[SEX] = sex_mislabeled.set_index(epirr_no_v)[\n",
    "        \"EpiClass_pred_Sex\"\n",
    "    ].to_dict()\n",
    "\n",
    "    life_stage_mislabeled = pd.read_csv(\n",
    "        metadata_dir / \"official_Life_stage_mislabeled.csv\"\n",
    "    )\n",
    "    epirr_to_corrections[LIFE_STAGE] = life_stage_mislabeled.set_index(epirr_no_v)[\n",
    "        \"EpiClass_pred_Life_stage\"\n",
    "    ].to_dict()\n",
    "\n",
    "    return md5sum_to_epirr, epirr_to_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_a_content(\n",
    "    results_dir: Path,\n",
    "    exclude_categories: List[str] | None = None,\n",
    "    exclude_names: List[str] | None = None,\n",
    ") -> Tuple[Dict[str, Dict[str, Dict[str, float]]], Dict[str, Dict[str, pd.DataFrame]]]:\n",
    "    \"\"\"Create the content data for figure 2a. (get metrics for each task)\n",
    "\n",
    "    Currently only using oversampled runs.\n",
    "\n",
    "    Args:\n",
    "        results_dir (Path): Directory containing the results. Needs to be parent over category folders.\n",
    "        exclude_categories (List[str]): Task categories to exclude (first level directory names).\n",
    "        exclude_names (List[str]): Names of folders to exclude (ex: 7c or no-mix).\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, float]]] A metrics dictionary with the following structure:\n",
    "            {split_name: {task_name: metrics_dict}}\n",
    "        Dict[str, Dict[str, pd.DataFrame]] A split results dictionary with the following structure:\n",
    "            {task_name: {split_name: split_results_df}}\n",
    "    \"\"\"\n",
    "    all_split_results = {}\n",
    "    split_results_handler = SplitResultsHandler()\n",
    "\n",
    "    md5sum_to_epirr, epirr_to_corrections = create_mislabel_corrector()\n",
    "\n",
    "    for parent, _, _ in os.walk(results_dir):\n",
    "        # Looking for oversampling only results\n",
    "        parent = Path(parent)\n",
    "        if parent.name != \"10fold-oversampling\":\n",
    "            continue\n",
    "\n",
    "        # Get the category\n",
    "        relpath = parent.relative_to(results_dir)\n",
    "        category = relpath.parts[0].rstrip(\"_1l_3000n\")\n",
    "        if exclude_categories is not None:\n",
    "            if any(exclude_str in category for exclude_str in exclude_categories):\n",
    "                continue\n",
    "\n",
    "        # Get the rest of the name, ignore certain runs\n",
    "        rest_of_name = list(relpath.parts[1:])\n",
    "        rest_of_name.remove(\"10fold-oversampling\")\n",
    "\n",
    "        if len(rest_of_name) > 1:\n",
    "            raise ValueError(\n",
    "                f\"Too many parts in the name: {rest_of_name}. Path: {relpath}\"\n",
    "            )\n",
    "        if rest_of_name:\n",
    "            rest_of_name = rest_of_name[0]\n",
    "\n",
    "        if exclude_names is not None:\n",
    "            if any(name in rest_of_name for name in exclude_names):\n",
    "                continue\n",
    "\n",
    "        full_task_name = category\n",
    "        if rest_of_name:\n",
    "            full_task_name += f\"_{rest_of_name}\"\n",
    "\n",
    "        # Get the split results\n",
    "        split_results = split_results_handler.read_split_results(parent)\n",
    "        if not split_results:\n",
    "            raise ValueError(f\"No split results found in {parent}\")\n",
    "\n",
    "        if \"sex\" in full_task_name or \"life_stage\" in full_task_name:\n",
    "            corrections = epirr_to_corrections[category]\n",
    "            for split_name in split_results:\n",
    "                split_result_df = split_results[split_name]\n",
    "                current_true_class = split_result_df[\"True class\"].to_dict()\n",
    "                new_true_class = {\n",
    "                    k: corrections.get(md5sum_to_epirr[k], v)\n",
    "                    for k, v in current_true_class.items()\n",
    "                }\n",
    "                split_result_df[\"True class\"] = new_true_class.values()\n",
    "\n",
    "                split_results[split_name] = split_result_df\n",
    "\n",
    "        all_split_results[full_task_name] = split_results\n",
    "\n",
    "    try:\n",
    "        split_results_metrics = split_results_handler.compute_split_metrics(\n",
    "            all_split_results, concat_first_level=True\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        logging.error(\"KeyError: %s\", e)\n",
    "        logging.error(\"all_split_results: %s\", all_split_results)\n",
    "        logging.error(\"check folder: %s\", results_dir)\n",
    "        raise e\n",
    "    return split_results_metrics, all_split_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_a(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    exclude_categories: List[str],\n",
    "    y_range: List[float] | None = None,\n",
    "    sort_by_acc: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Render box plots of metrics per classifier and split, each in its own subplot.\n",
    "\n",
    "    This function generates a figure with subplots, each representing a different\n",
    "    metric. Each subplot contains box plots for each classifier, ordered by accuracy.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A nested dictionary with structure {split: {classifier: {metric: score}}}.\n",
    "        logdir: The directory path to save the output plots.\n",
    "        name: The base name for the output plot files.\n",
    "        exclude_categories: Task categories to exclude from the plot.\n",
    "    \"\"\"\n",
    "    metrics = [\"Accuracy\", \"F1_macro\"]\n",
    "    # metrics = [\"AUC_micro\", \"AUC_macro\"]\n",
    "    # metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "\n",
    "    # Exclude some categories\n",
    "    classifier_names = list(next(iter(split_metrics.values())).keys())\n",
    "    for category in exclude_categories:\n",
    "        classifier_names = [c for c in classifier_names if category not in c]\n",
    "\n",
    "    # Sort classifiers by accuracy\n",
    "    if sort_by_acc:\n",
    "        mean_acc = {}\n",
    "        for classifier in classifier_names:\n",
    "            mean_acc[classifier] = np.mean(\n",
    "                [split_metrics[split][classifier][\"Accuracy\"] for split in split_metrics]\n",
    "            )\n",
    "        classifier_names = sorted(\n",
    "            classifier_names, key=lambda x: mean_acc[x], reverse=True\n",
    "        )\n",
    "\n",
    "    # Create subplots, one column for each metric\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(metrics),\n",
    "        subplot_titles=metrics,\n",
    "        horizontal_spacing=0.03,\n",
    "    )\n",
    "\n",
    "    colors = {\n",
    "        classifier: px.colors.qualitative.Plotly[i]\n",
    "        for i, classifier in enumerate(classifier_names)\n",
    "    }\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        for classifier_name in classifier_names:\n",
    "            values = [\n",
    "                split_metrics[split][classifier_name][metric] for split in split_metrics\n",
    "            ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=values,\n",
    "                    name=classifier_name,\n",
    "                    fillcolor=colors[classifier_name],\n",
    "                    line=dict(color=\"black\", width=1.5),\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    pointpos=0,\n",
    "                    showlegend=i == 0,  # Only show legend in the first subplot\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in zip(split_metrics, values)\n",
    "                    ],\n",
    "                    legendgroup=classifier_name,\n",
    "                    width=0.5,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=i + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Neural network classification - Metric distribution for 10-fold cross-validation\",\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode=\"group\",\n",
    "        height=1200 * 0.8,\n",
    "        width=1750 * 0.8,\n",
    "    )\n",
    "\n",
    "    # Acc, F1\n",
    "    range_acc = [0.86, 1.001]\n",
    "    fig.update_layout(yaxis=dict(range=range_acc))\n",
    "    fig.update_layout(yaxis2=dict(range=range_acc))\n",
    "\n",
    "    # AUC\n",
    "    range_auc = [0.986, 1.0001]\n",
    "    fig.update_layout(yaxis3=dict(range=range_auc))\n",
    "    fig.update_layout(yaxis4=dict(range=range_auc))\n",
    "\n",
    "    if y_range is not None:\n",
    "        fig.update_yaxes(range=y_range)\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split_name, split_metrics in all_split_metrics.items():\n",
    "#     print(split_name)\n",
    "#     for task_name, task_metrics in split_metrics.items():\n",
    "#         print(task_name)\n",
    "#         print(task_metrics)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\"groups_second_level_name\", \"track_type\", \"disease\"]\n",
    "exclude_names = [\"chip-seq\", \"7c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 100kb\n",
    "# results_dir = base_data_dir / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "# if not results_dir.exists():\n",
    "#     raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "# split_results_metrics, all_split_results = fig2_a_content(results_dir, exclude_categories, exclude_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 10kb\n",
    "# results_dir = base_data_dir / \"dfreeze_v2\" / \"hg38_10kb_all_none\"\n",
    "# if not results_dir.exists():\n",
    "#     raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "# split_results_metrics, all_split_results = fig2_a_content(results_dir, exclude_categories, exclude_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenated_results = split_results_handler.concatenate_split_results(all_split_results, concat_first_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_split_results.keys()\n",
    "# split_results_metrics[\"split0\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\"\n",
    "fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "fig_name = \"fig2_A\"\n",
    "\n",
    "exclude_categories = [\"disease\", \"sex_no-mixed\"]\n",
    "# fig2_a(split_results_metrics, fig_logdir, fig_name, exclude_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network performance per assay across metadata categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_performance_per_assay_across_categories(\n",
    "    all_split_results: Dict[str, Dict[str, pd.DataFrame]],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    exclude_categories: List[str],\n",
    "    y_range: None | List[float] = None,\n",
    "):\n",
    "    \"\"\"Create a box plot of assay accuracy for each classifier.\"\"\"\n",
    "    all_split_results = copy.deepcopy(all_split_results)\n",
    "\n",
    "    # Exclude some categories\n",
    "    classifier_names = list(all_split_results.keys())\n",
    "    for category in exclude_categories:\n",
    "        classifier_names = [c for c in classifier_names if category not in c]\n",
    "\n",
    "    metadata_df = MetadataHandler(paper_dir).load_metadata_df(\"v2\")\n",
    "    metadata_df = metadata_df[ASSAY]  # only need assay column\n",
    "    metadata_df.replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # One graph per metadata category\n",
    "    for task_name in classifier_names:\n",
    "        split_results = all_split_results[task_name]\n",
    "        if ASSAY in task_name:\n",
    "            for split_name in split_results:\n",
    "                split_results[split_name] = merge_similar_assays(\n",
    "                    split_results[split_name]\n",
    "                )\n",
    "\n",
    "        assay_acc = defaultdict(dict)\n",
    "        for split_name, split_result_df in split_results.items():\n",
    "            # Merge metadata\n",
    "            split_result_df = split_result_df.merge(\n",
    "                metadata_df, left_index=True, right_index=True\n",
    "            )\n",
    "\n",
    "            # Compute accuracy per assay\n",
    "            assay_groupby = split_result_df.groupby(ASSAY)\n",
    "            for assay, assay_df in assay_groupby:\n",
    "                assay_acc[assay][split_name] = np.mean(\n",
    "                    assay_df[\"True class\"].astype(str).str.lower()\n",
    "                    == assay_df[\"Predicted class\"].astype(str).str.lower()\n",
    "                )\n",
    "\n",
    "        assay_acc_df = pd.DataFrame(assay_acc)\n",
    "        fig = go.Figure()\n",
    "        for assay in ASSAY_ORDER:\n",
    "            try:\n",
    "                assay_accuracies = assay_acc_df[assay]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=assay_accuracies.values,\n",
    "                    name=assay,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=True,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=assay_colors[assay],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in assay_accuracies.items()\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if \"sample_ontology\" in task_name:\n",
    "            yrange = [0.59, 1.001]\n",
    "        elif ASSAY in task_name:\n",
    "            yrange = [0.985, 1.001]\n",
    "        else:\n",
    "            yrange = [assay_acc_df.min(), 1.001]  # type: ignore\n",
    "\n",
    "        if y_range is not None:\n",
    "            yrange = y_range\n",
    "\n",
    "        fig.update_yaxes(range=yrange)\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"Neural network classification - {task_name} - Assay accuracy\",\n",
    "            yaxis_title=\"Accuracy\",\n",
    "            xaxis_title=\"Assay\",\n",
    "        )\n",
    "\n",
    "        # Save figure\n",
    "        this_name = name + f\"_{task_name}\"\n",
    "        fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "        fig.write_image(logdir / f\"{this_name}.png\")\n",
    "        fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\"disease\", \"sex_no-mixed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\" / \"per_assay\"\n",
    "# NN_performance_per_assay_across_categories(all_split_results, logdir, fig_name, exclude_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track type effect on NN performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = base_data_dir / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "assay_parent_dir = parent_dir / \"assay_epiclass_1l_3000n\" / \"11c\"\n",
    "ct_parent_dir = parent_dir / \"harmonized_sample_ontology_intermediate_1l_3000n\"\n",
    "\n",
    "assay_results = {\n",
    "    folder.name: split_results_handler.read_split_results(folder)\n",
    "    for folder in assay_parent_dir.iterdir()\n",
    "    if \"chip\" not in folder.name\n",
    "}\n",
    "ct_results = {\n",
    "    folder.name: split_results_handler.read_split_results(folder)\n",
    "    for folder in ct_parent_dir.iterdir()\n",
    "    if \"l1\" not in folder.name\n",
    "}\n",
    "\n",
    "_ = assay_results.pop(\"10fold-oversampling\")\n",
    "_ = ct_results.pop(\"10fold-oversampling\")\n",
    "_ = ct_results.pop(\"10fold-oversampling_chip-seq-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_assay_results = copy.deepcopy(assay_results)\n",
    "for task_name, split_dfs in list(corrected_assay_results.items()):\n",
    "    for split_name in split_dfs:\n",
    "        split_dfs[split_name] = merge_similar_assays(split_dfs[split_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_metrics = split_results_handler.compute_split_metrics(\n",
    "    corrected_assay_results, concat_first_level=True\n",
    ")\n",
    "ct_metrics = split_results_handler.compute_split_metrics(\n",
    "    ct_results, concat_first_level=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\"\n",
    "name = f\"{ASSAY}_global_track_type_effect\"\n",
    "# fig2_a(\n",
    "#     assay_metrics,\n",
    "#     logdir,\n",
    "#     name,\n",
    "#     exclude_categories=[],\n",
    "#     y_range=[0.99, 1.0001],\n",
    "#     sort_by_acc=False,\n",
    "# )\n",
    "\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\"\n",
    "name = f\"{CELL_TYPE}_global_track_type_effect\"\n",
    "# fig2_a(\n",
    "#     ct_metrics,\n",
    "#     logdir,\n",
    "#     name,\n",
    "#     exclude_categories=[],\n",
    "#     y_range=[0.91, 1.001],\n",
    "#     sort_by_acc=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"{ASSAY}_global_track_type_effect_per_assay\"\n",
    "# NN_performance_per_assay_across_categories(\n",
    "#     corrected_assay_results, logdir, name, exclude_categories=[], y_range=[0.96, 1.001]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_keep_core_assays(\n",
    "    results_dfs: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Exclude non core-assays from split results. Also exclude input.\"\"\"\n",
    "    accepted_assays = ASSAY_ORDER[0:-3]\n",
    "    new_results = copy.deepcopy(results_dfs)\n",
    "    for task_name, split_dfs in list(new_results.items()):\n",
    "        for split_name in split_dfs:\n",
    "            df = split_dfs[split_name]\n",
    "            if ASSAY not in df.columns:\n",
    "                merged_df = df.merge(\n",
    "                    metadata_v2_df, how=\"left\", left_index=True, right_index=True\n",
    "                )\n",
    "                df = df[merged_df[ASSAY].isin(accepted_assays)]\n",
    "                new_results[task_name][split_name] = df\n",
    "    return new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recompute metrics considering only histones\n",
    "# for result_df, category_name, y_range in zip(\n",
    "#     [corrected_assay_results, ct_results],\n",
    "#     [ASSAY, CELL_TYPE],\n",
    "#     [[0.85, 1.001], [0.91, 1.001]],\n",
    "# ):\n",
    "#     print(category_name)\n",
    "#     name = f\"{category_name}_core6c_track_type_effect\"\n",
    "\n",
    "#     core_result_df = only_keep_core_assays(result_df)\n",
    "#     metrics = split_results_handler.compute_split_metrics(\n",
    "#         core_result_df, concat_first_level=True\n",
    "#     )\n",
    "\n",
    "#     fig2_a(metrics, logdir, name, exclude_categories=[], y_range=y_range)\n",
    "\n",
    "#     if category_name == ASSAY:\n",
    "#         name = f\"{ASSAY}_core6_track_type_effect_per_assay\"\n",
    "#         NN_performance_per_assay_across_categories(\n",
    "#             core_result_df, logdir, name, exclude_categories=[], y_range=[0.97, 1.001]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex chrY z-score distribution vs predictions\n",
    "\n",
    "Violin plot of average z-score on chrY per sex, black dots for pred same class and red for pred different class.  \n",
    "\n",
    "- Do the split male female violin per assay (only FC, merge 2xwgbs and 2xrna, no rna unique_raw). \n",
    "- Use scatter for points on each side, agree same color as violin, disagree other.\n",
    "- Point labels: uuid, epirr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute chrY coverage z-score VS assay distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chrY_zscores(version: str):\n",
    "    \"\"\"Compute z-scores for chrY coverage data, per assay distribution.\n",
    "\n",
    "    Excludes raw, pval, and Unique_raw tracks.\n",
    "    \"\"\"\n",
    "    # Get chrY coverage data\n",
    "    chrY_coverage_dir = base_data_dir / \"chrY_coverage\"\n",
    "    if not chrY_coverage_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {chrY_coverage_dir} does not exist.\")\n",
    "    chrY_coverage_df = pd.read_csv(chrY_coverage_dir / \"chrXY_coverage_all.csv\", header=0)\n",
    "\n",
    "    # Filter out md5s not in metadata version\n",
    "    metadata = MetadataHandler(paper_dir).load_metadata(version)\n",
    "    md5s = set(metadata.md5s)\n",
    "    chrY_coverage_df = chrY_coverage_df[chrY_coverage_df[\"filename\"].isin(md5s)]\n",
    "\n",
    "    # Make sure all values are non-zero\n",
    "    assert (chrY_coverage_df[\"chrY\"] != 0).all()\n",
    "\n",
    "    # These tracks are excluded from z-score computation\n",
    "    metadata.remove_category_subsets(\"track_type\", [\"raw\", \"pval\", \"Unique_raw\"])\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n",
    "    metadata_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # Merge with metadata\n",
    "    chrY_coverage_df = chrY_coverage_df.merge(\n",
    "        metadata_df[[\"md5sum\", ASSAY]], left_on=\"filename\", right_on=\"md5sum\"\n",
    "    )\n",
    "\n",
    "    # Compute z-score per assay\n",
    "    chrY_dists = chrY_coverage_df.groupby(ASSAY).agg({\"chrY\": [\"mean\", \"std\", \"count\"]})\n",
    "\n",
    "    output_dir = chrY_coverage_dir / f\"dfreeze_{version}_stats\"\n",
    "    output_dir.mkdir(parents=False, exist_ok=True)\n",
    "    chrY_dists.to_csv(output_dir / \"chrY_coverage_stats.csv\")\n",
    "\n",
    "    # Compute z-score per assay group, merge back into the dataframe, save results\n",
    "    metric_name = \"chrY_zscore_vs_assay\"\n",
    "    groupby_df = chrY_coverage_df.groupby(ASSAY)\n",
    "    for _, group in groupby_df:\n",
    "        group[\"chrY_zscore\"] = zscore(group[\"chrY\"])\n",
    "        chrY_coverage_df.loc[group.index, metric_name] = group[\"chrY_zscore\"]\n",
    "\n",
    "    output_cols = [\"filename\", \"chrY\", metric_name, ASSAY]\n",
    "    chrY_coverage_df[output_cols].to_csv(\n",
    "        output_dir / \"chrY_coverage_zscore_vs_assay.csv\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chrY_zscores(\"v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot z-scores according to sex\n",
    "\n",
    "main Fig: chrY per EpiRR (excluding WGBS): only boxplot with all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_label = \"chrY_zscore_vs_assay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fig_2B_data(version: str, prediction_data_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Prepare data for figure 2b.\"\"\"\n",
    "    # Load metadata\n",
    "    meta_cols = [\"md5sum\", \"EpiRR\", SEX]\n",
    "    metadata = MetadataHandler(paper_dir).load_metadata(version)\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n",
    "    metadata_df = metadata_df[meta_cols]\n",
    "\n",
    "    # Load z-score data\n",
    "    zscore_dir = base_data_dir / \"chrY_coverage\" / f\"dfreeze_{version}_stats\"\n",
    "    zscore_df = pd.read_csv(zscore_dir / \"chrY_coverage_zscore_vs_assay.csv\", header=0)\n",
    "\n",
    "    # Load NN predictions\n",
    "    pred_df = pd.read_csv(\n",
    "        prediction_data_dir / \"full-10fold-validation_prediction.csv\",\n",
    "        header=0,\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    # Merge all\n",
    "    zscore_df = zscore_df.merge(metadata_df, left_on=\"filename\", right_on=\"md5sum\")\n",
    "    zscore_df = zscore_df.merge(pred_df, left_on=\"filename\", right_index=True)\n",
    "    zscore_df[\"Max pred\"] = zscore_df[[\"female\", \"male\", \"mixed\"]].max(axis=1)\n",
    "    zscore_df.set_index(\"md5sum\", inplace=True)\n",
    "    return zscore_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_B(zscore_df: pd.DataFrame, logdir: Path, name: str) -> None:\n",
    "    \"\"\"Create figure 2B.\n",
    "\n",
    "    Args:\n",
    "        zscore_df: The dataframe with z-score data.\n",
    "    \"\"\"\n",
    "    assay_sizes = zscore_df[ASSAY].value_counts()\n",
    "    assays = sorted(assay_sizes.index)\n",
    "\n",
    "    # x_title = \"Assay+Sex z-score distributions - Male/Female classification disagreement separate\"\n",
    "    x_title = \"Assay+Sex z-score distributions\"\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(assays),\n",
    "        shared_yaxes=True,\n",
    "        x_title=x_title,\n",
    "        y_title=\"z-score\",\n",
    "        horizontal_spacing=0.02,\n",
    "        subplot_titles=[\n",
    "            f\"{assay_label} ({assay_sizes[assay_label]})\" for assay_label in assays\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    for i, assay_label in enumerate(sorted(assays)):\n",
    "        sub_df = zscore_df[zscore_df[ASSAY] == assay_label]\n",
    "\n",
    "        y_values = sub_df[metric_label]\n",
    "        hovertext = [\n",
    "            f\"{epirr}: z-score={z_score:.3f}, pred={pred:.3f}\"\n",
    "            for epirr, pred, z_score in zip(\n",
    "                sub_df[\"EpiRR\"],\n",
    "                sub_df[\"Max pred\"],\n",
    "                sub_df[metric_label],\n",
    "            )\n",
    "        ]\n",
    "        hovertext = np.array(hovertext)\n",
    "\n",
    "        female_idx = np.argwhere((sub_df[\"True class\"] == \"female\").values).flatten()\n",
    "        male_idx = np.argwhere((sub_df[\"True class\"] == \"male\").values).flatten()\n",
    "\n",
    "        # predicted_as_female_idx = np.argwhere(\n",
    "        #     (\n",
    "        #         (sub_df[\"Predicted class\"] == \"female\") & (sub_df[\"True class\"] == \"male\")\n",
    "        #     ).values\n",
    "        # ).flatten()\n",
    "        # predicted_as_male_idx = np.argwhere(\n",
    "        #     (\n",
    "        #         (sub_df[\"Predicted class\"] == \"male\") & (sub_df[\"True class\"] == \"female\")\n",
    "        #     ).values\n",
    "        # ).flatten()\n",
    "\n",
    "        # fig.add_trace(\n",
    "        #     go.Violin(\n",
    "        #         name=\"\",\n",
    "        #         x0=i,\n",
    "        #         y=y_values[female_idx],\n",
    "        #         box_visible=True,\n",
    "        #         meanline_visible=True,\n",
    "        #         points=\"all\",\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[female_idx],\n",
    "        #         side=\"negative\",\n",
    "        #         line_color=sex_colors[\"male\"],\n",
    "        #         spanmode=\"hard\",\n",
    "        #         showlegend=False,\n",
    "        #         marker=dict(size=1),\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "        # fig.add_trace(\n",
    "        #     go.Violin(\n",
    "        #         name=\"\",\n",
    "        #         x0=i,\n",
    "        #         y=y_values[male_idx],\n",
    "        #         box_visible=True,\n",
    "        #         meanline_visible=True,\n",
    "        #         points=\"all\",\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[male_idx],\n",
    "        #         side=\"positive\",\n",
    "        #         line_color=sex_colors[\"male\"],\n",
    "        #         spanmode=\"hard\",\n",
    "        #         showlegend=False,\n",
    "        #         marker=dict(size=1),\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=assay_label,\n",
    "                y=y_values[female_idx],\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[female_idx],\n",
    "                marker=dict(\n",
    "                    size=2,\n",
    "                    color=sex_colors[\"female\"],\n",
    "                    line=dict(width=0.5, color=\"black\"),\n",
    "                ),\n",
    "                fillcolor=sex_colors[\"female\"],\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                showlegend=False,\n",
    "                legendgroup=\"Female\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=assay_label,\n",
    "                y=y_values[male_idx],\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[male_idx],\n",
    "                marker=dict(\n",
    "                    size=2, color=sex_colors[\"male\"], line=dict(width=0.5, color=\"black\")\n",
    "                ),\n",
    "                fillcolor=sex_colors[\"male\"],\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                showlegend=False,\n",
    "                legendgroup=\"Male\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "        # temp_y_values = y_values[predicted_as_female_idx]\n",
    "        # temp_size = 1 + 5 * sub_df[\"Max pred\"].values[predicted_as_female_idx]\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         name=\"\",\n",
    "        #         x=[i - 0.2] * len(temp_y_values),\n",
    "        #         y=temp_y_values,\n",
    "        #         mode=\"markers\",\n",
    "        #         marker=dict(color=sex_colors[\"female\"], size=temp_size),\n",
    "        #         showlegend=False,\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[predicted_as_female_idx],\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "        # temp_y_values = y_values[predicted_as_male_idx]\n",
    "        # temp_size = 1 + 5 * sub_df[\"Max pred\"].values[predicted_as_male_idx]\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         name=\"\",\n",
    "        #         x=[i - 0.25] * len(temp_y_values),\n",
    "        #         y=temp_y_values,\n",
    "        #         mode=\"markers\",\n",
    "        #         marker=dict(color=sex_colors[\"male\"], size=temp_size),\n",
    "        #         showlegend=False,\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[predicted_as_male_idx],\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "    # Add a dummy scatter plot for legend\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Female\",\n",
    "            marker=dict(color=sex_colors[\"female\"], size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Female\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Male\",\n",
    "            marker=dict(color=sex_colors[\"male\"], size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Male\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(range=[-1.5, 3])\n",
    "    title = \"z-score(mean chrY coverage per file) distribution per assay\"\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        width=3000,\n",
    "        height=1000,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_chrY_zscore\"\n",
    "logdir.mkdir(parents=False, exist_ok=True)\n",
    "name = \"fig2--sex_chrY_zscore_only_box\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v2\"\n",
    "pred_data_dir = (\n",
    "    base_data_dir\n",
    "    / f\"dfreeze_{version}\"\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / f\"{SEX}_1l_3000n\"\n",
    "    / \"w-mixed\"\n",
    "    / \"10fold-oversampling\"\n",
    ")\n",
    "if not pred_data_dir:\n",
    "    raise FileNotFoundError(f\"Directory {pred_data_dir} does not exist.\")\n",
    "# zscore_df = prepare_fig_2B_data(version, pred_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig2_B(zscore_df, logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot z-score according to sex, merge assays except wgbs (1 violin plot, 1 point = 1 epirr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_B_merged_assays(\n",
    "    zscore_df: pd.DataFrame,\n",
    "    sex_mislabels: Dict[str, str],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    min_pred: float | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Create figure 2B.\n",
    "\n",
    "    Args:\n",
    "        zscore_df (pd.DataFrame): The dataframe with z-score data.\n",
    "        sex_mislabels (Dict[str, str]): {EpiRR_no-v: corrected_sex_label}\n",
    "        logdir (Path): The directory path to save the output plots.\n",
    "        name (str): The base name for the output plot files.\n",
    "        min_pred (float|None): Minimum prediction value to include in the plot. Used on average EpiRR 'Max pred' values.\n",
    "    \"\"\"\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "    zscore_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # wgbs reverses male/female chrY tendency, so removed here\n",
    "    zscore_df = zscore_df[zscore_df[ASSAY] != \"wgbs\"]\n",
    "\n",
    "    # Average chrY z-score values\n",
    "    mean_chrY_values_df = zscore_df.groupby([\"EpiRR\", SEX]).agg(\n",
    "        {metric_label: \"mean\", \"Max pred\": \"mean\"}\n",
    "    )\n",
    "    mean_chrY_values_df.reset_index(inplace=True)\n",
    "    if not mean_chrY_values_df[\"EpiRR\"].is_unique:\n",
    "        raise ValueError(\"EpiRR is not unique.\")\n",
    "\n",
    "    # Filter out low prediction values\n",
    "    if min_pred is not None:\n",
    "        mean_chrY_values_df = mean_chrY_values_df[\n",
    "            mean_chrY_values_df[\"Max pred\"] > min_pred\n",
    "        ]\n",
    "\n",
    "    mean_chrY_values_df.reset_index(drop=True, inplace=True)\n",
    "    chrY_values = mean_chrY_values_df[metric_label]\n",
    "    female_idx = np.argwhere((mean_chrY_values_df[SEX] == \"female\").values).flatten()  # type: ignore\n",
    "    male_idx = np.argwhere((mean_chrY_values_df[SEX] == \"male\").values).flatten()  # type: ignore\n",
    "\n",
    "    # Mislabels\n",
    "    binary_mislabels = set(\n",
    "        epirr_no_v\n",
    "        for epirr_no_v, label in sex_mislabels.items()\n",
    "        if label in [\"male\", \"female\"]\n",
    "    )\n",
    "    epirr_no_v = mean_chrY_values_df[\"EpiRR\"].str.extract(pat=r\"(\\w+\\d+).\\d+\")[0]\n",
    "    mislabels_idx = np.argwhere(\n",
    "        epirr_no_v.isin(binary_mislabels).values  # type: ignore\n",
    "    ).flatten()\n",
    "\n",
    "    mislabel_color_dict = {\"female\": sex_colors[\"male\"], \"male\": sex_colors[\"female\"]}\n",
    "    mislabel_colors = [mislabel_color_dict[mean_chrY_values_df[SEX][i]] for i in mislabels_idx]  # type: ignore\n",
    "\n",
    "    # Hovertext\n",
    "    hovertext = [\n",
    "        f\"{epirr}: <z-score>={z_score:.3f}\"\n",
    "        for epirr, z_score in zip(\n",
    "            mean_chrY_values_df[\"EpiRR\"],\n",
    "            mean_chrY_values_df[metric_label],\n",
    "        )\n",
    "    ]\n",
    "    hovertext = np.array(hovertext)\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            name=\"Female\",\n",
    "            y=chrY_values[female_idx],\n",
    "            boxmean=True,\n",
    "            boxpoints=\"all\",\n",
    "            pointpos=0,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[female_idx],\n",
    "            marker=dict(size=1, color=\"black\"),\n",
    "            line=dict(width=1, color=\"black\"),\n",
    "            fillcolor=sex_colors[\"female\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            name=\"Male\",\n",
    "            y=chrY_values[male_idx],\n",
    "            boxmean=True,\n",
    "            boxpoints=\"all\",\n",
    "            pointpos=0,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[male_idx],\n",
    "            marker=dict(size=1, color=\"black\"),\n",
    "            line=dict(width=1, color=\"black\"),\n",
    "            fillcolor=sex_colors[\"male\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Mislabel\",\n",
    "            x=np.zeros(len(mislabels_idx)),\n",
    "            y=chrY_values[mislabels_idx],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=4, color=mislabel_colors, line=dict(width=1, color=\"black\")),\n",
    "            showlegend=False,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[mislabels_idx],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(range=[-1.5, 3])\n",
    "    title = \"z-score(mean chrY coverage per file) distribution - z-scores averaged over assays\"\n",
    "    if min_pred is not None:\n",
    "        title += f\"<br>avg_maxPred>{min_pred}\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(text=title, x=0.5),\n",
    "        xaxis_title=SEX,\n",
    "        yaxis_title=\"Average z-score\",\n",
    "        width=750,\n",
    "        height=750,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    this_name = f\"{name}_n{mean_chrY_values_df.shape[0]}\"\n",
    "    fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "    fig.write_image(logdir / f\"{this_name}.png\")\n",
    "    fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, epirr_mislabels = create_mislabel_corrector()\n",
    "sex_mislabels = epirr_mislabels[SEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pred = None\n",
    "name = \"fig2--sex_chrY_zscore_merged_assays\"\n",
    "if min_pred is not None:\n",
    "    name = f\"fig2--sex_chrY_zscore_merged_assays_avg_maxPred>{min_pred}\"\n",
    "\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_chrY_zscore\"\n",
    "# fig2_B_merged_assays(zscore_df, sex_mislabels, logdir, name, min_pred=min_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex: prediction score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_score_violin(\n",
    "    results_df: pd.DataFrame, logdir: Path, name: str, min_y: float | None = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with violin plots and associated scatter plots for each class.\n",
    "    Red scatter points, indicating a mismatch, appear on top and have a larger size.\n",
    "\n",
    "    Args:\n",
    "        results_df (pd.DataFrame): The DataFrame containing the neural network results, including metadata.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "        name (str): The name of the figure.\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Class ordering\n",
    "    class_labels = (\n",
    "        results_df[\"True class\"].unique().tolist()\n",
    "        + results_df[\"Predicted class\"].unique().tolist()\n",
    "    )\n",
    "    class_labels_sorted = sorted(set(class_labels))\n",
    "    class_index = {label: i for i, label in enumerate(class_labels_sorted)}\n",
    "\n",
    "    for label in class_labels_sorted:\n",
    "        df = results_df[results_df[\"True class\"] == label]\n",
    "\n",
    "        # Majority vote, mean prediction score\n",
    "        groupby_epirr = df.groupby([\"EpiRR\", \"Predicted class\"])[\"Max pred\"].aggregate(\n",
    "            [\"size\", \"mean\"]\n",
    "        )\n",
    "\n",
    "        groupby_epirr = groupby_epirr.reset_index().sort_values(\n",
    "            [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "        )\n",
    "        groupby_epirr = groupby_epirr.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "        assert groupby_epirr[\"EpiRR\"].is_unique\n",
    "\n",
    "        mean_pred = groupby_epirr[\"mean\"]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[class_index[label]] * len(mean_pred),\n",
    "                y=mean_pred,\n",
    "                name=label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=False,\n",
    "                fillcolor=sex_colors[label],\n",
    "                line_color=\"black\",\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Prepare data for scatter plots\n",
    "        match_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] == label\n",
    "        ]\n",
    "        mismatch_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] != label\n",
    "        ]\n",
    "\n",
    "        jitter_match = np.random.uniform(-0.01, 0.01, len(match_pred))\n",
    "\n",
    "        # Add scatter plots for matches in black\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[class_index[label]] * len(match_pred) + jitter_match,\n",
    "                y=match_pred,\n",
    "                mode=\"markers\",\n",
    "                name=label,\n",
    "                marker=dict(\n",
    "                    color=\"black\",\n",
    "                    size=2,  # Standard size for matches\n",
    "                ),\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.2f}\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] == label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "                legendgroup=\"match\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add scatter plots for mismatches in red, with larger size\n",
    "        mismatch_info = groupby_epirr[groupby_epirr[\"Predicted class\"] != label]\n",
    "        strong_mismatch = mismatch_info[mismatch_info[\"mean\"] > 0.9]\n",
    "        display(strong_mismatch)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[class_index[label]] * len(mismatch_pred),\n",
    "                y=mismatch_pred,\n",
    "                mode=\"markers\",\n",
    "                name=label,\n",
    "                marker=dict(\n",
    "                    color=\"red\",\n",
    "                    size=5,  # Larger size for mismatches\n",
    "                ),\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.3f} (n={row[1]['size']})\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] != label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "                legendgroup=\"mismatch\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - black points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Match\",\n",
    "            marker=dict(color=\"black\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"match\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - red points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Mismatch\",\n",
    "            marker=dict(color=\"red\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"mismatch\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tickvals = list(class_index.values())\n",
    "    ticktext = list(class_index.keys())\n",
    "    fig.update_xaxes(tickvals=tickvals, ticktext=ticktext)\n",
    "\n",
    "    if min_y is None:\n",
    "        min_y = min(results_df[\"Max pred\"])\n",
    "\n",
    "    fig.update_yaxes(range=[min_y, 1.001])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Prediction score distribution class\",\n",
    "        yaxis_title=\"Avg. prediction score (majority class)\",\n",
    "        xaxis_title=\"Expected class label\",\n",
    "        width=750,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            title_text=\"Legend\",\n",
    "            itemsizing=\"constant\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    # fig.write_html(logdir / f\"{name}.html\")\n",
    "    # fig.write_image(logdir / f\"{name}.svg\")\n",
    "    # fig.write_image(logdir / f\"{name}.png\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sex_df = concatenated_results[\"harmonized_donor_sex_w-mixed\"]\n",
    "# sex_df = split_results_handler.add_max_pred(sex_df)\n",
    "# sex_df = metadata_handler.join_metadata(sex_df, metadata_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"fig2--sex_pred_score_post_correction\"\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_pred_score\"\n",
    "# pred_score_violin(sex_df, logdir, name, min_y=0.485)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRIORITY\n",
    "\n",
    "- REDUCED FEATURES: Graph of performance metrics per feature reduction for assay and cellType (100-0.1%)\n",
    "--> naive version for now\n",
    "\n",
    "-  Blacklisted et Winsorization\n",
    "\n",
    "THEN\n",
    "\n",
    "- CELL TYPE: Graph of performance metrics on cell-types after removing some assays --> waiting for details\n",
    "\n",
    "- SEX: Distribution of pred scores, black dots for pred same class and red for pred different class\n",
    "\n",
    "- SEX: Confusion matrix with pred scores>0.9 to identify the potential mislabeled and complementation (including Unknows) --> waiting for details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced features sets NN metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_task_metrics(df: pd.DataFrame, category: str, output_dir: Path) -> None:\n",
    "    \"\"\"Graph the metrics of a task.\"\"\"\n",
    "    for metric in [\"val_Accuracy\", \"val_F1Score\"]:\n",
    "        label_order = [\n",
    "            \"all\",\n",
    "            \"global_tasks_union\",\n",
    "            \"random_n4510\",\n",
    "            \"global_tasks_intersection\",\n",
    "            \"random_n118\",\n",
    "        ]\n",
    "        fig = px.box(\n",
    "            df,\n",
    "            x=\"HDF5 filter\",\n",
    "            y=metric,\n",
    "            title=f\"{category}: {metric}\",\n",
    "            points=\"all\",\n",
    "            category_orders={\n",
    "                \"HDF5 filter\": label_order,\n",
    "                \"HDF5 Resolution\": [\"1.0kb\", \"10.0kb\", \"100.0kb\"],\n",
    "            },\n",
    "            color=\"HDF5 Resolution\",\n",
    "            color_discrete_sequence=px.colors.qualitative.Safe,\n",
    "            width=800,\n",
    "            height=800,\n",
    "        )\n",
    "        fig.update_traces(boxmean=True)\n",
    "        fig.write_html(output_dir / f\"{category}_{metric}.html\")\n",
    "        fig.write_image(output_dir / f\"{category}_{metric}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_dir = base_data_dir / \"dfreeze_v2\"\n",
    "all_metrics = {}\n",
    "input_sizes = defaultdict(set)\n",
    "for folder in gen_data_dir.iterdir():\n",
    "    feature_set = folder.name\n",
    "    split_results_metrics, _ = fig2_a_content(folder)\n",
    "    inverted_dict = split_results_handler.invert_metrics_dict(split_results_metrics)\n",
    "    all_metrics[feature_set] = inverted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify input size\n",
    "import re\n",
    "\n",
    "input_size_line = \"(1): Linear\"\n",
    "input_sizes = defaultdict(set)\n",
    "folders = list(parent_dir.iterdir())\n",
    "for folder in gen_data_dir.iterdir():\n",
    "    for stdout_file in folder.rglob(\"output_job*.o\"):\n",
    "        with open(stdout_file, \"r\", encoding=\"utf8\") as f:\n",
    "            line = [l.rstrip() for l in f if input_size_line in l]\n",
    "            if len(line) == 0:\n",
    "                # print(f\"Skipping {stdout_file.name}, no model description found.\")\n",
    "                continue\n",
    "            if len(line) > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Incorrect file reading, captured more than one line in {stdout_file.name}: {line}\"\n",
    "                )\n",
    "            input_size = int(re.match(pattern=r\".*in_features=(\\d+).*\", string=line[0]).group(1))  # type: ignore\n",
    "            input_sizes[folder.name].add(input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine input size of runs after epigeec_filter was fixed, according to run metadata\n",
    "# run_metadata = RUN_METADATA.copy(deep=True)\n",
    "# run_metadata = run_metadata[run_metadata[\"startTimeMillis\"] > 1706943404420]\n",
    "\n",
    "# run_metadata[\"feature_set\"] = run_metadata[\"Name\"].str.extract(pat=r\"(^hg38_1[0]{0,2}kb_.*_none).*$\")\n",
    "\n",
    "# input_sizes_count = run_metadata.groupby([\"feature_set\", \"input_size\"]).size()\n",
    "# accepted_input_sizes = {idx[0]:int(idx[1]) for idx in input_sizes_count.index}\n",
    "\n",
    "# assert len(input_sizes_count) == len(accepted_input_sizes)\n",
    "# accepted_input_sizes.update({\"hg38_100kb_all_none\": 30321})\n",
    "\n",
    "# for k,v in sorted(accepted_input_sizes.items()):\n",
    "#     print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_metrics[\"hg38_100kb_all_none\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg38_100kb_all_none_models = [\n",
    "    \"assay_epiclass_11c\",\n",
    "    \"harmonized_donor_life_stage\",\n",
    "    \"harmonized_donor_sex_w-mixed\",\n",
    "    \"harmonized_sample_cancer_high\",\n",
    "    \"harmonized_sample_ontology_intermediate\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
