{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Workbook to create figures (fig2) destined for the paper.\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Workbook to create figures (fig2) destined for the paper.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Set, Tuple\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.core.metadata import Metadata\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    display_perc,\n",
    "    extract_experiment_keys_from_output_files,\n",
    "    extract_input_sizes_from_output_files,\n",
    "    merge_similar_assays,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map\n",
    "sex_colors = IHECColorMap.sex_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()\n",
    "metadata_handler = MetadataHandler(paper_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:172: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  metadata_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "metadata_v2 = metadata_handler.load_metadata(\"v2\")\n",
    "metadata_v2_df = metadata_handler.load_metadata_df(\"v2\", merge_assays=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_metadata_path = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"all_results_cometml_filtered_oversampling-fixed.csv\"\n",
    ")\n",
    "RUN_METADATA = pd.read_csv(parameters_metadata_path, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig 2 - EpiClass results on EpiAtlas other metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN performance across metadata categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if oversampling is uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_oversampling(parent_dir: Path, verbose: bool = False):\n",
    "    \"\"\"Check for oversampling status in the results, using \"output_job*.o\" files.\n",
    "    Args:\n",
    "        parent_dir (Path): Parent directory of the results. (classifier type level, e.g. assay_epiclass_1l_3000n)\n",
    "\n",
    "    \"\"\"\n",
    "    # Identify experiments\n",
    "    exp_keys_dict = extract_experiment_keys_from_output_files(parent_dir)\n",
    "\n",
    "    # Filter metadata to only include experiments in the results\n",
    "    all_exp_keys = set()\n",
    "    for exp_keys in exp_keys_dict.values():\n",
    "        all_exp_keys.update(exp_keys)\n",
    "\n",
    "    df = RUN_METADATA[RUN_METADATA[\"experimentKey\"].isin(all_exp_keys)]\n",
    "    df[\"general_name\"] = df[\"Name\"].str.replace(r\"[_-]?split\\d+$\", \"\", regex=True)\n",
    "    # print(df[[\"general_name\"] + [f\"run_arg_{i}\" for i in range(5)]].value_counts())\n",
    "\n",
    "    # Check oversampling values, ignore nan\n",
    "    df_na = df[df[\"hparams/oversampling\"].isna()]\n",
    "    df = df[df[\"hparams/oversampling\"].notna()]\n",
    "    if not (df[\"hparams/oversampling\"] == \"true\").all():\n",
    "        err_df = df.groupby([\"general_name\", \"hparams/oversampling\"]).agg(\"size\")\n",
    "        print(\n",
    "            \"Not all experiments have oversampling:\\n%s\",\n",
    "            err_df,\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Checked {len(exp_keys_dict)} folders and found {len(df)} oversampling values.\"\n",
    "    )\n",
    "    if len(df_na) != 0:\n",
    "        print(\n",
    "            \"Could not read oversampling value of all visited experiments. Values missing in:\"\n",
    "        )\n",
    "        print(df_na[[\"general_name\"] + [f\"run_arg_{i}\" for i in range(5)]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_for_oversampling(base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_10kb_all_none\" / \"assay_epiclass_1l_3000n\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read/correct results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mislabel_corrector():\n",
    "    \"\"\"Obtain information necessary to correct sex and life_stage mislabels.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: {md5sum: EpiRR_no-v}\n",
    "        Dict[str, Dict[str, str]]: {label_category: {EpiRR_no-v: corrected_label}}\n",
    "    \"\"\"\n",
    "    epirr_no_v = \"EpiRR_no-v\"\n",
    "    # Associate epirrs to md5sums\n",
    "    metadata = MetadataHandler(paper_dir).load_metadata(\"v2\")\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n",
    "    md5sum_to_epirr = metadata_df.set_index(\"md5sum\")[epirr_no_v].to_dict()\n",
    "\n",
    "    # Load mislabels\n",
    "    epirr_to_corrections = {}\n",
    "    metadata_dir = base_data_dir / \"metadata\" / \"official\" / \"BadQual-mislabels\"\n",
    "\n",
    "    sex_mislabeled = pd.read_csv(metadata_dir / \"official_Sex_mislabeled.csv\")\n",
    "    epirr_to_corrections[SEX] = sex_mislabeled.set_index(epirr_no_v)[\n",
    "        \"EpiClass_pred_Sex\"\n",
    "    ].to_dict()\n",
    "\n",
    "    life_stage_mislabeled = pd.read_csv(\n",
    "        metadata_dir / \"official_Life_stage_mislabeled.csv\"\n",
    "    )\n",
    "    epirr_to_corrections[LIFE_STAGE] = life_stage_mislabeled.set_index(epirr_no_v)[\n",
    "        \"EpiClass_pred_Life_stage\"\n",
    "    ].to_dict()\n",
    "\n",
    "    return md5sum_to_epirr, epirr_to_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_split_metrics(\n",
    "    results_dir: Path,\n",
    "    merge_assays: bool,\n",
    "    exclude_categories: List[str] | None = None,\n",
    "    exclude_names: List[str] | None = None,\n",
    "    return_type: str = \"both\",\n",
    "    verbose: bool = False,\n",
    ") -> (\n",
    "    Dict[str, Dict[str, Dict[str, float]]]\n",
    "    | Dict[str, Dict[str, pd.DataFrame]]\n",
    "    | Tuple[Dict[str, Dict[str, Dict[str, float]]], Dict[str, Dict[str, pd.DataFrame]]]\n",
    "):\n",
    "    \"\"\"Create the content data for figure 2a. (get metrics for each task)\n",
    "\n",
    "    Currently only using oversampled runs.\n",
    "\n",
    "    Args:\n",
    "        results_dir (Path): Directory containing the results. Needs to be parent over category folders.\n",
    "        merge_assays (bool): Merge similar assays (rna-seq x2, wgbs x2)\n",
    "        exclude_categories (List[str]): Task categories to exclude (first level directory names).\n",
    "        exclude_names (List[str]): Names of folders to exclude (ex: 7c or no-mix).\n",
    "        return_type (str): Type of data to return ('metrics', 'split_results', 'both').\n",
    "        verbose (bool): Print additional information.\n",
    "\n",
    "    Returns:\n",
    "        Union[Dict[str, Dict[str, Dict[str, float]]],\n",
    "              Dict[str, Dict[str, pd.DataFrame]],\n",
    "              Tuple[Dict[str, Dict[str, Dict[str, float]]], Dict[str, Dict[str, pd.DataFrame]]]]\n",
    "              Depending on return_type, it returns:\n",
    "              - 'metrics': A metrics dictionary with the structure {split_name: {task_name: metrics_dict}}\n",
    "              - 'split_results': A split results dictionary with the structure {task_name: {split_name: split_results_df}}\n",
    "              - 'both': A tuple with both dictionaries described above\n",
    "    \"\"\"\n",
    "    if return_type not in [\"metrics\", \"split_results\", \"both\"]:\n",
    "        raise ValueError(\n",
    "            f\"Invalid return_type: {return_type}. Choose from 'metrics', 'split_results', or 'both'.\"\n",
    "        )\n",
    "\n",
    "    all_split_results = {}\n",
    "    split_results_handler = SplitResultsHandler()\n",
    "\n",
    "    md5sum_to_epirr, epirr_to_corrections = create_mislabel_corrector()\n",
    "\n",
    "    for parent, _, _ in os.walk(results_dir):\n",
    "        # Looking for oversampling only results\n",
    "        parent = Path(parent)\n",
    "        if parent.name != \"10fold-oversampling\":\n",
    "            continue\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Checking {parent}\")\n",
    "\n",
    "        # Get the category\n",
    "        relpath = parent.relative_to(results_dir)\n",
    "        category = relpath.parts[0].rstrip(\"_1l_3000n\")\n",
    "        if exclude_categories is not None:\n",
    "            if any(exclude_str in category for exclude_str in exclude_categories):\n",
    "                continue\n",
    "\n",
    "        # Get the rest of the name, ignore certain runs\n",
    "        rest_of_name = list(relpath.parts[1:])\n",
    "        rest_of_name.remove(\"10fold-oversampling\")\n",
    "\n",
    "        if len(rest_of_name) > 1:\n",
    "            raise ValueError(\n",
    "                f\"Too many parts in the name: {rest_of_name}. Path: {relpath}\"\n",
    "            )\n",
    "        if rest_of_name:\n",
    "            rest_of_name = rest_of_name[0]\n",
    "\n",
    "        if exclude_names is not None:\n",
    "            if any(name in rest_of_name for name in exclude_names):\n",
    "                if verbose:\n",
    "                    print(f\"Skipping {category} {rest_of_name}: in {exclude_names}\")\n",
    "                continue\n",
    "\n",
    "        full_task_name = category\n",
    "        if rest_of_name:\n",
    "            full_task_name += f\"_{rest_of_name}\"\n",
    "\n",
    "        # Get the split results\n",
    "        split_results = split_results_handler.read_split_results(parent)\n",
    "        if not split_results:\n",
    "            raise ValueError(f\"No split results found in {parent}\")\n",
    "\n",
    "        if \"sex\" in full_task_name or \"life_stage\" in full_task_name:\n",
    "            corrections = epirr_to_corrections[category]\n",
    "            for split_name in split_results:\n",
    "                split_result_df = split_results[split_name]\n",
    "                current_true_class = split_result_df[\"True class\"].to_dict()\n",
    "                new_true_class = {\n",
    "                    k: corrections.get(md5sum_to_epirr[k], v)\n",
    "                    for k, v in current_true_class.items()\n",
    "                }\n",
    "                split_result_df[\"True class\"] = new_true_class.values()\n",
    "\n",
    "                split_results[split_name] = split_result_df\n",
    "\n",
    "        if (\"assay\" in full_task_name) and merge_assays:\n",
    "            for split_name in split_results:\n",
    "                try:\n",
    "                    split_result_df = merge_similar_assays(split_results[split_name])\n",
    "                except ValueError as e:\n",
    "                    print(f\"Skipping {full_task_name} assay merging: {e}\")\n",
    "                    break\n",
    "                split_results[split_name] = split_result_df\n",
    "\n",
    "        all_split_results[full_task_name] = split_results\n",
    "\n",
    "    if return_type in [\"metrics\", \"both\"]:\n",
    "        try:\n",
    "            split_results_metrics = split_results_handler.compute_split_metrics(\n",
    "                all_split_results, concat_first_level=True\n",
    "            )\n",
    "        except KeyError as e:\n",
    "            logging.error(\"KeyError: %s\", e)\n",
    "            logging.error(\"all_split_results: %s\", all_split_results)\n",
    "            logging.error(\"check folder: %s\", results_dir)\n",
    "            raise e\n",
    "\n",
    "    if return_type == \"metrics\":\n",
    "        return split_results_metrics\n",
    "    if return_type == \"split_results\":\n",
    "        return all_split_results\n",
    "\n",
    "    # the default return type is 'both'\n",
    "    return split_results_metrics, all_split_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=dangerous-default-value\n",
    "def fig2_a(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    exclude_categories: List[str] | None = None,\n",
    "    y_range: List[float] | None = None,\n",
    "    sort_by_acc: bool = False,\n",
    "    metric_names: List[str] = [\"Accuracy\", \"F1_macro\"],\n",
    "    show_plot: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Render box plots of metrics per classifier and split, each in its own subplot.\n",
    "\n",
    "    This function generates a figure with subplots, each representing a different\n",
    "    metric. Each subplot contains box plots for each classifier, ordered by accuracy.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A nested dictionary with structure {split: {classifier: {metric: score}}}.\n",
    "        logdir: The directory path to save the output plots.\n",
    "        name: The base name for the output plot files.\n",
    "        exclude_categories: Task categories to exclude from the plot.\n",
    "        y_range: The y-axis range for the plots.\n",
    "        sort_by_acc: Whether to sort the classifiers by accuracy.\n",
    "        metrics: The metrics to include in the plot.\n",
    "    \"\"\"\n",
    "    # Exclude some categories\n",
    "    classifier_names = list(split_metrics[\"split0\"].keys())\n",
    "    if exclude_categories is not None:\n",
    "        for category in exclude_categories:\n",
    "            classifier_names = [c for c in classifier_names if category not in c]\n",
    "\n",
    "    available_metrics = list(split_metrics[\"split0\"][classifier_names[0]].keys())\n",
    "    if any(metric not in available_metrics for metric in metric_names):\n",
    "        raise ValueError(f\"Invalid metric. Metrics need to be in {available_metrics}\")\n",
    "\n",
    "    # Sort classifiers by accuracy\n",
    "    if sort_by_acc:\n",
    "        mean_acc = {}\n",
    "        for classifier in classifier_names:\n",
    "            mean_acc[classifier] = np.mean(\n",
    "                [split_metrics[split][classifier][\"Accuracy\"] for split in split_metrics]\n",
    "            )\n",
    "        classifier_names = sorted(\n",
    "            classifier_names, key=lambda x: mean_acc[x], reverse=True\n",
    "        )\n",
    "\n",
    "    # Create subplots, one column for each metric\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(metric_names),\n",
    "        subplot_titles=metric_names,\n",
    "        horizontal_spacing=0.03,\n",
    "    )\n",
    "\n",
    "    color_group = px.colors.qualitative.Plotly\n",
    "    colors = {\n",
    "        classifier: color_group[i % len(color_group)]\n",
    "        for i, classifier in enumerate(classifier_names)\n",
    "    }\n",
    "\n",
    "    # point_pos = -1.35\n",
    "    point_pos = 0\n",
    "    for i, metric in enumerate(metric_names):\n",
    "        for classifier_name in classifier_names:\n",
    "            values = [\n",
    "                split_metrics[split][classifier_name][metric] for split in split_metrics\n",
    "            ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=values,\n",
    "                    name=classifier_name,\n",
    "                    fillcolor=colors[classifier_name],\n",
    "                    line=dict(color=\"black\", width=1.5),\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    pointpos=point_pos,\n",
    "                    showlegend=i == 0,  # Only show legend in the first subplot\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in zip(split_metrics, values)\n",
    "                    ],\n",
    "                    legendgroup=classifier_name,\n",
    "                    width=0.5,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=i + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Neural network classification - Metric distribution for 10-fold cross-validation\",\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode=\"group\",\n",
    "        height=1200 * 0.8,\n",
    "        width=1750 * 0.8,\n",
    "    )\n",
    "\n",
    "    # Acc, F1\n",
    "    # range_acc = [0.86, 1.001]\n",
    "    # fig.update_layout(yaxis=dict(range=range_acc))\n",
    "    # fig.update_layout(yaxis2=dict(range=range_acc))\n",
    "    fig.update_layout(yaxis=dict(range=[0.88, 1.001]))\n",
    "    fig.update_layout(yaxis2=dict(range=[0.80, 1.001]))\n",
    "\n",
    "    # AUC\n",
    "    range_auc = [0.986, 1.0001]\n",
    "    fig.update_layout(yaxis3=dict(range=range_auc))\n",
    "    fig.update_layout(yaxis4=dict(range=range_auc))\n",
    "\n",
    "    if y_range is not None:\n",
    "        fig.update_yaxes(range=y_range)\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    if show_plot:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping assay_epiclass_harmonized_sample_ontology_intermediate_16ct assay merging: Wrong results dataframe, rna or wgbs columns missing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:105: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"True class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:105: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"True class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:105: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"True class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:105: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"True class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:105: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"True class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:105: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"True class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:105: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"True class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:105: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"True class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:105: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"True class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:105: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"True class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n",
      "/home/rabj2301/projects/sources/epi_ml/src/python/epi_ml/utils/notebooks/paper/paper_utilities.py:106: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Predicted class\"].replace(ASSAY_MERGE_DICT, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot compute ROC AUC. At least one ground truth class missing from split5 for assay_epiclass_harmonized_sample_ontology_intermediate_16ct: ({'h3k4me1_extraembryonic cell', 'h3k36me3_extraembryonic cell'})\n",
      "WARNING:root:Cannot compute ROC AUC. At least one ground truth class missing from split6 for assay_epiclass_harmonized_sample_ontology_intermediate_16ct: ({'h3k36me3_extraembryonic cell'})\n"
     ]
    }
   ],
   "source": [
    "# exclude_categories = [\"track_type\", \"groups\", \"disease\", \"no-mixed\"]\n",
    "exclude_categories = [\"track_type\"]\n",
    "exclude_names = [\"chip-seq\", \"7c\"]\n",
    "\n",
    "hdf5_type = \"hg38_100kb_all_none\"\n",
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "split_results_metrics, all_split_results = general_split_metrics(\n",
    "    results_dir,\n",
    "    merge_assays=True,\n",
    "    exclude_categories=exclude_categories,\n",
    "    exclude_names=exclude_names,\n",
    "    return_type=\"both\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\"\n",
    "fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "fig_name = f\"{hdf5_type}_perf_across_categories_full_internal_2024-07-22\"\n",
    "\n",
    "metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_micro\"]\n",
    "# fig2_a(\n",
    "#     split_results_metrics, # type: ignore\n",
    "#     fig_logdir,\n",
    "#     fig_name,\n",
    "#     sort_by_acc=True,\n",
    "#     metric_names=metrics,\n",
    "#     exclude_categories=None,\n",
    "#     show_plot=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\"\n",
    "fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "metrics_full = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "metrics_AUC = [\"AUC_micro\", \"AUC_macro\"]\n",
    "metrics_acc_F1 = [\"Accuracy\", \"F1_macro\"]\n",
    "exclude_categories = [\"sex_no-mixed\", \"disease\"]\n",
    "y_range_AUC = [0.986, 1.0001]\n",
    "y_range_acc = [0.86, 1.001]\n",
    "\n",
    "for name, metrics, y_range in zip(\n",
    "    [\"full\", \"acc_F1\", \"AUC\"],\n",
    "    [metrics_full, metrics_acc_F1, metrics_AUC],\n",
    "    [None, y_range_acc, y_range_AUC],\n",
    "):\n",
    "    fig_name = f\"{hdf5_type}_perf_across_categories_{name}\"\n",
    "    # fig2_a(\n",
    "    #     split_results_metrics,\n",
    "    #     fig_logdir,\n",
    "    #     fig_name,\n",
    "    #     sort_by_acc=True,\n",
    "    #     metric_names=metrics,\n",
    "    #     exclude_categories=exclude_categories,\n",
    "    #     show_plot=False,\n",
    "    #     y_range=y_range,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\"\n",
    "fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "metrics_acc_F1 = [\"Accuracy\", \"F1_macro\"]\n",
    "fig_name = f\"{hdf5_type}_perf_across_categories_acc_F1\"\n",
    "# fig2_a(\n",
    "#     split_results_metrics,\n",
    "#     fig_logdir,\n",
    "#     fig_name,\n",
    "#     sort_by_acc=True,\n",
    "#     metric_names=metrics_acc_F1,\n",
    "#     exclude_categories=exclude_categories,\n",
    "#     show_plot=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_imbalance(\n",
    "    all_split_results: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute class imbalance for each task and split.\n",
    "\n",
    "    Args:\n",
    "        all_split_results: A dictionary with structure {task_name: {split_name: split_results_df}}.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the following columns:\n",
    "            - avg(balance_ratio): The average balance ratio for each task.\n",
    "            - n: The number of classes for each task (used for the average).\n",
    "    \"\"\"\n",
    "    # combine md5 lists\n",
    "    task_md5s = {\n",
    "        classifier_task: [split_df.index for split_df in split_results.values()]\n",
    "        for classifier_task, split_results in all_split_results.items()\n",
    "    }\n",
    "    task_md5s = {\n",
    "        classifier_task: [list(split_md5s) for split_md5s in md5s]\n",
    "        for classifier_task, md5s in task_md5s.items()\n",
    "    }\n",
    "    task_md5s = {\n",
    "        classifier_task: list(itertools.chain(*md5s))\n",
    "        for classifier_task, md5s in task_md5s.items()\n",
    "    }\n",
    "\n",
    "    # get metadata\n",
    "    metadata_df = metadata_handler.load_metadata_df(\"v2-encode\")\n",
    "\n",
    "    label_counts = {}\n",
    "    for classifier_task, md5s in task_md5s.items():\n",
    "        try:\n",
    "            label_counts[classifier_task] = metadata_df.loc[md5s][\n",
    "                classifier_task\n",
    "            ].value_counts()\n",
    "        except KeyError as e:\n",
    "            category_name = classifier_task.rsplit(\"_\", maxsplit=1)[0]\n",
    "            try:\n",
    "                label_counts[classifier_task] = metadata_df.loc[md5s][\n",
    "                    category_name\n",
    "                ].value_counts()\n",
    "            except KeyError as e:\n",
    "                raise e\n",
    "\n",
    "    # Compute average class ratio vs majority class\n",
    "    # class_ratios = {}\n",
    "    # for classifier_task, counts in label_counts.items():\n",
    "    #     class_ratios[classifier_task] = (np.mean(counts / max(counts)), len(counts))\n",
    "\n",
    "    # Compute Shannon Entropy\n",
    "    class_balance = {}\n",
    "    for classifier_task, counts in label_counts.items():\n",
    "        total_count = counts.sum()\n",
    "        k = len(counts)\n",
    "        p_x = counts / total_count  # class proportions\n",
    "        p_x = p_x.values\n",
    "        shannon_entropy = -np.sum(p_x * np.log2(p_x))\n",
    "        balance = shannon_entropy / np.log2(k)\n",
    "        class_balance[classifier_task] = (balance, k)\n",
    "\n",
    "    df_class_balance = pd.DataFrame.from_dict(\n",
    "        class_balance, orient=\"index\", columns=[\"Normalized Shannon Entropy\", \"k\"]\n",
    "    ).sort_index()\n",
    "\n",
    "    return df_class_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_type = \"hg38_100kb_all_none\"\n",
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "# all_split_results = general_split_metrics(\n",
    "#     results_dir,\n",
    "#     exclude_categories=None,\n",
    "#     exclude_names=None,\n",
    "#     merge_assays=True,\n",
    "#     return_type=\"split_results\",\n",
    "# )\n",
    "\n",
    "# fig_logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\"\n",
    "# df_class_balance = compute_class_imbalance(all_split_results)\n",
    "# df_class_balance.to_csv(fig_logdir / \"class_balance_Shannon.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN performance per assay across metadata categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_performance_per_assay_across_categories(\n",
    "    all_split_results: Dict[str, Dict[str, pd.DataFrame]],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    title_end: str = \"\",\n",
    "    exclude_categories: List[str] | None = None,\n",
    "    y_range: None | List[float] = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"Create a box plot of assay accuracy for each classifier.\"\"\"\n",
    "    all_split_results = copy.deepcopy(all_split_results)\n",
    "\n",
    "    # Exclude some categories\n",
    "    classifier_names = list(all_split_results.keys())\n",
    "    if exclude_categories is not None:\n",
    "        for category in exclude_categories:\n",
    "            classifier_names = [c for c in classifier_names if category not in c]\n",
    "\n",
    "    metadata_df = MetadataHandler(paper_dir).load_metadata_df(\"v2-encode\")\n",
    "\n",
    "    # One graph per metadata category\n",
    "    for task_name in classifier_names:\n",
    "        if verbose:\n",
    "            print(f\"Processing {task_name}\")\n",
    "        split_results = all_split_results[task_name]\n",
    "        if ASSAY in task_name:\n",
    "            for split_name in split_results:\n",
    "                try:\n",
    "                    split_results[split_name] = merge_similar_assays(\n",
    "                        split_results[split_name]\n",
    "                    )\n",
    "                except ValueError as e:\n",
    "                    print(f\"Skipping {task_name} assay merging: {e}\")\n",
    "                    break\n",
    "\n",
    "        assay_acc_df = split_results_handler.compute_acc_per_assay(\n",
    "            split_results, metadata_df\n",
    "        )\n",
    "\n",
    "        fig = go.Figure()\n",
    "        for assay in ASSAY_ORDER:\n",
    "            try:\n",
    "                assay_accuracies = assay_acc_df[assay]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=assay_accuracies.values,\n",
    "                    name=assay,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=True,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=assay_colors[assay],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in assay_accuracies.items()\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # if \"sample_ontology\" in task_name:\n",
    "        #     yrange = [0.59, 1.001]\n",
    "        # elif ASSAY in task_name:\n",
    "        #     yrange = [0.985, 1.001]\n",
    "        # else:\n",
    "        yrange = [assay_acc_df.min(), 1.001]  # type: ignore\n",
    "\n",
    "        if y_range is not None:\n",
    "            yrange = y_range\n",
    "\n",
    "        fig.update_yaxes(range=yrange)\n",
    "\n",
    "        title_text = f\"NN classification - {task_name}\"\n",
    "        if title_end:\n",
    "            title_text += f\" - {title_end}\"\n",
    "        fig.update_layout(\n",
    "            title_text=title_text,\n",
    "            yaxis_title=\"Accuracy\",\n",
    "            xaxis_title=\"Assay\",\n",
    "            width=1000,\n",
    "            height=700,\n",
    "        )\n",
    "\n",
    "        # Save figure\n",
    "        this_name = name + f\"_{task_name}\"\n",
    "        fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "        fig.write_image(logdir / f\"{this_name}.png\")\n",
    "        fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\"track_type\", \"groups\", \"disease\"]\n",
    "exclude_names = [\"chip-seq\", \"7c\", \"no-mixed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 303114\n",
    "# # N = 30321\n",
    "# hdf5_type = f\"hg38_regulatory_regions_n{N}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hg38_100kb_random_n316_none\n",
      "hg38_100kb_random_n3044_none\n"
     ]
    }
   ],
   "source": [
    "hdf5_types = []\n",
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "# exclude_names = [\"random\", \"global\", \"cpg\", \"gene\", \"regulatory\", \"100kb\", \"10kb\"]\n",
    "exclude_names = [\n",
    "    \"global\",\n",
    "    \"cpg\",\n",
    "    \"gene\",\n",
    "    \"regulatory\",\n",
    "    \"coord\",\n",
    "    \"10kb\",\n",
    "    \"1kb\",\n",
    "    \"1mb\",\n",
    "    \"4510\",\n",
    "    \"118\",\n",
    "    \"all_none\",\n",
    "]\n",
    "for folder in results_dir.iterdir():\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    if any(label in folder.name for label in exclude_names):\n",
    "        continue\n",
    "\n",
    "    hdf5_types.append(folder.name)\n",
    "\n",
    "for hdf5_type in hdf5_types:\n",
    "    print(hdf5_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf5_type_1mb = \"hg38_1mb_all_none\"\n",
    "# hdf5_type_gene = \"hg38_gene_regions_100kb_coord_n19864\"\n",
    "# hdf5_type_cpg_300k = \"hg38_cpg_regions_10kb_coord_n300k\"\n",
    "# hdf5_type_cpg_30k = \"hg38_cpg_regions_10kb_coord_n30k\"\n",
    "# hdf5_types = [hdf5_type_1mb, hdf5_type_gene, hdf5_type_cpg_300k, hdf5_type_cpg_30k]\n",
    "# hdf5_types = [\"hg38_100kb_all_none\"]\n",
    "hdf5_types = [\"hg38_100kb_all_none\"]\n",
    "\n",
    "for hdf5_type in hdf5_types:\n",
    "    results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "    if not results_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "\n",
    "    # all_split_results = general_split_metrics(\n",
    "    #     results_dir,\n",
    "    #     merge_assays=True,\n",
    "    #     exclude_categories=exclude_categories\n",
    "    #     + [\"disease\", \"cancer\", \"life\", \"sex\", \"end\", \"bio\", \"project\"],\n",
    "    #     exclude_names=exclude_names + [\"11c\", \"chip\"],\n",
    "    #     return_type=\"split_results\",\n",
    "    #     # verbose=True,\n",
    "    # )\n",
    "\n",
    "    logdir = (\n",
    "        base_fig_dir\n",
    "        / \"fig2_EpiAtlas_other\"\n",
    "        / \"fig2--NN_perf_across_categories\"\n",
    "        / \"per_assay\"\n",
    "        / hdf5_type\n",
    "    )\n",
    "    logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "    # min_y = 0.65\n",
    "    # fig_name = f\"perf_per_assay_Y_{min_y:.2f}\"\n",
    "    # NN_performance_per_assay_across_categories(\n",
    "    #     all_split_results,  # type: ignore\n",
    "    #     logdir,\n",
    "    #     fig_name,\n",
    "    #     title_end=hdf5_type.replace(\"hg38_\", \"\"),\n",
    "    #     # exclude_categories=[CELL_TYPE],\n",
    "    #     y_range=[min_y, 1.001],\n",
    "    # )\n",
    "\n",
    "    # min_y = 0.3\n",
    "    # fig_name = f\"perf_per_assay_Y_{min_y:.2f}\"\n",
    "    # NN_performance_per_assay_across_categories(\n",
    "    #     all_split_results, # type: ignore\n",
    "    #     logdir,\n",
    "    #     fig_name,\n",
    "    #     title_end=hdf5_type.replace(\"hg38_\", \"\"),\n",
    "    #     exclude_categories=[ASSAY],\n",
    "    #     y_range=[min_y, 1.001],\n",
    "    # )\n",
    "\n",
    "    # min_y = 0.65\n",
    "    # fig_name = f\"perf_per_assay_Y_{min_y:.2f}\"\n",
    "    # NN_performance_per_assay_across_categories(\n",
    "    #     all_split_results, # type: ignore\n",
    "    #     logdir,\n",
    "    #     fig_name,\n",
    "    #     title_end=hdf5_type.replace(\"hg38_\", \"\"),\n",
    "    #     y_range=[min_y, 1.001],\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assay & CT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSAY_AND_CT = \"assay_epiclass_harmonized_sample_ontology_intermediate_16ct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_results = split_results_handler.concatenate_split_results(\n",
    "#     all_split_results, concat_first_level=True\n",
    "# )\n",
    "# concat_results = concat_results[ASSAY_AND_CT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(\n",
    "    df: pd.DataFrame, name1: str, name2: str, new_name: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge columns starting with \"name1_\" and \"name2_\" in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame\n",
    "    name1 (str): First prefix to match\n",
    "    name2 (str): Second prefix to match\n",
    "    new_name (str): Prefix for the new merged columns\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with merged columns\n",
    "    \"\"\"\n",
    "    # Get all column names\n",
    "    columns = df.columns\n",
    "\n",
    "    # Find columns starting with 'name1_' and 'name2_'\n",
    "    cols1 = [col for col in columns if col.startswith(f\"{name1}_\")]\n",
    "    cols2 = [col for col in columns if col.startswith(f\"{name2}_\")]\n",
    "\n",
    "    # Create a dictionary to store new column names and their corresponding columns to merge\n",
    "    merge_dict = {}\n",
    "\n",
    "    for col1 in cols1:\n",
    "        suffix = col1.split(f\"{name1}_\")[1]\n",
    "        col2 = f\"{name2}_{suffix}\"\n",
    "\n",
    "        if col2 in cols2:\n",
    "            new_col_name = f\"{new_name}_{suffix}\"\n",
    "            merge_dict[new_col_name] = [col1, col2]\n",
    "\n",
    "    # Merge columns\n",
    "    for new_col, cols_to_merge in merge_dict.items():\n",
    "        df[new_col] = df[cols_to_merge].sum(axis=1)\n",
    "\n",
    "    # Drop original columns\n",
    "    df = df.drop(columns=[col for sublist in merge_dict.values() for col in sublist])\n",
    "\n",
    "    # Adjust values in other columns, change name1 and name2 for new_name\n",
    "    for col in [\"True class\", \"Predicted class\"]:\n",
    "        df[col] = df[col].apply(lambda x: x.replace(name1, new_name))\n",
    "        df[col] = df[col].apply(lambda x: x.replace(name2, new_name))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assay_ct_results = merge_columns(concat_results, \"wgbs-standard\", \"wgbs-pbat\", \"wgbs\")\n",
    "# assay_ct_results = merge_columns(assay_ct_results, \"rna_seq\", \"mrna_seq\", \"rna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assay_ct_results[\"Max pred\"] = assay_ct_results[assay_ct_results.columns[2:]].max(axis=1)\n",
    "# classes = sorted(set(assay_ct_results[\"True class\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_pred in [0, 0.6, 0.8]:\n",
    "    # df = assay_ct_results[assay_ct_results[\"Max pred\"] > min_pred]\n",
    "    # conf_matrix = sk_cm(\n",
    "    #     df[\"True class\"], df[\"Predicted class\"], normalize=None, labels=classes\n",
    "    # )\n",
    "    # mat_writer = ConfusionMatrixWriter(labels=classes, confusion_matrix=conf_matrix)\n",
    "    # mat_writer.to_all_formats(\n",
    "    #     logdir, name=f\"{ASSAY_AND_CT}_confusion_matrix_min_pred_{min_pred:.2f}\"\n",
    "    # )\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to create a comprehensive confusion matrix report\n",
    "\n",
    "# y_true = assay_ct_results[\"True class\"].values\n",
    "# y_pred = assay_ct_results[\"Predicted class\"].values\n",
    "# classes = sorted(set(y_true) | set(y_pred))\n",
    "# from pycm import ConfusionMatrix\n",
    "# cm = ConfusionMatrix(y_true, y_pred, classes=list(classes))\n",
    "# output_name = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\" / f\"{ASSAY_AND_CT}_confusion_matrix\"\n",
    "# cm.save_html(str(output_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN performance per assay, scatterplot\n",
    "\n",
    "model_X split_n vs model_Y split_n for all n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_performance_scatterplot(\n",
    "    all_split_results: Dict[str, Dict[str, pd.DataFrame]],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    label_category: str,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    For the two given classification tasks split results (need to be from same category),\n",
    "    create a scatter plot of split performance per assay, split for split.\n",
    "\n",
    "    Args:\n",
    "        all_split_results: A dictionary with structure {task_name: {split_name: split_results_df}}.\n",
    "        logdir (Path): The directory path to save the output plots.\n",
    "        name (str): The base name for the output plot files.\n",
    "        label_category (str): category used for labels, used for title and axis labels.\n",
    "        verbose (bool): Print more information.\n",
    "    \"\"\"\n",
    "    all_split_results = copy.deepcopy(all_split_results)\n",
    "    metadata_df = MetadataHandler(paper_dir).load_metadata_df(\n",
    "        \"v2-encode\", merge_assays=True\n",
    "    )\n",
    "\n",
    "    for task_name_1, task_name_2 in itertools.combinations(all_split_results.keys(), 2):\n",
    "        if verbose:\n",
    "            print(task_name_1, task_name_2)\n",
    "        split_results_1 = all_split_results[task_name_1]\n",
    "        split_results_2 = all_split_results[task_name_2]\n",
    "\n",
    "        if ASSAY in task_name_1:\n",
    "            for split_name in split_results_1:\n",
    "                split_results_1[split_name] = merge_similar_assays(\n",
    "                    split_results_1[split_name]\n",
    "                )\n",
    "                split_results_2[split_name] = merge_similar_assays(\n",
    "                    split_results_2[split_name]\n",
    "                )\n",
    "\n",
    "        if split_results_1[\"split0\"].shape != split_results_2[\"split0\"].shape:\n",
    "            raise ValueError(\n",
    "                f\"Split results for {task_name_1} and {task_name_2} do not have the same shape: {split_results_1['split0'].shape} != {split_results_2['split0'].shape}\"\n",
    "            )\n",
    "        assay_acc_df_1 = split_results_handler.compute_acc_per_assay(\n",
    "            split_results_1, metadata_df\n",
    "        )\n",
    "        assay_acc_df_2 = split_results_handler.compute_acc_per_assay(\n",
    "            split_results_2, metadata_df\n",
    "        )\n",
    "\n",
    "        fig = go.Figure()\n",
    "        min_x = 1\n",
    "        min_y = 1\n",
    "        for assay in ASSAY_ORDER:\n",
    "            if verbose:\n",
    "                print(assay)\n",
    "            try:\n",
    "                assay_accuracies_1 = assay_acc_df_1[assay]\n",
    "                assay_accuracies_2 = assay_acc_df_2[assay]\n",
    "            except KeyError as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{task_name_1}: {assay_accuracies_1}\")\n",
    "                print(f\"{task_name_2}: {assay_accuracies_2}\")\n",
    "\n",
    "            hovertext = [\n",
    "                f\"{split}: ({assay_accuracies_1[split]:.4f},{assay_accuracies_2[split]:.4f})\"\n",
    "                for split in assay_accuracies_1.keys()\n",
    "            ]\n",
    "\n",
    "            x_gt_y = sum(assay_accuracies_1 > assay_accuracies_2)\n",
    "            y_gt_x = sum(assay_accuracies_1 < assay_accuracies_2)\n",
    "            trace_name = f\"{assay} ({y_gt_x},{x_gt_y})\"\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=assay_accuracies_1.values,\n",
    "                    y=assay_accuracies_2.values,\n",
    "                    mode=\"markers\",\n",
    "                    name=trace_name,\n",
    "                    marker=dict(size=5, color=assay_colors[assay]),\n",
    "                    text=hovertext,\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "            min_x = min(min_x, *assay_accuracies_1.values)\n",
    "            min_y = min(min_y, *assay_accuracies_2.values)\n",
    "\n",
    "        # diagonal line\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[0, 1],\n",
    "                y=[0, 1],\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        range_x = 1 - min_x\n",
    "        range_y = 1 - min_y\n",
    "        fig.update_xaxes(range=[min_x - 0.01 * range_x, 1 + 0.01 * range_x])\n",
    "        fig.update_yaxes(range=[min_y - 0.01 * range_y, 1 + 0.01 * range_y])\n",
    "\n",
    "        x_name = task_name_1.replace(f\"_{label_category}\", \"\")\n",
    "        y_name = task_name_2.replace(f\"_{label_category}\", \"\")\n",
    "        fig.update_layout(\n",
    "            title_text=f\"Neural network classification - {label_category} - 10-fold cross-validation\",\n",
    "            xaxis_title=f\"{x_name} accuracy\",\n",
    "            yaxis_title=f\"{y_name} accuracy\",\n",
    "        )\n",
    "\n",
    "        fig.update_layout(legend_title_text=\"Assay: (y>x, x>y)\")\n",
    "\n",
    "        # Save figure\n",
    "        this_name = f\"{name}-{label_category}-{x_name}_VS_{y_name}\"\n",
    "        this_name = this_name.replace(ASSAY, \"assay\")\n",
    "        this_name = this_name.replace(CELL_TYPE, \"sample_ontology\")\n",
    "        fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "        fig.write_image(logdir / f\"{this_name}.png\")\n",
    "        fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "        # fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\"track_type\", \"groups\", \"disease\"]\n",
    "exclude_names = [\"chip-seq\", \"7c\", \"no-mixed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 303114\n",
    "N2 = 30321\n",
    "hdf5_type_reg1 = f\"hg38_regulatory_regions_n{N1}\"\n",
    "hdf5_type_reg2 = f\"hg38_regulatory_regions_n{N2}\"\n",
    "hdf5_type_100kb = \"hg38_100kb_all_none\"\n",
    "hdf5_type_10kb = \"hg38_10kb_all_none\"\n",
    "hdf5_type_gene = \"hg38_gene_regions_100kb_coord_n19864\"\n",
    "hdf5_type_1mb = \"hg38_1mb_all_none\"\n",
    "hdf5_type_5mb = \"hg38_5mb_all_none_1mb_coord\"\n",
    "hdf5_type_10mb = \"hg38_10mb_all_none_1mb_coord\"\n",
    "hdf5_type_100kb_n316 = \"hg38_100kb_random_n316_none\"\n",
    "hdf5_type_100kb_n3044 = \"hg38_100kb_random_n3044_none\"\n",
    "\n",
    "scatter_fig_results = {}\n",
    "for hdf5_type in [\n",
    "    hdf5_type_reg1,\n",
    "    hdf5_type_reg2,\n",
    "    hdf5_type_100kb,\n",
    "    hdf5_type_10kb,\n",
    "    hdf5_type_gene,\n",
    "    hdf5_type_1mb,\n",
    "    hdf5_type_5mb,\n",
    "    hdf5_type_10mb,\n",
    "    hdf5_type_100kb_n316,\n",
    "    hdf5_type_100kb_n3044,\n",
    "]:\n",
    "    results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "    if not results_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "\n",
    "    # all_split_results = general_split_metrics(\n",
    "    #     results_dir,\n",
    "    #     merge_assays=True,\n",
    "    #     exclude_categories=exclude_categories,\n",
    "    #     exclude_names=exclude_names,\n",
    "    #     return_type=\"split_results\"\n",
    "    # )\n",
    "\n",
    "    # scatter_fig_results.update(\n",
    "    #     {\n",
    "    #         f\"{hdf5_type}_{task_name}\": split_results\n",
    "    #         for task_name, split_results in all_split_results.items() # type: ignore\n",
    "    #     }\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter_fig_results[\"hg38_regulatory_regions_n30321_harmonized_sample_ontology_intermediate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_category in [ASSAY, CELL_TYPE]:\n",
    "    # results = {k: v for k, v in scatter_fig_results.items() if label_category in k}\n",
    "    # pairwise_performance_scatterplot(\n",
    "    #     results,\n",
    "    #     logdir=base_fig_dir / \"flagship\" / \"pairwise_scatterplot_acc\" / label_category,\n",
    "    #     name=\"acc_per_assay\",\n",
    "    #     label_category=label_category,\n",
    "    #     verbose=False,\n",
    "    # )\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track type effect on NN performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "assay_parent_dir = parent_dir / \"assay_epiclass_1l_3000n\" / \"11c\"\n",
    "ct_parent_dir = parent_dir / \"harmonized_sample_ontology_intermediate_1l_3000n\"\n",
    "\n",
    "assay_results = {\n",
    "    folder.name: split_results_handler.read_split_results(folder)\n",
    "    for folder in assay_parent_dir.iterdir()\n",
    "    if \"chip\" not in folder.name\n",
    "}\n",
    "ct_results = {\n",
    "    folder.name: split_results_handler.read_split_results(folder)\n",
    "    for folder in ct_parent_dir.iterdir()\n",
    "    if \"l1\" not in folder.name\n",
    "}\n",
    "\n",
    "_ = assay_results.pop(\"10fold-oversampling\")\n",
    "_ = ct_results.pop(\"10fold-oversampling\")\n",
    "_ = ct_results.pop(\"10fold-oversampling_chip-seq-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected_assay_results = copy.deepcopy(assay_results)\n",
    "# for task_name, split_dfs in list(corrected_assay_results.items()):\n",
    "#     for split_name in split_dfs:\n",
    "#         split_dfs[split_name] = merge_similar_assays(split_dfs[split_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assay_metrics = split_results_handler.compute_split_metrics(\n",
    "#     corrected_assay_results, concat_first_level=True\n",
    "# )\n",
    "# ct_metrics = split_results_handler.compute_split_metrics(\n",
    "#     ct_results, concat_first_level=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\"\n",
    "name = f\"{ASSAY}_global_track_type_effect\"\n",
    "# fig2_a(\n",
    "#     assay_metrics,\n",
    "#     logdir,\n",
    "#     name,\n",
    "#     exclude_categories=None,\n",
    "#     y_range=[0.99, 1.0001],\n",
    "#     sort_by_acc=False,\n",
    "# )\n",
    "\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\"\n",
    "name = f\"{CELL_TYPE}_global_track_type_effect\"\n",
    "# fig2_a(\n",
    "#     ct_metrics,\n",
    "#     logdir,\n",
    "#     name,\n",
    "#     exclude_categories=None,\n",
    "#     y_range=[0.91, 1.001],\n",
    "#     sort_by_acc=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"{ASSAY}_global_track_type_effect_per_assay\"\n",
    "# NN_performance_per_assay_across_categories(\n",
    "#     corrected_assay_results, logdir, name, exclude_categories=None, y_range=[0.96, 1.001]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_keep_core_assays(\n",
    "    results_dfs: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Exclude non core-assays from split results. Also exclude input.\"\"\"\n",
    "    accepted_assays = ASSAY_ORDER[0:-3]\n",
    "    new_results = copy.deepcopy(results_dfs)\n",
    "    for task_name, split_dfs in list(new_results.items()):\n",
    "        for split_name in split_dfs:\n",
    "            df = split_dfs[split_name]\n",
    "            if ASSAY not in df.columns:\n",
    "                merged_df = df.merge(\n",
    "                    metadata_v2_df, how=\"left\", left_index=True, right_index=True\n",
    "                )\n",
    "                df = df[merged_df[ASSAY].isin(accepted_assays)]\n",
    "                new_results[task_name][split_name] = df\n",
    "    return new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recompute metrics considering only histones\n",
    "# for result_df, category_name, y_range in zip(\n",
    "#     [corrected_assay_results, ct_results],\n",
    "#     [ASSAY, CELL_TYPE],\n",
    "#     [[0.85, 1.001], [0.91, 1.001]],\n",
    "# ):\n",
    "#     print(category_name)\n",
    "#     name = f\"{category_name}_core6c_track_type_effect\"\n",
    "\n",
    "#     core_result_df = only_keep_core_assays(result_df)\n",
    "#     metrics = split_results_handler.compute_split_metrics(\n",
    "#         core_result_df, concat_first_level=True\n",
    "#     )\n",
    "\n",
    "#     fig2_a(metrics, logdir, name, exclude_categories=None, y_range=y_range)\n",
    "\n",
    "#     if category_name == ASSAY:\n",
    "#         name = f\"{ASSAY}_core6_track_type_effect_per_assay\"\n",
    "#         NN_performance_per_assay_across_categories(\n",
    "#             core_result_df, logdir, name, exclude_categories=None, y_range=[0.97, 1.001]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex chrY z-score distribution vs predictions\n",
    "\n",
    "Violin plot of average z-score on chrY per sex, black dots for pred same class and red for pred different class.  \n",
    "\n",
    "- Do the split male female violin per assay (only FC, merge 2xwgbs and 2xrna, no rna unique_raw). \n",
    "- Use scatter for points on each side, agree same color as violin, disagree other.\n",
    "- Point labels: uuid, epirr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute chrY coverage z-score VS assay distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chrY_zscores(version: str):\n",
    "    \"\"\"Compute z-scores for chrY coverage data, per assay distribution.\n",
    "\n",
    "    Excludes raw, pval, and Unique_raw tracks.\n",
    "    \"\"\"\n",
    "    # Get chrY coverage data\n",
    "    chrY_coverage_dir = base_data_dir / \"chrY_coverage\"\n",
    "    if not chrY_coverage_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {chrY_coverage_dir} does not exist.\")\n",
    "    chrY_coverage_df = pd.read_csv(chrY_coverage_dir / \"chrXY_coverage_all.csv\", header=0)\n",
    "\n",
    "    # Filter out md5s not in metadata version\n",
    "    metadata = MetadataHandler(paper_dir).load_metadata(version)\n",
    "    md5s = set(metadata.md5s)\n",
    "    chrY_coverage_df = chrY_coverage_df[chrY_coverage_df[\"filename\"].isin(md5s)]\n",
    "\n",
    "    # Make sure all values are non-zero\n",
    "    assert (chrY_coverage_df[\"chrY\"] != 0).all()\n",
    "\n",
    "    # These tracks are excluded from z-score computation\n",
    "    metadata.remove_category_subsets(\"track_type\", [\"raw\", \"pval\", \"Unique_raw\"])\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n",
    "    metadata_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # Merge with metadata\n",
    "    chrY_coverage_df = chrY_coverage_df.merge(\n",
    "        metadata_df[[\"md5sum\", ASSAY]], left_on=\"filename\", right_on=\"md5sum\"\n",
    "    )\n",
    "\n",
    "    # Compute z-score per assay\n",
    "    chrY_dists = chrY_coverage_df.groupby(ASSAY).agg({\"chrY\": [\"mean\", \"std\", \"count\"]})\n",
    "\n",
    "    output_dir = chrY_coverage_dir / f\"dfreeze_{version}_stats\"\n",
    "    output_dir.mkdir(parents=False, exist_ok=True)\n",
    "    chrY_dists.to_csv(output_dir / \"chrY_coverage_stats.csv\")\n",
    "\n",
    "    # Compute z-score per assay group, merge back into the dataframe, save results\n",
    "    metric_name = \"chrY_zscore_vs_assay\"\n",
    "    groupby_df = chrY_coverage_df.groupby(ASSAY)\n",
    "    for _, group in groupby_df:\n",
    "        group[\"chrY_zscore\"] = zscore(group[\"chrY\"])\n",
    "        chrY_coverage_df.loc[group.index, metric_name] = group[\"chrY_zscore\"]\n",
    "\n",
    "    output_cols = [\"filename\", \"chrY\", metric_name, ASSAY]\n",
    "    chrY_coverage_df[output_cols].to_csv(\n",
    "        output_dir / \"chrY_coverage_zscore_vs_assay.csv\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_chrY_zscores(\"v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot z-scores according to sex\n",
    "\n",
    "main Fig: chrY per EpiRR (excluding WGBS): only boxplot with all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_label = \"chrY_zscore_vs_assay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fig_2B_data(version: str, prediction_data_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Prepare data for figure 2b.\"\"\"\n",
    "    # Load metadata\n",
    "    meta_cols = [\"md5sum\", \"EpiRR\", SEX]\n",
    "    metadata = MetadataHandler(paper_dir).load_metadata(version)\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n",
    "    metadata_df = metadata_df[meta_cols]\n",
    "\n",
    "    # Load z-score data\n",
    "    zscore_dir = base_data_dir / \"chrY_coverage\" / f\"dfreeze_{version}_stats\"\n",
    "    zscore_df = pd.read_csv(zscore_dir / \"chrY_coverage_zscore_vs_assay.csv\", header=0)\n",
    "\n",
    "    # Load NN predictions\n",
    "    split_results = split_results_handler.read_split_results(prediction_data_dir)\n",
    "    pred_df = split_results_handler.concatenate_split_results(\n",
    "        {\"sex\": split_results}, concat_first_level=True\n",
    "    )[\"sex\"]\n",
    "\n",
    "    # Merge all\n",
    "    zscore_df = zscore_df.merge(metadata_df, left_on=\"filename\", right_on=\"md5sum\")\n",
    "    zscore_df = zscore_df.merge(pred_df, left_on=\"filename\", right_index=True)\n",
    "    zscore_df[\"Max pred\"] = zscore_df[[\"female\", \"male\", \"mixed\"]].max(axis=1)\n",
    "    zscore_df.set_index(\"md5sum\", inplace=True)\n",
    "    return zscore_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_B(zscore_df: pd.DataFrame, logdir: Path, name: str) -> None:\n",
    "    \"\"\"Create figure 2B.\n",
    "\n",
    "    Args:\n",
    "        zscore_df: The dataframe with z-score data.\n",
    "    \"\"\"\n",
    "    assay_sizes = zscore_df[ASSAY].value_counts()\n",
    "    assays = sorted(assay_sizes.index)\n",
    "\n",
    "    # x_title = \"Assay+Sex z-score distributions - Male/Female classification disagreement separate\"\n",
    "    x_title = \"Assay+Sex z-score distributions\"\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(assays),\n",
    "        shared_yaxes=True,\n",
    "        x_title=x_title,\n",
    "        y_title=\"z-score\",\n",
    "        horizontal_spacing=0.02,\n",
    "        subplot_titles=[\n",
    "            f\"{assay_label} ({assay_sizes[assay_label]})\" for assay_label in assays\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    for i, assay_label in enumerate(sorted(assays)):\n",
    "        sub_df = zscore_df[zscore_df[ASSAY] == assay_label]\n",
    "\n",
    "        y_values = sub_df[metric_label]\n",
    "        hovertext = [\n",
    "            f\"{epirr}: z-score={z_score:.3f}, pred={pred:.3f}\"\n",
    "            for epirr, pred, z_score in zip(\n",
    "                sub_df[\"EpiRR\"],\n",
    "                sub_df[\"Max pred\"],\n",
    "                sub_df[metric_label],\n",
    "            )\n",
    "        ]\n",
    "        hovertext = np.array(hovertext)\n",
    "\n",
    "        female_idx = np.argwhere((sub_df[\"True class\"] == \"female\").values).flatten()\n",
    "        male_idx = np.argwhere((sub_df[\"True class\"] == \"male\").values).flatten()\n",
    "\n",
    "        # predicted_as_female_idx = np.argwhere(\n",
    "        #     (\n",
    "        #         (sub_df[\"Predicted class\"] == \"female\") & (sub_df[\"True class\"] == \"male\")\n",
    "        #     ).values\n",
    "        # ).flatten()\n",
    "        # predicted_as_male_idx = np.argwhere(\n",
    "        #     (\n",
    "        #         (sub_df[\"Predicted class\"] == \"male\") & (sub_df[\"True class\"] == \"female\")\n",
    "        #     ).values\n",
    "        # ).flatten()\n",
    "\n",
    "        # fig.add_trace(\n",
    "        #     go.Violin(\n",
    "        #         name=\"\",\n",
    "        #         x0=i,\n",
    "        #         y=y_values[female_idx],\n",
    "        #         box_visible=True,\n",
    "        #         meanline_visible=True,\n",
    "        #         points=\"all\",\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[female_idx],\n",
    "        #         side=\"negative\",\n",
    "        #         line_color=sex_colors[\"male\"],\n",
    "        #         spanmode=\"hard\",\n",
    "        #         showlegend=False,\n",
    "        #         marker=dict(size=1),\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "        # fig.add_trace(\n",
    "        #     go.Violin(\n",
    "        #         name=\"\",\n",
    "        #         x0=i,\n",
    "        #         y=y_values[male_idx],\n",
    "        #         box_visible=True,\n",
    "        #         meanline_visible=True,\n",
    "        #         points=\"all\",\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[male_idx],\n",
    "        #         side=\"positive\",\n",
    "        #         line_color=sex_colors[\"male\"],\n",
    "        #         spanmode=\"hard\",\n",
    "        #         showlegend=False,\n",
    "        #         marker=dict(size=1),\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=assay_label,\n",
    "                y=y_values[female_idx],\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[female_idx],\n",
    "                marker=dict(\n",
    "                    size=2,\n",
    "                    color=sex_colors[\"female\"],\n",
    "                    line=dict(width=0.5, color=\"black\"),\n",
    "                ),\n",
    "                fillcolor=sex_colors[\"female\"],\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                showlegend=False,\n",
    "                legendgroup=\"Female\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=assay_label,\n",
    "                y=y_values[male_idx],\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[male_idx],\n",
    "                marker=dict(\n",
    "                    size=2, color=sex_colors[\"male\"], line=dict(width=0.5, color=\"black\")\n",
    "                ),\n",
    "                fillcolor=sex_colors[\"male\"],\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                showlegend=False,\n",
    "                legendgroup=\"Male\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "        # temp_y_values = y_values[predicted_as_female_idx]\n",
    "        # temp_size = 1 + 5 * sub_df[\"Max pred\"].values[predicted_as_female_idx]\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         name=\"\",\n",
    "        #         x=[i - 0.2] * len(temp_y_values),\n",
    "        #         y=temp_y_values,\n",
    "        #         mode=\"markers\",\n",
    "        #         marker=dict(color=sex_colors[\"female\"], size=temp_size),\n",
    "        #         showlegend=False,\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[predicted_as_female_idx],\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "        # temp_y_values = y_values[predicted_as_male_idx]\n",
    "        # temp_size = 1 + 5 * sub_df[\"Max pred\"].values[predicted_as_male_idx]\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         name=\"\",\n",
    "        #         x=[i - 0.25] * len(temp_y_values),\n",
    "        #         y=temp_y_values,\n",
    "        #         mode=\"markers\",\n",
    "        #         marker=dict(color=sex_colors[\"male\"], size=temp_size),\n",
    "        #         showlegend=False,\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[predicted_as_male_idx],\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "    # Add a dummy scatter plot for legend\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Female\",\n",
    "            marker=dict(color=sex_colors[\"female\"], size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Female\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Male\",\n",
    "            marker=dict(color=sex_colors[\"male\"], size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Male\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(range=[-1.5, 3])\n",
    "    title = \"z-score(mean chrY coverage per file) distribution per assay\"\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        width=3000,\n",
    "        height=1000,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_chrY_zscore\"\n",
    "logdir.mkdir(parents=False, exist_ok=True)\n",
    "name = \"fig2--sex_chrY_zscore_only_box\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v2\"\n",
    "pred_data_dir = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / f\"dfreeze_{version}\"\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / f\"{SEX}_1l_3000n\"\n",
    "    / \"w-mixed\"\n",
    "    / \"10fold-oversampling\"\n",
    ")\n",
    "if not pred_data_dir:\n",
    "    raise FileNotFoundError(f\"Directory {pred_data_dir} does not exist.\")\n",
    "zscore_df = prepare_fig_2B_data(version, pred_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig2_B(zscore_df, logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot z-score according to sex, merge assays except wgbs (1 violin plot, 1 point = 1 epirr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_B_merged_assays(\n",
    "    zscore_df: pd.DataFrame,\n",
    "    sex_mislabels: Dict[str, str],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    min_pred: float | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Create figure 2B.\n",
    "\n",
    "    Args:\n",
    "        zscore_df (pd.DataFrame): The dataframe with z-score data.\n",
    "        sex_mislabels (Dict[str, str]): {EpiRR_no-v: corrected_sex_label}\n",
    "        logdir (Path): The directory path to save the output plots.\n",
    "        name (str): The base name for the output plot files.\n",
    "        min_pred (float|None): Minimum prediction value to include in the plot. Used on average EpiRR 'Max pred' values.\n",
    "    \"\"\"\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "    zscore_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # wgbs reverses male/female chrY tendency, so removed here\n",
    "    zscore_df = zscore_df[zscore_df[ASSAY] != \"wgbs\"]\n",
    "\n",
    "    # Average chrY z-score values\n",
    "    mean_chrY_values_df = zscore_df.groupby([\"EpiRR\", SEX]).agg(\n",
    "        {metric_label: \"mean\", \"Max pred\": \"mean\"}\n",
    "    )\n",
    "    mean_chrY_values_df.reset_index(inplace=True)\n",
    "    if not mean_chrY_values_df[\"EpiRR\"].is_unique:\n",
    "        raise ValueError(\"EpiRR is not unique.\")\n",
    "\n",
    "    # Filter out low prediction values\n",
    "    if min_pred is not None:\n",
    "        mean_chrY_values_df = mean_chrY_values_df[\n",
    "            mean_chrY_values_df[\"Max pred\"] > min_pred\n",
    "        ]\n",
    "\n",
    "    mean_chrY_values_df.reset_index(drop=True, inplace=True)\n",
    "    chrY_values = mean_chrY_values_df[metric_label]\n",
    "    female_idx = np.argwhere((mean_chrY_values_df[SEX] == \"female\").values).flatten()  # type: ignore\n",
    "    male_idx = np.argwhere((mean_chrY_values_df[SEX] == \"male\").values).flatten()  # type: ignore\n",
    "\n",
    "    # Mislabels\n",
    "    binary_mislabels = set(\n",
    "        epirr_no_v\n",
    "        for epirr_no_v, label in sex_mislabels.items()\n",
    "        if label in [\"male\", \"female\"]\n",
    "    )\n",
    "    epirr_no_v = mean_chrY_values_df[\"EpiRR\"].str.extract(pat=r\"(\\w+\\d+).\\d+\")[0]\n",
    "    mislabels_idx = np.argwhere(\n",
    "        epirr_no_v.isin(binary_mislabels).values  # type: ignore\n",
    "    ).flatten()\n",
    "\n",
    "    mislabel_color_dict = {\"female\": sex_colors[\"male\"], \"male\": sex_colors[\"female\"]}\n",
    "    mislabel_colors = [mislabel_color_dict[mean_chrY_values_df[SEX][i]] for i in mislabels_idx]  # type: ignore\n",
    "\n",
    "    # Hovertext\n",
    "    hovertext = [\n",
    "        f\"{epirr}: <z-score>={z_score:.3f}\"\n",
    "        for epirr, z_score in zip(\n",
    "            mean_chrY_values_df[\"EpiRR\"],\n",
    "            mean_chrY_values_df[metric_label],\n",
    "        )\n",
    "    ]\n",
    "    hovertext = np.array(hovertext)\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            name=\"Female\",\n",
    "            y=chrY_values[female_idx],  # type: ignore\n",
    "            boxmean=True,\n",
    "            boxpoints=\"all\",\n",
    "            pointpos=0,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[female_idx],\n",
    "            marker=dict(size=1, color=\"black\"),\n",
    "            line=dict(width=1, color=\"black\"),\n",
    "            fillcolor=sex_colors[\"female\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            name=\"Male\",\n",
    "            y=chrY_values[male_idx],  # type: ignore\n",
    "            boxmean=True,\n",
    "            boxpoints=\"all\",\n",
    "            pointpos=0,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[male_idx],\n",
    "            marker=dict(size=1, color=\"black\"),\n",
    "            line=dict(width=1, color=\"black\"),\n",
    "            fillcolor=sex_colors[\"male\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Mislabel\",\n",
    "            x=np.zeros(len(mislabels_idx)),\n",
    "            y=chrY_values[mislabels_idx],  # type: ignore\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=4, color=mislabel_colors, line=dict(width=1, color=\"black\")),\n",
    "            showlegend=False,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[mislabels_idx],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(range=[-1.5, 3])\n",
    "    title = \"z-score(mean chrY coverage per file) distribution - z-scores averaged over assays\"\n",
    "    if min_pred is not None:\n",
    "        title += f\"<br>avg_maxPred>{min_pred}\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(text=title, x=0.5),\n",
    "        xaxis_title=SEX,\n",
    "        yaxis_title=\"Average z-score\",\n",
    "        width=750,\n",
    "        height=750,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    this_name = f\"{name}_n{mean_chrY_values_df.shape[0]}\"\n",
    "    fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "    fig.write_image(logdir / f\"{this_name}.png\")\n",
    "    fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, epirr_mislabels = create_mislabel_corrector()\n",
    "# sex_mislabels = epirr_mislabels[SEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pred = None\n",
    "name = \"fig2--sex_chrY_zscore_merged_assays\"\n",
    "if min_pred is not None:\n",
    "    name = f\"fig2--sex_chrY_zscore_merged_assays_avg_maxPred>{min_pred}\"\n",
    "\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_chrY_zscore\"\n",
    "# fig2_B_merged_assays(zscore_df, sex_mislabels, logdir, name, min_pred=min_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_assays_separation_distance(\n",
    "    zscore_df: pd.DataFrame, logdir: Path, name: str\n",
    ") -> None:\n",
    "    \"\"\"Complement to figure 2B, showing separation distance (mean, median)\n",
    "    between male/female zscore clusters.\n",
    "\n",
    "    Args:\n",
    "        zscore_df (pd.DataFrame): The dataframe with z-score data.\n",
    "        logdir (Path): The directory path to save the output plots.\n",
    "        name (str): The base name for the output plot files.\n",
    "    \"\"\"\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "    zscore_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # wgbs reverses male/female chrY tendency, so removed here\n",
    "    zscore_df = zscore_df[zscore_df[ASSAY] != \"wgbs\"]\n",
    "\n",
    "    # Average chrY z-score values\n",
    "    mean_chrY_values_df = zscore_df.groupby([\"EpiRR\", SEX]).agg(\n",
    "        {metric_label: \"mean\", \"Max pred\": \"mean\"}\n",
    "    )\n",
    "    mean_chrY_values_df.reset_index(inplace=True)\n",
    "    if not mean_chrY_values_df[\"EpiRR\"].is_unique:\n",
    "        raise ValueError(\"EpiRR is not unique.\")\n",
    "\n",
    "    mean_chrY_values_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    distances = {\"mean\": [], \"median\": []}\n",
    "    min_preds = np.arange(0, 1.0, 0.01)\n",
    "    sample_count = []\n",
    "    for min_pred in min_preds:\n",
    "        subset_chrY_values_df = mean_chrY_values_df[\n",
    "            mean_chrY_values_df[\"Max pred\"] > min_pred\n",
    "        ]\n",
    "        sample_count.append(subset_chrY_values_df.shape[0])\n",
    "\n",
    "        # Compute separation distances\n",
    "        chrY_vals_female = subset_chrY_values_df[subset_chrY_values_df[SEX] == \"female\"][\n",
    "            metric_label\n",
    "        ]\n",
    "        chrY_vals_male = subset_chrY_values_df[subset_chrY_values_df[SEX] == \"male\"][\n",
    "            metric_label\n",
    "        ]\n",
    "\n",
    "        if not chrY_vals_female.empty and not chrY_vals_male.empty:\n",
    "            mean_distance = np.abs(chrY_vals_female.mean() - chrY_vals_male.mean())\n",
    "            median_distance = np.abs(chrY_vals_female.median() - chrY_vals_male.median())\n",
    "\n",
    "            distances[\"mean\"].append(mean_distance)\n",
    "            distances[\"median\"].append(median_distance)\n",
    "        else:\n",
    "            distances[\"mean\"].append(np.nan)\n",
    "            distances[\"median\"].append(np.nan)\n",
    "\n",
    "    # Plotting the results\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for mean and median distances\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=min_preds,\n",
    "            y=distances[\"mean\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Mean Distance (left)\",\n",
    "            line=dict(color=\"blue\"),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=min_preds,\n",
    "            y=distances[\"median\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Median Distance (left)\",\n",
    "            line=dict(color=\"green\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add trace for number of files\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=min_preds,\n",
    "            y=sample_count,\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Number of Files (right)\",\n",
    "            line=dict(color=\"red\"),\n",
    "            yaxis=\"y2\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout for secondary y-axis\n",
    "    fig.update_layout(\n",
    "        title=\"Separation Distance of chrY z-scores male/female clusters\",\n",
    "        xaxis_title=\"Average Prediction Score minimum threshold\",\n",
    "        yaxis_title=\"Z-score Distance\",\n",
    "        yaxis2=dict(title=\"Number of Files\", overlaying=\"y\", side=\"right\"),\n",
    "        legend=dict(\n",
    "            x=1.08,\n",
    "        ),\n",
    "    )\n",
    "    # Save the plot\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"sex_chrY_zscore_merged_assays_distance\"\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_chrY_zscore\"\n",
    "# merged_assays_separation_distance(zscore_df, logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex: prediction score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_score_violin(\n",
    "    results_df: pd.DataFrame, logdir: Path, name: str, min_y: float | None = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with violin plots and associated scatter plots for each class.\n",
    "    Red scatter points, indicating a mismatch, appear on top and have a larger size.\n",
    "\n",
    "    Args:\n",
    "        results_df (pd.DataFrame): The DataFrame containing the neural network results, including metadata.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "        name (str): The name of the figure.\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Class ordering\n",
    "    class_labels = (\n",
    "        results_df[\"True class\"].unique().tolist()\n",
    "        + results_df[\"Predicted class\"].unique().tolist()\n",
    "    )\n",
    "    class_labels_sorted = sorted(set(class_labels))\n",
    "    class_index = {label: i for i, label in enumerate(class_labels_sorted)}\n",
    "\n",
    "    for label in class_labels_sorted:\n",
    "        df = results_df[results_df[\"True class\"] == label]\n",
    "\n",
    "        # Majority vote, mean prediction score\n",
    "        groupby_epirr = df.groupby([\"EpiRR\", \"Predicted class\"])[\"Max pred\"].aggregate(\n",
    "            [\"size\", \"mean\"]\n",
    "        )\n",
    "\n",
    "        groupby_epirr = groupby_epirr.reset_index().sort_values(\n",
    "            [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "        )\n",
    "        groupby_epirr = groupby_epirr.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "        assert groupby_epirr[\"EpiRR\"].is_unique\n",
    "\n",
    "        mean_pred = groupby_epirr[\"mean\"]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[class_index[label]] * len(mean_pred),\n",
    "                y=mean_pred,\n",
    "                name=label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=False,\n",
    "                fillcolor=sex_colors[label],\n",
    "                line_color=\"black\",\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Prepare data for scatter plots\n",
    "        match_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] == label\n",
    "        ]\n",
    "        mismatch_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] != label\n",
    "        ]\n",
    "\n",
    "        jitter_match = np.random.uniform(-0.01, 0.01, len(match_pred))\n",
    "\n",
    "        # Add scatter plots for matches in black\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[class_index[label]] * len(match_pred) + jitter_match,\n",
    "                y=match_pred,\n",
    "                mode=\"markers\",\n",
    "                name=label,\n",
    "                marker=dict(\n",
    "                    color=\"black\",\n",
    "                    size=2,  # Standard size for matches\n",
    "                ),\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.2f}\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] == label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "                legendgroup=\"match\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add scatter plots for mismatches in red, with larger size\n",
    "        mismatch_info = groupby_epirr[groupby_epirr[\"Predicted class\"] != label]\n",
    "        strong_mismatch = mismatch_info[mismatch_info[\"mean\"] > 0.9]\n",
    "        display(strong_mismatch)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[class_index[label]] * len(mismatch_pred),\n",
    "                y=mismatch_pred,\n",
    "                mode=\"markers\",\n",
    "                name=label,\n",
    "                marker=dict(\n",
    "                    color=\"red\",\n",
    "                    size=5,  # Larger size for mismatches\n",
    "                ),\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.3f} (n={row[1]['size']})\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] != label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "                legendgroup=\"mismatch\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - black points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Match\",\n",
    "            marker=dict(color=\"black\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"match\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - red points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Mismatch\",\n",
    "            marker=dict(color=\"red\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"mismatch\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tickvals = list(class_index.values())\n",
    "    ticktext = list(class_index.keys())\n",
    "    fig.update_xaxes(tickvals=tickvals, ticktext=ticktext)\n",
    "\n",
    "    if min_y is None:\n",
    "        min_y = min(results_df[\"Max pred\"])\n",
    "\n",
    "    fig.update_yaxes(range=[min_y, 1.001])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Prediction score distribution class\",\n",
    "        yaxis_title=\"Avg. prediction score (majority class)\",\n",
    "        xaxis_title=\"Expected class label\",\n",
    "        width=750,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            title_text=\"Legend\",\n",
    "            itemsizing=\"constant\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    # fig.write_html(logdir / f\"{name}.html\")\n",
    "    # fig.write_image(logdir / f\"{name}.svg\")\n",
    "    # fig.write_image(logdir / f\"{name}.png\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_df = zscore_df\n",
    "sex_df = split_results_handler.add_max_pred(sex_df)\n",
    "sex_df = metadata_handler.join_metadata(sex_df, metadata_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"fig2--sex_pred_score_post_correction\"\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_pred_score\"\n",
    "# pred_score_violin(sex_df, logdir, name, use_aggregate_vote=False, min_y=0.485)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample ontology prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_score_violin_alt(\n",
    "    results_df: pd.DataFrame,\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    min_y: float | None = None,\n",
    "    use_aggregate_vote: bool = True,\n",
    "    group_by_column: str = \"True class\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with violin plots and associated scatter plots for each group.\n",
    "    Args:\n",
    "        results_df (pd.DataFrame): The DataFrame containing the neural network results, including metadata.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "        name (str): The name of the figure.\n",
    "        min_y (float | None): Minimum y-axis value. If None, calculated from the data.\n",
    "        use_aggregate_vote (bool): If True, use EpiRR for aggregation. If False, use md5sum without aggregation.\n",
    "        group_by_column (str): The column name to use for grouping traces. Defaults to \"True class\".\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Group ordering\n",
    "    group_labels = results_df[group_by_column].unique().tolist()\n",
    "    group_labels_sorted = sorted(set(group_labels))\n",
    "    group_index = {label: i for i, label in enumerate(group_labels_sorted)}\n",
    "\n",
    "    for label in group_labels_sorted:\n",
    "        df = results_df[results_df[group_by_column] == label]\n",
    "\n",
    "        if use_aggregate_vote:\n",
    "            # Majority vote, mean prediction score\n",
    "            groupby = df.groupby([\"EpiRR\", \"Predicted class\"])[\"Max pred\"].aggregate(\n",
    "                [\"size\", \"mean\"]\n",
    "            )\n",
    "            groupby = groupby.reset_index().sort_values(\n",
    "                [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "            )\n",
    "            groupby = groupby.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "            assert groupby[\"EpiRR\"].is_unique\n",
    "            mean_pred = groupby[\"mean\"]\n",
    "            hover_text = [\n",
    "                f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.2f}, n={row[1]['size']}\"\n",
    "                for row in groupby.iterrows()\n",
    "            ]\n",
    "        else:\n",
    "            # Use md5sum without aggregation\n",
    "            mean_pred = df[\"Max pred\"]\n",
    "            hover_text = [\n",
    "                f\"md5sum: {row['md5sum']}, Pred class: {row['Predicted class']}, Pred: {row['Max pred']:.2f}\"\n",
    "                for _, row in df.iterrows()\n",
    "            ]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[group_index[label]] * len(mean_pred),\n",
    "                y=mean_pred,\n",
    "                name=label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=False,\n",
    "                points=\"all\",\n",
    "                fillcolor=assay_colors[label],\n",
    "                line_color=\"black\",\n",
    "                showlegend=False,\n",
    "                marker=dict(\n",
    "                    color=\"black\",\n",
    "                    size=2,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # # Prepare data for scatter plots\n",
    "        # jitter = np.random.uniform(-0.01, 0.01, len(mean_pred))\n",
    "\n",
    "        # # Add scatter plots\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         x=[group_index[label]] * len(mean_pred) + jitter,\n",
    "        #         y=mean_pred,\n",
    "        #         mode=\"markers\",\n",
    "        #         name=label,\n",
    "        #         marker=dict(\n",
    "        #             color=\"black\",\n",
    "        #             size=2,\n",
    "        #         ),\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hover_text,\n",
    "        #         showlegend=False,\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "    tickvals = list(group_index.values())\n",
    "    ticktext = list(group_index.keys())\n",
    "    fig.update_xaxes(tickvals=tickvals, ticktext=ticktext)\n",
    "\n",
    "    if min_y is None:\n",
    "        min_y = min(results_df[\"Max pred\"])\n",
    "\n",
    "    fig.update_yaxes(range=[min_y, 1.001])\n",
    "\n",
    "    title = \"Prediction score distribution\"\n",
    "    if use_aggregate_vote:\n",
    "        title += \" (EpiRR majority vote)\"\n",
    "        y_axis_title = \"Avg. prediction score (majority class)\"\n",
    "        filename = f\"{name}_epirr\"\n",
    "    else:\n",
    "        title += \" (per file)\"\n",
    "        y_axis_title = \"Prediction score\"\n",
    "        filename = name\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        yaxis_title=y_axis_title,\n",
    "        xaxis_title=group_by_column,\n",
    "        width=len(group_index)*100,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    fig.write_html(logdir / f\"{filename}.html\")\n",
    "    fig.write_image(logdir / f\"{filename}.svg\")\n",
    "    fig.write_image(logdir / f\"{filename}.png\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_split_dfs = all_split_results[CELL_TYPE]\n",
    "ct_concat_pred = split_results_handler.concatenate_split_results({CELL_TYPE:ct_split_dfs}, concat_first_level=True)[CELL_TYPE]\n",
    "ct_concat_pred = split_results_handler.add_max_pred(ct_concat_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1124/3281540322.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  ct_concat_pred[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "ct_concat_pred = metadata_handler.join_metadata(ct_concat_pred, metadata_v2)\n",
    "ct_concat_pred[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "box": {
          "visible": true
         },
         "fillcolor": "rgb(225,148,37)",
         "line": {
          "color": "black"
         },
         "marker": {
          "color": "black",
          "size": 2
         },
         "meanline": {
          "visible": false
         },
         "name": "h3k27ac",
         "points": "all",
         "showlegend": false,
         "spanmode": "hard",
         "type": "violin",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ],
         "y": [
          0.9997694999999999,
          0.9973300166666667,
          0.9895747166666666,
          0.9979191,
          0.9921785333333334,
          0.9989898,
          0.8958773666666667,
          0.9859907333333333,
          0.9953129033333333,
          0.9983407133333334,
          0.9964030066666667,
          0.9903697,
          0.9976281433333334,
          0.99711815,
          0.9989555500000001,
          0.9582664,
          0.9997598499999999,
          0.9976723833333333,
          0.9931914566666666,
          0.99144421,
          0.9896487366666666,
          0.9983376566666666,
          0.9928124199999999,
          0.47219354,
          0.99824397,
          0.9960745166666666,
          0.9952742899999999,
          0.9940779666666666,
          0.9048065,
          0.9953843133333334,
          0.99494607,
          0.9315372766666666,
          0.9641573833333333,
          0.9995955566666668,
          0.9961078666666667,
          0.9677465866666667,
          0.9996132333333333,
          0.9975999566666666,
          0.9996861566666667,
          0.9957736666666667,
          0.9790418366666667,
          0.9994122366666667,
          0.9993701433333332,
          0.9506854166666666,
          0.99786915,
          0.9894228833333334,
          0.9545681766666667,
          0.99878645,
          0.9975011500000001,
          0.9992551466666666,
          0.9997163866666666,
          0.9964517,
          0.98624848,
          0.9990809833333333,
          0.9999070066666667,
          0.9993012,
          0.99967445,
          0.9972762999999999,
          0.9984574500000001,
          0.9994092666666666,
          0.9766714100000001,
          0.9830707533333333,
          0.9957563333333334,
          0.9664126199999999,
          0.9960567533333333,
          0.9992453333333332,
          0.9905317666666668,
          0.9994464933333332,
          0.9989772933333333,
          0.9997918833333334,
          0.9461488266666667,
          0.9981719866666667,
          0.9992207233333333,
          0.9791667133333334,
          0.9993045833333333,
          0.9998399,
          0.9912847166666667,
          0.8631844066666666,
          0.9995111766666667,
          0.98156468,
          0.9985950533333333,
          0.9095204733333334,
          0.9366910866666668,
          0.9393087250000001,
          0.9970277766666666,
          0.9926071,
          0.9855787233333334,
          0.9983325866666667,
          0.9801498866666667,
          0.99941302,
          0.9997207133333333,
          0.9999971,
          0.8875913999999999,
          0.9990975533333334,
          0.80975221,
          0.9989419700000001,
          0.88772599,
          0.9745806899999999,
          0.99702035,
          0.9967696033333334,
          0.9751327433333333,
          0.9999966166666666,
          0.99891065,
          0.66850275,
          0.9982341,
          0.9996511066666667,
          0.9892215566666667,
          0.8810959566666666,
          0.9828529,
          0.99899738,
          0.9982490366666666,
          0.99284901,
          0.9998216366666667,
          0.9611893,
          0.9857671833333334,
          0.9996570366666667,
          0.9866528,
          0.9115320333333333,
          0.9980059766666667,
          0.9980806000000001,
          0.99937855,
          0.7459098166666666,
          0.9987387633333333,
          0.9974733699999999,
          0.97356812,
          0.9821640233333334,
          0.99996892,
          0.9982363099999999,
          0.9991765533333333,
          0.9974729633333334,
          0.9982911666666667,
          0.9965875633333333,
          0.99402474,
          0.8694053833333334,
          0.9985460866666668,
          0.9975506,
          0.9971191899999999,
          0.9932638300000001,
          0.9876201333333334,
          0.99852812,
          0.9992739466666668,
          0.99796085,
          0.6358293066666666,
          0.9988313866666667,
          0.87774348,
          0.9693795233333334,
          0.9994509533333332,
          0.9956586000000001,
          0.9967822333333333,
          0.9997431166666667,
          0.38167007,
          0.9944021266666666,
          0.9982231666666667,
          0.9970835899999999,
          0.9999987733333334,
          0.9999949233333334,
          0.9998830666666666,
          0.9997670333333334,
          0.9998633133333333,
          0.9944399133333334,
          0.9999134766666667,
          0.9996819666666666,
          0.9985219166666667,
          0.9985406,
          0.9997173500000001,
          0.99966425,
          0.9948434966666667,
          0.9997171666666667,
          0.9992784333333334,
          0.9992923166666667,
          0.9668011233333335,
          0.9434165,
          0.9983067966666667,
          0.9957471666666667,
          0.9999204466666667,
          0.9682656233333334,
          0.9999798966666668,
          0.99976641,
          0.9998870000000001,
          0.99965765,
          0.99924508,
          0.9997916333333334,
          0.9957910033333333,
          0.9953920900000001,
          0.9822807,
          0.9999788666666666,
          0.9975545,
          0.99936947,
          0.99940316,
          0.99949809,
          0.9996627233333334,
          0.99973893,
          0.9991055133333333,
          0.9998525766666667,
          0.9971589200000001,
          0.9999547300000001,
          0.9998131333333333,
          0.99736831,
          0.9665485133333332,
          0.9965949766666666,
          0.9995455066666666,
          0.9995088633333333,
          0.99999062,
          0.9995702666666667,
          0.99975628,
          0.9999956166666667,
          0.9884107000000001,
          0.99999465,
          0.9998682133333333,
          0.9999396433333333,
          0.9958894433333333,
          0.99644762,
          0.9998247600000001,
          0.8767664666666667,
          0.9386246033333334,
          0.98084619,
          0.99948979,
          0.9980014566666666,
          0.9959375,
          0.9998655666666667,
          0.99607739,
          0.9999903433333334,
          0.9998833633333333,
          0.9983170366666666,
          0.9998923033333332,
          0.9993895199999999,
          0.9997428666666667,
          0.9997728266666667,
          0.99993543,
          0.98161726,
          0.99977039,
          0.9998404566666667,
          0.9998797766666666,
          0.9999324833333333,
          0.9997887666666667,
          0.9999778,
          0.8657677766666666,
          0.9999682999999999,
          0.9998256,
          0.99870444,
          0.9998766733333332,
          0.99985992,
          0.9996316766666666,
          0.9994207666666667,
          0.9952085766666666,
          0.9891635166666667,
          0.99992665,
          0.9428689666666666,
          0.9903577966666667,
          0.9968528000000001,
          0.9999855366666667,
          0.9531399566666666,
          0.9996734433333333,
          0.9972150333333333,
          0.99576371,
          0.9981399766666668,
          0.9959804933333333,
          0.39568252333333337,
          0.99399241,
          0.99668057,
          0.9988851899999999,
          0.99931505,
          0.9926288799999999,
          0.9998249666666666,
          0.91171475,
          0.99558789,
          0.9934472666666667,
          0.965274,
          0.9995976033333333,
          0.9997576,
          0.9967121333333333,
          0.9975321333333333,
          0.99960449,
          0.9989120333333333,
          0.9994782966666667,
          0.9993958666666667,
          0.9989391799999999,
          0.9958159766666667,
          0.9990293166666667,
          0.6991791333333334,
          0.9987371333333334,
          0.9983993266666666,
          0.9953740433333333,
          0.9434058100000001,
          0.9984687666666666,
          0.9995225166666666,
          0.9974043233333333,
          0.99684461,
          0.9997427766666668,
          0.9991683933333334,
          0.7193282,
          0.9994220533333333,
          0.9977169,
          0.9572886233333334,
          0.99814083,
          0.9988164333333334,
          0.9857374533333333,
          0.9908780533333333,
          0.99966366,
          0.9995349399999999,
          0.99935455,
          0.9998694499999999,
          0.9978780199999999,
          0.9987128733333334,
          0.99962878,
          0.9868903166666666,
          0.9839921166666666,
          0.7832026333333334,
          0.99971802,
          0.99852885,
          0.99260556,
          0.9016970299999999,
          0.9985014466666667,
          0.9996454866666666,
          0.90817625,
          0.9991389233333333,
          0.999942,
          0.99908775,
          0.9985301,
          0.96174824,
          0.9983991333333333,
          0.9993803666666666,
          0.9997538666666667,
          0.99942952,
          0.9994980433333334,
          0.9995155633333334,
          0.9947206799999999,
          0.9954040233333333,
          0.9996840833333334,
          0.9995894633333333,
          0.9996185833333332,
          0.8680610666666667,
          0.9981907466666667,
          0.9985741766666667,
          0.7105417766666666,
          0.9984736533333334,
          0.9986737,
          0.851476,
          0.9993901,
          0.83487218,
          0.9995979166666666,
          0.9988227833333333,
          0.9982028666666668,
          0.98691556,
          0.9911699766666667,
          0.99259545,
          0.9992192000000001,
          0.9975506166666667,
          0.9994585133333334,
          0.9985014666666666,
          0.9998856000000002,
          0.9973839766666667,
          0.9406349833333333,
          0.99915695,
          0.9997954266666667,
          0.9996942,
          0.9897550566666666,
          0.99972077,
          0.9998991466666666,
          0.9995702300000001,
          0.9997126666666666,
          0.9925706999999999,
          0.9957653433333333,
          0.9957045433333334,
          0.9974786300000001,
          0.99978788,
          0.98151918,
          0.9980870333333334,
          0.9987417566666666,
          0.9980327433333334,
          0.9858296566666667,
          0.800106125,
          0.9926871,
          0.9885214433333332,
          0.9999230366666666,
          0.9979600566666665,
          0.9995591033333334,
          0.9995579,
          0.9999013833333333,
          0.9992792666666667,
          0.99712938,
          0.9995802633333333,
          0.9988283033333333,
          0.97746727,
          0.72815685,
          0.9992394766666667,
          0.9991977466666667,
          0.9995508666666667,
          0.99892831,
          0.92711775,
          0.9182712533333334,
          0.9994822133333333,
          0.9897598799999999,
          0.9951798466666667,
          0.9962177233333334,
          0.99977594,
          0.9997128133333333,
          0.9998387333333333,
          0.9277223,
          0.9996872566666667,
          0.9998577233333332,
          0.9993298833333334,
          0.9941033699999999,
          0.9996020633333332,
          0.9910146000000001,
          0.8237435233333333,
          0.9999904700000001,
          0.9995528866666668,
          0.9010166533333334,
          0.9691747999999999,
          0.9919097133333333,
          0.9986974200000001,
          0.9994735,
          0.9968187199999999,
          0.9892731,
          0.9993435366666666,
          0.9884343800000001,
          0.9947902966666667,
          0.9967701099999999,
          0.9979774133333333,
          0.9934261233333334,
          0.9994726866666667,
          0.9991196333333333,
          0.9992045666666667,
          0.9879389633333333,
          0.9976517833333333,
          0.9535802333333333,
          0.9989837933333333,
          0.9911369466666667,
          0.9969764733333334,
          0.9998203866666667,
          0.9996064766666667,
          0.9989935799999999,
          0.9819778333333332,
          0.9369703166666666,
          0.99982885,
          0.9990664533333332,
          0.99275675,
          0.9994951333333333,
          0.9906347666666667,
          0.9744446066666667,
          0.9752438166666666,
          0.9908172666666667,
          0.9998543833333334,
          0.99981238,
          0.9960555833333333,
          0.9961518833333334,
          0.9999208866666667,
          0.9998650000000001,
          0.7356051466666665,
          0.9983716666666665,
          0.9997823333333334,
          0.9943947799999999,
          0.98073941,
          0.9996731799999999,
          0.9988755266666667,
          0.9856876633333332,
          0.9999171066666667,
          0.9646845366666666,
          0.9989861333333333,
          0.9996263966666666,
          0.9897584,
          0.9987608766666667,
          0.638749985,
          0.9986158333333334,
          0.9522952333333333,
          0.9944047566666666,
          0.9999455933333333,
          0.9996815033333334,
          0.9973364366666667,
          0.9991114433333333,
          0.9999067166666666,
          0.9965151333333333,
          0.9999338533333333,
          0.9993819500000001,
          0.9991985200000001,
          0.9999693666666666,
          0.99999089,
          0.99997711,
          0.99942275,
          0.99888605,
          0.9996691533333334,
          0.9966027433333333,
          0.98831632,
          0.9984956,
          0.9963112266666666,
          0.9958699433333335,
          0.9778199366666667,
          0.9978767566666665,
          0.9038874633333333,
          0.9961403999999999,
          0.9996363,
          0.25705257,
          0.16800840666666664,
          0.99720047,
          0.9906189633333332,
          0.9973663666666667,
          0.9999860133333334,
          0.9999331666666667,
          0.9999846266666667,
          0.9998509866666666,
          0.99958001,
          0.9999919,
          0.9999742,
          0.9994575133333333,
          0.9998708733333334,
          0.9999012666666666,
          0.9999923333333333,
          0.9996992666666666,
          0.9993259666666666,
          0.9999949566666667,
          0.9999981366666667,
          0.9999959566666666,
          0.9999988566666667,
          0.99999213,
          0.9999932466666666,
          0.9999987633333333,
          0.9999977333333333,
          0.9999968566666667,
          0.9999974166666666,
          0.9999965133333334,
          0.99994035,
          0.9967669666666668,
          0.9999376833333334,
          0.9999192899999999,
          0.9999252133333334,
          0.9986471,
          0.9936431433333333,
          0.9949588199999999,
          0.9998269666666667,
          0.99981609,
          0.9982528199999999,
          0.9904531466666667,
          0.99854252,
          0.9994132666666666,
          0.9897257233333333,
          0.9983317133333335,
          0.9999041133333333,
          0.9997082833333334,
          0.9998430533333332,
          0.999851,
          0.99905378,
          0.9998209066666668,
          0.9995984333333334,
          0.9918353,
          0.9997957333333334,
          0.9998064733333334,
          0.9991230666666667,
          0.99977043,
          0.9971356733333333,
          0.9998033666666667,
          0.9999465166666667,
          0.9995717333333333,
          0.9999449766666667,
          0.9995426766666666,
          0.9998440033333335,
          0.9972169866666666,
          0.9998258,
          0.9989982500000001,
          0.9976121099999999,
          0.9998777,
          0.9964325633333333,
          0.9931625766666666,
          0.9997620666666668,
          0.9999013366666666,
          0.9896304533333334,
          0.9962056433333334,
          0.9989461766666666,
          0.8945794399999999,
          0.9980545166666667,
          0.9999329733333333,
          0.9998226466666665,
          0.9985211633333333,
          0.99933972,
          0.9998299333333334,
          0.9999165566666667,
          0.9999614466666666,
          0.9972283533333334,
          0.9995718266666667,
          0.99881177,
          0.9972755466666667,
          0.9994730666666666,
          0.9757787233333334,
          0.9998518933333335,
          0.9999316133333332,
          0.9989263033333334,
          0.9994405,
          0.9997670733333334,
          0.99975638,
          0.99954376,
          0.9988382633333334,
          0.9988539166666667,
          0.9873594666666667,
          0.9996236999999999,
          0.9995708666666667,
          0.9975692500000001,
          0.9994165266666667,
          0.9952646133333333,
          0.99981332,
          0.99965198,
          0.9965134666666667,
          0.9966055866666667,
          0.9999330366666667,
          0.9997532000000001,
          0.99991935,
          0.998885,
          0.9998728,
          0.9997105333333334,
          0.9981166666666667,
          0.9996412533333333,
          0.9974563533333334,
          0.99858222,
          0.9994376466666667,
          0.9999039133333333,
          0.9995508766666666,
          0.9953843299999999,
          0.9999004766666667,
          0.9990088,
          0.9979111333333334,
          0.9967695366666667,
          0.9986870066666667,
          0.9983521333333334,
          0.9992499499999999,
          0.9989005933333334,
          0.9992058533333333,
          0.99987171,
          0.9961204766666666,
          0.9936302933333333,
          0.99977777,
          0.9986570433333334,
          0.9999018866666667,
          0.9995463899999999,
          0.9985328833333332,
          0.9999232366666666,
          0.9950758566666668,
          0.9999008766666666,
          0.99983742,
          0.9996851000000001,
          0.9999279433333333,
          0.99377548,
          0.9961443066666668,
          0.99984128,
          0.99797789,
          0.9980775166666667,
          0.99994885,
          0.997127,
          0.99988701,
          0.9981973466666667,
          0.9962573166666666,
          0.99814202,
          0.9988355200000001,
          0.9999603333333335,
          0.9998382100000001,
          0.99979592,
          0.99913598,
          0.9986385866666666,
          0.99961508,
          0.9988295733333333,
          0.9999014166666665,
          0.9996830533333334,
          0.98747168,
          0.9996222666666666,
          0.99016658,
          0.99417319,
          0.99761408,
          0.9998418299999999,
          0.9912062666666667,
          0.9999451333333332,
          0.9950808333333333,
          0.9997041866666666,
          0.9851178033333333,
          0.99965549,
          0.99831719,
          0.9950007533333333,
          0.9985783333333332,
          0.9999674166666667,
          0.9992672699999999,
          0.9998958,
          0.9996594833333333,
          0.9971831866666667,
          0.9997769566666667,
          0.9998210433333333,
          0.9970801466666667,
          0.9993875133333333,
          0.9996760766666667,
          0.9985145733333334,
          0.9998748166666666,
          0.9956665766666667,
          0.9990801666666668,
          0.9992474466666668,
          0.9811446166666666,
          0.99983361,
          0.9999031566666666,
          0.9967709,
          0.99942067,
          0.99775371,
          0.9998854566666667,
          0.9993409466666666,
          0.9984645833333333,
          0.9965688199999999,
          0.99953276,
          0.9997195733333334,
          0.9999436833333334,
          0.99465264,
          0.9989201799999999,
          0.9999187733333333,
          0.9999166333333332,
          0.9988080666666667,
          0.99375802,
          0.9989434999999999,
          0.9999172,
          0.9999734166666666,
          0.99319574,
          0.9999214799999999,
          0.9999316033333333,
          0.9979315133333334,
          0.9996045366666667,
          0.9999172999999999,
          0.9998627666666667,
          0.999135,
          0.99976101,
          0.9968766533333334,
          0.99933883,
          0.99620535,
          0.9998957333333333,
          0.9998994866666667,
          0.9979743333333334,
          0.5695825433333334,
          0.9992096466666668,
          0.9958147333333334,
          0.9994372133333332,
          0.9997270433333334,
          0.9994507066666666,
          0.9995997500000001,
          0.9998634933333334,
          0.9998094833333333,
          0.9990029133333334,
          0.9999499,
          0.9999002933333333,
          0.99473309,
          0.99931845,
          0.9999119133333334,
          0.9973458533333334,
          0.9962432666666666,
          0.9998554199999999,
          0.9995419966666667,
          0.9997210133333333,
          0.9998586633333333,
          0.9989921666666667,
          0.9998064666666666,
          0.9950853533333334,
          0.9997117533333334,
          0.9999199366666667,
          0.9997905,
          0.9892713766666666,
          0.9988430233333334,
          0.9967746133333334,
          0.9977483333333333,
          0.99991325,
          0.99937154,
          0.9989191666666667,
          0.9997092766666666,
          0.9975012733333334,
          0.99854048,
          0.9980086866666666,
          0.9965020833333332,
          0.9927522299999999,
          0.9991926533333334,
          0.9976736,
          0.9990901333333334,
          0.9993862833333335,
          0.9960712733333333,
          0.9993774200000001,
          0.99925476,
          0.9999580333333333,
          0.99936522,
          0.9989314133333332,
          0.9984137666666667,
          0.9882171533333333,
          0.9987461033333332,
          0.99935404,
          0.9998562866666667,
          0.9996012833333333,
          0.9997328033333334,
          0.9996961566666668,
          0.99987795,
          0.9998667833333333,
          0.99934542,
          0.9997099666666666,
          0.9996315333333333,
          0.9974305933333333,
          0.9991168766666667,
          0.9977040766666666,
          0.9998095166666667,
          0.9997111666666667,
          0.9956428333333333,
          0.99986372,
          0.99100665,
          0.99929217,
          0.9994860433333334,
          0.99946151,
          0.99983802,
          0.9970022799999999,
          0.9999300999999999,
          0.9999293466666667,
          0.9989042,
          0.9992656200000001,
          0.9996217466666666,
          0.9935227333333333,
          0.9997073433333333,
          0.9996473333333333,
          0.9997870666666667,
          0.9993297233333333,
          0.9979017666666667,
          0.9994847333333333,
          0.9999146533333333,
          0.9998438799999999,
          0.9999335199999999,
          0.9998851166666668,
          0.99992775,
          0.9962504466666666,
          0.9998393366666667,
          0.9946389466666666,
          0.9999092833333334,
          0.9979531166666668,
          0.9941456299999999,
          0.9996670166666667,
          0.9998408966666666,
          0.9963230433333333,
          0.99949455,
          0.9878827033333333,
          0.9957280466666667,
          0.9997989533333334,
          0.9880724000000001,
          0.9998262866666666,
          0.99884029,
          0.9998909666666665,
          0.9999054566666666,
          0.99939655,
          0.99829992,
          0.9996452033333334,
          0.9996434433333333,
          0.9701247666666667,
          0.9997751133333334,
          0.9999050233333334,
          0.9997794999999999,
          0.9953254899999999,
          0.9990014399999999,
          0.9993086433333334,
          0.9993172,
          0.9975034366666667,
          0.9994093666666667,
          0.9996032333333332,
          0.9968406166666667,
          0.8552321766666667,
          0.9991829333333332,
          0.9969575666666666,
          0.9990111333333332,
          0.99964229,
          0.99765315,
          0.9994175100000001,
          0.9996644466666668,
          0.9997414766666667,
          0.9804013,
          0.9986896766666667,
          0.9996745466666667,
          0.9998613500000001,
          0.99994719,
          0.9979560766666666,
          0.9945511499999999,
          0.9998588,
          0.9982770666666667,
          0.9999342433333333,
          0.9985432333333334,
          0.99669826,
          0.99973601,
          0.9951806066666666,
          0.99955566,
          0.9991958400000001,
          0.9997627033333334,
          0.9989878166666667,
          0.9999450466666667,
          0.9998883033333333,
          0.99947205,
          0.99961299,
          0.9998668366666666,
          0.9986018333333333,
          0.9972460066666667,
          0.999851,
          0.9984622166666667,
          0.9963756233333334,
          0.9997244333333333,
          0.9998591333333332,
          0.9969417166666666,
          0.9998379466666667,
          0.9999343933333332,
          0.9991412566666668,
          0.9977424733333334,
          0.9968693966666667,
          0.9307277266666668,
          0.99775255,
          0.9994248566666667,
          0.99976924,
          0.9998566733333334,
          0.9988550166666667,
          0.9998283333333333,
          0.9985553833333333,
          0.9999237500000001,
          0.9945432,
          0.9998963999999999,
          0.99355832,
          0.9994555533333332,
          0.9997221166666667,
          0.9997493966666666,
          0.9999320333333334,
          0.8639573866666668,
          0.9990456499999999,
          0.9986788333333333,
          0.99984735,
          0.9998225566666666,
          0.9995320366666666,
          0.9990209866666667,
          0.9989627333333333,
          0.9996863333333333,
          0.9925138966666666,
          0.99939906,
          0.9924521666666667,
          0.9977686833333334,
          0.9996754666666666,
          0.99989399,
          0.9990579866666667,
          0.99991699,
          0.9982565,
          0.9998474066666666,
          0.9977809333333334,
          0.99955243,
          0.9997695233333334,
          0.9998771666666667,
          0.9970407833333333,
          0.9996413166666667,
          0.9998547033333334,
          0.9998912133333334,
          0.9997292833333334,
          0.9999507333333333,
          0.9999199000000001,
          0.99933634,
          0.9959704866666667,
          0.9996089166666667,
          0.9997391333333333,
          0.99142692,
          0.9992855199999999,
          0.9996316333333333,
          0.9999027766666666,
          0.9999298666666666,
          0.9980935666666667,
          0.9989853133333334,
          0.9998616666666668,
          0.9996677633333334,
          0.9998479333333333,
          0.9993910933333333,
          0.99970525,
          0.9932721666666667,
          0.9999783066666666,
          0.9959470133333334,
          0.9983983433333333,
          0.9998798433333334,
          0.9984369633333333,
          0.9989087166666666,
          0.99517606,
          0.9986835166666667,
          0.9998510266666667,
          0.9999674833333333,
          0.98939858,
          0.9997560433333333,
          0.99937055,
          0.9890954033333333,
          0.9966044366666665,
          0.9998833500000001,
          0.9998043133333333,
          0.9985584,
          0.99996166,
          0.9999940199999999,
          0.9999953133333332,
          0.9999575866666666,
          0.99999201,
          0.9999926133333333,
          0.9999938566666667,
          0.9999878333333333,
          0.9999907366666667,
          0.9999566033333332,
          0.9999773833333334,
          0.9999958699999999,
          0.9998829766666667,
          0.9997963366666666,
          0.99999085,
          0.9999336866666667,
          0.9999916333333333,
          0.9999956333333334,
          0.9998011333333334,
          0.9999762866666666,
          0.9999849,
          0.9999880000000001,
          0.9999917666666667,
          0.94955658,
          0.9999037999999999,
          0.9999973766666667,
          0.9999939000000001,
          0.9999707999999999,
          0.99986703,
          0.9999869333333334,
          0.9999491466666667,
          0.9999842666666666,
          0.9999861366666667,
          0.9999971766666667,
          0.9988236,
          0.9999480166666667,
          0.9999818,
          0.9999415099999999,
          0.99998555,
          0.9999955833333333,
          0.9999834333333334,
          0.99998792,
          0.9999964166666667,
          0.9999886,
          0.9998556733333333,
          0.9999554466666667,
          0.9999977666666666,
          0.9999225233333333,
          0.99999226,
          0.9999591200000001,
          0.9768517166666667,
          0.9930049433333333,
          0.9999975333333332,
          0.9999545400000001,
          0.9372591833333334,
          0.999994,
          0.9999475766666667,
          0.9999583200000001,
          0.9996654566666666,
          0.9999963833333334,
          0.9999884333333333,
          0.9999490866666667,
          0.9999943966666667,
          0.99999115,
          0.9999734866666666,
          0.9999610666666667,
          0.9996578399999999,
          0.9999772666666668,
          0.9999982866666667,
          0.9999196633333334,
          0.99971025,
          0.9999883266666667,
          0.9999849,
          0.99997792,
          0.9997331666666667,
          0.9999997666666666,
          0.9999966233333333,
          0.9999921133333333,
          0.9999582366666666,
          0.9999392433333334,
          0.9999474300000001,
          0.9999391699999999,
          0.9999906633333334,
          0.99997298,
          0.9997181999999999,
          0.9999201166666666,
          0.9999797999999999,
          0.9999980900000001,
          0.9997701033333333,
          0.9991382533333333,
          0.9997098333333333,
          0.9999812,
          0.9999847833333333,
          0.9999061899999999,
          0.9999865666666666,
          0.9999970166666667,
          0.9999609666666668,
          0.9999947833333334,
          0.9999942333333333,
          0.9999946333333334,
          0.9999624833333334,
          0.8385427999999999,
          0.99999722,
          0.9999934466666667,
          0.9999907,
          0.99999055,
          0.9999898133333334,
          0.99975903,
          0.9999955833333333,
          0.9999845133333333,
          0.9999801,
          0.9999974566666667,
          0.9998980533333333,
          0.9999800533333333,
          0.9999821833333332,
          0.99999301,
          0.9125713466666667,
          0.9999558166666667,
          0.99999849,
          0.9999792466666667,
          0.9993551866666667,
          0.99996249,
          0.9999905099999999,
          0.9999939666666666,
          0.9999393666666666,
          0.99999788,
          0.9998970833333334,
          0.9999878366666667,
          0.9977468533333335,
          0.9999886,
          0.9993427633333334,
          0.9999969433333332,
          0.9999766,
          0.9999972600000001,
          0.9999884333333333,
          0.9999913833333333,
          0.99963605,
          0.9999598666666666,
          0.9999954333333333,
          0.9999936800000001,
          0.9999970166666667,
          0.9999954566666668,
          0.99999153,
          0.9999265833333334,
          0.9880247966666666,
          0.9999821666666667,
          0.9999980966666667,
          0.9997814100000001,
          0.9999708666666667,
          0.9999424866666667,
          0.9434512666666667,
          0.99988843,
          0.9999941199999999,
          0.99998837,
          0.9999957333333334,
          0.9999946033333332,
          0.9999904966666667,
          0.999456,
          0.9999937133333333,
          0.9998132233333333,
          0.9411429,
          0.9999653566666667,
          0.9999798533333334,
          0.99999701,
          0.9999825866666666,
          0.9999689333333333,
          0.9999981633333334,
          0.9999666966666666,
          0.9999884766666667,
          0.9997013899999999,
          0.99999834,
          0.99980102,
          0.9891709,
          0.9999968666666668,
          0.9999476333333334,
          0.9999966333333333,
          0.9999952866666667,
          0.99996253,
          0.99998955,
          0.9958901333333333,
          0.9999740933333333,
          0.99999201,
          0.9998542666666667,
          0.9999168766666666,
          0.9999925166666667,
          0.9999908966666666,
          0.9982381533333333,
          0.99999158,
          0.9999833166666666,
          0.9996424933333333,
          0.9999906666666667,
          0.9999768000000001,
          0.9999448000000001,
          0.9881007533333334,
          0.9999975733333333,
          0.9999837500000001,
          0.9999856500000001,
          0.8615911433333334,
          0.9999878833333334,
          0.9999979733333334,
          0.9541731333333333,
          0.9999706,
          0.9999194533333333,
          0.99998049,
          0.99997778,
          0.9999767966666666,
          0.9999812,
          0.9999985566666667,
          0.9999855000000001,
          0.99999562,
          0.9999448433333334,
          0.9999945566666666,
          0.9999872466666666,
          0.9953756566666666,
          0.9979675966666667,
          0.9999965666666667,
          0.9999799333333333,
          0.9999939266666668,
          0.9999682066666667,
          0.9999955933333333,
          0.9999859233333334,
          0.9999896333333332,
          0.9999914333333333,
          0.9999815999999999,
          0.99998797,
          0.9999969333333333,
          0.99996484,
          0.9999972,
          0.9999875933333334,
          0.9999990133333334,
          0.9999499266666666,
          0.9999953866666668,
          0.9994053433333333,
          0.9999760466666666,
          0.9993708666666666,
          0.9999365333333333,
          0.9999832333333334,
          0.9999723066666667,
          0.9999715,
          0.9999889066666666,
          0.9999473999999999,
          0.9999957333333332,
          0.9999946,
          0.9996204366666667,
          0.99995748,
          0.9999886766666667,
          0.9999929366666667,
          0.99996837,
          0.9999611,
          0.9999922133333333,
          0.9998973966666668,
          0.9999895866666667,
          0.99997477,
          0.9999966233333333,
          0.99998195,
          0.9999138566666667,
          0.9925743833333334,
          0.9999901166666666,
          0.9999965433333333,
          0.9999920066666667,
          0.9999942,
          0.99999348,
          0.9999892300000001,
          0.9998972,
          0.9999928766666667,
          0.99999153,
          0.9999954833333334,
          0.9993160200000001,
          0.6306440233333334,
          0.9998661866666666,
          0.9999876799999999,
          0.9999901466666666,
          0.9999895466666667,
          0.9999629766666667,
          0.9999926,
          0.9999974666666667,
          0.9999941200000001,
          0.9999744666666667,
          0.99997981,
          0.9999861000000001,
          0.9999889566666665,
          0.9706519233333334,
          0.9999952333333333,
          0.9999985666666666,
          0.9999341199999999,
          0.9999987200000001,
          0.9999993666666667,
          0.99989142,
          0.9998870166666668,
          0.9999467933333334,
          0.9940137466666666,
          0.99985695,
          0.99971865,
          0.9431794333333333,
          0.9486315766666666,
          0.9552153166666667,
          0.9998701033333334,
          0.9999865333333333,
          0.9996970733333334,
          0.9999632433333333,
          0.99995248,
          0.9999843166666667,
          0.99998064,
          0.9979599766666666,
          0.9997387333333333,
          0.9979112133333333,
          0.9953628133333333,
          0.9998600366666667,
          0.9999295566666667,
          0.99996902,
          0.9997783533333333,
          0.9997076366666667,
          0.9991976966666667,
          0.9921905333333333,
          0.9989404333333334,
          0.9999434633333334,
          0.98631423,
          0.9996838866666667,
          0.97612655,
          0.9998314866666668,
          0.9995836833333334,
          0.9846304866666666,
          0.8549037133333334,
          0.9997225,
          0.99801127,
          0.9971628433333333,
          0.9995092033333334,
          0.93786318,
          0.9320726833333334,
          0.9997257999999999,
          0.9565934633333333,
          0.9830034333333334,
          0.8914264333333334,
          0.9995924666666666,
          0.9813214666666666,
          0.9887701633333333,
          0.9995413766666666,
          0.9996104233333334,
          0.99960332,
          0.99925418,
          0.9999029666666667,
          0.99987557,
          0.9992870466666667,
          0.9998667433333334,
          0.9997160066666666,
          0.99974635,
          0.9997615999999999,
          0.9999178766666666,
          0.9995397766666666,
          0.9999881833333334,
          0.9999100466666667,
          0.9835829866666667,
          0.9999739633333333,
          0.9998524266666666,
          0.9923582866666667,
          0.9998957966666667,
          0.9981865666666666,
          0.8245790999999999,
          0.93120102,
          0.9291860633333333,
          0.9950621233333333,
          0.8912347866666668,
          0.9823622333333333,
          0.9999784166666666,
          0.9995339666666667,
          0.9375291166666666,
          0.99291822,
          0.9943510166666666,
          0.9986021533333332,
          0.9999211,
          0.9999466233333334,
          0.9942729266666667,
          0.9999304166666666,
          0.9999819533333333,
          0.8882269666666667,
          0.9999709999999999,
          0.9989184333333333,
          0.9952389433333333,
          0.9998029333333333,
          0.9997741233333333,
          0.9997099866666667,
          0.9960261899999999,
          0.9998981100000001,
          0.9999087666666666,
          0.99701198,
          0.9986234499999999,
          0.9999518733333334,
          0.9999598666666666,
          0.99998339,
          0.99991007,
          0.9999555,
          0.9302465,
          0.9991700166666666
         ]
        },
        {
         "box": {
          "visible": true
         },
         "fillcolor": "rgb(184,185,189)",
         "line": {
          "color": "black"
         },
         "marker": {
          "color": "black",
          "size": 2
         },
         "meanline": {
          "visible": false
         },
         "name": "h3k27me3",
         "points": "all",
         "showlegend": false,
         "spanmode": "hard",
         "type": "violin",
         "x": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
         ],
         "y": [
          0.9972126466666666,
          0.9567753899999999,
          0.9823033166666667,
          0.9797847566666666,
          0.97192735,
          0.9906381999999999,
          0.9859719533333333,
          0.9452923200000001,
          0.8885833966666666,
          0.9646664533333333,
          0.9600989333333333,
          0.8712039533333334,
          0.9550925499999999,
          0.9820163333333333,
          0.9903864333333333,
          0.9925218633333334,
          0.5690832066666667,
          0.9772923800000001,
          0.9901866566666667,
          0.9941002666666666,
          0.9689322766666667,
          0.9657470333333333,
          0.9695942299999999,
          0.9622044166666667,
          0.9512741266666667,
          0.9702859466666666,
          0.9450827166666667,
          0.9952264999999999,
          0.9947547,
          0.7437970533333335,
          0.93670572,
          0.9566221666666667,
          0.8538048033333334,
          0.7895894666666666,
          0.8847618333333332,
          0.97814107,
          0.9666004766666667,
          0.9816338333333334,
          0.9927449166666666,
          0.9511895199999999,
          0.9912543866666667,
          0.9732599266666666,
          0.97204269,
          0.9883141000000001,
          0.9875768766666667,
          0.9912752666666668,
          0.9740430666666667,
          0.9350522666666666,
          0.9792188533333334,
          0.9953151433333334,
          0.9793615533333333,
          0.8903745399999999,
          0.7239328066666667,
          0.7212224300000001,
          0.9670199999999999,
          0.9829179,
          0.99000864,
          0.98546311,
          0.9910936666666667,
          0.9245526666666667,
          0.9917374333333333,
          0.96665885,
          0.9930097033333333,
          0.9830470766666667,
          0.8205616,
          0.9762002266666667,
          0.98087137,
          0.9875363533333333,
          0.4758871,
          0.9898497266666667,
          0.97598018,
          0.9792408300000001,
          0.9200207833333334,
          0.84938628,
          0.8961716666666666,
          0.9941831899999999,
          0.7508081166666667,
          0.9663005866666667,
          0.9178046700000001,
          0.9297538233333333,
          0.9961923666666667,
          0.9981743866666667,
          0.9958076633333334,
          0.9659679333333333,
          0.9935816700000001,
          0.9308196566666668,
          0.9872936433333334,
          0.8679768333333332,
          0.9664080899999999,
          0.9582838333333333,
          0.9148109333333333,
          0.9708590666666667,
          0.9949487433333334,
          0.9383907633333334,
          0.7065984533333333,
          0.9643283333333333,
          0.2896774266666667,
          0.96618748,
          0.9637203099999999,
          0.9683783166666666,
          0.9551699399999999,
          0.9008168333333333,
          0.9323892366666667,
          0.99173812,
          0.9794582866666667,
          0.9653146399999999,
          0.7364242,
          0.9488180666666667,
          0.9695970333333334,
          0.96419197,
          0.9494641333333332,
          0.8411877533333333,
          0.9292258833333333,
          0.9467916033333333,
          0.9615700966666667,
          0.9387220333333334,
          0.84921369,
          0.9735767066666666,
          0.7755922166666668,
          0.9805394999999999,
          0.9568397000000001,
          0.9505173866666666,
          0.9385711,
          0.9673681666666667,
          0.8344741,
          0.8706671866666666,
          0.9334273999999999,
          0.9622974666666666,
          0.9938448900000001,
          0.7521186666666667,
          0.8514316966666667,
          0.97412928,
          0.8077775666666667,
          0.8779070666666667,
          0.7209822333333333,
          0.9782112866666667,
          0.9475027300000001,
          0.9791324666666666,
          0.8905523233333333,
          0.9728346999999999,
          0.9054458966666666,
          0.9842292866666668,
          0.9561079166666667,
          0.9702054266666668,
          0.9954293499999999,
          0.9632768633333333,
          0.99537808,
          0.99073735,
          0.9941252566666666,
          0.9992965166666666,
          0.9990232233333334,
          0.9930328533333334,
          0.9973099533333333,
          0.9942921299999999,
          0.9651021466666667,
          0.9889002433333333,
          0.9906189166666667,
          0.98331277,
          0.97112984,
          0.9900071,
          0.9935215133333334,
          0.99609862,
          0.97771454,
          0.96576611,
          0.9927802366666666,
          0.9642191000000001,
          0.9897649566666665,
          0.9902391133333334,
          0.9955479333333334,
          0.9890779233333333,
          0.9986811533333334,
          0.9564938666666666,
          0.9805256833333335,
          0.9919407066666667,
          0.9846670966666666,
          0.99030868,
          0.9950459333333334,
          0.9820461333333333,
          0.9922492666666667,
          0.9912906,
          0.47828223000000003,
          0.6852310300000001,
          0.97627715,
          0.29901664,
          0.6496311433333334,
          0.2271837,
          0.219795395,
          0.6869888666666667,
          0.9826693733333333,
          0.36153081666666664,
          0.8066154666666666,
          0.9747798233333334,
          0.45598542333333336,
          0.887543,
          0.9432300666666666,
          0.7077864433333333,
          0.9762584133333334,
          0.9445560866666667,
          0.9950277933333332,
          0.9468757333333334,
          0.9506008,
          0.9125972,
          0.32298784333333336,
          0.9330886233333334,
          0.93193245,
          0.9380496233333333,
          0.382247045,
          0.8487687866666667,
          0.9851828166666667,
          0.22185833333333335,
          0.9641236566666667,
          0.9601897566666667,
          0.9618600133333334,
          0.9768274333333333,
          0.81299559,
          0.96594203,
          0.9045567166666667,
          0.9868265166666667,
          0.9979103666666665,
          0.9825989166666668,
          0.9715940333333334,
          0.9933299533333333,
          0.9785441166666667,
          0.7249371333333334,
          0.9839689233333333,
          0.9743912533333333,
          0.9659420866666667,
          0.9799431766666666,
          0.9877998833333334,
          0.9915351833333332,
          0.62154889,
          0.9930855833333334,
          0.9936709333333332,
          0.9235897266666667,
          0.9779104666666667,
          0.9574619166666666,
          0.9748214466666667,
          0.9941898533333333,
          0.40700428,
          0.96495882,
          0.6032208,
          0.9595692166666666,
          0.97742688,
          0.9198810466666667,
          0.9495764133333333,
          0.38603863,
          0.5626468666666666,
          0.9918503166666666,
          0.45850977666666665,
          0.9195952433333333,
          0.90540276,
          0.87479338,
          0.96192771,
          0.9626397766666667,
          0.9952232566666667,
          0.99647703,
          0.6198397033333333,
          0.9128447333333334,
          0.66831192,
          0.9663771233333334,
          0.8324921233333334,
          0.7242602033333333,
          0.9897613333333334,
          0.9068295166666666,
          0.8548636,
          0.7735363466666666,
          0.9846547166666667,
          0.9870002133333333,
          0.8818168233333333,
          0.93284862,
          0.8698923966666667,
          0.9308913766666667,
          0.98490945,
          0.9752216833333334,
          0.9013702499999999,
          0.7905932333333333,
          0.9791671866666666,
          0.4941251,
          0.9709226900000001,
          0.70863275,
          0.8692848433333333,
          0.6347083333333333,
          0.8487244466666667,
          0.60706374,
          0.70613271,
          0.6070361333333333,
          0.9866881666666667,
          0.7729079066666666,
          0.8341901333333333,
          0.6238966933333333,
          0.9529494433333333,
          0.98476226,
          0.58665785,
          0.9535446333333333,
          0.8620987166666666,
          0.9695909333333333,
          0.9704154366666667,
          0.61245179,
          0.9085136,
          0.9178545766666666,
          0.8502499433333334,
          0.9570739166666667,
          0.63446902,
          0.6304335533333333,
          0.6681737766666668,
          0.9727701333333334,
          0.9884044966666666,
          0.9870729033333333,
          0.96953265,
          0.9480386366666668,
          0.9582582966666666,
          0.7133364566666667,
          0.7093055566666666,
          0.8978519500000001,
          0.9085425866666667,
          0.9650468333333334,
          0.97733628,
          0.7120396933333333,
          0.9461371166666667,
          0.9690943399999999,
          0.7254210066666666,
          0.9402434833333334,
          0.4127457633333333,
          0.9653759133333333,
          0.9641568,
          0.7760778733333332,
          0.78526907,
          0.46540279500000004,
          0.9445929766666668,
          0.8994416900000001,
          0.9719820933333333,
          0.41875642333333335,
          0.9604271999999999,
          0.9877819200000001,
          0.9847017299999999,
          0.9903804933333333,
          0.9854528666666668,
          0.9854502233333333,
          0.9976438133333333,
          0.99786772,
          0.9975916533333334,
          0.5433321866666666,
          0.9959403666666667,
          0.9985385999999999,
          0.9987494766666667,
          0.9433250666666666,
          0.9161921166666667,
          0.9263694,
          0.7897885333333333,
          0.9503397133333333,
          0.5205686700000001,
          0.9756978033333333,
          0.9440448333333333,
          0.9245847700000001,
          0.9908687,
          0.8953496133333333,
          0.9568811966666666,
          0.8834938166666667,
          0.9486620833333334,
          0.9386723,
          0.97098525,
          0.9874734066666666,
          0.9887037333333334,
          0.9790130733333333,
          0.9849203033333334,
          0.9743574033333333,
          0.9927760466666666,
          0.9911057666666666,
          0.8617783499999999,
          0.9850940966666667,
          0.96920061,
          0.8755750966666667,
          0.96875952,
          0.9733027000000001,
          0.99537976,
          0.9842731499999999,
          0.9874244033333334,
          0.9929174333333334,
          0.6823130866666668,
          0.9932057466666667,
          0.9863572366666666,
          0.98393499,
          0.9951382666666667,
          0.9959855666666666,
          0.9862233466666667,
          0.7831753,
          0.83113644,
          0.9791386699999999,
          0.5796699966666666,
          0.9609743666666667,
          0.9588252066666666,
          0.9646058133333333,
          0.9466115199999999,
          0.9703115133333333,
          0.8110506466666667,
          0.99809671,
          0.9877681933333333,
          0.9859757033333333,
          0.9072963666666668,
          0.9784376633333333,
          0.97783367,
          0.9728046866666666,
          0.406100185,
          0.9949662300000001,
          0.9984613133333333,
          0.9968372799999999,
          0.8694371833333333,
          0.65913243,
          0.7439515999999999,
          0.81680033,
          0.9998733466666666,
          0.9953853566666666,
          0.9673309666666667,
          0.9996583133333333,
          0.9998179866666667,
          0.9998825266666667,
          0.9563040733333333,
          0.9974031600000001,
          0.9573288866666667,
          0.7053995233333333,
          0.9989682666666666,
          0.9992467766666667,
          0.9993164499999999,
          0.9932375333333333,
          0.9991060666666667,
          0.9968583333333333,
          0.9665168533333333,
          0.9989407666666666,
          0.9988933166666666,
          0.9944124333333333,
          0.9913063000000001,
          0.9706783866666667,
          0.99003202,
          0.9957777466666666,
          0.8302493466666666,
          0.42019191,
          0.9953704566666666,
          0.90064774,
          0.8419666666666666,
          0.9938274166666666,
          0.99434048,
          0.9967921333333334,
          0.9939450666666666,
          0.7616946866666666,
          0.8586004033333333,
          0.9945421900000001,
          0.7236369333333333,
          0.8731152233333334,
          0.9842375133333334,
          0.9949490966666666,
          0.9977136,
          0.9996750866666666,
          0.9989948666666666,
          0.9981250533333332,
          0.9990869466666666,
          0.9984753966666666,
          0.9969812833333332,
          0.9927690266666667,
          0.9995441166666668,
          0.9958355933333333,
          0.9976129666666665,
          0.9990781666666667,
          0.9981597566666668,
          0.9990550333333333,
          0.99924427,
          0.9038798199999999,
          0.9061043,
          0.99772473,
          0.9894459533333334,
          0.5743229733333334,
          0.6415560533333333,
          0.47711852000000005,
          0.8451043833333333,
          0.7155164466666667,
          0.33643368,
          0.3818321633333333,
          0.7868195333333333,
          0.9682678433333333,
          0.5734641533333333,
          0.6785910233333334,
          0.8923081700000001,
          0.27367172333333334,
          0.9516888066666667,
          0.9964413899999999,
          0.9995942433333335,
          0.9986627333333334,
          0.9691364466666667,
          0.9988839666666666,
          0.9998097333333332,
          0.8435500333333333,
          0.9013976499999999,
          0.8794745333333333
         ]
        },
        {
         "box": {
          "visible": true
         },
         "fillcolor": "rgb(13,127,66)",
         "line": {
          "color": "black"
         },
         "marker": {
          "color": "black",
          "size": 2
         },
         "meanline": {
          "visible": false
         },
         "name": "h3k36me3",
         "points": "all",
         "showlegend": false,
         "spanmode": "hard",
         "type": "violin",
         "x": [
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2
         ],
         "y": [
          0.9983597999999999,
          0.9951467633333334,
          0.9967049,
          0.9814994533333333,
          0.9791922,
          0.9955326133333333,
          0.9984450633333334,
          0.99126981,
          0.96923768,
          0.9768503333333333,
          0.9915791866666667,
          0.9325708,
          0.98625712,
          0.7874325799999999,
          0.54362885,
          0.9860088033333333,
          0.9511119366666666,
          0.9232124333333332,
          0.9932437133333334,
          0.8794145533333334,
          0.98760951,
          0.9834235466666668,
          0.99732295,
          0.97756131,
          0.9571129933333333,
          0.9937834100000001,
          0.9945961366666666,
          0.9945486,
          0.9913343499999999,
          0.9815827433333334,
          0.9619207866666667,
          0.9980847633333333,
          0.9969847933333332,
          0.9876805333333333,
          0.96759038,
          0.8653080666666666,
          0.8902671099999999,
          0.9782651266666665,
          0.9985104833333334,
          0.9772507866666666,
          0.9878435333333333,
          0.9987565233333333,
          0.9963850166666667,
          0.9956741600000001,
          0.9735604333333333,
          0.9721450666666667,
          0.9238819433333333,
          0.9988380233333333,
          0.9931692566666667,
          0.9778669333333333,
          0.8501841,
          0.9476786333333332,
          0.9962887333333333,
          0.9968221866666668,
          0.9247613966666667,
          0.9256964999999999,
          0.82827382,
          0.99736168,
          0.98491273,
          0.99359991,
          0.9786731,
          0.9968544166666667,
          0.98008228,
          0.9964218633333334,
          0.9893308566666666,
          0.9975944866666667,
          0.9893557333333334,
          0.8453127433333334,
          0.9829317733333333,
          0.9769066433333333,
          0.9943567366666667,
          0.43964638,
          0.9934653999999999,
          0.92409797,
          0.9459546799999999,
          0.9640893833333334,
          0.5935151799999999,
          0.81073691,
          0.9923149666666666,
          0.8093963866666667,
          0.9873312200000001,
          0.9785483199999999,
          0.9746243,
          0.9968776,
          0.9987385099999999,
          0.9856824566666665,
          0.9446146666666667,
          0.9404532099999999,
          0.9840084933333334,
          0.9848545666666667,
          0.9895952533333333,
          0.9847854566666667,
          0.9881844533333334,
          0.9705092533333334,
          0.9859548233333334,
          0.9977622833333334,
          0.9769518933333333,
          0.8346858666666667,
          0.9863636833333334,
          0.9932950133333334,
          0.97133237,
          0.9648860833333334,
          0.9935919333333333,
          0.9595562833333333,
          0.9671666266666668,
          0.9919203200000001,
          0.9979047799999999,
          0.9745629433333334,
          0.8992444833333333,
          0.9400583366666666,
          0.9827735566666668,
          0.99927713,
          0.9941686666666666,
          0.9904175199999999,
          0.7050634499999999,
          0.91906542,
          0.96745284,
          0.9786404666666666,
          0.9476233333333334,
          0.9588023,
          0.9849619233333332,
          0.83015889,
          0.9743545333333333,
          0.9795758666666666,
          0.9911284333333333,
          0.8157113033333333,
          0.9869885066666666,
          0.9316483999999999,
          0.9785939466666665,
          0.95143215,
          0.9963831500000001,
          0.9954600966666667,
          0.9514353833333334,
          0.9861977333333334,
          0.98959048,
          0.6783612899999999,
          0.9980457899999999,
          0.8701278800000001,
          0.97716526,
          0.9956321333333333,
          0.99213321,
          0.9874482566666667,
          0.9939920733333333,
          0.9202847433333333,
          0.9940100333333334,
          0.9901624466666666,
          0.9857483333333333,
          0.9989552266666667,
          0.9993996466666667,
          0.9355748133333334,
          0.9910962133333333,
          0.9939128266666666,
          0.9901270866666666,
          0.9294567933333333,
          0.9997554966666667,
          0.9736881199999999,
          0.9827160199999999,
          0.9920774533333333,
          0.995143,
          0.9894556566666667,
          0.9932665533333332,
          0.99316614,
          0.9859348866666666,
          0.9922121666666667,
          0.8546016700000001,
          0.99721211,
          0.9995868899999999,
          0.9882326,
          0.9882834133333334,
          0.99280242,
          0.9938371633333333,
          0.9830704499999999,
          0.99223253,
          0.9940748866666667,
          0.9283448766666668,
          0.9944733666666666,
          0.9759488533333333,
          0.9999934466666667,
          0.9935630166666667,
          0.9905295133333333,
          0.9877146466666668,
          0.99518768,
          0.9783486333333333,
          0.9870218166666667,
          0.9870834133333334,
          0.9834135233333333,
          0.9952545599999999,
          0.9998920833333335,
          0.9983140266666667,
          0.45424300333333334,
          0.85868018,
          0.87694085,
          0.99349491,
          0.58735883,
          0.7388083,
          0.9965229,
          0.95956841,
          0.9784436033333334,
          0.9286224566666667,
          0.9445631233333334,
          0.9597175133333332,
          0.9782237333333333,
          0.9909082233333333,
          0.99485722,
          0.9825506333333333,
          0.9986719333333333,
          0.9915140466666666,
          0.9980242433333334,
          0.7602877100000001,
          0.9977465666666667,
          0.9942165900000001,
          0.9210374133333333,
          0.39648374000000003,
          0.9907846333333333,
          0.6942232,
          0.98501194,
          0.7641477333333334,
          0.9923992233333333,
          0.9981093133333333,
          0.9653779,
          0.9719745333333334,
          0.9886655533333334,
          0.9496692000000001,
          0.9844787666666667,
          0.9969045400000001,
          0.9977837466666667,
          0.9967614433333334,
          0.9966665,
          0.9989210766666666,
          0.9992762166666666,
          0.9583216866666667,
          0.9905665166666667,
          0.9936302666666667,
          0.9966414466666667,
          0.99926753,
          0.9971709133333334,
          0.9984957799999999,
          0.9403903666666666,
          0.9977710466666667,
          0.9986025333333334,
          0.9937851000000001,
          0.9977118533333332,
          0.9877314333333334,
          0.9919295233333333,
          0.9969712833333334,
          0.9867360966666666,
          0.9685467499999999,
          0.8820383366666666,
          0.9988748433333333,
          0.9726473999999999,
          0.9973343900000001,
          0.9996321033333334,
          0.8941299866666667,
          0.8186745833333333,
          0.9540825866666666,
          0.67581068,
          0.9913397333333333,
          0.9979766099999999,
          0.99704096,
          0.99896935,
          0.9924109166666666,
          0.9897787666666668,
          0.5294812666666667,
          0.9746130333333335,
          0.9917416600000001,
          0.9088911333333334,
          0.8694414866666667,
          0.9864991700000001,
          0.9571526766666668,
          0.9453751133333334,
          0.9919922333333333,
          0.97172812,
          0.8256204199999999,
          0.9982337466666666,
          0.9947408666666666,
          0.9978802866666667,
          0.9917479166666666,
          0.7999686,
          0.6382070533333334,
          0.9967350466666667,
          0.9990501966666666,
          0.9664329666666666,
          0.9976949300000001,
          0.8590646666666667,
          0.9944037,
          0.84464775,
          0.9190907733333334,
          0.94596542,
          0.9962160766666667,
          0.9373654133333332,
          0.9984322766666667,
          0.96626205,
          0.81552408,
          0.9936757366666665,
          0.6458318133333333,
          0.99653223,
          0.7750736466666668,
          0.6398898333333333,
          0.9587622166666666,
          0.9976339066666666,
          0.9504472133333334,
          0.7453985900000001,
          0.8873849633333334,
          0.9556740666666667,
          0.9089157833333333,
          0.9655697666666666,
          0.9147774366666667,
          0.6364178266666666,
          0.9679429,
          0.9258336666666667,
          0.9758876266666666,
          0.98903997,
          0.8684240533333334,
          0.9881713799999999,
          0.9085085999999999,
          0.9842088333333333,
          0.98315567,
          0.8995240466666666,
          0.8714449633333333,
          0.9758514633333334,
          0.99120369,
          0.9967026166666667,
          0.9091225933333332,
          0.9677217133333333,
          0.7151854666666666,
          0.9859869666666666,
          0.9588902866666666,
          0.9714052766666667,
          0.9693974533333334,
          0.9776010966666666,
          0.9941695666666667,
          0.9738323000000001,
          0.9999908633333333,
          0.9998888766666667,
          0.9999291499999999,
          0.9999539466666666,
          0.9992156333333333,
          0.99993981,
          0.9999300333333334,
          0.95680882,
          0.99541979,
          0.9854747033333333,
          0.9990876666666666,
          0.9982258333333333,
          0.9725617999999999,
          0.8732897300000001,
          0.9799973066666666,
          0.97765134,
          0.7020621,
          0.9987526,
          0.9862945000000001,
          0.9820485333333333,
          0.9912063933333334,
          0.9628557966666667,
          0.8800380166666667,
          0.97747925,
          0.9538298433333333,
          0.9794513333333333,
          0.9999352199999999,
          0.9946078133333334,
          0.9996999133333334,
          0.9983175733333334,
          0.99915172,
          0.9999100866666667,
          0.9982049366666667,
          0.98416548,
          0.99896248,
          0.9994447233333332,
          0.99939132,
          0.9828586666666667,
          0.9993579,
          0.9890662833333334,
          0.9730954333333334,
          0.99582945,
          0.9954894866666666,
          0.9921342699999999,
          0.96635838,
          0.9967089666666666,
          0.9990043166666668,
          0.9968955533333334,
          0.9845107799999999,
          0.9846295866666667,
          0.9978950866666666,
          0.8941299266666668,
          0.99919985,
          0.9569326866666666,
          0.9990394433333334,
          0.99020919,
          0.9874577233333334,
          0.9931063333333334,
          0.9921292899999999,
          0.94954392,
          0.99471429,
          0.9971697333333335,
          0.9986151799999999,
          0.9395379833333334,
          0.6930944299999999,
          0.756671425,
          0.9830911366666667,
          0.9964217,
          0.9971226933333334,
          0.9995589666666667,
          0.9983389766666667,
          0.9996898666666666,
          0.9596714999999999,
          0.9921451466666666,
          0.9898062666666667,
          0.7161113233333333,
          0.9990947666666666,
          0.9988066533333333,
          0.9989947433333334,
          0.9989540199999999,
          0.99716258,
          0.9976403133333333,
          0.9850068333333333,
          0.9967126433333333,
          0.9951604666666666,
          0.9969319866666666,
          0.9834970799999999,
          0.9031438666666666,
          0.93958138,
          0.9989260999999999,
          0.9946681866666666,
          0.9885991233333332,
          0.94657518,
          0.9937398566666666,
          0.7905625000000001,
          0.9987393866666667,
          0.9993607333333333,
          0.9982659133333334,
          0.9671980666666666,
          0.9537774899999999,
          0.9799988766666666,
          0.98863332,
          0.70701355,
          0.9602379966666668,
          0.9904660033333333,
          0.9669595466666667,
          0.9966755333333334,
          0.98096798,
          0.99686845,
          0.9982067,
          0.9635964333333332,
          0.9973359466666666,
          0.9901077033333333,
          0.9967712999999999,
          0.96401773,
          0.8805070733333333,
          0.9883259266666666,
          0.9279297666666667,
          0.9987971333333334,
          0.9937503900000001,
          0.9993984533333333,
          0.7108810066666668,
          0.6775713366666666,
          0.5421101566666666,
          0.9652717566666666,
          0.6970082333333334,
          0.9877647633333333,
          0.9982872333333334,
          0.6339351666666667,
          0.8343036333333332,
          0.48206957,
          0.6727694966666666,
          0.9654290933333334,
          0.9863757166666667,
          0.85851668,
          0.5132675233333334,
          0.7414879666666666,
          0.8844387733333333,
          0.48193692499999996,
          0.541069485,
          0.99489567,
          0.6437497633333333,
          0.9831016866666666,
          0.9822753,
          0.9104992666666667,
          0.9968385666666667,
          0.9849202233333333,
          0.9873547133333332,
          0.9857934866666667,
          0.9216918500000001,
          0.9968279833333332,
          0.9963591333333334,
          0.9992212666666666,
          0.7473673366666667,
          0.9978021033333334
         ]
        },
        {
         "box": {
          "visible": true
         },
         "fillcolor": "rgb(236,232,56)",
         "line": {
          "color": "black"
         },
         "marker": {
          "color": "black",
          "size": 2
         },
         "meanline": {
          "visible": false
         },
         "name": "h3k4me1",
         "points": "all",
         "showlegend": false,
         "spanmode": "hard",
         "type": "violin",
         "x": [
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3
         ],
         "y": [
          0.9993751,
          0.9989352566666666,
          0.9912537133333333,
          0.9996688133333334,
          0.9959343966666667,
          0.98044019,
          0.9951805833333333,
          0.99285002,
          0.9926788833333333,
          0.99715638,
          0.9880737833333333,
          0.9693312500000001,
          0.9565293800000001,
          0.9881275333333334,
          0.998097,
          0.9945134866666666,
          0.9759945633333333,
          0.9439407866666666,
          0.9911225566666667,
          0.9943221233333333,
          0.9952763533333333,
          0.9960376166666668,
          0.9985768166666666,
          0.9934818999999999,
          0.9892325133333334,
          0.98247654,
          0.9944191566666666,
          0.9958566433333335,
          0.98650767,
          0.9096985333333333,
          0.9684346866666665,
          0.7190332333333332,
          0.9772715,
          0.96777112,
          0.9991767466666667,
          0.99834856,
          0.9241624333333333,
          0.9928403,
          0.9770690333333333,
          0.99509893,
          0.9978846366666666,
          0.9995179866666666,
          0.8297561099999999,
          0.9979539000000001,
          0.9898733866666666,
          0.89413474,
          0.9988932866666667,
          0.99791328,
          0.9847862666666667,
          0.9949098833333334,
          0.9953658000000001,
          0.9963619433333334,
          0.9289557966666666,
          0.9962710333333334,
          0.99872322,
          0.9987543899999999,
          0.9623777466666666,
          0.9723915466666666,
          0.95218993,
          0.9992785333333334,
          0.9972511233333333,
          0.97743748,
          0.9444486333333333,
          0.9970500233333333,
          0.9846319,
          0.99693821,
          0.98040245,
          0.9991534866666667,
          0.9995118333333334,
          0.9606986566666667,
          0.9988775433333333,
          0.9366003966666666,
          0.9522982433333333,
          0.9972638333333333,
          0.9978861933333333,
          0.98127465,
          0.9532379633333333,
          0.9942300933333333,
          0.9018816333333334,
          0.9836281000000001,
          0.90499908,
          0.89967666,
          0.8276155,
          0.99431878,
          0.8532862,
          0.9876018466666666,
          0.9957898833333333,
          0.9910316899999999,
          0.9992498866666667,
          0.9996029700000001,
          0.9998694066666666,
          0.99022792,
          0.9997869666666667,
          0.9700248033333333,
          0.9951367333333333,
          0.97459162,
          0.9909357233333335,
          0.9920408766666666,
          0.9979276333333332,
          0.9784277466666667,
          0.9999884,
          0.9971609366666666,
          0.9072375166666666,
          0.9994466333333333,
          0.9965248566666668,
          0.9970786166666666,
          0.9763800799999999,
          0.9993432133333333,
          0.9844508433333333,
          0.9977052666666667,
          0.9819065433333334,
          0.9985844199999999,
          0.9745369,
          0.8518971,
          0.9955780666666666,
          0.9956058366666666,
          0.9990966133333333,
          0.9995426533333333,
          0.9985109233333334,
          0.9029315233333334,
          0.9863819533333333,
          0.9906565233333332,
          0.9855433166666666,
          0.9220932966666666,
          0.9915688666666668,
          0.9991607333333333,
          0.9936347199999999,
          0.9959270866666667,
          0.9988302966666667,
          0.9933638166666667,
          0.9189318166666666,
          0.9991320533333333,
          0.9949246766666667,
          0.9913766599999999,
          0.9911590133333333,
          0.9993695966666666,
          0.98982589,
          0.9954798666666665,
          0.9150356866666667,
          0.9961475966666667,
          0.9664217800000001,
          0.9727313166666667,
          0.9999552,
          0.9948309866666666,
          0.9993222833333334,
          0.9992387766666666,
          0.8988870433333332,
          0.99916618,
          0.9992940899999999,
          0.99815305,
          0.9781390333333334,
          0.9997503366666667,
          0.99768585,
          0.9851197466666667,
          0.9650069566666667,
          0.9998967,
          0.9990932933333333,
          0.9963375,
          0.9815079433333334,
          0.9994016800000001,
          0.99940512,
          0.9977062633333333,
          0.9994727866666667,
          0.9992224333333333,
          0.9997649333333333,
          0.99909318,
          0.9668382000000001,
          0.9989800666666667,
          0.9990972333333333,
          0.9977576300000001,
          0.9928238666666666,
          0.9995773533333333,
          0.9708383500000001,
          0.9999759899999999,
          0.9967932333333334,
          0.9998223433333333,
          0.9989120666666667,
          0.9953802666666666,
          0.99926841,
          0.9992384533333333,
          0.9996760133333332,
          0.9991913433333334,
          0.9976359133333333,
          0.9982384599999999,
          0.9984956666666666,
          0.86579417,
          0.9976541033333333,
          0.9929072900000001,
          0.9999823166666667,
          0.9987797466666667,
          0.9923616833333333,
          0.9996998666666667,
          0.9995008333333333,
          0.9985025200000001,
          0.9869263533333333,
          0.9948080666666668,
          0.9984397333333334,
          0.9995583333333333,
          0.9999079766666666,
          0.6780121133333333,
          0.86728003,
          0.99666482,
          0.98289961,
          0.9457711999999999,
          0.9847484666666667,
          0.9722303,
          0.9997744066666666,
          0.99447685,
          0.99989592,
          0.9397965200000001,
          0.9982133333333333,
          0.9937484333333333,
          0.99942559,
          0.9999460066666667,
          0.9634016766666665,
          0.9999424666666666,
          0.9999167,
          0.9818678666666667,
          0.9945640133333334,
          0.7585182466666667,
          0.9988111000000001,
          0.9989369466666668,
          0.9993901799999999,
          0.5682986666666666,
          0.9973129100000001,
          0.93556687,
          0.9969961,
          0.88631299,
          0.9987712833333333,
          0.99835361,
          0.9929137333333333,
          0.9981397133333333,
          0.99996484,
          0.9998953733333332,
          0.9960221833333334,
          0.9988098566666667,
          0.9993333666666667,
          0.9980521866666666,
          0.99930063,
          0.9996959333333333,
          0.9990712566666667,
          0.91037985,
          0.9959825466666666,
          0.9987658866666665,
          0.9989833333333333,
          0.99987248,
          0.9991694066666666,
          0.9998758099999999,
          0.8278396833333334,
          0.9990464566666667,
          0.9989651300000001,
          0.9985831333333333,
          0.99845554,
          0.9986113133333333,
          0.9998696533333332,
          0.9956244033333332,
          0.9913897466666667,
          0.9996100866666667,
          0.9851046333333334,
          0.99996209,
          0.9992439666666666,
          0.9996408266666666,
          0.99999555,
          0.9982963466666667,
          0.9978037466666666,
          0.9997903733333334,
          0.9914427199999999,
          0.9974599666666667,
          0.9943560333333333,
          0.9991589033333333,
          0.9981887,
          0.9994831933333334,
          0.9907327100000001,
          0.7733590333333332,
          0.9876994333333333,
          0.8387947466666666,
          0.9284169666666666,
          0.9125605999999999,
          0.5683729049999999,
          0.9990509466666667,
          0.9735867433333333,
          0.9802992333333332,
          0.9598930466666666,
          0.9979346866666666,
          0.9894361333333332,
          0.54460305,
          0.9446558666666666,
          0.9129249,
          0.9888383333333334,
          0.9865262333333334,
          0.9957868466666667,
          0.9734249666666667,
          0.9907692200000001,
          0.9856351999999999,
          0.9166528666666666,
          0.9978021466666668,
          0.9996438,
          0.9193060866666666,
          0.5569448850000001,
          0.99999491,
          0.66199838,
          0.99737257,
          0.9927081333333333,
          0.9922198366666667,
          0.9598445233333334,
          0.9980299333333335,
          0.9259257666666666,
          0.9967106666666666,
          0.9427862666666668,
          0.9761112133333333,
          0.9520865966666667,
          0.9974421866666666,
          0.9783868866666667,
          0.98415053,
          0.9926915666666667,
          0.8952424333333333,
          0.9808215633333334,
          0.9644032100000001,
          0.9781372199999999,
          0.9845729333333333,
          0.9446502200000001,
          0.8981846,
          0.9976367266666667,
          0.95568562,
          0.9914211533333334,
          0.9862165666666667,
          0.9746531833333334,
          0.7593129666666667,
          0.9819166133333334,
          0.9601405000000001,
          0.9597385666666667,
          0.94216195,
          0.8863305333333334,
          0.9721053533333333,
          0.7491762033333332,
          0.9755102466666666,
          0.9955824333333333,
          0.9872867533333333,
          0.9907051666666667,
          0.97350572,
          0.9996539333333333,
          0.95319828,
          0.9789125133333334,
          0.9722185,
          0.9448728466666667,
          0.9501709566666667,
          0.9182796333333334,
          0.8931501466666667,
          0.9204495,
          0.9997449433333333,
          0.9978463666666667,
          0.9579232666666666,
          0.9899915899999999,
          0.9365021833333333,
          0.9691739199999999,
          0.9855487966666666,
          0.7045749466666668,
          0.9964508733333334,
          0.9799611466666667,
          0.9975949666666667,
          0.97044327,
          0.9655370099999999,
          0.9976876433333333,
          0.9842744833333333,
          0.9664883,
          0.9989296233333333,
          0.9918392800000001,
          0.9999932033333333,
          0.9904467566666666,
          0.9999878466666666,
          0.9999909800000001,
          0.99999848,
          0.9988161433333334,
          0.9998937666666666,
          0.9996569766666666,
          0.9997617233333335,
          0.9945333399999999,
          0.9857517366666667,
          0.9512939366666666,
          0.9960968,
          0.9944069333333333,
          0.9031102999999999,
          0.9992092533333333,
          0.9967756533333333,
          0.9915928333333334,
          0.9993249333333334,
          0.98369349,
          0.99507665,
          0.9637857133333334,
          0.9941623433333332,
          0.9905279066666667,
          0.98861921,
          0.99983228,
          0.99993002,
          0.9997798566666667,
          0.9999795666666667,
          0.9999717,
          0.9985095133333334,
          0.99929652,
          0.9993394000000001,
          0.9248490333333333,
          0.9683688333333333,
          0.9998218866666667,
          0.9997216999999999,
          0.9998660366666666,
          0.9999351566666667,
          0.9999826366666666,
          0.9998696833333334,
          0.9988347000000001,
          0.9999283533333333,
          0.9999487333333335,
          0.9998615133333333,
          0.9998873666666667,
          0.9996406166666666,
          0.9999478666666667,
          0.9998984666666667,
          0.9998841133333333,
          0.9998452166666666,
          0.9988902999999999,
          0.9991514933333333,
          0.9998762999999999,
          0.9998922666666666,
          0.9998577333333333,
          0.9996470833333334,
          0.9997840833333335,
          0.9983931566666667,
          0.9995890599999999,
          0.9994123666666667,
          0.9990522833333334,
          0.9999137033333333,
          0.9998817433333334,
          0.9998879666666666,
          0.99941478,
          0.99981577,
          0.9989802166666667,
          0.99987028,
          0.9992757,
          0.9989244133333334,
          0.9999052,
          0.99507624,
          0.9996748333333333,
          0.9989378133333333,
          0.9968073866666667,
          0.999679,
          0.9988566766666667,
          0.9999383266666667,
          0.99980792,
          0.9987489766666666,
          0.9995901666666667,
          0.9989909333333333,
          0.9985981133333333,
          0.9999216733333333,
          0.9999022000000001,
          0.9992046733333333,
          0.9998772366666667,
          0.9999550166666666,
          0.9992912,
          0.9985805833333333,
          0.9989159166666667,
          0.9978090333333333,
          0.99927153,
          0.99959831,
          0.9988346166666666,
          0.9998754666666666,
          0.99882551,
          0.9997606533333334,
          0.9994266666666666,
          0.9992007833333334,
          0.9998263833333333,
          0.9997488900000001,
          0.9999079866666666,
          0.9988091666666667,
          0.99979562,
          0.9990825799999999,
          0.99779977,
          0.9992305833333334,
          0.9999700433333333,
          0.9990059666666666,
          0.9999173533333333,
          0.9995037333333334,
          0.9996502366666666,
          0.99957194,
          0.9999481000000001,
          0.9999000933333333,
          0.9991168300000001,
          0.99925768,
          0.9995921333333334,
          0.9991934499999999,
          0.9999500933333333,
          0.99879798,
          0.9995003333333333,
          0.9998845933333333,
          0.99986276,
          0.9998363666666666,
          0.9986949333333334,
          0.9997084466666667,
          0.99906929,
          0.9991247533333333,
          0.99985238,
          0.9989582666666666,
          0.9983893433333333,
          0.9980131,
          0.9996949033333333,
          0.9998835100000001,
          0.9998079466666666,
          0.9994681133333333,
          0.9985810866666668,
          0.9992237966666666,
          0.9999435766666666,
          0.9992353533333334,
          0.9999553666666667,
          0.9945324833333333,
          0.9911581666666667,
          0.9961015466666666,
          0.9998529166666666,
          0.9994982666666666,
          0.9998906333333334,
          0.9998202833333334,
          0.9998772033333333,
          0.9999567699999999,
          0.99974922,
          0.9999127333333333,
          0.99948733,
          0.9996015133333334,
          0.9975934333333334,
          0.9998449666666667,
          0.9999197300000001,
          0.9991918733333334,
          0.9990983299999999,
          0.9991816,
          0.9987893466666667,
          0.9998636799999999,
          0.9989176999999999,
          0.9997444766666667,
          0.9989950200000001,
          0.9993527333333333,
          0.9991492833333334,
          0.9993191333333332,
          0.99789198,
          0.99940018,
          0.99892739,
          0.9998157000000001,
          0.9987974199999999,
          0.9993453533333333,
          0.99970116,
          0.9996915,
          0.99885432,
          0.99987298,
          0.9996154766666666,
          0.9987538466666667,
          0.9991515666666667,
          0.9999633933333333,
          0.9993024833333334,
          0.9996668999999999,
          0.9998327300000001,
          0.9998981366666667,
          0.99963218,
          0.99902428,
          0.9998640766666668,
          0.9998458499999999,
          0.9996957700000001,
          0.9972349500000001,
          0.9999198466666667,
          0.9995939666666667,
          0.9982581433333334,
          0.9994129333333334,
          0.9992355266666667,
          0.9994339233333333,
          0.9999004533333333,
          0.9992578999999999,
          0.99972545,
          0.9989687266666666,
          0.9996727666666666,
          0.99876781,
          0.99920426,
          0.9970529566666667,
          0.99988172,
          0.9999408000000001,
          0.99985595,
          0.9998936633333333,
          0.9978491666666667,
          0.99992934,
          0.9995770299999999,
          0.9996828666666667,
          0.9960359333333333,
          0.9998428333333335,
          0.9944954866666667,
          0.9994946100000001,
          0.9987011733333334,
          0.9987434500000001,
          0.9997014200000001,
          0.9987260333333333,
          0.9788237000000001,
          0.9991824666666665,
          0.9992962,
          0.9992852666666666,
          0.9998239766666668,
          0.9987598200000001,
          0.9985252333333333,
          0.9987320566666668,
          0.9997433333333333,
          0.99918019,
          0.9995647433333333,
          0.9987870333333332,
          0.9986742333333334,
          0.9997848133333335,
          0.9995998466666666,
          0.9972592800000001,
          0.9990132800000001,
          0.98278183,
          0.9999520333333334,
          0.9991797033333333,
          0.9999605800000001,
          0.9995235833333332,
          0.9999287066666667,
          0.9985748,
          0.99975968,
          0.99985738,
          0.9998878833333333,
          0.99983859,
          0.9962057333333334,
          0.9996846233333333,
          0.9991545333333334,
          0.9998612933333333,
          0.9993481700000001,
          0.99884071,
          0.9987583,
          0.9989521866666667,
          0.9997644000000001,
          0.9992508933333334,
          0.9998901099999999,
          0.9994986233333334,
          0.99993964,
          0.9987362533333334,
          0.9992596666666667,
          0.9994689466666666,
          0.9991355500000001,
          0.998068,
          0.9997780666666666,
          0.9977074,
          0.9997264433333334,
          0.9999492833333333,
          0.9982519999999999,
          0.9966270466666667,
          0.99942558,
          0.9995832266666667,
          0.9998120666666667,
          0.9978328333333334,
          0.9996498433333333,
          0.9998536533333334,
          0.999216,
          0.9987583,
          0.9990816833333334,
          0.9997788333333334,
          0.9997304133333333,
          0.9988721666666667,
          0.9994761866666666,
          0.9985803166666667,
          0.9999031766666667,
          0.9997344333333333,
          0.9998373866666667,
          0.9990170666666667,
          0.9993903833333334,
          0.9998117166666667,
          0.9994833333333334,
          0.9996792633333333,
          0.9985382633333333,
          0.9991657533333335,
          0.9998699166666668,
          0.9998127499999999,
          0.8225480533333333,
          0.99998982,
          0.9998034066666667,
          0.99999094,
          0.9999936333333334,
          0.9999530466666666,
          0.9999420633333332,
          0.9999808433333333,
          0.9985471800000001,
          0.9969091333333333,
          0.9999737366666667,
          0.99992342,
          0.8336764133333334,
          0.693026935,
          0.85979565,
          0.9999747666666666,
          0.9999854333333333,
          0.99962668,
          0.9999830666666667,
          0.9999653933333333,
          0.9999090433333334,
          0.9999586666666667,
          0.99831541,
          0.99998062,
          0.9962139333333333,
          0.8134057866666667,
          0.9999311733333333,
          0.99978907,
          0.9999104466666666,
          0.99932345,
          0.9998626833333333,
          0.99965318,
          0.9950576433333334,
          0.9998146333333334,
          0.9996388733333333,
          0.9997696733333333,
          0.99857815,
          0.9898075533333334,
          0.9983213900000001,
          0.99975911,
          0.9580116233333333,
          0.9666514233333334,
          0.9991041333333334,
          0.9897570166666666,
          0.9888552,
          0.9990888666666667,
          0.9994443500000001,
          0.9992724700000001,
          0.9993729833333332,
          0.8668783333333333,
          0.9392287133333334,
          0.9994131533333332,
          0.67122823,
          0.8269260333333334,
          0.9835200133333334,
          0.9986989666666667,
          0.9988621133333334,
          0.9990635766666666,
          0.9985673,
          0.9995520433333334,
          0.9996100466666666,
          0.9989257666666665,
          0.9996772833333334,
          0.99853346,
          0.9956556333333334,
          0.9986534100000001,
          0.99935749,
          0.9996641299999999,
          0.9993681533333333,
          0.9993662466666667,
          0.9991902866666668,
          0.9999778633333333,
          0.7969076999999999,
          0.99997668,
          0.9998567399999999,
          0.9572603333333333,
          0.9997585333333333,
          0.9929848533333333,
          0.9609912733333333,
          0.8885340566666667,
          0.9974618666666667,
          0.998833,
          0.9905396266666667,
          0.6895987,
          0.9801337,
          0.9902682566666666,
          0.9993979066666667,
          0.9971668433333334,
          0.9149253033333333,
          0.9730576599999999,
          0.9929448666666666,
          0.9991089833333332,
          0.9772136033333334,
          0.9994017933333333,
          0.9988188466666666,
          0.9998757333333333,
          0.9984881533333333,
          0.99857804,
          0.9997239133333333,
          0.9999431333333334,
          0.9999797466666666,
          0.6323778866666667,
          0.9991571466666667
         ]
        },
        {
         "box": {
          "visible": true
         },
         "fillcolor": "rgb(238,30,37)",
         "line": {
          "color": "black"
         },
         "marker": {
          "color": "black",
          "size": 2
         },
         "meanline": {
          "visible": false
         },
         "name": "h3k4me3",
         "points": "all",
         "showlegend": false,
         "spanmode": "hard",
         "type": "violin",
         "x": [
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4
         ],
         "y": [
          0.9755845566666667,
          0.9920526733333334,
          0.89396809,
          0.9875121099999999,
          0.9628885199999999,
          0.9851967500000001,
          0.9494181233333333,
          0.8891695033333334,
          0.9839074833333333,
          0.9831023,
          0.9816855666666667,
          0.6524336333333333,
          0.9270237200000001,
          0.8910932666666667,
          0.89167428,
          0.8891195866666667,
          0.9876666333333333,
          0.9472308666666667,
          0.8378970333333333,
          0.9522117233333334,
          0.53146755,
          0.67419679,
          0.9079577733333334,
          0.9469904666666666,
          0.97904315,
          0.9807581500000001,
          0.99140452,
          0.9268952166666665,
          0.83210586,
          0.9625007099999999,
          0.9431514,
          0.9434188533333333,
          0.8301733200000001,
          0.9842936999999999,
          0.9587671999999999,
          0.9925663400000001,
          0.9837196566666667,
          0.87356336,
          0.9462778233333333,
          0.9106788666666666,
          0.9006337499999999,
          0.80669472,
          0.9128658666666666,
          0.9912831166666667,
          0.6308154633333333,
          0.97139422,
          0.972298,
          0.6302343,
          0.9791177666666666,
          0.9896106866666666,
          0.9305295666666668,
          0.9767077766666666,
          0.9901785133333334,
          0.9369388633333333,
          0.9445076833333333,
          0.9066649566666666,
          0.9718801133333334,
          0.9632939633333333,
          0.93005585,
          0.9743060066666667,
          0.9284412333333334,
          0.8634550666666666,
          0.9922184500000001,
          0.9386101333333333,
          0.9603924266666667,
          0.7457434166666667,
          0.9633591333333333,
          0.9087199333333333,
          0.9020735433333332,
          0.8556503133333333,
          0.9920263666666665,
          0.9901072333333333,
          0.8758025466666667,
          0.9676146233333333,
          0.9517946,
          0.9719744,
          0.9828626333333333,
          0.9845663333333333,
          0.9820388466666667,
          0.4738823466666666,
          0.9615677533333334,
          0.8621464666666666,
          0.9079458866666666,
          0.9248160566666667,
          0.76161007,
          0.9505273333333334,
          0.7200953,
          0.79579133,
          0.9754393933333333,
          0.9384182000000001,
          0.9434327533333334,
          0.9925267433333333,
          0.9896206433333333,
          0.9278893766666667,
          0.8564475500000001,
          0.9949987333333334,
          0.90457979,
          0.90099046,
          0.9478555533333334,
          0.9692509166666667,
          0.9348263500000001,
          0.9347308733333334,
          0.9607579833333334,
          0.99537398,
          0.9689731,
          0.8439029333333333,
          0.9785749466666666,
          0.9765259866666667,
          0.97974503,
          0.9266887233333333,
          0.984801,
          0.9087035533333333,
          0.8874916233333333,
          0.97525761,
          0.9394345,
          0.97984501,
          0.8992011933333334,
          0.6809988666666666,
          0.560662,
          0.8584610566666667,
          0.9750775966666666,
          0.7916567133333333,
          0.9400074466666667,
          0.9917631466666667,
          0.9856887,
          0.8946317433333334,
          0.8324739333333334,
          0.9321362833333332,
          0.9768576033333334,
          0.9580120500000001,
          0.7883264933333333,
          0.94831792,
          0.9769779766666667,
          0.9553970233333334,
          0.9403685133333334,
          0.8787893800000001,
          0.9866218333333333,
          0.62326136,
          0.9849540333333334,
          0.9697846066666665,
          0.9791090333333333,
          0.9588941266666667,
          0.96289079,
          0.9873726866666667,
          0.9322137666666667,
          0.9712885099999999,
          0.9309823333333332,
          0.6301708333333333,
          0.9651349233333333,
          0.901369,
          0.8221504266666667,
          0.9573652199999999,
          0.9920252333333334,
          0.9778249099999999,
          0.484787975,
          0.9571322533333334,
          0.9682809766666667,
          0.9727622633333333,
          0.9636624666666668,
          0.9784302500000001,
          0.9719423300000001,
          0.9777942466666666,
          0.99913908,
          0.9960536333333333,
          0.9866364233333332,
          0.9988134233333333,
          0.9667071666666667,
          0.99645683,
          0.9987233,
          0.9971895633333334,
          0.9899116466666666,
          0.99198288,
          0.9893919666666666,
          0.9962668,
          0.9971600666666666,
          0.9971249666666667,
          0.99623521,
          0.9244156666666666,
          0.99895018,
          0.9754450533333333,
          0.99630977,
          0.99794568,
          0.9991355500000001,
          0.99542181,
          0.8054735900000001,
          0.9993109499999999,
          0.9920270133333333,
          0.99236555,
          0.9824516666666666,
          0.9916543133333334,
          0.9938136433333334,
          0.9991416466666667,
          0.9876303666666666,
          0.9971645200000001,
          0.9776789333333333,
          0.99858621,
          0.8735161333333333,
          0.9854996666666667,
          0.9988289900000001,
          0.9989481766666667,
          0.9955271166666666,
          0.9946388066666666,
          0.9934200833333334,
          0.8819904733333334,
          0.9974087333333334,
          0.9895726933333333,
          0.994853,
          0.9980876733333334,
          0.99527518,
          0.32992924333333334,
          0.8849648,
          0.5188886666666667,
          0.9647852299999999,
          0.6815479433333334,
          0.8680328666666667,
          0.9340987866666667,
          0.62685866,
          0.9006665533333335,
          0.9937509499999999,
          0.45643731000000004,
          0.9681626066666666,
          0.99739301,
          0.9892517366666667,
          0.9924790666666666,
          0.9977223000000001,
          0.9958017199999999,
          0.9913935500000001,
          0.9705306466666667,
          0.9934958333333332,
          0.8520343499999999,
          0.99765157,
          0.99707081,
          0.9382562166666667,
          0.6183112033333332,
          0.97680263,
          0.26393454,
          0.9853006999999999,
          0.24358820000000003,
          0.9800429666666667,
          0.9989733900000001,
          0.7710573833333333,
          0.98623494,
          0.9952817899999999,
          0.9936098900000001,
          0.92883364,
          0.99914944,
          0.9966161433333333,
          0.9887736,
          0.9984988766666666,
          0.99871224,
          0.80473673,
          0.9720527433333332,
          0.9826775466666667,
          0.9909786333333334,
          0.9942464233333334,
          0.9993635366666668,
          0.9980034766666668,
          0.9974987666666667,
          0.8644314666666667,
          0.99887683,
          0.9980740666666666,
          0.9952306933333332,
          0.9980894666666668,
          0.9927301866666666,
          0.9631124999999999,
          0.98138015,
          0.5864131699999999,
          0.43548980666666665,
          0.9899464833333335,
          0.9901313333333333,
          0.9933687099999999,
          0.7971578966666666,
          0.99834982,
          0.9948780133333334,
          0.9784048266666666,
          0.9769504166666666,
          0.9880945366666666,
          0.96542,
          0.9968679333333333,
          0.6662999199999999,
          0.9890207333333333,
          0.86262838,
          0.76654319,
          0.9793365033333333,
          0.8561158566666668,
          0.7880571199999999,
          0.9839881866666667,
          0.96670413,
          0.9857121333333333,
          0.7749576333333333,
          0.74810052,
          0.6024645550000001,
          0.96428439,
          0.98735938,
          0.9301599466666666,
          0.9632651233333333,
          0.9221823333333333,
          0.9089195566666667,
          0.9063340666666667,
          0.9831187033333334,
          0.65656487,
          0.9896112966666667,
          0.72665389,
          0.9710602700000001,
          0.9112050433333333,
          0.9507262,
          0.9849776633333334,
          0.9874906733333333,
          0.9910375199999999,
          0.9261664533333334,
          0.9808474,
          0.5357416466666667,
          0.9886700666666667,
          0.89168308,
          0.51139,
          0.9721147499999999,
          0.7767080533333334,
          0.93888975,
          0.8148672966666667,
          0.9884838533333333,
          0.9907454333333333,
          0.9620380599999999,
          0.6710182000000001,
          0.79217003,
          0.68407013,
          0.9749186333333334,
          0.9680383533333333,
          0.9804190366666666,
          0.564890075,
          0.9454842866666667,
          0.9099728866666666,
          0.9384757533333333,
          0.92525345,
          0.9737405566666667,
          0.80351068,
          0.9272616666666668,
          0.9809418533333334,
          0.8588683100000001,
          0.8693085333333334,
          0.8404755733333333,
          0.9675261900000001,
          0.9935012,
          0.5637722000000001,
          0.9391392066666667,
          0.61281553,
          0.9607739466666666,
          0.987807,
          0.9441251466666666,
          0.3453532033333333,
          0.9539515466666666,
          0.9873742833333333,
          0.8884774766666667,
          0.92131749,
          0.9264350666666666,
          0.5721570166666666,
          0.9252305866666667,
          0.9862985333333333,
          0.9676763233333334,
          0.92417017,
          0.49931837333333334,
          0.8180326466666666,
          0.9581975866666667,
          0.9173657666666667,
          0.93967645,
          0.9385928666666666,
          0.9947936833333334,
          0.9647648666666667,
          0.80115472,
          0.9176813166666666,
          0.9378777033333333,
          0.9827819333333333,
          0.7886609233333334,
          0.6950645,
          0.9619472666666667,
          0.79331058,
          0.7521166266666667,
          0.9598259933333333,
          0.9847393666666666,
          0.9396403266666667,
          0.97685543,
          0.8495756966666667,
          0.9603781666666666,
          0.9231818333333334,
          0.9671889666666668,
          0.5960533666666666,
          0.9918647666666667,
          0.8172778733333333,
          0.8866536833333334,
          0.9866072,
          0.9621378666666667,
          0.8973961433333333,
          0.69687913,
          0.9733602533333333,
          0.30851097000000005,
          0.9928123566666667,
          0.9997689200000001,
          0.9983895633333333,
          0.62490565,
          0.9527203266666667,
          0.9975341566666667,
          0.8420348,
          0.9987561899999999,
          0.9970513166666667,
          0.99529845,
          0.9989128333333334,
          0.9922577833333334,
          0.564844465,
          0.9924832499999999,
          0.96884719,
          0.9866046666666666,
          0.9755719666666667,
          0.93493102,
          0.7344461866666667,
          0.9414697466666667,
          0.92519078,
          0.6899858633333333,
          0.9964212333333333,
          0.9887190866666667,
          0.9824669,
          0.9982804533333334,
          0.9424726133333333,
          0.59228426,
          0.5002769800000001,
          0.9605178133333334,
          0.98359143,
          0.9683619266666668,
          0.9996101466666666,
          0.9849616333333334,
          0.9980376266666666,
          0.9950314566666666,
          0.9930152233333334,
          0.9993866466666667,
          0.9964855,
          0.9516667466666666,
          0.9986648833333334,
          0.9976778433333333,
          0.9956309433333334,
          0.9733093833333334,
          0.9882812833333334,
          0.9552990399999999,
          0.9953087933333333,
          0.9547381533333333,
          0.9991003833333334,
          0.9899833766666667,
          0.9998234699999999,
          0.9993771333333333,
          0.9985024433333333,
          0.99894309,
          0.9910266766666666,
          0.98497991,
          0.98743916,
          0.9921502833333333,
          0.9909444333333334,
          0.99359333,
          0.9835009499999999,
          0.9553688766666667,
          0.9824635333333332,
          0.9927984666666667,
          0.98824072,
          0.9846730866666666,
          0.8651217133333334,
          0.9999924033333333,
          0.99980124,
          0.9999085133333333,
          0.9999944333333333,
          0.9999932500000001,
          0.9997478066666666,
          0.9998019,
          0.9265858866666666,
          0.9953376166666666,
          0.9990295866666666,
          0.9967898233333333,
          0.8427922966666667,
          0.9389327666666668,
          0.8630668666666667,
          0.9998079633333333,
          0.99993686,
          0.9991693433333334,
          0.9997646066666667,
          0.99983338,
          0.9999506833333333,
          0.9999665466666667,
          0.9927304133333333,
          0.98003772,
          0.9543818666666667,
          0.62600708,
          0.9965484333333334,
          0.9971521333333332,
          0.99887556,
          0.99125158,
          0.9950479666666666,
          0.9965353833333334,
          0.9443652533333333,
          0.9972309633333333,
          0.9979443466666668,
          0.9984268633333334,
          0.9963968,
          0.97574995,
          0.9973750833333334,
          0.9963535433333334,
          0.9527518433333334,
          0.9491628899999999,
          0.9994040766666666,
          0.9944399533333333,
          0.9898112333333334,
          0.9992901666666668,
          0.9981716433333334,
          0.9989099333333332,
          0.9991089999999999,
          0.9853483066666667,
          0.9900961933333333,
          0.9987743133333332,
          0.7268853000000001,
          0.9920487800000001,
          0.97399106,
          0.99852475,
          0.9990878533333333,
          0.99739529,
          0.9994754833333334,
          0.9983624,
          0.99819311,
          0.99747161,
          0.9988908333333333,
          0.99932762,
          0.9877374099999999,
          0.9396444366666667,
          0.98932122,
          0.9854645,
          0.9970549666666667,
          0.9988206833333333,
          0.9992946333333332,
          0.8875352266666666,
          0.8162121333333333,
          0.7815161666666667,
          0.9955540666666667,
          0.93125887,
          0.6831017433333333,
          0.9843888199999999,
          0.7624263433333333,
          0.82162331,
          0.99210023,
          0.35190324,
          0.61345883,
          0.9483147566666666,
          0.6010487800000001,
          0.731349425,
          0.9809496666666666,
          0.9946935166666666,
          0.9578809266666667,
          0.6364258533333333,
          0.8921557933333334,
          0.8586981366666667,
          0.99051835,
          0.9269801566666667,
          0.8506193133333334,
          0.6863007333333333,
          0.9878817433333333,
          0.9937839766666667,
          0.82265938,
          0.9893329866666667,
          0.9971328866666666,
          0.9991083,
          0.9993455299999999,
          0.9970751099999999,
          0.9997714833333333,
          0.9997186333333333,
          0.47631113666666663,
          0.88024804,
          0.34545974333333335,
          0.92144878
         ]
        },
        {
         "box": {
          "visible": true
         },
         "fillcolor": "rgb(144,206,219)",
         "line": {
          "color": "black"
         },
         "marker": {
          "color": "black",
          "size": 2
         },
         "meanline": {
          "visible": false
         },
         "name": "h3k9me3",
         "points": "all",
         "showlegend": false,
         "spanmode": "hard",
         "type": "violin",
         "x": [
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5
         ],
         "y": [
          0.88933742,
          0.6036613833333333,
          0.9814392966666666,
          0.9727554666666668,
          0.9241630399999999,
          0.9968832366666667,
          0.9809233766666666,
          0.9444170033333333,
          0.91531757,
          0.9792614166666667,
          0.95761672,
          0.8899421833333333,
          0.8916956766666667,
          0.8973679333333333,
          0.8947459499999999,
          0.9551393666666667,
          0.9877490866666667,
          0.7234250866666666,
          0.9436910666666667,
          0.8568064233333333,
          0.9617940333333334,
          0.9015133966666666,
          0.8669161366666667,
          0.94862155,
          0.9216895800000001,
          0.9558774333333333,
          0.9394540333333333,
          0.9488868666666667,
          0.8971707800000001,
          0.7887189333333334,
          0.9877463400000001,
          0.496311395,
          0.8427889333333334,
          0.8180797333333333,
          0.6999094499999999,
          0.67205111,
          0.8737033233333333,
          0.8872959566666667,
          0.9857923766666666,
          0.8377150433333332,
          0.98146491,
          0.9116903933333332,
          0.9889195733333334,
          0.8908791666666667,
          0.8685754600000001,
          0.9144572466666666,
          0.96042252,
          0.9858135166666666,
          0.9843934266666666,
          0.61748435,
          0.83361558,
          0.6959660666666667,
          0.9651129300000001,
          0.9908984433333333,
          0.9170682766666666,
          0.9909418899999999,
          0.9938390333333333,
          0.8992235700000001,
          0.9816910933333333,
          0.8872663533333333,
          0.9456512333333333,
          0.9065313633333334,
          0.9513423200000001,
          0.97389782,
          0.9680538599999999,
          0.99649472,
          0.37936008,
          0.9859606900000001,
          0.9847442333333333,
          0.98787463,
          0.9651737466666667,
          0.8300513199999999,
          0.9734897166666666,
          0.9946904,
          0.9727618866666666,
          0.9494086933333333,
          0.98910099,
          0.94723009,
          0.9960679533333333,
          0.95377078,
          0.8899938666666666,
          0.9785898999999999,
          0.9594937,
          0.9516393533333334,
          0.9778835899999999,
          0.9295074933333334,
          0.8835499966666666,
          0.98423682,
          0.9724116466666667,
          0.7577521766666667,
          0.9802381033333333,
          0.9541911933333335,
          0.8599336000000001,
          0.9442333999999999,
          0.3866087,
          0.9441018900000001,
          0.9529052866666667,
          0.9323264966666667,
          0.9547664666666668,
          0.9547006166666666,
          0.8296105300000001,
          0.9538895666666667,
          0.92241465,
          0.88826588,
          0.29174497,
          0.9399443,
          0.9812686533333334,
          0.88895043,
          0.9497361966666666,
          0.7989628666666667,
          0.9553571766666668,
          0.9059804866666666,
          0.66922358,
          0.7553014466666667,
          0.9028959099999999,
          0.9685571533333333,
          0.6912854333333334,
          0.9736610466666668,
          0.9386306166666666,
          0.9138793266666667,
          0.64699757,
          0.9036920466666666,
          0.8793542766666667,
          0.9494544133333332,
          0.94589998,
          0.9510684833333333,
          0.54814191,
          0.7144593533333333,
          0.9132911766666667,
          0.9297326,
          0.9920422666666666,
          0.9192284599999999,
          0.8845779833333335,
          0.98173941,
          0.9114803866666666,
          0.9069911999999999,
          0.9889649,
          0.87996365,
          0.8144924333333333,
          0.96350006,
          0.8879540833333334,
          0.9784277833333334,
          0.9280521199999999,
          0.7816184333333333,
          0.9580800766666666,
          0.9663309866666667,
          0.9138992566666667,
          0.9318359266666666,
          0.9973351566666667,
          0.9440505433333333,
          0.9686419666666667,
          0.9320386333333334,
          0.9520598366666667,
          0.8745045466666667,
          0.9127331766666668,
          0.9244304766666667,
          0.9315888333333332,
          0.9688355566666665,
          0.8915286933333334,
          0.99915495,
          0.9961130800000001,
          0.8316673466666668,
          0.9576300466666666,
          0.9654182633333334,
          0.9354548233333334,
          0.7532696666666666,
          0.63096713,
          0.8216853833333334,
          0.56503148,
          0.9931554,
          0.8956452,
          0.9209576666666667,
          0.9453749,
          0.7127546333333333,
          0.8402365333333334,
          0.8522664833333332,
          0.7851223266666666,
          0.9265003433333333,
          0.99813863,
          0.8635198166666666,
          0.36524518,
          0.6859314,
          0.34584681500000003,
          0.9311583750000001,
          0.6966447666666667,
          0.45707948499999995,
          0.280589895,
          0.55711725,
          0.8150796233333333,
          0.7639155766666667,
          0.53829804,
          0.9525852466666667,
          0.8924728200000001,
          0.8350971333333334,
          0.8905815166666667,
          0.9175657199999999,
          0.9523543800000001,
          0.9889703566666667,
          0.9371858533333333,
          0.87300639,
          0.43878665,
          0.9585643566666667,
          0.8898604,
          0.4871632000000001,
          0.9516504,
          0.40612329,
          0.9674604533333334,
          0.27205811999999996,
          0.9647842566666668,
          0.8749257633333333,
          0.8335058933333332,
          0.8876385566666668,
          0.6303670433333334,
          0.8530636033333333,
          0.9687825533333333,
          0.9621513500000001,
          0.9886183966666667,
          0.9877026,
          0.9872294666666667,
          0.9983670666666667,
          0.9914014999999999,
          0.6018310666666666,
          0.8947260466666668,
          0.9864175,
          0.9895660833333334,
          0.9948509433333333,
          0.99816849,
          0.958759,
          0.79210031,
          0.9990040033333334,
          0.9883591800000001,
          0.9386509333333333,
          0.9911007166666668,
          0.9805838866666666,
          0.9148319533333332,
          0.9951515299999999,
          0.6467439333333332,
          0.9467476,
          0.5282034766666667,
          0.98730807,
          0.7215072100000001,
          0.96883361,
          0.28579324333333334,
          0.9196949866666667,
          0.8457565833333334,
          0.8762456633333334,
          0.8317779066666667,
          0.991444,
          0.9974120333333333,
          0.99312457,
          0.9498962333333334,
          0.468940305,
          0.8522810966666666,
          0.9094114000000001,
          0.8276330333333334,
          0.7804700666666666,
          0.8256942733333333,
          0.9270126433333333,
          0.7113022399999999,
          0.90159238,
          0.9589081,
          0.8375962100000001,
          0.9152409333333335,
          0.93130335,
          0.77217205,
          0.65156919,
          0.7167575433333333,
          0.9399695566666667,
          0.9740394766666668,
          0.9225277166666667,
          0.7452256199999999,
          0.75289745,
          0.8254552500000001,
          0.54342185,
          0.9694044333333333,
          0.489160935,
          0.3500304166666666,
          0.9051477333333334,
          0.8094316533333333,
          0.83993784,
          0.9530326499999999,
          0.9632957000000001,
          0.9829502433333334,
          0.9226214,
          0.6583465199999999,
          0.8024298666666666,
          0.5322784500000001,
          0.8534806466666667,
          0.9688893333333333,
          0.8797197433333332,
          0.6864727233333333,
          0.88425936,
          0.9265880333333333,
          0.6762142466666666,
          0.8315819866666666,
          0.8638963333333334,
          0.60248225,
          0.8812761333333333,
          0.9377719166666667,
          0.8882755566666667,
          0.3373665266666667,
          0.8646200266666667,
          0.78008366,
          0.9532339,
          0.27415151,
          0.94245293,
          0.9217940666666666,
          0.8726527900000001,
          0.9428974,
          0.45702344333333333,
          0.9649861333333334,
          0.9695591566666666,
          0.9710295666666666,
          0.841098,
          0.8440294833333333,
          0.90366761,
          0.7629786766666666,
          0.9349858366666667,
          0.9799049866666666,
          0.8757755566666666,
          0.8029181166666666,
          0.9666453966666667,
          0.7437008833333333,
          0.9478760733333335,
          0.85058268,
          0.9396446466666667,
          0.8079178100000001,
          0.8488392666666668,
          0.8136137466666667,
          0.9260476566666668,
          0.8535379333333334,
          0.9742717766666665,
          0.5219284000000001,
          0.9798276333333332,
          0.9836507666666666,
          0.9956393,
          0.9516586866666666,
          0.9100055766666667,
          0.6571628533333334,
          0.88234095,
          0.8256842,
          0.58117966,
          0.9698819100000001,
          0.9822379133333333,
          0.98335151,
          0.9618510666666666,
          0.8711799066666667,
          0.5473847633333334,
          0.9746364466666666,
          0.9665327466666667,
          0.9289264866666667,
          0.9931661433333333,
          0.9336490866666667,
          0.9744674833333332,
          0.9972589300000001,
          0.9669912866666667,
          0.9923549633333334,
          0.9940874366666667,
          0.9744499766666667,
          0.9955549933333333,
          0.99672348,
          0.9676834666666667,
          0.99383994,
          0.9893938366666667,
          0.9949801633333334,
          0.98944449,
          0.9691295333333333,
          0.9848603300000001,
          0.9816654233333333,
          0.4747978033333333,
          0.9795083433333334,
          0.9772412533333332,
          0.9894950433333333,
          0.9950823099999999,
          0.9967411899999999,
          0.9730920633333332,
          0.9384622566666666,
          0.9717411866666666,
          0.9785715233333333,
          0.9874563,
          0.88830507,
          0.9587522533333334,
          0.9725308833333334,
          0.9521803000000001,
          0.91246029,
          0.9468401333333333,
          0.9905925766666667,
          0.9901961766666666,
          0.8286663166666667,
          0.836617175,
          0.8311064533333333,
          0.9743763466666667,
          0.9820384999999999,
          0.9978210133333333,
          0.9918285333333333,
          0.9871642666666668,
          0.99826668,
          0.9951690566666667,
          0.96739732,
          0.9748187033333333,
          0.9767320833333333,
          0.8355127866666666,
          0.9952450066666666,
          0.9978257666666667,
          0.9977216999999999,
          0.97448625,
          0.9960881366666667,
          0.98803741,
          0.9569300566666666,
          0.99121329,
          0.9279378,
          0.9905105633333333,
          0.8898968333333332,
          0.9646681766666667,
          0.9091137266666666,
          0.95133412,
          0.9653722833333332,
          0.6728744133333334,
          0.9503689,
          0.9841801800000001,
          0.9647802200000001,
          0.7318038033333334,
          0.9950548133333333,
          0.9766195,
          0.9148839866666667,
          0.372439405,
          0.9849229466666666,
          0.84237486,
          0.88376661,
          0.9852832133333335,
          0.9328672,
          0.9569736666666667,
          0.9972269666666667,
          0.9971835466666666,
          0.9910064966666666,
          0.9940674366666666,
          0.94769924,
          0.9993274,
          0.9767130700000001,
          0.99154844,
          0.9255097,
          0.9659896466666668,
          0.9985239,
          0.95193031,
          0.9971623466666667,
          0.8885986533333333,
          0.9983531533333334,
          0.91912015,
          0.714694175,
          0.7940699233333333,
          0.565003165,
          0.8456388,
          0.6642498933333334,
          0.6196154733333333,
          0.4585171,
          0.5594913866666666,
          0.618800415,
          0.5852608666666667,
          0.7915440466666667,
          0.5429835233333333,
          0.48466531333333335,
          0.5306197466666667,
          0.93339673,
          0.7840202199999999,
          0.8677400666666667,
          0.8487822533333333,
          0.5109510033333333,
          0.95475552,
          0.9310310866666667,
          0.92310294,
          0.9438282166666667,
          0.8213439466666667,
          0.6309511333333333
         ]
        },
        {
         "box": {
          "visible": true
         },
         "fillcolor": "rgb(30,30,30)",
         "line": {
          "color": "black"
         },
         "marker": {
          "color": "black",
          "size": 2
         },
         "meanline": {
          "visible": false
         },
         "name": "input",
         "points": "all",
         "showlegend": false,
         "spanmode": "hard",
         "type": "violin",
         "x": [
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6
         ],
         "y": [
          0.20267743,
          0.491453,
          0.947898,
          0.7382399,
          0.35095435,
          0.48994333,
          0.33787772,
          0.600013,
          0.4860671,
          0.342758,
          0.21667495,
          0.20402044,
          0.7524124,
          0.5934801,
          0.89888173,
          0.44481012,
          0.18785635,
          0.96511334,
          0.367742,
          0.39534575,
          0.5913922,
          0.594552,
          0.30300704,
          0.41551122,
          0.32988554,
          0.9728396,
          0.9346419,
          0.48993865,
          0.5548768,
          0.5977243,
          0.7240188,
          0.34569022,
          0.19995084,
          0.23058301,
          0.22635956,
          0.24550134,
          0.26345646,
          0.9018077,
          0.3991933,
          0.24768268,
          0.57169765,
          0.95077175,
          0.34849888,
          0.27421746,
          0.19621755,
          0.3773671,
          0.28708088,
          0.29600063,
          0.8612168,
          0.40213212,
          0.64151245,
          0.95904464,
          0.53724605,
          0.544946,
          0.6016806,
          0.260122,
          0.23171522,
          0.80391324,
          0.57229716,
          0.22961769,
          0.60817236,
          0.7033464,
          0.64710563,
          0.34610328,
          0.24511856,
          0.943178,
          0.8840954,
          0.62835413,
          0.7913389,
          0.9569598,
          0.31280878,
          0.36730653,
          0.6487205,
          0.31320146,
          0.36710367,
          0.3742392,
          0.5249672,
          0.42708805,
          0.33726647,
          0.24053384,
          0.3361613,
          0.4531253,
          0.53476596,
          0.30402303,
          0.67286503,
          0.31002164,
          0.3401498,
          0.30173486,
          0.29406187,
          0.35907426,
          0.5169036,
          0.35470173,
          0.27951518,
          0.28178376,
          0.18218814,
          0.23899446,
          0.26390857,
          0.9858342,
          0.1448916,
          0.671476,
          0.2596539,
          0.91410506,
          0.7937936,
          0.23915,
          0.28347695,
          0.26859564,
          0.37994027,
          0.38606182,
          0.9765127,
          0.35921714,
          0.33096257,
          0.36831543,
          0.19124348,
          0.22079928,
          0.80286825,
          0.19078875,
          0.4333434,
          0.1964838,
          0.25969514,
          0.19453916,
          0.22528438,
          0.254135,
          0.8756371,
          0.5248199,
          0.23989592,
          0.22104639,
          0.17644735,
          0.1520848,
          0.25843892,
          0.3465234,
          0.2717214,
          0.9863593,
          0.8732699,
          0.2482945,
          0.3200427,
          0.64093727,
          0.98786944,
          0.3109513,
          0.8277504,
          0.18957956,
          0.48557302,
          0.59824616,
          0.22501752,
          0.1974464,
          0.9721816,
          0.39344442,
          0.8876769,
          0.16991457,
          0.14765099,
          0.27922705,
          0.29085577,
          0.84775835,
          0.5643362,
          0.23004423,
          0.4039174,
          0.9860253,
          0.9063318,
          0.46304917,
          0.96572757,
          0.98762053,
          0.8898808,
          0.3462481,
          0.6984864,
          0.94004023,
          0.96554935,
          0.8806923,
          0.4710289,
          0.23223981,
          0.48019093,
          0.9809183,
          0.9962845,
          0.5729693,
          0.9692096,
          0.9865328,
          0.8136837,
          0.42112857,
          0.9669822,
          0.69547975,
          0.8746798,
          0.87982804,
          0.5030772,
          0.9389754,
          0.2604238,
          0.9873592,
          0.69069344,
          0.7731994,
          0.7563018,
          0.7635013,
          0.97176874,
          0.9870684,
          0.9965545,
          0.98390526,
          0.825229,
          0.98821056,
          0.9213433,
          0.5709991,
          0.2118311,
          0.95118856,
          0.7116536,
          0.5261169,
          0.98130566,
          0.98269296,
          0.4462938,
          0.9689048,
          0.59434557,
          0.97237194,
          0.7956224,
          0.95461273,
          0.4467043,
          0.52938914,
          0.9908765,
          0.9457951,
          0.9858139,
          0.9624426,
          0.14915769,
          0.26369503,
          0.16502772,
          0.84539205,
          0.2550716,
          0.14796706,
          0.6355483,
          0.59897316,
          0.2012647,
          0.19518857,
          0.2241842,
          0.24241984,
          0.768382,
          0.40917125,
          0.32929638,
          0.93764794,
          0.29587662,
          0.2507237,
          0.45028797,
          0.95108426,
          0.20471649,
          0.9688825,
          0.9447836,
          0.9515634,
          0.19331843,
          0.545321,
          0.24841215,
          0.7060318,
          0.35660186,
          0.70254177,
          0.94174504,
          0.40914175,
          0.997928,
          0.3750056,
          0.50720274,
          0.3068703,
          0.94523,
          0.7368219,
          0.6055341,
          0.61624026,
          0.78272116,
          0.7712641,
          0.8410996,
          0.93939346,
          0.21063598,
          0.32694596,
          0.36300963,
          0.8018261,
          0.52251595,
          0.827315,
          0.68078107,
          0.81784475,
          0.8509621,
          0.8059869,
          0.60772955,
          0.9475426,
          0.9659928,
          0.20103412999999998,
          0.3703994,
          0.17789426,
          0.34369242,
          0.9677577,
          0.15349516,
          0.95476973,
          0.9683211,
          0.2544245,
          0.20712213,
          0.2933815,
          0.18683699,
          0.76068497,
          0.4194622,
          0.5474375,
          0.48791906,
          0.75163156,
          0.33403578,
          0.74901325,
          0.2055008,
          0.21608073,
          0.5161094,
          0.85106635,
          0.23565647,
          0.25177282,
          0.94021153,
          0.2225581,
          0.91461957,
          0.70696026,
          0.2105201,
          0.2984312,
          0.4340176,
          0.47703132,
          0.34965807,
          0.24133657,
          0.4036668,
          0.96804184,
          0.24049875,
          0.5546419,
          0.9836512,
          0.24747112,
          0.3321841,
          0.58631426,
          0.4651687,
          0.8273652,
          0.3508404,
          0.23762219,
          0.6282512,
          0.44733942,
          0.46082053,
          0.39270636,
          0.6144872,
          0.30999973,
          0.69880027,
          0.17606589,
          0.7839679,
          0.34911913,
          0.16670696,
          0.22619446,
          0.47547075,
          0.50202775,
          0.42362478,
          0.3680724,
          0.23852886,
          0.79220283,
          0.93928343,
          0.37223613,
          0.20352957,
          0.24327803,
          0.2514515,
          0.8809837,
          0.28198645,
          0.26337203,
          0.9658066,
          0.90214276,
          0.90208274,
          0.72060776,
          0.47654834,
          0.41390383,
          0.7566997,
          0.51283026,
          0.91449064,
          0.20036198,
          0.26821905,
          0.28450516,
          0.40028852,
          0.23221971,
          0.99875295,
          0.23267724,
          0.429285,
          0.21228185,
          0.9974245,
          0.19097447,
          0.28859925,
          0.8796645,
          0.639261,
          0.25504926,
          0.9659872,
          0.41164035,
          0.75755686,
          0.62292165,
          0.99222577,
          0.20972581,
          0.44542724,
          0.2261819,
          0.92861855,
          0.9987876,
          0.91533464,
          0.3302278,
          0.5194542,
          0.19852297,
          0.99916613,
          0.99846005,
          0.99935347,
          0.5045,
          0.59282494,
          0.59185714,
          0.56691575,
          0.377352,
          0.4414935,
          0.3237074,
          0.30593184,
          0.5828933,
          0.27100676,
          0.99768627,
          0.60264194,
          0.58907145,
          0.41952562,
          0.35319826,
          0.3399685,
          0.25480473,
          0.28754824,
          0.5733097,
          0.94025266,
          0.5984913,
          0.8503455,
          0.87857723,
          0.3975613,
          0.8945726,
          0.90900034,
          0.798053,
          0.48432282,
          0.34372517,
          0.53364533,
          0.47207725,
          0.16691317,
          0.3689796,
          0.4946968,
          0.23537041,
          0.24345335,
          0.28770038,
          0.25164366,
          0.40383005,
          0.26719356,
          0.53325367,
          0.29484773,
          0.28330603,
          0.17655067,
          0.38148928,
          0.28844115,
          0.3686358,
          0.53948665,
          0.95766324,
          0.4167823,
          0.98195213,
          0.5455404,
          0.6268983,
          0.9878001,
          0.98342323,
          0.32091874,
          0.47009724,
          0.717814,
          0.59276044,
          0.362195,
          0.40545583,
          0.49097687,
          0.7088569,
          0.28655043,
          0.43269715,
          0.5392796,
          0.7119678,
          0.63290197,
          0.80573744,
          0.23661445,
          0.21611816,
          0.43037915,
          0.520281,
          0.41458133,
          0.37972838,
          0.2646445,
          0.55208504,
          0.6360209,
          0.38024205,
          0.49006572,
          0.47061747,
          0.5141119,
          0.25354457,
          0.2872164,
          0.3676971,
          0.43861777,
          0.38252264,
          0.3429026,
          0.25784937,
          0.32932544,
          0.26606587,
          0.3689593,
          0.5614997,
          0.41831622,
          0.3398038,
          0.41294965,
          0.3471288,
          0.22884582,
          0.5706069,
          0.469959,
          0.24103571,
          0.29650214,
          0.4793413,
          0.51073754,
          0.47415683,
          0.33400595,
          0.46380785,
          0.5328644,
          0.42836574,
          0.57424444,
          0.34791365,
          0.33671328,
          0.401313,
          0.3528203,
          0.38121784,
          0.54627734,
          0.50947225,
          0.3492057,
          0.98757577,
          0.67099535,
          0.9988165,
          0.9978157,
          0.9182689,
          0.9569037,
          0.29492524,
          0.38828328,
          0.9284369,
          0.9548402,
          0.18274978,
          0.9825926,
          0.8383138,
          0.17776266,
          0.56109506,
          0.31694776,
          0.5566225,
          0.2524998,
          0.98736763,
          0.4309132,
          0.26572555,
          0.8218636,
          0.44616106,
          0.9269771,
          0.83062154,
          0.36863047,
          0.8669826,
          0.4636543,
          0.56453663,
          0.44327945,
          0.4352943,
          0.34274393,
          0.57491076,
          0.37417638,
          0.5710074,
          0.7082305,
          0.27071694,
          0.7198524,
          0.63606,
          0.6194477,
          0.5584503,
          0.33573368,
          0.31770006,
          0.20288947,
          0.29039848
         ]
        },
        {
         "box": {
          "visible": true
         },
         "fillcolor": "rgb(30,176,75)",
         "line": {
          "color": "black"
         },
         "marker": {
          "color": "black",
          "size": 2
         },
         "meanline": {
          "visible": false
         },
         "name": "rna_seq",
         "points": "all",
         "showlegend": false,
         "spanmode": "hard",
         "type": "violin",
         "x": [
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7
         ],
         "y": [
          0.99672225,
          0.996363035,
          0.999006925,
          0.99042238,
          0.998451985,
          0.99966055,
          0.9393781999999999,
          0.9960745200000001,
          0.99848433,
          0.981477325,
          0.9969292750000001,
          0.9835654,
          0.9876294999999999,
          0.9988704500000001,
          0.98693705,
          0.8869906350000001,
          0.9728432,
          0.97823547,
          0.9991039500000001,
          0.9922924900000001,
          0.9985035099999999,
          0.959392275,
          0.92037808,
          0.97043105,
          0.986622225,
          0.99930236,
          0.99964325,
          0.99921463,
          0.99194095,
          0.994881825,
          0.9938101500000001,
          0.998786215,
          0.99673935,
          0.9605468500000001,
          0.993976765,
          0.9706110800000001,
          0.99157566,
          0.99904685,
          0.95506,
          0.9991405,
          0.97176412,
          0.9979516150000001,
          0.989444625,
          0.9976241,
          0.9947442,
          0.9762265999999999,
          0.997187,
          0.9983396499999999,
          0.997372865,
          0.993212625,
          0.9977876999999999,
          0.9947657999999999,
          0.9987825699999999,
          0.98936415,
          0.9960424,
          0.9927483850000001,
          0.99874195,
          0.9900811199999999,
          0.9855689400000001,
          0.9763476200000001,
          0.99955032,
          0.94244534,
          0.995750425,
          0.998910925,
          0.997669625,
          0.9920793,
          0.99848282,
          0.98408768,
          0.998229425,
          0.93919088,
          0.9376848,
          0.98981545,
          0.95685665,
          0.9959872750000001,
          0.9987288999999999,
          0.990801225,
          0.99627455,
          0.9767413,
          0.9984259,
          0.9925965,
          0.9987270500000001,
          0.9887075750000001,
          0.98366049,
          0.9987017149999999,
          0.99707255,
          0.998723215,
          0.989988715,
          0.98155886,
          0.99709606,
          0.987392075,
          0.979064065,
          0.98888934,
          0.662248265,
          0.99771005,
          0.998597415,
          0.9904936,
          0.959049015,
          0.9984472200000001,
          0.95792557,
          0.9958644999999999,
          0.9978889550000001,
          0.9701148500000001,
          0.99987065,
          0.9706549950000001,
          0.99937579,
          0.9996308,
          0.998010085,
          0.978678275,
          0.996915735,
          0.92238058,
          0.98613368,
          0.9888241849999999,
          0.9003148999999999,
          0.9899435999999999,
          0.994655175,
          0.88656165,
          0.98789627,
          0.6885337,
          0.99796483,
          0.7911663,
          0.9980373499999999,
          0.9963902250000001,
          0.9975024,
          0.7245395,
          0.89678182,
          0.67187287,
          0.99077034,
          0.970015855,
          0.99864656,
          0.99878568,
          0.99630262,
          0.995058805,
          0.99979013,
          0.9872195500000001,
          0.9913127,
          0.99398264,
          0.98540616,
          0.98485595,
          0.9985319,
          0.96423849,
          0.9991417,
          0.9988146,
          0.9959222000000001,
          0.995039665,
          0.9990181,
          0.99114815,
          0.99662936,
          0.9815294649999999,
          0.9509864450000001,
          0.9718064500000001,
          0.9987977,
          0.99721853,
          0.994505915,
          0.99714965,
          0.8959865,
          0.9955461,
          0.955397,
          0.9988323,
          0.9966644499999999,
          0.99934523,
          0.91039795,
          0.99822542,
          0.9899957,
          0.9777792000000001,
          0.9946759,
          0.880163195,
          0.9534832,
          0.96276935,
          0.994456205,
          0.9887211,
          0.9004292199999999,
          0.98752815,
          0.99965227,
          0.992851,
          0.993282495,
          0.981568235,
          0.9925079,
          0.9988542499999999,
          0.9989432133333334,
          0.99568993,
          0.9910736499999999,
          0.99964557,
          0.96232438,
          0.9933911799999999,
          0.996286,
          0.997759575,
          0.986252225,
          0.998161215,
          0.99375312,
          0.9921607,
          0.9971672,
          0.9982650799999999,
          0.9976,
          0.99208005,
          0.9994802700000001,
          0.9677103,
          0.9896358000000001,
          0.99885862,
          0.9981736,
          0.9981121399999999,
          0.9980679800000001,
          0.99497265,
          0.999084965,
          0.9970593,
          0.9918214249999999,
          0.99792412,
          0.9953315,
          0.9926217500000001,
          0.99568835,
          0.9974729,
          0.99129993,
          0.9967174,
          0.996578065,
          0.996107845,
          0.95985505,
          0.99783415,
          0.997734755,
          0.990079765,
          0.9967817750000001,
          0.998515815,
          0.99003565,
          0.99758988,
          0.99760425,
          0.998472005,
          0.98666195,
          0.99726945,
          0.996034225,
          0.9991589000000001,
          0.9978001,
          0.9966926,
          0.9996178,
          0.99804213,
          0.9995289199999999,
          0.99091782,
          0.996771675,
          0.9989281,
          0.995143,
          0.994238935,
          0.99871697,
          0.99558675,
          0.98982742,
          0.997929455,
          0.997729585,
          0.9974604,
          0.99005788,
          0.9968315649999999,
          0.998367035,
          0.99865297,
          0.99716925,
          0.9953527799999999,
          0.996697755,
          0.99029693,
          0.996375,
          0.99483469,
          0.9955677700000001,
          0.9943649,
          0.997915085,
          0.9795072499999999,
          0.99913525,
          0.99351293,
          0.99879028,
          0.9975815699999999,
          0.9929879,
          0.99936634,
          0.98738675,
          0.99531795,
          0.992098275,
          0.9980855,
          0.9890480749999999,
          0.9959667999999999,
          0.996397515,
          0.99619535,
          0.9946196,
          0.99518775,
          0.9979331499999999,
          0.998301025,
          0.9939681,
          0.998917735,
          0.9877681,
          0.9977542500000001,
          0.99130255,
          0.997546645,
          0.99646759,
          0.9988558700000001,
          0.990601085,
          0.99446165,
          0.9996898000000001,
          0.992701975,
          0.997559575,
          0.9988524,
          0.9900423,
          0.99533498,
          0.989120625,
          0.99531865,
          0.9949972149999999,
          0.998277115,
          0.99452403,
          0.99671569,
          0.99914675,
          0.99866395,
          0.9933105849999999,
          0.99873203,
          0.989659275,
          0.996669665,
          0.9966578500000001,
          0.997186305,
          0.994142635,
          0.9943133,
          0.9995023000000001,
          0.99438373,
          0.99881025,
          0.97983396,
          0.992331025,
          0.9958619200000001,
          0.98458607,
          0.9973867,
          0.99671368,
          0.99719095,
          0.997353725,
          0.9978434,
          0.993131605,
          0.99635945,
          0.99283052,
          0.98787728,
          0.9981994,
          0.99632875,
          0.9957571,
          0.99882598,
          0.993366215,
          0.99901952,
          0.725459015,
          0.99908882,
          0.9921301899999999,
          0.9560019,
          0.99951112,
          0.9969361999999999,
          0.99285215,
          0.9981534000000001,
          0.9956293700000001,
          0.982193055,
          0.994985,
          0.99479825,
          0.9953198,
          0.98578595,
          0.99899205,
          0.98439895,
          0.994724385,
          0.99891995,
          0.99378994,
          0.98823898,
          0.9979392300000001,
          0.999383065,
          0.994402865,
          0.99706425,
          0.9977540250000001,
          0.9912222500000001,
          0.9976704750000001,
          0.99824365,
          0.99314927,
          0.997062685,
          0.997084795,
          0.99245472,
          0.990512915,
          0.996410675,
          0.9974056250000001,
          0.99401162,
          0.9976775,
          0.99580625,
          0.9972613,
          0.9884306,
          0.997884725,
          0.99912263,
          0.99505025,
          0.9964063999999999,
          0.9906614499999999,
          0.9914274000000001,
          0.9936944,
          0.9969097499999999,
          0.99467463,
          0.9929264849999999,
          0.9970627,
          0.9881640199999999,
          0.9991622,
          0.995116425,
          0.9919778,
          0.99691485,
          0.9964276700000001,
          0.99248132,
          0.9952166,
          0.9994154,
          0.999472275,
          0.991579875,
          0.99820983,
          0.9864624,
          0.9920823000000001,
          0.9771519,
          0.999817575,
          0.9691557,
          0.9922469700000001,
          0.9928796200000001,
          0.99105275,
          0.3162173,
          0.6982071,
          0.991602565,
          0.9932569200000001,
          0.981051675,
          0.999176775,
          0.9994787,
          0.9995038350000001,
          0.99939112,
          0.8589453,
          0.9993494,
          0.998965925,
          0.999471585,
          0.9995568850000001,
          0.9989015450000001,
          0.9995518000000001,
          0.980443865,
          0.99691258,
          0.9991086499999999,
          0.9506395750000001,
          0.9981027,
          0.9991685699999999,
          0.977979665,
          0.9999832,
          0.9956121149999999,
          0.980134625,
          0.57324189,
          0.99288362,
          0.9995207500000001,
          0.998966815,
          0.99632115,
          0.990680775,
          0.87367968,
          0.9613891299999999,
          0.766779185,
          0.893868385,
          0.998730785,
          0.98459797,
          0.98211815,
          0.97912741,
          0.998325855,
          0.9549704999999999,
          0.9948351200000001,
          0.94943615,
          0.9523132,
          0.990407365,
          0.99638357,
          0.804727215,
          0.9913437249999999,
          0.99381038,
          0.96977227,
          0.90172585,
          0.9991252500000001,
          0.99660007,
          0.9864614700000001,
          0.9724711699999999,
          0.97335465,
          0.9874633500000001,
          0.87205815,
          0.98961512,
          0.99869963,
          0.9747967,
          0.640791215,
          0.9995086,
          0.9939464,
          0.9113823999999999,
          0.97295192,
          0.6255205,
          0.997426965,
          0.97253995,
          0.992147,
          0.65778195,
          0.96347776,
          0.638537775,
          0.842039185,
          0.98101132,
          0.98404013,
          0.96756418,
          0.9934145249999999,
          0.97382422,
          0.89707445,
          0.924369,
          0.99955431,
          0.99952782,
          0.9981998999999999,
          0.9577884999999999,
          0.9995007124999999,
          0.9977652625,
          0.9997494825,
          0.99364622,
          0.9961557249999999,
          0.9166804500000001,
          0.996458625,
          0.99521663,
          0.9928747499999999,
          0.9869360125,
          0.9890199725,
          0.974704215,
          0.991300465,
          0.9921349349999999,
          0.9859228225,
          0.99579615,
          0.98745456,
          0.97652125,
          0.9929782225,
          0.9877101,
          0.97856595,
          0.880109655,
          0.99523815,
          0.9785488499999999,
          0.7558787,
          0.80666152,
          0.891047955,
          0.9964205500000001,
          0.997977965,
          0.9981379850000001,
          0.9966074,
          0.9985678,
          0.94934338,
          0.998584245,
          0.9988668549999999,
          0.96623077,
          0.995708775,
          0.9949602500000001,
          0.9937386850000001,
          0.9973369000000001,
          0.9803526499999999,
          0.99782495,
          0.996249765,
          0.997766685,
          0.9850996999999999,
          0.9984132,
          0.9964180149999999,
          0.99788305,
          0.9960693700000001,
          0.99820465,
          0.99533843,
          0.9961345349999999,
          0.9968941,
          0.99507295,
          0.9934327350000001,
          0.9984562,
          0.993755755,
          0.9902409999999999,
          0.9943472,
          0.99797653,
          0.9943697149999999,
          0.99533703,
          0.99667105,
          0.994962465,
          0.987841465,
          0.98584202,
          0.993732955,
          0.992409475,
          0.994309305,
          0.993700535,
          0.9971021200000001,
          0.99860423,
          0.9986115499999999,
          0.996098,
          0.996060935,
          0.99798845,
          0.996277225,
          0.98208064,
          0.9966602,
          0.988973135,
          0.996123925,
          0.9966196350000001,
          0.9993121333333334,
          0.99912878,
          0.9988269000000001,
          0.99767575,
          0.998412845,
          0.99883363,
          0.99899455,
          0.99875757,
          0.997838585,
          0.9986222650000001,
          0.99909546,
          0.998894535,
          0.997599085,
          0.9961532,
          0.9989179349999999,
          0.99839285,
          0.9987212700000001,
          0.99917049,
          0.9980976500000001,
          0.998475,
          0.9981524049999999,
          0.998625665,
          0.999255715,
          0.9988060599999999,
          0.9985711500000001,
          0.997839175,
          0.9992696750000001,
          0.99843682,
          0.998977985,
          0.998962175,
          0.99731613,
          0.99815472,
          0.9986210233333334,
          0.99898818,
          0.99877315,
          0.99886895,
          0.997958985,
          0.9985606499999999,
          0.998578545,
          0.99878068,
          0.998377725,
          0.9986029599999999,
          0.998653825,
          0.9980941400000001,
          0.9991090499999999,
          0.998871595,
          0.998113225,
          0.9979116,
          0.99860695,
          0.9988414800000001,
          0.998228315,
          0.99867987,
          0.99883755,
          0.998543835,
          0.99862902,
          0.9986239,
          0.9988379000000001,
          0.9989139149999999,
          0.9985644499999999,
          0.99860317,
          0.998798785,
          0.998615415,
          0.9982506250000001,
          0.9986184149999999,
          0.9985569000000001,
          0.9990429000000001,
          0.999106485,
          0.998219465,
          0.9984709350000001,
          0.99875054,
          0.998620275,
          0.998636095,
          0.999042,
          0.99912305,
          0.9988901,
          0.998109085,
          0.9983688749999999,
          0.998433925,
          0.998817085,
          0.9988786000000001,
          0.99817657,
          0.99877375,
          0.99883795,
          0.998227975,
          0.996234975,
          0.998485835,
          0.9978207699999999,
          0.9987973,
          0.9986620500000001,
          0.998680225,
          0.9983891,
          0.998373655,
          0.99866862,
          0.99917815,
          0.9982174699999999,
          0.99822833,
          0.9973893,
          0.9989709,
          0.99878093,
          0.9985927,
          0.99798812,
          0.998244515,
          0.99906045,
          0.9978744500000001,
          0.99866208,
          0.996637765,
          0.998724685,
          0.997601785,
          0.99863282,
          0.99886192,
          0.99861129,
          0.998624815,
          0.9986313400000001,
          0.9990508300000001,
          0.9979306450000001,
          0.9988695700000001,
          0.9988477,
          0.996557295,
          0.998472455,
          0.9980292049999999,
          0.99823083,
          0.99867482,
          0.9982853549999999,
          0.9986814749999999,
          0.9987947349999999,
          0.998129625,
          0.998707285,
          0.998864,
          0.99853775,
          0.9988543,
          0.998273425,
          0.9990873950000001,
          0.9987735,
          0.99729882,
          0.99844105,
          0.996656465,
          0.998976915,
          0.99839645,
          0.99826152,
          0.9983162999999999,
          0.99796642,
          0.998887675,
          0.99885045,
          0.9991470650000001,
          0.9983026500000001,
          0.9981185699999999,
          0.99841015,
          0.9982696,
          0.9983639799999999,
          0.99760145,
          0.99874005,
          0.999114135,
          0.998824805,
          0.9991973999999999,
          0.99812935,
          0.9984074000000001,
          0.9985596649999999,
          0.999110185,
          0.99905658,
          0.998646825,
          0.9983316849999999,
          0.9984435566666666,
          0.99796346,
          0.9987424,
          0.998975665,
          0.9982488,
          0.997283175,
          0.9970707000000001,
          0.9990318,
          0.99846613,
          0.9981655300000001,
          0.998847885,
          0.9969912000000001,
          0.9982137,
          0.99852845,
          0.99836505,
          0.9981180199999999,
          0.9981675,
          0.997609335,
          0.998771635,
          0.9989048,
          0.9989338,
          0.998900425,
          0.9988868200000001,
          0.9990989,
          0.99766872,
          0.99874745,
          0.9988192499999999,
          0.9984982,
          0.9993536199999999,
          0.9986783,
          0.99878372,
          0.9983711533333334,
          0.9985888,
          0.997500745,
          0.997759915,
          0.9986135,
          0.99787877,
          0.99867005,
          0.99723637,
          0.99886248,
          0.9977254499999999,
          0.9989712,
          0.977912165,
          0.99658322,
          0.998768975,
          0.99895332,
          0.99726695,
          0.9986974900000001,
          0.998840155,
          0.99901035,
          0.998783185,
          0.998288525,
          0.9984518,
          0.998591885,
          0.998773715,
          0.99869205,
          0.9977491,
          0.99896626,
          0.9985361100000001,
          0.99808438,
          0.99894525,
          0.9989393799999999,
          0.9989287099999999,
          0.99882083,
          0.9986314000000001,
          0.99835457,
          0.9980502250000001,
          0.9990281000000001,
          0.998115035,
          0.9980868,
          0.99829762,
          0.931936215,
          0.99742248,
          0.9985668766666667,
          0.9985048000000001,
          0.9987922,
          0.9984825599999999,
          0.9982996,
          0.998782095,
          0.997969745,
          0.9988358,
          0.9975976,
          0.99899715,
          0.99732429,
          0.998557025,
          0.997826285,
          0.99861038,
          0.99753707,
          0.998177675,
          0.9982255,
          0.9988634750000001,
          0.9988582850000001,
          0.99746685,
          0.9979009000000001,
          0.998538725,
          0.9977802499999999,
          0.99860595,
          0.99869145,
          0.998786095,
          0.99840092,
          0.998595875,
          0.99866555,
          0.998750575,
          0.9988497199999999,
          0.9984994,
          0.9978257,
          0.9982749200000001,
          0.998909185,
          0.998155275,
          0.9982146199999999,
          0.9987901,
          0.99765318,
          0.99879965,
          0.9982896,
          0.99857818,
          0.99849,
          0.998332215,
          0.997792,
          0.9981868,
          0.99808991,
          0.9984137,
          0.99765138,
          0.9960804000000001,
          0.9979423199999999,
          0.9993262500000001,
          0.998711235,
          0.9990416449999999,
          0.99862308,
          0.997123185,
          0.998526065,
          0.998454425,
          0.99850653,
          0.9984936799999999,
          0.9989095,
          0.998696,
          0.99920398,
          0.9986166000000001,
          0.9984751999999999,
          0.9985691999999999,
          0.9976184,
          0.9976138299999999,
          0.9990588300000001,
          0.9986648333333333,
          0.9981701000000001,
          0.9985890199999999,
          0.998916025,
          0.99892565,
          0.9987264499999999,
          0.998984725,
          0.9976637349999999,
          0.997811075,
          0.9988217699999999,
          0.99844453,
          0.9990043033333333,
          0.99911075,
          0.9991576,
          0.9990763066666667,
          0.99886233,
          0.998767,
          0.99860997,
          0.9983482699999999,
          0.9984677500000001,
          0.9979451500000001,
          0.99903846,
          0.99868974,
          0.99871405,
          0.997294455,
          0.99853598,
          0.99709355,
          0.9989806,
          0.99849678,
          0.998594165,
          0.998216725,
          0.9978511,
          0.9984108,
          0.998324575,
          0.99830755,
          0.998584565,
          0.9991931199999999,
          0.9987184,
          0.9990621,
          0.99851833,
          0.998358015,
          0.99853242,
          0.9977657799999999,
          0.9983196999999999,
          0.99903935,
          0.99914055,
          0.9981207000000001,
          0.9990833,
          0.9980483,
          0.9987771750000001,
          0.9985786533333334,
          0.998605785,
          0.9990711999999999,
          0.9989955500000001,
          0.99872122,
          0.9985438,
          0.99925157,
          0.9980563,
          0.9982522149999999,
          0.9991717149999999,
          0.99880931,
          0.99917285,
          0.9987996766666667,
          0.99817729,
          0.9984728866666667,
          0.99774312,
          0.998537985,
          0.9988106299999999,
          0.99907615,
          0.998970975,
          0.998526165,
          0.9986162000000001,
          0.9988561499999999,
          0.9992156,
          0.998694435,
          0.998538045,
          0.9977239499999999,
          0.998787575,
          0.9977399250000001,
          0.9989033,
          0.998918185,
          0.996829185,
          0.99873995,
          0.998869675,
          0.99704548,
          0.9993769766666666,
          0.998024225,
          0.9988946999999999,
          0.998938145,
          0.9984165666666667,
          0.9986570866666667,
          0.99898285,
          0.9979326,
          0.9979848,
          0.9972645333333334,
          0.99871015,
          0.998511925,
          0.998955925,
          0.9985788449999999,
          0.9988101,
          0.9992205000000001,
          0.99901995,
          0.9991644749999999,
          0.9988337249999999,
          0.999070065,
          0.9989036466666666,
          0.99836375,
          0.99866475,
          0.998740125,
          0.9988284000000001,
          0.997657875,
          0.99797994,
          0.99882242,
          0.99768425,
          0.9986787500000001,
          0.9985775800000001,
          0.9983305650000001,
          0.9988769799999999,
          0.998704075,
          0.99848169,
          0.99857085,
          0.9991157500000001,
          0.999058125,
          0.99873138,
          0.99867454,
          0.9986169949999999,
          0.99835315,
          0.99886535,
          0.998956205,
          0.99864315,
          0.998667895,
          0.9988510500000001,
          0.9985628,
          0.9977048500000001,
          0.99884217,
          0.99874302,
          0.99770565,
          0.998765965,
          0.9983229,
          0.99843293,
          0.9990010300000001,
          0.99942273,
          0.99907038,
          0.999265835,
          0.99923715,
          0.9992162,
          0.9987018249999999,
          0.9980264,
          0.9982998949999999,
          0.99871337,
          0.997567715,
          0.99837248,
          0.9979235,
          0.999235985,
          0.99787973,
          0.99876735,
          0.9987944049999999,
          0.9992105,
          0.9982433500000001,
          0.998792785,
          0.9988938199999999,
          0.99756065,
          0.9991301349999999,
          0.99900455,
          0.99796375,
          0.9983814099999999,
          0.99874388,
          0.99860413,
          0.99789587,
          0.998313775,
          0.99844227,
          0.999080575,
          0.99870593,
          0.99930955,
          0.99762443,
          0.99895878,
          0.9992412500000001,
          0.99837725,
          0.9983665349999999,
          0.99851885,
          0.998543795,
          0.9987828999999999,
          0.9981520500000001,
          0.9982587249999999,
          0.9983888700000001,
          0.9987347799999999,
          0.9986952,
          0.99900778,
          0.9983940499999999,
          0.9984628,
          0.9988582500000001,
          0.998813715,
          0.99856342,
          0.99859705,
          0.99845647,
          0.9985377,
          0.9972864349999999,
          0.9988960499999999,
          0.998925075,
          0.998877535,
          0.9987264,
          0.99902415,
          0.998304275,
          0.9987076500000001,
          0.9990714,
          0.99800736,
          0.9987710999999999,
          0.9988576666666668,
          0.998697195,
          0.9989979,
          0.9984014649999999,
          0.99910375,
          0.9990234,
          0.9987765,
          0.9993727800000001,
          0.9989865149999999,
          0.9985691,
          0.998791725,
          0.99883168,
          0.99752985,
          0.99899328,
          0.99739501,
          0.9987356949999999,
          0.998886675,
          0.998213115,
          0.9987418,
          0.99926417,
          0.99898188,
          0.9988651,
          0.998310655,
          0.99868375,
          0.9990854499999999,
          0.9989780450000001,
          0.99808683,
          0.9986579250000001,
          0.998489875,
          0.99741127,
          0.998886715,
          0.983664635,
          0.99887405,
          0.9985988,
          0.9989558,
          0.99777255,
          0.9980576566666667,
          0.99862435,
          0.998123025,
          0.9985102699999999,
          0.9987535,
          0.9984222300000001,
          0.99879445,
          0.99856605,
          0.998193125,
          0.99886688,
          0.9988166300000001,
          0.9985572500000001,
          0.9988366333333333,
          0.99920565,
          0.992071095,
          0.98331453,
          0.99347898,
          0.99130092,
          0.99403888,
          0.99274394,
          0.98531535,
          0.9978138000000001,
          0.97474153,
          0.99811288,
          0.99699117,
          0.9840889,
          0.99641377,
          0.98293902,
          0.99900537,
          0.99936545,
          0.97810782,
          0.9991922950000001,
          0.9992975,
          0.999488375,
          0.9985735499999999,
          0.99710625,
          0.9438565999999999,
          0.993217425,
          0.934842685,
          0.9193434,
          0.9982173625,
          0.9582771,
          0.991152235,
          0.9993519775,
          0.999346925,
          0.9994574775,
          0.9992496200000001,
          0.99928071,
          0.9996587575,
          0.9750408075,
          0.74845022,
          0.9994857975,
          0.9995848766666667,
          0.99311065,
          0.9869892600000001,
          0.954824,
          0.99480032,
          0.992395025,
          0.823389565,
          0.9667413300000001,
          0.990783125,
          0.9477638,
          0.71866925,
          0.99663715,
          0.9946583250000001,
          0.99442082,
          0.99329803,
          0.52209365,
          0.99600082,
          0.5093521,
          0.88413712,
          0.9991007000000001,
          0.99600697,
          0.9987349249999999,
          0.99675965,
          0.988238225,
          0.99920988,
          0.998547495,
          0.99871065,
          0.9971382049999999,
          0.9979337500000001,
          0.997655585,
          0.9948231700000001,
          0.9934817549999999,
          0.9978519,
          0.993622825,
          0.998919675,
          0.9964045,
          0.9964463400000001,
          0.70073137,
          0.9888387750000001,
          0.9938806,
          0.99820525,
          0.996235125,
          0.99580712,
          0.998215175,
          0.948006675,
          0.76673114,
          0.99688365,
          0.8895031,
          0.9921012499999999,
          0.9950514500000001,
          0.9887629,
          0.9951379600000001,
          0.992810215,
          0.9963476499999999,
          0.9978743999999999,
          0.9977498,
          0.99792625,
          0.9448311,
          0.967645535,
          0.99344813,
          0.98300225,
          0.99453708,
          0.59626537,
          0.99877065
         ]
        },
        {
         "box": {
          "visible": true
         },
         "fillcolor": "rgb(81,89,168)",
         "line": {
          "color": "black"
         },
         "marker": {
          "color": "black",
          "size": 2
         },
         "meanline": {
          "visible": false
         },
         "name": "wgbs",
         "points": "all",
         "showlegend": false,
         "spanmode": "hard",
         "type": "violin",
         "x": [
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8
         ],
         "y": [
          0.19903432,
          0.165404265,
          0.22667138,
          0.2628587,
          0.41611962999999996,
          0.45564456,
          0.454311295,
          0.19306402,
          0.19462242000000002,
          0.29476875,
          0.256769455,
          0.638127475,
          0.86894855,
          0.464552805,
          0.32345254,
          0.61538497,
          0.20553573,
          0.27522275,
          0.38073256,
          0.241435475,
          0.19721866500000002,
          0.202380915,
          0.204102895,
          0.17104160499999999,
          0.16924144,
          0.43914279,
          0.306232825,
          0.13137161,
          0.652080575,
          0.5223996049999999,
          0.3434802,
          0.20726645,
          0.22878241500000002,
          0.24800539000000002,
          0.2961484,
          0.97181875,
          0.26673121,
          0.6412411650000001,
          0.390174,
          0.16103201,
          0.17845982,
          0.33664364,
          0.404729155,
          0.5270522,
          0.20866126000000002,
          0.16202569,
          0.748307125,
          0.21860956999999998,
          0.25818178,
          0.291770665,
          0.5194371,
          0.233773515,
          0.30276248,
          0.18510664999999998,
          0.47461110500000003,
          0.71020624,
          0.80112877,
          0.599476465,
          0.8445151,
          0.6455820999999999,
          0.564922275,
          0.50055315,
          0.68394407,
          0.5058768,
          0.43586224500000004,
          0.7106880550000001,
          0.688439785,
          0.9187488500000001,
          0.633855515,
          0.6446094,
          0.5528511700000001,
          0.75769728,
          0.5268564250000001,
          0.5149834,
          0.47345368,
          0.230821365,
          0.9919583249999999,
          0.21901702,
          0.30028856,
          0.40754328500000003,
          0.259018365,
          0.22847144,
          0.21768362000000002,
          0.24517760500000002,
          0.508195525,
          0.5184811300000001,
          0.19177929500000002,
          0.584140925,
          0.14412555,
          0.44461578,
          0.72858145,
          0.23085972,
          0.494116605,
          0.62495382,
          0.550979165,
          0.14503685,
          0.23117521000000002,
          0.72335109,
          0.24138491,
          0.199795485,
          0.20675759,
          0.24007704,
          0.227736335,
          0.28361425,
          0.300809325,
          0.19829940000000001,
          0.24939462499999998,
          0.33252423,
          0.77654833,
          0.98087318,
          0.319797905,
          0.2891387,
          0.651064885,
          0.197345015,
          0.6353670499999999,
          0.309826045,
          0.32030228000000005,
          0.30885039000000003,
          0.21640747999999999,
          0.36130359999999995,
          0.38216344999999996,
          0.23839365,
          0.18788451,
          0.16745423,
          0.973090355,
          0.8728722,
          0.54305056,
          0.567891725,
          0.98912052,
          0.64753412,
          0.59653785,
          0.186406445,
          0.190788805,
          0.35962992,
          0.31268114999999996,
          0.94825737,
          0.55712172,
          0.25678095,
          0.30387606,
          0.8575058,
          0.6337250000000001,
          0.6305206,
          0.47854653,
          0.6791866,
          0.61009935,
          0.24329785999999998,
          0.89972688,
          0.27925305,
          0.905055625,
          0.833073325,
          0.14969298,
          0.6225703,
          0.628311215,
          0.5341136,
          0.670332215,
          0.9144734,
          0.41647349499999997,
          0.45344458499999996,
          0.66742039,
          0.61515657,
          0.125591755,
          0.17846838,
          0.29019329,
          0.19872146499999999,
          0.87748775,
          0.19811223500000003,
          0.962345435,
          0.9771994500000001,
          0.19359088,
          0.5923591,
          0.42681646500000003,
          0.57181865,
          0.70512555,
          0.33537093,
          0.37724002,
          0.6657482800000001,
          0.44039989999999996,
          0.616430925,
          0.5872831199999999,
          0.4226598,
          0.42679448499999995,
          0.444464175,
          0.483384255,
          0.16211973,
          0.214211665,
          0.916810585,
          0.8786183000000001,
          0.923084185,
          0.9278917200000001,
          0.922129235,
          0.5200380099999999,
          0.8153520999999999,
          0.91823554,
          0.75301425,
          0.7366731,
          0.65968845,
          0.83804245,
          0.941668535,
          0.83415222,
          0.9503455000000001,
          0.77833007,
          0.945591955,
          0.858515615,
          0.866621525,
          0.88981888,
          0.81910422,
          0.49821227999999995,
          0.7828563500000001,
          0.6606753000000001,
          0.7739256999999999,
          0.22531211499999998,
          0.17968756000000002,
          0.5778172,
          0.316560745,
          0.54246975,
          0.373235715,
          0.5466195899999999,
          0.9875065199999999,
          0.47013262,
          0.31596465500000004,
          0.5076080000000001,
          0.40315066,
          0.26302148999999997,
          0.456501485,
          0.60567777,
          0.24056694,
          0.7024435499999999,
          0.197453295,
          0.33661382500000003,
          0.4557478,
          0.267944065,
          0.47667453,
          0.525991025,
          0.240785365,
          0.419995675,
          0.40082954,
          0.29158201,
          0.45312958000000003,
          0.26970373999999997,
          0.46059517,
          0.35970682,
          0.218906975,
          0.6147826000000001,
          0.94406108,
          0.65617435,
          0.309907005,
          0.57810123,
          0.3259672,
          0.42014689,
          0.5205668999999999,
          0.916673315,
          0.82246112,
          0.905839235,
          0.98475322,
          0.9202274,
          0.8702375,
          0.942344325,
          0.95532555,
          0.6680207650000001,
          0.86243175,
          0.962142175,
          0.9303581000000001,
          0.6080720500000001,
          0.412737875,
          0.380107935,
          0.138208045,
          0.426866605,
          0.38454139,
          0.1620007,
          0.365407885,
          0.34153206999999997,
          0.9863893,
          0.990073055,
          0.930091145,
          0.98452502,
          0.97173647,
          0.99129218,
          0.9793445700000001,
          0.9983635200000001,
          0.9897866,
          0.9750352250000001,
          0.915274665,
          0.85386142,
          0.712766125,
          0.6164477150000001,
          0.44169406,
          0.880230665,
          0.467495785,
          0.5061809349999999,
          0.66348635,
          0.88393268,
          0.3246392,
          0.19941331,
          0.22443995,
          0.8099651,
          0.480901025,
          0.8203460899999999,
          0.86421822,
          0.7013202000000001,
          0.3335852,
          0.874856675,
          0.8579869600000001,
          0.779224675,
          0.7253229000000001,
          0.5213584,
          0.455171985,
          0.78338833,
          0.87986358,
          0.63546122,
          0.41010636,
          0.42967821500000003,
          0.577331305,
          0.89271715,
          0.85872568,
          0.8161473,
          0.8388926999999999,
          0.879403295,
          0.86115735,
          0.8729599699999999,
          0.721468225,
          0.816149,
          0.88526938,
          0.841221005,
          0.8241287749999999,
          0.651602275,
          0.82349622,
          0.29275255,
          0.5358076,
          0.981446465,
          0.9952581199999999,
          0.9800211,
          0.90602878,
          0.926796535,
          0.5574140000000001,
          0.98580035,
          0.97421802,
          0.9722697149999999,
          0.920000975,
          0.97907923,
          0.9660086649999999,
          0.3500019,
          0.952306315,
          0.9635838999999999,
          0.693541425,
          0.41495335,
          0.5669300749999999,
          0.31073440500000005,
          0.577483885,
          0.56282373,
          0.652439235,
          0.23631859,
          0.66928035,
          0.5224541149999999,
          0.7378287699999999,
          0.67090142,
          0.6031093249999999,
          0.22913751,
          0.5473898349999999,
          0.63081265,
          0.239471145,
          0.6700744000000001,
          0.43992627,
          0.23332471500000002,
          0.985589375,
          0.9893796450000001,
          0.96784526,
          0.981577245,
          0.9200713,
          0.65082067,
          0.560652825,
          0.890765995,
          0.7916887,
          0.80066465,
          0.817094035,
          0.485505955,
          0.9520887149999999,
          0.92359402,
          0.93014575,
          0.83747758,
          0.74609305,
          0.845816165,
          0.939941475,
          0.30644396,
          0.61078525,
          0.25056581,
          0.29558262,
          0.30253256500000003,
          0.657063475,
          0.39957980000000004,
          0.25470331,
          0.9641650900000001,
          0.9511769800000001,
          0.8783422000000001,
          0.75480333,
          0.9425174000000001,
          0.9730847499999999,
          0.64590835,
          0.65650443,
          0.7396631499999999,
          0.862981965,
          0.16620125000000002,
          0.860288505
         ]
        }
       ],
       "layout": {
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Sample ontology - Prediction score distribution (EpiRR majority vote)"
        },
        "width": 900,
        "xaxis": {
         "ticktext": [
          "h3k27ac",
          "h3k27me3",
          "h3k36me3",
          "h3k4me1",
          "h3k4me3",
          "h3k9me3",
          "input",
          "rna_seq",
          "wgbs"
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8
         ],
         "title": {
          "text": "assay_epiclass"
         }
        },
        "yaxis": {
         "range": [
          0,
          1.001
         ],
         "title": {
          "text": "Avg. prediction score (majority class)"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"c4f66091-c8ce-4bf3-8c86-271d3568bf35\" class=\"plotly-graph-div\" style=\"height:600px; width:900px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c4f66091-c8ce-4bf3-8c86-271d3568bf35\")) {                    Plotly.newPlot(                        \"c4f66091-c8ce-4bf3-8c86-271d3568bf35\",                        [{\"box\":{\"visible\":true},\"fillcolor\":\"rgb(225,148,37)\",\"line\":{\"color\":\"black\"},\"marker\":{\"color\":\"black\",\"size\":2},\"meanline\":{\"visible\":false},\"name\":\"h3k27ac\",\"points\":\"all\",\"showlegend\":false,\"spanmode\":\"hard\",\"x\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"y\":[0.9997694999999999,0.9973300166666667,0.9895747166666666,0.9979191,0.9921785333333334,0.9989898,0.8958773666666667,0.9859907333333333,0.9953129033333333,0.9983407133333334,0.9964030066666667,0.9903697,0.9976281433333334,0.99711815,0.9989555500000001,0.9582664,0.9997598499999999,0.9976723833333333,0.9931914566666666,0.99144421,0.9896487366666666,0.9983376566666666,0.9928124199999999,0.47219354,0.99824397,0.9960745166666666,0.9952742899999999,0.9940779666666666,0.9048065,0.9953843133333334,0.99494607,0.9315372766666666,0.9641573833333333,0.9995955566666668,0.9961078666666667,0.9677465866666667,0.9996132333333333,0.9975999566666666,0.9996861566666667,0.9957736666666667,0.9790418366666667,0.9994122366666667,0.9993701433333332,0.9506854166666666,0.99786915,0.9894228833333334,0.9545681766666667,0.99878645,0.9975011500000001,0.9992551466666666,0.9997163866666666,0.9964517,0.98624848,0.9990809833333333,0.9999070066666667,0.9993012,0.99967445,0.9972762999999999,0.9984574500000001,0.9994092666666666,0.9766714100000001,0.9830707533333333,0.9957563333333334,0.9664126199999999,0.9960567533333333,0.9992453333333332,0.9905317666666668,0.9994464933333332,0.9989772933333333,0.9997918833333334,0.9461488266666667,0.9981719866666667,0.9992207233333333,0.9791667133333334,0.9993045833333333,0.9998399,0.9912847166666667,0.8631844066666666,0.9995111766666667,0.98156468,0.9985950533333333,0.9095204733333334,0.9366910866666668,0.9393087250000001,0.9970277766666666,0.9926071,0.9855787233333334,0.9983325866666667,0.9801498866666667,0.99941302,0.9997207133333333,0.9999971,0.8875913999999999,0.9990975533333334,0.80975221,0.9989419700000001,0.88772599,0.9745806899999999,0.99702035,0.9967696033333334,0.9751327433333333,0.9999966166666666,0.99891065,0.66850275,0.9982341,0.9996511066666667,0.9892215566666667,0.8810959566666666,0.9828529,0.99899738,0.9982490366666666,0.99284901,0.9998216366666667,0.9611893,0.9857671833333334,0.9996570366666667,0.9866528,0.9115320333333333,0.9980059766666667,0.9980806000000001,0.99937855,0.7459098166666666,0.9987387633333333,0.9974733699999999,0.97356812,0.9821640233333334,0.99996892,0.9982363099999999,0.9991765533333333,0.9974729633333334,0.9982911666666667,0.9965875633333333,0.99402474,0.8694053833333334,0.9985460866666668,0.9975506,0.9971191899999999,0.9932638300000001,0.9876201333333334,0.99852812,0.9992739466666668,0.99796085,0.6358293066666666,0.9988313866666667,0.87774348,0.9693795233333334,0.9994509533333332,0.9956586000000001,0.9967822333333333,0.9997431166666667,0.38167007,0.9944021266666666,0.9982231666666667,0.9970835899999999,0.9999987733333334,0.9999949233333334,0.9998830666666666,0.9997670333333334,0.9998633133333333,0.9944399133333334,0.9999134766666667,0.9996819666666666,0.9985219166666667,0.9985406,0.9997173500000001,0.99966425,0.9948434966666667,0.9997171666666667,0.9992784333333334,0.9992923166666667,0.9668011233333335,0.9434165,0.9983067966666667,0.9957471666666667,0.9999204466666667,0.9682656233333334,0.9999798966666668,0.99976641,0.9998870000000001,0.99965765,0.99924508,0.9997916333333334,0.9957910033333333,0.9953920900000001,0.9822807,0.9999788666666666,0.9975545,0.99936947,0.99940316,0.99949809,0.9996627233333334,0.99973893,0.9991055133333333,0.9998525766666667,0.9971589200000001,0.9999547300000001,0.9998131333333333,0.99736831,0.9665485133333332,0.9965949766666666,0.9995455066666666,0.9995088633333333,0.99999062,0.9995702666666667,0.99975628,0.9999956166666667,0.9884107000000001,0.99999465,0.9998682133333333,0.9999396433333333,0.9958894433333333,0.99644762,0.9998247600000001,0.8767664666666667,0.9386246033333334,0.98084619,0.99948979,0.9980014566666666,0.9959375,0.9998655666666667,0.99607739,0.9999903433333334,0.9998833633333333,0.9983170366666666,0.9998923033333332,0.9993895199999999,0.9997428666666667,0.9997728266666667,0.99993543,0.98161726,0.99977039,0.9998404566666667,0.9998797766666666,0.9999324833333333,0.9997887666666667,0.9999778,0.8657677766666666,0.9999682999999999,0.9998256,0.99870444,0.9998766733333332,0.99985992,0.9996316766666666,0.9994207666666667,0.9952085766666666,0.9891635166666667,0.99992665,0.9428689666666666,0.9903577966666667,0.9968528000000001,0.9999855366666667,0.9531399566666666,0.9996734433333333,0.9972150333333333,0.99576371,0.9981399766666668,0.9959804933333333,0.39568252333333337,0.99399241,0.99668057,0.9988851899999999,0.99931505,0.9926288799999999,0.9998249666666666,0.91171475,0.99558789,0.9934472666666667,0.965274,0.9995976033333333,0.9997576,0.9967121333333333,0.9975321333333333,0.99960449,0.9989120333333333,0.9994782966666667,0.9993958666666667,0.9989391799999999,0.9958159766666667,0.9990293166666667,0.6991791333333334,0.9987371333333334,0.9983993266666666,0.9953740433333333,0.9434058100000001,0.9984687666666666,0.9995225166666666,0.9974043233333333,0.99684461,0.9997427766666668,0.9991683933333334,0.7193282,0.9994220533333333,0.9977169,0.9572886233333334,0.99814083,0.9988164333333334,0.9857374533333333,0.9908780533333333,0.99966366,0.9995349399999999,0.99935455,0.9998694499999999,0.9978780199999999,0.9987128733333334,0.99962878,0.9868903166666666,0.9839921166666666,0.7832026333333334,0.99971802,0.99852885,0.99260556,0.9016970299999999,0.9985014466666667,0.9996454866666666,0.90817625,0.9991389233333333,0.999942,0.99908775,0.9985301,0.96174824,0.9983991333333333,0.9993803666666666,0.9997538666666667,0.99942952,0.9994980433333334,0.9995155633333334,0.9947206799999999,0.9954040233333333,0.9996840833333334,0.9995894633333333,0.9996185833333332,0.8680610666666667,0.9981907466666667,0.9985741766666667,0.7105417766666666,0.9984736533333334,0.9986737,0.851476,0.9993901,0.83487218,0.9995979166666666,0.9988227833333333,0.9982028666666668,0.98691556,0.9911699766666667,0.99259545,0.9992192000000001,0.9975506166666667,0.9994585133333334,0.9985014666666666,0.9998856000000002,0.9973839766666667,0.9406349833333333,0.99915695,0.9997954266666667,0.9996942,0.9897550566666666,0.99972077,0.9998991466666666,0.9995702300000001,0.9997126666666666,0.9925706999999999,0.9957653433333333,0.9957045433333334,0.9974786300000001,0.99978788,0.98151918,0.9980870333333334,0.9987417566666666,0.9980327433333334,0.9858296566666667,0.800106125,0.9926871,0.9885214433333332,0.9999230366666666,0.9979600566666665,0.9995591033333334,0.9995579,0.9999013833333333,0.9992792666666667,0.99712938,0.9995802633333333,0.9988283033333333,0.97746727,0.72815685,0.9992394766666667,0.9991977466666667,0.9995508666666667,0.99892831,0.92711775,0.9182712533333334,0.9994822133333333,0.9897598799999999,0.9951798466666667,0.9962177233333334,0.99977594,0.9997128133333333,0.9998387333333333,0.9277223,0.9996872566666667,0.9998577233333332,0.9993298833333334,0.9941033699999999,0.9996020633333332,0.9910146000000001,0.8237435233333333,0.9999904700000001,0.9995528866666668,0.9010166533333334,0.9691747999999999,0.9919097133333333,0.9986974200000001,0.9994735,0.9968187199999999,0.9892731,0.9993435366666666,0.9884343800000001,0.9947902966666667,0.9967701099999999,0.9979774133333333,0.9934261233333334,0.9994726866666667,0.9991196333333333,0.9992045666666667,0.9879389633333333,0.9976517833333333,0.9535802333333333,0.9989837933333333,0.9911369466666667,0.9969764733333334,0.9998203866666667,0.9996064766666667,0.9989935799999999,0.9819778333333332,0.9369703166666666,0.99982885,0.9990664533333332,0.99275675,0.9994951333333333,0.9906347666666667,0.9744446066666667,0.9752438166666666,0.9908172666666667,0.9998543833333334,0.99981238,0.9960555833333333,0.9961518833333334,0.9999208866666667,0.9998650000000001,0.7356051466666665,0.9983716666666665,0.9997823333333334,0.9943947799999999,0.98073941,0.9996731799999999,0.9988755266666667,0.9856876633333332,0.9999171066666667,0.9646845366666666,0.9989861333333333,0.9996263966666666,0.9897584,0.9987608766666667,0.638749985,0.9986158333333334,0.9522952333333333,0.9944047566666666,0.9999455933333333,0.9996815033333334,0.9973364366666667,0.9991114433333333,0.9999067166666666,0.9965151333333333,0.9999338533333333,0.9993819500000001,0.9991985200000001,0.9999693666666666,0.99999089,0.99997711,0.99942275,0.99888605,0.9996691533333334,0.9966027433333333,0.98831632,0.9984956,0.9963112266666666,0.9958699433333335,0.9778199366666667,0.9978767566666665,0.9038874633333333,0.9961403999999999,0.9996363,0.25705257,0.16800840666666664,0.99720047,0.9906189633333332,0.9973663666666667,0.9999860133333334,0.9999331666666667,0.9999846266666667,0.9998509866666666,0.99958001,0.9999919,0.9999742,0.9994575133333333,0.9998708733333334,0.9999012666666666,0.9999923333333333,0.9996992666666666,0.9993259666666666,0.9999949566666667,0.9999981366666667,0.9999959566666666,0.9999988566666667,0.99999213,0.9999932466666666,0.9999987633333333,0.9999977333333333,0.9999968566666667,0.9999974166666666,0.9999965133333334,0.99994035,0.9967669666666668,0.9999376833333334,0.9999192899999999,0.9999252133333334,0.9986471,0.9936431433333333,0.9949588199999999,0.9998269666666667,0.99981609,0.9982528199999999,0.9904531466666667,0.99854252,0.9994132666666666,0.9897257233333333,0.9983317133333335,0.9999041133333333,0.9997082833333334,0.9998430533333332,0.999851,0.99905378,0.9998209066666668,0.9995984333333334,0.9918353,0.9997957333333334,0.9998064733333334,0.9991230666666667,0.99977043,0.9971356733333333,0.9998033666666667,0.9999465166666667,0.9995717333333333,0.9999449766666667,0.9995426766666666,0.9998440033333335,0.9972169866666666,0.9998258,0.9989982500000001,0.9976121099999999,0.9998777,0.9964325633333333,0.9931625766666666,0.9997620666666668,0.9999013366666666,0.9896304533333334,0.9962056433333334,0.9989461766666666,0.8945794399999999,0.9980545166666667,0.9999329733333333,0.9998226466666665,0.9985211633333333,0.99933972,0.9998299333333334,0.9999165566666667,0.9999614466666666,0.9972283533333334,0.9995718266666667,0.99881177,0.9972755466666667,0.9994730666666666,0.9757787233333334,0.9998518933333335,0.9999316133333332,0.9989263033333334,0.9994405,0.9997670733333334,0.99975638,0.99954376,0.9988382633333334,0.9988539166666667,0.9873594666666667,0.9996236999999999,0.9995708666666667,0.9975692500000001,0.9994165266666667,0.9952646133333333,0.99981332,0.99965198,0.9965134666666667,0.9966055866666667,0.9999330366666667,0.9997532000000001,0.99991935,0.998885,0.9998728,0.9997105333333334,0.9981166666666667,0.9996412533333333,0.9974563533333334,0.99858222,0.9994376466666667,0.9999039133333333,0.9995508766666666,0.9953843299999999,0.9999004766666667,0.9990088,0.9979111333333334,0.9967695366666667,0.9986870066666667,0.9983521333333334,0.9992499499999999,0.9989005933333334,0.9992058533333333,0.99987171,0.9961204766666666,0.9936302933333333,0.99977777,0.9986570433333334,0.9999018866666667,0.9995463899999999,0.9985328833333332,0.9999232366666666,0.9950758566666668,0.9999008766666666,0.99983742,0.9996851000000001,0.9999279433333333,0.99377548,0.9961443066666668,0.99984128,0.99797789,0.9980775166666667,0.99994885,0.997127,0.99988701,0.9981973466666667,0.9962573166666666,0.99814202,0.9988355200000001,0.9999603333333335,0.9998382100000001,0.99979592,0.99913598,0.9986385866666666,0.99961508,0.9988295733333333,0.9999014166666665,0.9996830533333334,0.98747168,0.9996222666666666,0.99016658,0.99417319,0.99761408,0.9998418299999999,0.9912062666666667,0.9999451333333332,0.9950808333333333,0.9997041866666666,0.9851178033333333,0.99965549,0.99831719,0.9950007533333333,0.9985783333333332,0.9999674166666667,0.9992672699999999,0.9998958,0.9996594833333333,0.9971831866666667,0.9997769566666667,0.9998210433333333,0.9970801466666667,0.9993875133333333,0.9996760766666667,0.9985145733333334,0.9998748166666666,0.9956665766666667,0.9990801666666668,0.9992474466666668,0.9811446166666666,0.99983361,0.9999031566666666,0.9967709,0.99942067,0.99775371,0.9998854566666667,0.9993409466666666,0.9984645833333333,0.9965688199999999,0.99953276,0.9997195733333334,0.9999436833333334,0.99465264,0.9989201799999999,0.9999187733333333,0.9999166333333332,0.9988080666666667,0.99375802,0.9989434999999999,0.9999172,0.9999734166666666,0.99319574,0.9999214799999999,0.9999316033333333,0.9979315133333334,0.9996045366666667,0.9999172999999999,0.9998627666666667,0.999135,0.99976101,0.9968766533333334,0.99933883,0.99620535,0.9998957333333333,0.9998994866666667,0.9979743333333334,0.5695825433333334,0.9992096466666668,0.9958147333333334,0.9994372133333332,0.9997270433333334,0.9994507066666666,0.9995997500000001,0.9998634933333334,0.9998094833333333,0.9990029133333334,0.9999499,0.9999002933333333,0.99473309,0.99931845,0.9999119133333334,0.9973458533333334,0.9962432666666666,0.9998554199999999,0.9995419966666667,0.9997210133333333,0.9998586633333333,0.9989921666666667,0.9998064666666666,0.9950853533333334,0.9997117533333334,0.9999199366666667,0.9997905,0.9892713766666666,0.9988430233333334,0.9967746133333334,0.9977483333333333,0.99991325,0.99937154,0.9989191666666667,0.9997092766666666,0.9975012733333334,0.99854048,0.9980086866666666,0.9965020833333332,0.9927522299999999,0.9991926533333334,0.9976736,0.9990901333333334,0.9993862833333335,0.9960712733333333,0.9993774200000001,0.99925476,0.9999580333333333,0.99936522,0.9989314133333332,0.9984137666666667,0.9882171533333333,0.9987461033333332,0.99935404,0.9998562866666667,0.9996012833333333,0.9997328033333334,0.9996961566666668,0.99987795,0.9998667833333333,0.99934542,0.9997099666666666,0.9996315333333333,0.9974305933333333,0.9991168766666667,0.9977040766666666,0.9998095166666667,0.9997111666666667,0.9956428333333333,0.99986372,0.99100665,0.99929217,0.9994860433333334,0.99946151,0.99983802,0.9970022799999999,0.9999300999999999,0.9999293466666667,0.9989042,0.9992656200000001,0.9996217466666666,0.9935227333333333,0.9997073433333333,0.9996473333333333,0.9997870666666667,0.9993297233333333,0.9979017666666667,0.9994847333333333,0.9999146533333333,0.9998438799999999,0.9999335199999999,0.9998851166666668,0.99992775,0.9962504466666666,0.9998393366666667,0.9946389466666666,0.9999092833333334,0.9979531166666668,0.9941456299999999,0.9996670166666667,0.9998408966666666,0.9963230433333333,0.99949455,0.9878827033333333,0.9957280466666667,0.9997989533333334,0.9880724000000001,0.9998262866666666,0.99884029,0.9998909666666665,0.9999054566666666,0.99939655,0.99829992,0.9996452033333334,0.9996434433333333,0.9701247666666667,0.9997751133333334,0.9999050233333334,0.9997794999999999,0.9953254899999999,0.9990014399999999,0.9993086433333334,0.9993172,0.9975034366666667,0.9994093666666667,0.9996032333333332,0.9968406166666667,0.8552321766666667,0.9991829333333332,0.9969575666666666,0.9990111333333332,0.99964229,0.99765315,0.9994175100000001,0.9996644466666668,0.9997414766666667,0.9804013,0.9986896766666667,0.9996745466666667,0.9998613500000001,0.99994719,0.9979560766666666,0.9945511499999999,0.9998588,0.9982770666666667,0.9999342433333333,0.9985432333333334,0.99669826,0.99973601,0.9951806066666666,0.99955566,0.9991958400000001,0.9997627033333334,0.9989878166666667,0.9999450466666667,0.9998883033333333,0.99947205,0.99961299,0.9998668366666666,0.9986018333333333,0.9972460066666667,0.999851,0.9984622166666667,0.9963756233333334,0.9997244333333333,0.9998591333333332,0.9969417166666666,0.9998379466666667,0.9999343933333332,0.9991412566666668,0.9977424733333334,0.9968693966666667,0.9307277266666668,0.99775255,0.9994248566666667,0.99976924,0.9998566733333334,0.9988550166666667,0.9998283333333333,0.9985553833333333,0.9999237500000001,0.9945432,0.9998963999999999,0.99355832,0.9994555533333332,0.9997221166666667,0.9997493966666666,0.9999320333333334,0.8639573866666668,0.9990456499999999,0.9986788333333333,0.99984735,0.9998225566666666,0.9995320366666666,0.9990209866666667,0.9989627333333333,0.9996863333333333,0.9925138966666666,0.99939906,0.9924521666666667,0.9977686833333334,0.9996754666666666,0.99989399,0.9990579866666667,0.99991699,0.9982565,0.9998474066666666,0.9977809333333334,0.99955243,0.9997695233333334,0.9998771666666667,0.9970407833333333,0.9996413166666667,0.9998547033333334,0.9998912133333334,0.9997292833333334,0.9999507333333333,0.9999199000000001,0.99933634,0.9959704866666667,0.9996089166666667,0.9997391333333333,0.99142692,0.9992855199999999,0.9996316333333333,0.9999027766666666,0.9999298666666666,0.9980935666666667,0.9989853133333334,0.9998616666666668,0.9996677633333334,0.9998479333333333,0.9993910933333333,0.99970525,0.9932721666666667,0.9999783066666666,0.9959470133333334,0.9983983433333333,0.9998798433333334,0.9984369633333333,0.9989087166666666,0.99517606,0.9986835166666667,0.9998510266666667,0.9999674833333333,0.98939858,0.9997560433333333,0.99937055,0.9890954033333333,0.9966044366666665,0.9998833500000001,0.9998043133333333,0.9985584,0.99996166,0.9999940199999999,0.9999953133333332,0.9999575866666666,0.99999201,0.9999926133333333,0.9999938566666667,0.9999878333333333,0.9999907366666667,0.9999566033333332,0.9999773833333334,0.9999958699999999,0.9998829766666667,0.9997963366666666,0.99999085,0.9999336866666667,0.9999916333333333,0.9999956333333334,0.9998011333333334,0.9999762866666666,0.9999849,0.9999880000000001,0.9999917666666667,0.94955658,0.9999037999999999,0.9999973766666667,0.9999939000000001,0.9999707999999999,0.99986703,0.9999869333333334,0.9999491466666667,0.9999842666666666,0.9999861366666667,0.9999971766666667,0.9988236,0.9999480166666667,0.9999818,0.9999415099999999,0.99998555,0.9999955833333333,0.9999834333333334,0.99998792,0.9999964166666667,0.9999886,0.9998556733333333,0.9999554466666667,0.9999977666666666,0.9999225233333333,0.99999226,0.9999591200000001,0.9768517166666667,0.9930049433333333,0.9999975333333332,0.9999545400000001,0.9372591833333334,0.999994,0.9999475766666667,0.9999583200000001,0.9996654566666666,0.9999963833333334,0.9999884333333333,0.9999490866666667,0.9999943966666667,0.99999115,0.9999734866666666,0.9999610666666667,0.9996578399999999,0.9999772666666668,0.9999982866666667,0.9999196633333334,0.99971025,0.9999883266666667,0.9999849,0.99997792,0.9997331666666667,0.9999997666666666,0.9999966233333333,0.9999921133333333,0.9999582366666666,0.9999392433333334,0.9999474300000001,0.9999391699999999,0.9999906633333334,0.99997298,0.9997181999999999,0.9999201166666666,0.9999797999999999,0.9999980900000001,0.9997701033333333,0.9991382533333333,0.9997098333333333,0.9999812,0.9999847833333333,0.9999061899999999,0.9999865666666666,0.9999970166666667,0.9999609666666668,0.9999947833333334,0.9999942333333333,0.9999946333333334,0.9999624833333334,0.8385427999999999,0.99999722,0.9999934466666667,0.9999907,0.99999055,0.9999898133333334,0.99975903,0.9999955833333333,0.9999845133333333,0.9999801,0.9999974566666667,0.9998980533333333,0.9999800533333333,0.9999821833333332,0.99999301,0.9125713466666667,0.9999558166666667,0.99999849,0.9999792466666667,0.9993551866666667,0.99996249,0.9999905099999999,0.9999939666666666,0.9999393666666666,0.99999788,0.9998970833333334,0.9999878366666667,0.9977468533333335,0.9999886,0.9993427633333334,0.9999969433333332,0.9999766,0.9999972600000001,0.9999884333333333,0.9999913833333333,0.99963605,0.9999598666666666,0.9999954333333333,0.9999936800000001,0.9999970166666667,0.9999954566666668,0.99999153,0.9999265833333334,0.9880247966666666,0.9999821666666667,0.9999980966666667,0.9997814100000001,0.9999708666666667,0.9999424866666667,0.9434512666666667,0.99988843,0.9999941199999999,0.99998837,0.9999957333333334,0.9999946033333332,0.9999904966666667,0.999456,0.9999937133333333,0.9998132233333333,0.9411429,0.9999653566666667,0.9999798533333334,0.99999701,0.9999825866666666,0.9999689333333333,0.9999981633333334,0.9999666966666666,0.9999884766666667,0.9997013899999999,0.99999834,0.99980102,0.9891709,0.9999968666666668,0.9999476333333334,0.9999966333333333,0.9999952866666667,0.99996253,0.99998955,0.9958901333333333,0.9999740933333333,0.99999201,0.9998542666666667,0.9999168766666666,0.9999925166666667,0.9999908966666666,0.9982381533333333,0.99999158,0.9999833166666666,0.9996424933333333,0.9999906666666667,0.9999768000000001,0.9999448000000001,0.9881007533333334,0.9999975733333333,0.9999837500000001,0.9999856500000001,0.8615911433333334,0.9999878833333334,0.9999979733333334,0.9541731333333333,0.9999706,0.9999194533333333,0.99998049,0.99997778,0.9999767966666666,0.9999812,0.9999985566666667,0.9999855000000001,0.99999562,0.9999448433333334,0.9999945566666666,0.9999872466666666,0.9953756566666666,0.9979675966666667,0.9999965666666667,0.9999799333333333,0.9999939266666668,0.9999682066666667,0.9999955933333333,0.9999859233333334,0.9999896333333332,0.9999914333333333,0.9999815999999999,0.99998797,0.9999969333333333,0.99996484,0.9999972,0.9999875933333334,0.9999990133333334,0.9999499266666666,0.9999953866666668,0.9994053433333333,0.9999760466666666,0.9993708666666666,0.9999365333333333,0.9999832333333334,0.9999723066666667,0.9999715,0.9999889066666666,0.9999473999999999,0.9999957333333332,0.9999946,0.9996204366666667,0.99995748,0.9999886766666667,0.9999929366666667,0.99996837,0.9999611,0.9999922133333333,0.9998973966666668,0.9999895866666667,0.99997477,0.9999966233333333,0.99998195,0.9999138566666667,0.9925743833333334,0.9999901166666666,0.9999965433333333,0.9999920066666667,0.9999942,0.99999348,0.9999892300000001,0.9998972,0.9999928766666667,0.99999153,0.9999954833333334,0.9993160200000001,0.6306440233333334,0.9998661866666666,0.9999876799999999,0.9999901466666666,0.9999895466666667,0.9999629766666667,0.9999926,0.9999974666666667,0.9999941200000001,0.9999744666666667,0.99997981,0.9999861000000001,0.9999889566666665,0.9706519233333334,0.9999952333333333,0.9999985666666666,0.9999341199999999,0.9999987200000001,0.9999993666666667,0.99989142,0.9998870166666668,0.9999467933333334,0.9940137466666666,0.99985695,0.99971865,0.9431794333333333,0.9486315766666666,0.9552153166666667,0.9998701033333334,0.9999865333333333,0.9996970733333334,0.9999632433333333,0.99995248,0.9999843166666667,0.99998064,0.9979599766666666,0.9997387333333333,0.9979112133333333,0.9953628133333333,0.9998600366666667,0.9999295566666667,0.99996902,0.9997783533333333,0.9997076366666667,0.9991976966666667,0.9921905333333333,0.9989404333333334,0.9999434633333334,0.98631423,0.9996838866666667,0.97612655,0.9998314866666668,0.9995836833333334,0.9846304866666666,0.8549037133333334,0.9997225,0.99801127,0.9971628433333333,0.9995092033333334,0.93786318,0.9320726833333334,0.9997257999999999,0.9565934633333333,0.9830034333333334,0.8914264333333334,0.9995924666666666,0.9813214666666666,0.9887701633333333,0.9995413766666666,0.9996104233333334,0.99960332,0.99925418,0.9999029666666667,0.99987557,0.9992870466666667,0.9998667433333334,0.9997160066666666,0.99974635,0.9997615999999999,0.9999178766666666,0.9995397766666666,0.9999881833333334,0.9999100466666667,0.9835829866666667,0.9999739633333333,0.9998524266666666,0.9923582866666667,0.9998957966666667,0.9981865666666666,0.8245790999999999,0.93120102,0.9291860633333333,0.9950621233333333,0.8912347866666668,0.9823622333333333,0.9999784166666666,0.9995339666666667,0.9375291166666666,0.99291822,0.9943510166666666,0.9986021533333332,0.9999211,0.9999466233333334,0.9942729266666667,0.9999304166666666,0.9999819533333333,0.8882269666666667,0.9999709999999999,0.9989184333333333,0.9952389433333333,0.9998029333333333,0.9997741233333333,0.9997099866666667,0.9960261899999999,0.9998981100000001,0.9999087666666666,0.99701198,0.9986234499999999,0.9999518733333334,0.9999598666666666,0.99998339,0.99991007,0.9999555,0.9302465,0.9991700166666666],\"type\":\"violin\"},{\"box\":{\"visible\":true},\"fillcolor\":\"rgb(184,185,189)\",\"line\":{\"color\":\"black\"},\"marker\":{\"color\":\"black\",\"size\":2},\"meanline\":{\"visible\":false},\"name\":\"h3k27me3\",\"points\":\"all\",\"showlegend\":false,\"spanmode\":\"hard\",\"x\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"y\":[0.9972126466666666,0.9567753899999999,0.9823033166666667,0.9797847566666666,0.97192735,0.9906381999999999,0.9859719533333333,0.9452923200000001,0.8885833966666666,0.9646664533333333,0.9600989333333333,0.8712039533333334,0.9550925499999999,0.9820163333333333,0.9903864333333333,0.9925218633333334,0.5690832066666667,0.9772923800000001,0.9901866566666667,0.9941002666666666,0.9689322766666667,0.9657470333333333,0.9695942299999999,0.9622044166666667,0.9512741266666667,0.9702859466666666,0.9450827166666667,0.9952264999999999,0.9947547,0.7437970533333335,0.93670572,0.9566221666666667,0.8538048033333334,0.7895894666666666,0.8847618333333332,0.97814107,0.9666004766666667,0.9816338333333334,0.9927449166666666,0.9511895199999999,0.9912543866666667,0.9732599266666666,0.97204269,0.9883141000000001,0.9875768766666667,0.9912752666666668,0.9740430666666667,0.9350522666666666,0.9792188533333334,0.9953151433333334,0.9793615533333333,0.8903745399999999,0.7239328066666667,0.7212224300000001,0.9670199999999999,0.9829179,0.99000864,0.98546311,0.9910936666666667,0.9245526666666667,0.9917374333333333,0.96665885,0.9930097033333333,0.9830470766666667,0.8205616,0.9762002266666667,0.98087137,0.9875363533333333,0.4758871,0.9898497266666667,0.97598018,0.9792408300000001,0.9200207833333334,0.84938628,0.8961716666666666,0.9941831899999999,0.7508081166666667,0.9663005866666667,0.9178046700000001,0.9297538233333333,0.9961923666666667,0.9981743866666667,0.9958076633333334,0.9659679333333333,0.9935816700000001,0.9308196566666668,0.9872936433333334,0.8679768333333332,0.9664080899999999,0.9582838333333333,0.9148109333333333,0.9708590666666667,0.9949487433333334,0.9383907633333334,0.7065984533333333,0.9643283333333333,0.2896774266666667,0.96618748,0.9637203099999999,0.9683783166666666,0.9551699399999999,0.9008168333333333,0.9323892366666667,0.99173812,0.9794582866666667,0.9653146399999999,0.7364242,0.9488180666666667,0.9695970333333334,0.96419197,0.9494641333333332,0.8411877533333333,0.9292258833333333,0.9467916033333333,0.9615700966666667,0.9387220333333334,0.84921369,0.9735767066666666,0.7755922166666668,0.9805394999999999,0.9568397000000001,0.9505173866666666,0.9385711,0.9673681666666667,0.8344741,0.8706671866666666,0.9334273999999999,0.9622974666666666,0.9938448900000001,0.7521186666666667,0.8514316966666667,0.97412928,0.8077775666666667,0.8779070666666667,0.7209822333333333,0.9782112866666667,0.9475027300000001,0.9791324666666666,0.8905523233333333,0.9728346999999999,0.9054458966666666,0.9842292866666668,0.9561079166666667,0.9702054266666668,0.9954293499999999,0.9632768633333333,0.99537808,0.99073735,0.9941252566666666,0.9992965166666666,0.9990232233333334,0.9930328533333334,0.9973099533333333,0.9942921299999999,0.9651021466666667,0.9889002433333333,0.9906189166666667,0.98331277,0.97112984,0.9900071,0.9935215133333334,0.99609862,0.97771454,0.96576611,0.9927802366666666,0.9642191000000001,0.9897649566666665,0.9902391133333334,0.9955479333333334,0.9890779233333333,0.9986811533333334,0.9564938666666666,0.9805256833333335,0.9919407066666667,0.9846670966666666,0.99030868,0.9950459333333334,0.9820461333333333,0.9922492666666667,0.9912906,0.47828223000000003,0.6852310300000001,0.97627715,0.29901664,0.6496311433333334,0.2271837,0.219795395,0.6869888666666667,0.9826693733333333,0.36153081666666664,0.8066154666666666,0.9747798233333334,0.45598542333333336,0.887543,0.9432300666666666,0.7077864433333333,0.9762584133333334,0.9445560866666667,0.9950277933333332,0.9468757333333334,0.9506008,0.9125972,0.32298784333333336,0.9330886233333334,0.93193245,0.9380496233333333,0.382247045,0.8487687866666667,0.9851828166666667,0.22185833333333335,0.9641236566666667,0.9601897566666667,0.9618600133333334,0.9768274333333333,0.81299559,0.96594203,0.9045567166666667,0.9868265166666667,0.9979103666666665,0.9825989166666668,0.9715940333333334,0.9933299533333333,0.9785441166666667,0.7249371333333334,0.9839689233333333,0.9743912533333333,0.9659420866666667,0.9799431766666666,0.9877998833333334,0.9915351833333332,0.62154889,0.9930855833333334,0.9936709333333332,0.9235897266666667,0.9779104666666667,0.9574619166666666,0.9748214466666667,0.9941898533333333,0.40700428,0.96495882,0.6032208,0.9595692166666666,0.97742688,0.9198810466666667,0.9495764133333333,0.38603863,0.5626468666666666,0.9918503166666666,0.45850977666666665,0.9195952433333333,0.90540276,0.87479338,0.96192771,0.9626397766666667,0.9952232566666667,0.99647703,0.6198397033333333,0.9128447333333334,0.66831192,0.9663771233333334,0.8324921233333334,0.7242602033333333,0.9897613333333334,0.9068295166666666,0.8548636,0.7735363466666666,0.9846547166666667,0.9870002133333333,0.8818168233333333,0.93284862,0.8698923966666667,0.9308913766666667,0.98490945,0.9752216833333334,0.9013702499999999,0.7905932333333333,0.9791671866666666,0.4941251,0.9709226900000001,0.70863275,0.8692848433333333,0.6347083333333333,0.8487244466666667,0.60706374,0.70613271,0.6070361333333333,0.9866881666666667,0.7729079066666666,0.8341901333333333,0.6238966933333333,0.9529494433333333,0.98476226,0.58665785,0.9535446333333333,0.8620987166666666,0.9695909333333333,0.9704154366666667,0.61245179,0.9085136,0.9178545766666666,0.8502499433333334,0.9570739166666667,0.63446902,0.6304335533333333,0.6681737766666668,0.9727701333333334,0.9884044966666666,0.9870729033333333,0.96953265,0.9480386366666668,0.9582582966666666,0.7133364566666667,0.7093055566666666,0.8978519500000001,0.9085425866666667,0.9650468333333334,0.97733628,0.7120396933333333,0.9461371166666667,0.9690943399999999,0.7254210066666666,0.9402434833333334,0.4127457633333333,0.9653759133333333,0.9641568,0.7760778733333332,0.78526907,0.46540279500000004,0.9445929766666668,0.8994416900000001,0.9719820933333333,0.41875642333333335,0.9604271999999999,0.9877819200000001,0.9847017299999999,0.9903804933333333,0.9854528666666668,0.9854502233333333,0.9976438133333333,0.99786772,0.9975916533333334,0.5433321866666666,0.9959403666666667,0.9985385999999999,0.9987494766666667,0.9433250666666666,0.9161921166666667,0.9263694,0.7897885333333333,0.9503397133333333,0.5205686700000001,0.9756978033333333,0.9440448333333333,0.9245847700000001,0.9908687,0.8953496133333333,0.9568811966666666,0.8834938166666667,0.9486620833333334,0.9386723,0.97098525,0.9874734066666666,0.9887037333333334,0.9790130733333333,0.9849203033333334,0.9743574033333333,0.9927760466666666,0.9911057666666666,0.8617783499999999,0.9850940966666667,0.96920061,0.8755750966666667,0.96875952,0.9733027000000001,0.99537976,0.9842731499999999,0.9874244033333334,0.9929174333333334,0.6823130866666668,0.9932057466666667,0.9863572366666666,0.98393499,0.9951382666666667,0.9959855666666666,0.9862233466666667,0.7831753,0.83113644,0.9791386699999999,0.5796699966666666,0.9609743666666667,0.9588252066666666,0.9646058133333333,0.9466115199999999,0.9703115133333333,0.8110506466666667,0.99809671,0.9877681933333333,0.9859757033333333,0.9072963666666668,0.9784376633333333,0.97783367,0.9728046866666666,0.406100185,0.9949662300000001,0.9984613133333333,0.9968372799999999,0.8694371833333333,0.65913243,0.7439515999999999,0.81680033,0.9998733466666666,0.9953853566666666,0.9673309666666667,0.9996583133333333,0.9998179866666667,0.9998825266666667,0.9563040733333333,0.9974031600000001,0.9573288866666667,0.7053995233333333,0.9989682666666666,0.9992467766666667,0.9993164499999999,0.9932375333333333,0.9991060666666667,0.9968583333333333,0.9665168533333333,0.9989407666666666,0.9988933166666666,0.9944124333333333,0.9913063000000001,0.9706783866666667,0.99003202,0.9957777466666666,0.8302493466666666,0.42019191,0.9953704566666666,0.90064774,0.8419666666666666,0.9938274166666666,0.99434048,0.9967921333333334,0.9939450666666666,0.7616946866666666,0.8586004033333333,0.9945421900000001,0.7236369333333333,0.8731152233333334,0.9842375133333334,0.9949490966666666,0.9977136,0.9996750866666666,0.9989948666666666,0.9981250533333332,0.9990869466666666,0.9984753966666666,0.9969812833333332,0.9927690266666667,0.9995441166666668,0.9958355933333333,0.9976129666666665,0.9990781666666667,0.9981597566666668,0.9990550333333333,0.99924427,0.9038798199999999,0.9061043,0.99772473,0.9894459533333334,0.5743229733333334,0.6415560533333333,0.47711852000000005,0.8451043833333333,0.7155164466666667,0.33643368,0.3818321633333333,0.7868195333333333,0.9682678433333333,0.5734641533333333,0.6785910233333334,0.8923081700000001,0.27367172333333334,0.9516888066666667,0.9964413899999999,0.9995942433333335,0.9986627333333334,0.9691364466666667,0.9988839666666666,0.9998097333333332,0.8435500333333333,0.9013976499999999,0.8794745333333333],\"type\":\"violin\"},{\"box\":{\"visible\":true},\"fillcolor\":\"rgb(13,127,66)\",\"line\":{\"color\":\"black\"},\"marker\":{\"color\":\"black\",\"size\":2},\"meanline\":{\"visible\":false},\"name\":\"h3k36me3\",\"points\":\"all\",\"showlegend\":false,\"spanmode\":\"hard\",\"x\":[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2],\"y\":[0.9983597999999999,0.9951467633333334,0.9967049,0.9814994533333333,0.9791922,0.9955326133333333,0.9984450633333334,0.99126981,0.96923768,0.9768503333333333,0.9915791866666667,0.9325708,0.98625712,0.7874325799999999,0.54362885,0.9860088033333333,0.9511119366666666,0.9232124333333332,0.9932437133333334,0.8794145533333334,0.98760951,0.9834235466666668,0.99732295,0.97756131,0.9571129933333333,0.9937834100000001,0.9945961366666666,0.9945486,0.9913343499999999,0.9815827433333334,0.9619207866666667,0.9980847633333333,0.9969847933333332,0.9876805333333333,0.96759038,0.8653080666666666,0.8902671099999999,0.9782651266666665,0.9985104833333334,0.9772507866666666,0.9878435333333333,0.9987565233333333,0.9963850166666667,0.9956741600000001,0.9735604333333333,0.9721450666666667,0.9238819433333333,0.9988380233333333,0.9931692566666667,0.9778669333333333,0.8501841,0.9476786333333332,0.9962887333333333,0.9968221866666668,0.9247613966666667,0.9256964999999999,0.82827382,0.99736168,0.98491273,0.99359991,0.9786731,0.9968544166666667,0.98008228,0.9964218633333334,0.9893308566666666,0.9975944866666667,0.9893557333333334,0.8453127433333334,0.9829317733333333,0.9769066433333333,0.9943567366666667,0.43964638,0.9934653999999999,0.92409797,0.9459546799999999,0.9640893833333334,0.5935151799999999,0.81073691,0.9923149666666666,0.8093963866666667,0.9873312200000001,0.9785483199999999,0.9746243,0.9968776,0.9987385099999999,0.9856824566666665,0.9446146666666667,0.9404532099999999,0.9840084933333334,0.9848545666666667,0.9895952533333333,0.9847854566666667,0.9881844533333334,0.9705092533333334,0.9859548233333334,0.9977622833333334,0.9769518933333333,0.8346858666666667,0.9863636833333334,0.9932950133333334,0.97133237,0.9648860833333334,0.9935919333333333,0.9595562833333333,0.9671666266666668,0.9919203200000001,0.9979047799999999,0.9745629433333334,0.8992444833333333,0.9400583366666666,0.9827735566666668,0.99927713,0.9941686666666666,0.9904175199999999,0.7050634499999999,0.91906542,0.96745284,0.9786404666666666,0.9476233333333334,0.9588023,0.9849619233333332,0.83015889,0.9743545333333333,0.9795758666666666,0.9911284333333333,0.8157113033333333,0.9869885066666666,0.9316483999999999,0.9785939466666665,0.95143215,0.9963831500000001,0.9954600966666667,0.9514353833333334,0.9861977333333334,0.98959048,0.6783612899999999,0.9980457899999999,0.8701278800000001,0.97716526,0.9956321333333333,0.99213321,0.9874482566666667,0.9939920733333333,0.9202847433333333,0.9940100333333334,0.9901624466666666,0.9857483333333333,0.9989552266666667,0.9993996466666667,0.9355748133333334,0.9910962133333333,0.9939128266666666,0.9901270866666666,0.9294567933333333,0.9997554966666667,0.9736881199999999,0.9827160199999999,0.9920774533333333,0.995143,0.9894556566666667,0.9932665533333332,0.99316614,0.9859348866666666,0.9922121666666667,0.8546016700000001,0.99721211,0.9995868899999999,0.9882326,0.9882834133333334,0.99280242,0.9938371633333333,0.9830704499999999,0.99223253,0.9940748866666667,0.9283448766666668,0.9944733666666666,0.9759488533333333,0.9999934466666667,0.9935630166666667,0.9905295133333333,0.9877146466666668,0.99518768,0.9783486333333333,0.9870218166666667,0.9870834133333334,0.9834135233333333,0.9952545599999999,0.9998920833333335,0.9983140266666667,0.45424300333333334,0.85868018,0.87694085,0.99349491,0.58735883,0.7388083,0.9965229,0.95956841,0.9784436033333334,0.9286224566666667,0.9445631233333334,0.9597175133333332,0.9782237333333333,0.9909082233333333,0.99485722,0.9825506333333333,0.9986719333333333,0.9915140466666666,0.9980242433333334,0.7602877100000001,0.9977465666666667,0.9942165900000001,0.9210374133333333,0.39648374000000003,0.9907846333333333,0.6942232,0.98501194,0.7641477333333334,0.9923992233333333,0.9981093133333333,0.9653779,0.9719745333333334,0.9886655533333334,0.9496692000000001,0.9844787666666667,0.9969045400000001,0.9977837466666667,0.9967614433333334,0.9966665,0.9989210766666666,0.9992762166666666,0.9583216866666667,0.9905665166666667,0.9936302666666667,0.9966414466666667,0.99926753,0.9971709133333334,0.9984957799999999,0.9403903666666666,0.9977710466666667,0.9986025333333334,0.9937851000000001,0.9977118533333332,0.9877314333333334,0.9919295233333333,0.9969712833333334,0.9867360966666666,0.9685467499999999,0.8820383366666666,0.9988748433333333,0.9726473999999999,0.9973343900000001,0.9996321033333334,0.8941299866666667,0.8186745833333333,0.9540825866666666,0.67581068,0.9913397333333333,0.9979766099999999,0.99704096,0.99896935,0.9924109166666666,0.9897787666666668,0.5294812666666667,0.9746130333333335,0.9917416600000001,0.9088911333333334,0.8694414866666667,0.9864991700000001,0.9571526766666668,0.9453751133333334,0.9919922333333333,0.97172812,0.8256204199999999,0.9982337466666666,0.9947408666666666,0.9978802866666667,0.9917479166666666,0.7999686,0.6382070533333334,0.9967350466666667,0.9990501966666666,0.9664329666666666,0.9976949300000001,0.8590646666666667,0.9944037,0.84464775,0.9190907733333334,0.94596542,0.9962160766666667,0.9373654133333332,0.9984322766666667,0.96626205,0.81552408,0.9936757366666665,0.6458318133333333,0.99653223,0.7750736466666668,0.6398898333333333,0.9587622166666666,0.9976339066666666,0.9504472133333334,0.7453985900000001,0.8873849633333334,0.9556740666666667,0.9089157833333333,0.9655697666666666,0.9147774366666667,0.6364178266666666,0.9679429,0.9258336666666667,0.9758876266666666,0.98903997,0.8684240533333334,0.9881713799999999,0.9085085999999999,0.9842088333333333,0.98315567,0.8995240466666666,0.8714449633333333,0.9758514633333334,0.99120369,0.9967026166666667,0.9091225933333332,0.9677217133333333,0.7151854666666666,0.9859869666666666,0.9588902866666666,0.9714052766666667,0.9693974533333334,0.9776010966666666,0.9941695666666667,0.9738323000000001,0.9999908633333333,0.9998888766666667,0.9999291499999999,0.9999539466666666,0.9992156333333333,0.99993981,0.9999300333333334,0.95680882,0.99541979,0.9854747033333333,0.9990876666666666,0.9982258333333333,0.9725617999999999,0.8732897300000001,0.9799973066666666,0.97765134,0.7020621,0.9987526,0.9862945000000001,0.9820485333333333,0.9912063933333334,0.9628557966666667,0.8800380166666667,0.97747925,0.9538298433333333,0.9794513333333333,0.9999352199999999,0.9946078133333334,0.9996999133333334,0.9983175733333334,0.99915172,0.9999100866666667,0.9982049366666667,0.98416548,0.99896248,0.9994447233333332,0.99939132,0.9828586666666667,0.9993579,0.9890662833333334,0.9730954333333334,0.99582945,0.9954894866666666,0.9921342699999999,0.96635838,0.9967089666666666,0.9990043166666668,0.9968955533333334,0.9845107799999999,0.9846295866666667,0.9978950866666666,0.8941299266666668,0.99919985,0.9569326866666666,0.9990394433333334,0.99020919,0.9874577233333334,0.9931063333333334,0.9921292899999999,0.94954392,0.99471429,0.9971697333333335,0.9986151799999999,0.9395379833333334,0.6930944299999999,0.756671425,0.9830911366666667,0.9964217,0.9971226933333334,0.9995589666666667,0.9983389766666667,0.9996898666666666,0.9596714999999999,0.9921451466666666,0.9898062666666667,0.7161113233333333,0.9990947666666666,0.9988066533333333,0.9989947433333334,0.9989540199999999,0.99716258,0.9976403133333333,0.9850068333333333,0.9967126433333333,0.9951604666666666,0.9969319866666666,0.9834970799999999,0.9031438666666666,0.93958138,0.9989260999999999,0.9946681866666666,0.9885991233333332,0.94657518,0.9937398566666666,0.7905625000000001,0.9987393866666667,0.9993607333333333,0.9982659133333334,0.9671980666666666,0.9537774899999999,0.9799988766666666,0.98863332,0.70701355,0.9602379966666668,0.9904660033333333,0.9669595466666667,0.9966755333333334,0.98096798,0.99686845,0.9982067,0.9635964333333332,0.9973359466666666,0.9901077033333333,0.9967712999999999,0.96401773,0.8805070733333333,0.9883259266666666,0.9279297666666667,0.9987971333333334,0.9937503900000001,0.9993984533333333,0.7108810066666668,0.6775713366666666,0.5421101566666666,0.9652717566666666,0.6970082333333334,0.9877647633333333,0.9982872333333334,0.6339351666666667,0.8343036333333332,0.48206957,0.6727694966666666,0.9654290933333334,0.9863757166666667,0.85851668,0.5132675233333334,0.7414879666666666,0.8844387733333333,0.48193692499999996,0.541069485,0.99489567,0.6437497633333333,0.9831016866666666,0.9822753,0.9104992666666667,0.9968385666666667,0.9849202233333333,0.9873547133333332,0.9857934866666667,0.9216918500000001,0.9968279833333332,0.9963591333333334,0.9992212666666666,0.7473673366666667,0.9978021033333334],\"type\":\"violin\"},{\"box\":{\"visible\":true},\"fillcolor\":\"rgb(236,232,56)\",\"line\":{\"color\":\"black\"},\"marker\":{\"color\":\"black\",\"size\":2},\"meanline\":{\"visible\":false},\"name\":\"h3k4me1\",\"points\":\"all\",\"showlegend\":false,\"spanmode\":\"hard\",\"x\":[3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3],\"y\":[0.9993751,0.9989352566666666,0.9912537133333333,0.9996688133333334,0.9959343966666667,0.98044019,0.9951805833333333,0.99285002,0.9926788833333333,0.99715638,0.9880737833333333,0.9693312500000001,0.9565293800000001,0.9881275333333334,0.998097,0.9945134866666666,0.9759945633333333,0.9439407866666666,0.9911225566666667,0.9943221233333333,0.9952763533333333,0.9960376166666668,0.9985768166666666,0.9934818999999999,0.9892325133333334,0.98247654,0.9944191566666666,0.9958566433333335,0.98650767,0.9096985333333333,0.9684346866666665,0.7190332333333332,0.9772715,0.96777112,0.9991767466666667,0.99834856,0.9241624333333333,0.9928403,0.9770690333333333,0.99509893,0.9978846366666666,0.9995179866666666,0.8297561099999999,0.9979539000000001,0.9898733866666666,0.89413474,0.9988932866666667,0.99791328,0.9847862666666667,0.9949098833333334,0.9953658000000001,0.9963619433333334,0.9289557966666666,0.9962710333333334,0.99872322,0.9987543899999999,0.9623777466666666,0.9723915466666666,0.95218993,0.9992785333333334,0.9972511233333333,0.97743748,0.9444486333333333,0.9970500233333333,0.9846319,0.99693821,0.98040245,0.9991534866666667,0.9995118333333334,0.9606986566666667,0.9988775433333333,0.9366003966666666,0.9522982433333333,0.9972638333333333,0.9978861933333333,0.98127465,0.9532379633333333,0.9942300933333333,0.9018816333333334,0.9836281000000001,0.90499908,0.89967666,0.8276155,0.99431878,0.8532862,0.9876018466666666,0.9957898833333333,0.9910316899999999,0.9992498866666667,0.9996029700000001,0.9998694066666666,0.99022792,0.9997869666666667,0.9700248033333333,0.9951367333333333,0.97459162,0.9909357233333335,0.9920408766666666,0.9979276333333332,0.9784277466666667,0.9999884,0.9971609366666666,0.9072375166666666,0.9994466333333333,0.9965248566666668,0.9970786166666666,0.9763800799999999,0.9993432133333333,0.9844508433333333,0.9977052666666667,0.9819065433333334,0.9985844199999999,0.9745369,0.8518971,0.9955780666666666,0.9956058366666666,0.9990966133333333,0.9995426533333333,0.9985109233333334,0.9029315233333334,0.9863819533333333,0.9906565233333332,0.9855433166666666,0.9220932966666666,0.9915688666666668,0.9991607333333333,0.9936347199999999,0.9959270866666667,0.9988302966666667,0.9933638166666667,0.9189318166666666,0.9991320533333333,0.9949246766666667,0.9913766599999999,0.9911590133333333,0.9993695966666666,0.98982589,0.9954798666666665,0.9150356866666667,0.9961475966666667,0.9664217800000001,0.9727313166666667,0.9999552,0.9948309866666666,0.9993222833333334,0.9992387766666666,0.8988870433333332,0.99916618,0.9992940899999999,0.99815305,0.9781390333333334,0.9997503366666667,0.99768585,0.9851197466666667,0.9650069566666667,0.9998967,0.9990932933333333,0.9963375,0.9815079433333334,0.9994016800000001,0.99940512,0.9977062633333333,0.9994727866666667,0.9992224333333333,0.9997649333333333,0.99909318,0.9668382000000001,0.9989800666666667,0.9990972333333333,0.9977576300000001,0.9928238666666666,0.9995773533333333,0.9708383500000001,0.9999759899999999,0.9967932333333334,0.9998223433333333,0.9989120666666667,0.9953802666666666,0.99926841,0.9992384533333333,0.9996760133333332,0.9991913433333334,0.9976359133333333,0.9982384599999999,0.9984956666666666,0.86579417,0.9976541033333333,0.9929072900000001,0.9999823166666667,0.9987797466666667,0.9923616833333333,0.9996998666666667,0.9995008333333333,0.9985025200000001,0.9869263533333333,0.9948080666666668,0.9984397333333334,0.9995583333333333,0.9999079766666666,0.6780121133333333,0.86728003,0.99666482,0.98289961,0.9457711999999999,0.9847484666666667,0.9722303,0.9997744066666666,0.99447685,0.99989592,0.9397965200000001,0.9982133333333333,0.9937484333333333,0.99942559,0.9999460066666667,0.9634016766666665,0.9999424666666666,0.9999167,0.9818678666666667,0.9945640133333334,0.7585182466666667,0.9988111000000001,0.9989369466666668,0.9993901799999999,0.5682986666666666,0.9973129100000001,0.93556687,0.9969961,0.88631299,0.9987712833333333,0.99835361,0.9929137333333333,0.9981397133333333,0.99996484,0.9998953733333332,0.9960221833333334,0.9988098566666667,0.9993333666666667,0.9980521866666666,0.99930063,0.9996959333333333,0.9990712566666667,0.91037985,0.9959825466666666,0.9987658866666665,0.9989833333333333,0.99987248,0.9991694066666666,0.9998758099999999,0.8278396833333334,0.9990464566666667,0.9989651300000001,0.9985831333333333,0.99845554,0.9986113133333333,0.9998696533333332,0.9956244033333332,0.9913897466666667,0.9996100866666667,0.9851046333333334,0.99996209,0.9992439666666666,0.9996408266666666,0.99999555,0.9982963466666667,0.9978037466666666,0.9997903733333334,0.9914427199999999,0.9974599666666667,0.9943560333333333,0.9991589033333333,0.9981887,0.9994831933333334,0.9907327100000001,0.7733590333333332,0.9876994333333333,0.8387947466666666,0.9284169666666666,0.9125605999999999,0.5683729049999999,0.9990509466666667,0.9735867433333333,0.9802992333333332,0.9598930466666666,0.9979346866666666,0.9894361333333332,0.54460305,0.9446558666666666,0.9129249,0.9888383333333334,0.9865262333333334,0.9957868466666667,0.9734249666666667,0.9907692200000001,0.9856351999999999,0.9166528666666666,0.9978021466666668,0.9996438,0.9193060866666666,0.5569448850000001,0.99999491,0.66199838,0.99737257,0.9927081333333333,0.9922198366666667,0.9598445233333334,0.9980299333333335,0.9259257666666666,0.9967106666666666,0.9427862666666668,0.9761112133333333,0.9520865966666667,0.9974421866666666,0.9783868866666667,0.98415053,0.9926915666666667,0.8952424333333333,0.9808215633333334,0.9644032100000001,0.9781372199999999,0.9845729333333333,0.9446502200000001,0.8981846,0.9976367266666667,0.95568562,0.9914211533333334,0.9862165666666667,0.9746531833333334,0.7593129666666667,0.9819166133333334,0.9601405000000001,0.9597385666666667,0.94216195,0.8863305333333334,0.9721053533333333,0.7491762033333332,0.9755102466666666,0.9955824333333333,0.9872867533333333,0.9907051666666667,0.97350572,0.9996539333333333,0.95319828,0.9789125133333334,0.9722185,0.9448728466666667,0.9501709566666667,0.9182796333333334,0.8931501466666667,0.9204495,0.9997449433333333,0.9978463666666667,0.9579232666666666,0.9899915899999999,0.9365021833333333,0.9691739199999999,0.9855487966666666,0.7045749466666668,0.9964508733333334,0.9799611466666667,0.9975949666666667,0.97044327,0.9655370099999999,0.9976876433333333,0.9842744833333333,0.9664883,0.9989296233333333,0.9918392800000001,0.9999932033333333,0.9904467566666666,0.9999878466666666,0.9999909800000001,0.99999848,0.9988161433333334,0.9998937666666666,0.9996569766666666,0.9997617233333335,0.9945333399999999,0.9857517366666667,0.9512939366666666,0.9960968,0.9944069333333333,0.9031102999999999,0.9992092533333333,0.9967756533333333,0.9915928333333334,0.9993249333333334,0.98369349,0.99507665,0.9637857133333334,0.9941623433333332,0.9905279066666667,0.98861921,0.99983228,0.99993002,0.9997798566666667,0.9999795666666667,0.9999717,0.9985095133333334,0.99929652,0.9993394000000001,0.9248490333333333,0.9683688333333333,0.9998218866666667,0.9997216999999999,0.9998660366666666,0.9999351566666667,0.9999826366666666,0.9998696833333334,0.9988347000000001,0.9999283533333333,0.9999487333333335,0.9998615133333333,0.9998873666666667,0.9996406166666666,0.9999478666666667,0.9998984666666667,0.9998841133333333,0.9998452166666666,0.9988902999999999,0.9991514933333333,0.9998762999999999,0.9998922666666666,0.9998577333333333,0.9996470833333334,0.9997840833333335,0.9983931566666667,0.9995890599999999,0.9994123666666667,0.9990522833333334,0.9999137033333333,0.9998817433333334,0.9998879666666666,0.99941478,0.99981577,0.9989802166666667,0.99987028,0.9992757,0.9989244133333334,0.9999052,0.99507624,0.9996748333333333,0.9989378133333333,0.9968073866666667,0.999679,0.9988566766666667,0.9999383266666667,0.99980792,0.9987489766666666,0.9995901666666667,0.9989909333333333,0.9985981133333333,0.9999216733333333,0.9999022000000001,0.9992046733333333,0.9998772366666667,0.9999550166666666,0.9992912,0.9985805833333333,0.9989159166666667,0.9978090333333333,0.99927153,0.99959831,0.9988346166666666,0.9998754666666666,0.99882551,0.9997606533333334,0.9994266666666666,0.9992007833333334,0.9998263833333333,0.9997488900000001,0.9999079866666666,0.9988091666666667,0.99979562,0.9990825799999999,0.99779977,0.9992305833333334,0.9999700433333333,0.9990059666666666,0.9999173533333333,0.9995037333333334,0.9996502366666666,0.99957194,0.9999481000000001,0.9999000933333333,0.9991168300000001,0.99925768,0.9995921333333334,0.9991934499999999,0.9999500933333333,0.99879798,0.9995003333333333,0.9998845933333333,0.99986276,0.9998363666666666,0.9986949333333334,0.9997084466666667,0.99906929,0.9991247533333333,0.99985238,0.9989582666666666,0.9983893433333333,0.9980131,0.9996949033333333,0.9998835100000001,0.9998079466666666,0.9994681133333333,0.9985810866666668,0.9992237966666666,0.9999435766666666,0.9992353533333334,0.9999553666666667,0.9945324833333333,0.9911581666666667,0.9961015466666666,0.9998529166666666,0.9994982666666666,0.9998906333333334,0.9998202833333334,0.9998772033333333,0.9999567699999999,0.99974922,0.9999127333333333,0.99948733,0.9996015133333334,0.9975934333333334,0.9998449666666667,0.9999197300000001,0.9991918733333334,0.9990983299999999,0.9991816,0.9987893466666667,0.9998636799999999,0.9989176999999999,0.9997444766666667,0.9989950200000001,0.9993527333333333,0.9991492833333334,0.9993191333333332,0.99789198,0.99940018,0.99892739,0.9998157000000001,0.9987974199999999,0.9993453533333333,0.99970116,0.9996915,0.99885432,0.99987298,0.9996154766666666,0.9987538466666667,0.9991515666666667,0.9999633933333333,0.9993024833333334,0.9996668999999999,0.9998327300000001,0.9998981366666667,0.99963218,0.99902428,0.9998640766666668,0.9998458499999999,0.9996957700000001,0.9972349500000001,0.9999198466666667,0.9995939666666667,0.9982581433333334,0.9994129333333334,0.9992355266666667,0.9994339233333333,0.9999004533333333,0.9992578999999999,0.99972545,0.9989687266666666,0.9996727666666666,0.99876781,0.99920426,0.9970529566666667,0.99988172,0.9999408000000001,0.99985595,0.9998936633333333,0.9978491666666667,0.99992934,0.9995770299999999,0.9996828666666667,0.9960359333333333,0.9998428333333335,0.9944954866666667,0.9994946100000001,0.9987011733333334,0.9987434500000001,0.9997014200000001,0.9987260333333333,0.9788237000000001,0.9991824666666665,0.9992962,0.9992852666666666,0.9998239766666668,0.9987598200000001,0.9985252333333333,0.9987320566666668,0.9997433333333333,0.99918019,0.9995647433333333,0.9987870333333332,0.9986742333333334,0.9997848133333335,0.9995998466666666,0.9972592800000001,0.9990132800000001,0.98278183,0.9999520333333334,0.9991797033333333,0.9999605800000001,0.9995235833333332,0.9999287066666667,0.9985748,0.99975968,0.99985738,0.9998878833333333,0.99983859,0.9962057333333334,0.9996846233333333,0.9991545333333334,0.9998612933333333,0.9993481700000001,0.99884071,0.9987583,0.9989521866666667,0.9997644000000001,0.9992508933333334,0.9998901099999999,0.9994986233333334,0.99993964,0.9987362533333334,0.9992596666666667,0.9994689466666666,0.9991355500000001,0.998068,0.9997780666666666,0.9977074,0.9997264433333334,0.9999492833333333,0.9982519999999999,0.9966270466666667,0.99942558,0.9995832266666667,0.9998120666666667,0.9978328333333334,0.9996498433333333,0.9998536533333334,0.999216,0.9987583,0.9990816833333334,0.9997788333333334,0.9997304133333333,0.9988721666666667,0.9994761866666666,0.9985803166666667,0.9999031766666667,0.9997344333333333,0.9998373866666667,0.9990170666666667,0.9993903833333334,0.9998117166666667,0.9994833333333334,0.9996792633333333,0.9985382633333333,0.9991657533333335,0.9998699166666668,0.9998127499999999,0.8225480533333333,0.99998982,0.9998034066666667,0.99999094,0.9999936333333334,0.9999530466666666,0.9999420633333332,0.9999808433333333,0.9985471800000001,0.9969091333333333,0.9999737366666667,0.99992342,0.8336764133333334,0.693026935,0.85979565,0.9999747666666666,0.9999854333333333,0.99962668,0.9999830666666667,0.9999653933333333,0.9999090433333334,0.9999586666666667,0.99831541,0.99998062,0.9962139333333333,0.8134057866666667,0.9999311733333333,0.99978907,0.9999104466666666,0.99932345,0.9998626833333333,0.99965318,0.9950576433333334,0.9998146333333334,0.9996388733333333,0.9997696733333333,0.99857815,0.9898075533333334,0.9983213900000001,0.99975911,0.9580116233333333,0.9666514233333334,0.9991041333333334,0.9897570166666666,0.9888552,0.9990888666666667,0.9994443500000001,0.9992724700000001,0.9993729833333332,0.8668783333333333,0.9392287133333334,0.9994131533333332,0.67122823,0.8269260333333334,0.9835200133333334,0.9986989666666667,0.9988621133333334,0.9990635766666666,0.9985673,0.9995520433333334,0.9996100466666666,0.9989257666666665,0.9996772833333334,0.99853346,0.9956556333333334,0.9986534100000001,0.99935749,0.9996641299999999,0.9993681533333333,0.9993662466666667,0.9991902866666668,0.9999778633333333,0.7969076999999999,0.99997668,0.9998567399999999,0.9572603333333333,0.9997585333333333,0.9929848533333333,0.9609912733333333,0.8885340566666667,0.9974618666666667,0.998833,0.9905396266666667,0.6895987,0.9801337,0.9902682566666666,0.9993979066666667,0.9971668433333334,0.9149253033333333,0.9730576599999999,0.9929448666666666,0.9991089833333332,0.9772136033333334,0.9994017933333333,0.9988188466666666,0.9998757333333333,0.9984881533333333,0.99857804,0.9997239133333333,0.9999431333333334,0.9999797466666666,0.6323778866666667,0.9991571466666667],\"type\":\"violin\"},{\"box\":{\"visible\":true},\"fillcolor\":\"rgb(238,30,37)\",\"line\":{\"color\":\"black\"},\"marker\":{\"color\":\"black\",\"size\":2},\"meanline\":{\"visible\":false},\"name\":\"h3k4me3\",\"points\":\"all\",\"showlegend\":false,\"spanmode\":\"hard\",\"x\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"y\":[0.9755845566666667,0.9920526733333334,0.89396809,0.9875121099999999,0.9628885199999999,0.9851967500000001,0.9494181233333333,0.8891695033333334,0.9839074833333333,0.9831023,0.9816855666666667,0.6524336333333333,0.9270237200000001,0.8910932666666667,0.89167428,0.8891195866666667,0.9876666333333333,0.9472308666666667,0.8378970333333333,0.9522117233333334,0.53146755,0.67419679,0.9079577733333334,0.9469904666666666,0.97904315,0.9807581500000001,0.99140452,0.9268952166666665,0.83210586,0.9625007099999999,0.9431514,0.9434188533333333,0.8301733200000001,0.9842936999999999,0.9587671999999999,0.9925663400000001,0.9837196566666667,0.87356336,0.9462778233333333,0.9106788666666666,0.9006337499999999,0.80669472,0.9128658666666666,0.9912831166666667,0.6308154633333333,0.97139422,0.972298,0.6302343,0.9791177666666666,0.9896106866666666,0.9305295666666668,0.9767077766666666,0.9901785133333334,0.9369388633333333,0.9445076833333333,0.9066649566666666,0.9718801133333334,0.9632939633333333,0.93005585,0.9743060066666667,0.9284412333333334,0.8634550666666666,0.9922184500000001,0.9386101333333333,0.9603924266666667,0.7457434166666667,0.9633591333333333,0.9087199333333333,0.9020735433333332,0.8556503133333333,0.9920263666666665,0.9901072333333333,0.8758025466666667,0.9676146233333333,0.9517946,0.9719744,0.9828626333333333,0.9845663333333333,0.9820388466666667,0.4738823466666666,0.9615677533333334,0.8621464666666666,0.9079458866666666,0.9248160566666667,0.76161007,0.9505273333333334,0.7200953,0.79579133,0.9754393933333333,0.9384182000000001,0.9434327533333334,0.9925267433333333,0.9896206433333333,0.9278893766666667,0.8564475500000001,0.9949987333333334,0.90457979,0.90099046,0.9478555533333334,0.9692509166666667,0.9348263500000001,0.9347308733333334,0.9607579833333334,0.99537398,0.9689731,0.8439029333333333,0.9785749466666666,0.9765259866666667,0.97974503,0.9266887233333333,0.984801,0.9087035533333333,0.8874916233333333,0.97525761,0.9394345,0.97984501,0.8992011933333334,0.6809988666666666,0.560662,0.8584610566666667,0.9750775966666666,0.7916567133333333,0.9400074466666667,0.9917631466666667,0.9856887,0.8946317433333334,0.8324739333333334,0.9321362833333332,0.9768576033333334,0.9580120500000001,0.7883264933333333,0.94831792,0.9769779766666667,0.9553970233333334,0.9403685133333334,0.8787893800000001,0.9866218333333333,0.62326136,0.9849540333333334,0.9697846066666665,0.9791090333333333,0.9588941266666667,0.96289079,0.9873726866666667,0.9322137666666667,0.9712885099999999,0.9309823333333332,0.6301708333333333,0.9651349233333333,0.901369,0.8221504266666667,0.9573652199999999,0.9920252333333334,0.9778249099999999,0.484787975,0.9571322533333334,0.9682809766666667,0.9727622633333333,0.9636624666666668,0.9784302500000001,0.9719423300000001,0.9777942466666666,0.99913908,0.9960536333333333,0.9866364233333332,0.9988134233333333,0.9667071666666667,0.99645683,0.9987233,0.9971895633333334,0.9899116466666666,0.99198288,0.9893919666666666,0.9962668,0.9971600666666666,0.9971249666666667,0.99623521,0.9244156666666666,0.99895018,0.9754450533333333,0.99630977,0.99794568,0.9991355500000001,0.99542181,0.8054735900000001,0.9993109499999999,0.9920270133333333,0.99236555,0.9824516666666666,0.9916543133333334,0.9938136433333334,0.9991416466666667,0.9876303666666666,0.9971645200000001,0.9776789333333333,0.99858621,0.8735161333333333,0.9854996666666667,0.9988289900000001,0.9989481766666667,0.9955271166666666,0.9946388066666666,0.9934200833333334,0.8819904733333334,0.9974087333333334,0.9895726933333333,0.994853,0.9980876733333334,0.99527518,0.32992924333333334,0.8849648,0.5188886666666667,0.9647852299999999,0.6815479433333334,0.8680328666666667,0.9340987866666667,0.62685866,0.9006665533333335,0.9937509499999999,0.45643731000000004,0.9681626066666666,0.99739301,0.9892517366666667,0.9924790666666666,0.9977223000000001,0.9958017199999999,0.9913935500000001,0.9705306466666667,0.9934958333333332,0.8520343499999999,0.99765157,0.99707081,0.9382562166666667,0.6183112033333332,0.97680263,0.26393454,0.9853006999999999,0.24358820000000003,0.9800429666666667,0.9989733900000001,0.7710573833333333,0.98623494,0.9952817899999999,0.9936098900000001,0.92883364,0.99914944,0.9966161433333333,0.9887736,0.9984988766666666,0.99871224,0.80473673,0.9720527433333332,0.9826775466666667,0.9909786333333334,0.9942464233333334,0.9993635366666668,0.9980034766666668,0.9974987666666667,0.8644314666666667,0.99887683,0.9980740666666666,0.9952306933333332,0.9980894666666668,0.9927301866666666,0.9631124999999999,0.98138015,0.5864131699999999,0.43548980666666665,0.9899464833333335,0.9901313333333333,0.9933687099999999,0.7971578966666666,0.99834982,0.9948780133333334,0.9784048266666666,0.9769504166666666,0.9880945366666666,0.96542,0.9968679333333333,0.6662999199999999,0.9890207333333333,0.86262838,0.76654319,0.9793365033333333,0.8561158566666668,0.7880571199999999,0.9839881866666667,0.96670413,0.9857121333333333,0.7749576333333333,0.74810052,0.6024645550000001,0.96428439,0.98735938,0.9301599466666666,0.9632651233333333,0.9221823333333333,0.9089195566666667,0.9063340666666667,0.9831187033333334,0.65656487,0.9896112966666667,0.72665389,0.9710602700000001,0.9112050433333333,0.9507262,0.9849776633333334,0.9874906733333333,0.9910375199999999,0.9261664533333334,0.9808474,0.5357416466666667,0.9886700666666667,0.89168308,0.51139,0.9721147499999999,0.7767080533333334,0.93888975,0.8148672966666667,0.9884838533333333,0.9907454333333333,0.9620380599999999,0.6710182000000001,0.79217003,0.68407013,0.9749186333333334,0.9680383533333333,0.9804190366666666,0.564890075,0.9454842866666667,0.9099728866666666,0.9384757533333333,0.92525345,0.9737405566666667,0.80351068,0.9272616666666668,0.9809418533333334,0.8588683100000001,0.8693085333333334,0.8404755733333333,0.9675261900000001,0.9935012,0.5637722000000001,0.9391392066666667,0.61281553,0.9607739466666666,0.987807,0.9441251466666666,0.3453532033333333,0.9539515466666666,0.9873742833333333,0.8884774766666667,0.92131749,0.9264350666666666,0.5721570166666666,0.9252305866666667,0.9862985333333333,0.9676763233333334,0.92417017,0.49931837333333334,0.8180326466666666,0.9581975866666667,0.9173657666666667,0.93967645,0.9385928666666666,0.9947936833333334,0.9647648666666667,0.80115472,0.9176813166666666,0.9378777033333333,0.9827819333333333,0.7886609233333334,0.6950645,0.9619472666666667,0.79331058,0.7521166266666667,0.9598259933333333,0.9847393666666666,0.9396403266666667,0.97685543,0.8495756966666667,0.9603781666666666,0.9231818333333334,0.9671889666666668,0.5960533666666666,0.9918647666666667,0.8172778733333333,0.8866536833333334,0.9866072,0.9621378666666667,0.8973961433333333,0.69687913,0.9733602533333333,0.30851097000000005,0.9928123566666667,0.9997689200000001,0.9983895633333333,0.62490565,0.9527203266666667,0.9975341566666667,0.8420348,0.9987561899999999,0.9970513166666667,0.99529845,0.9989128333333334,0.9922577833333334,0.564844465,0.9924832499999999,0.96884719,0.9866046666666666,0.9755719666666667,0.93493102,0.7344461866666667,0.9414697466666667,0.92519078,0.6899858633333333,0.9964212333333333,0.9887190866666667,0.9824669,0.9982804533333334,0.9424726133333333,0.59228426,0.5002769800000001,0.9605178133333334,0.98359143,0.9683619266666668,0.9996101466666666,0.9849616333333334,0.9980376266666666,0.9950314566666666,0.9930152233333334,0.9993866466666667,0.9964855,0.9516667466666666,0.9986648833333334,0.9976778433333333,0.9956309433333334,0.9733093833333334,0.9882812833333334,0.9552990399999999,0.9953087933333333,0.9547381533333333,0.9991003833333334,0.9899833766666667,0.9998234699999999,0.9993771333333333,0.9985024433333333,0.99894309,0.9910266766666666,0.98497991,0.98743916,0.9921502833333333,0.9909444333333334,0.99359333,0.9835009499999999,0.9553688766666667,0.9824635333333332,0.9927984666666667,0.98824072,0.9846730866666666,0.8651217133333334,0.9999924033333333,0.99980124,0.9999085133333333,0.9999944333333333,0.9999932500000001,0.9997478066666666,0.9998019,0.9265858866666666,0.9953376166666666,0.9990295866666666,0.9967898233333333,0.8427922966666667,0.9389327666666668,0.8630668666666667,0.9998079633333333,0.99993686,0.9991693433333334,0.9997646066666667,0.99983338,0.9999506833333333,0.9999665466666667,0.9927304133333333,0.98003772,0.9543818666666667,0.62600708,0.9965484333333334,0.9971521333333332,0.99887556,0.99125158,0.9950479666666666,0.9965353833333334,0.9443652533333333,0.9972309633333333,0.9979443466666668,0.9984268633333334,0.9963968,0.97574995,0.9973750833333334,0.9963535433333334,0.9527518433333334,0.9491628899999999,0.9994040766666666,0.9944399533333333,0.9898112333333334,0.9992901666666668,0.9981716433333334,0.9989099333333332,0.9991089999999999,0.9853483066666667,0.9900961933333333,0.9987743133333332,0.7268853000000001,0.9920487800000001,0.97399106,0.99852475,0.9990878533333333,0.99739529,0.9994754833333334,0.9983624,0.99819311,0.99747161,0.9988908333333333,0.99932762,0.9877374099999999,0.9396444366666667,0.98932122,0.9854645,0.9970549666666667,0.9988206833333333,0.9992946333333332,0.8875352266666666,0.8162121333333333,0.7815161666666667,0.9955540666666667,0.93125887,0.6831017433333333,0.9843888199999999,0.7624263433333333,0.82162331,0.99210023,0.35190324,0.61345883,0.9483147566666666,0.6010487800000001,0.731349425,0.9809496666666666,0.9946935166666666,0.9578809266666667,0.6364258533333333,0.8921557933333334,0.8586981366666667,0.99051835,0.9269801566666667,0.8506193133333334,0.6863007333333333,0.9878817433333333,0.9937839766666667,0.82265938,0.9893329866666667,0.9971328866666666,0.9991083,0.9993455299999999,0.9970751099999999,0.9997714833333333,0.9997186333333333,0.47631113666666663,0.88024804,0.34545974333333335,0.92144878],\"type\":\"violin\"},{\"box\":{\"visible\":true},\"fillcolor\":\"rgb(144,206,219)\",\"line\":{\"color\":\"black\"},\"marker\":{\"color\":\"black\",\"size\":2},\"meanline\":{\"visible\":false},\"name\":\"h3k9me3\",\"points\":\"all\",\"showlegend\":false,\"spanmode\":\"hard\",\"x\":[5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5],\"y\":[0.88933742,0.6036613833333333,0.9814392966666666,0.9727554666666668,0.9241630399999999,0.9968832366666667,0.9809233766666666,0.9444170033333333,0.91531757,0.9792614166666667,0.95761672,0.8899421833333333,0.8916956766666667,0.8973679333333333,0.8947459499999999,0.9551393666666667,0.9877490866666667,0.7234250866666666,0.9436910666666667,0.8568064233333333,0.9617940333333334,0.9015133966666666,0.8669161366666667,0.94862155,0.9216895800000001,0.9558774333333333,0.9394540333333333,0.9488868666666667,0.8971707800000001,0.7887189333333334,0.9877463400000001,0.496311395,0.8427889333333334,0.8180797333333333,0.6999094499999999,0.67205111,0.8737033233333333,0.8872959566666667,0.9857923766666666,0.8377150433333332,0.98146491,0.9116903933333332,0.9889195733333334,0.8908791666666667,0.8685754600000001,0.9144572466666666,0.96042252,0.9858135166666666,0.9843934266666666,0.61748435,0.83361558,0.6959660666666667,0.9651129300000001,0.9908984433333333,0.9170682766666666,0.9909418899999999,0.9938390333333333,0.8992235700000001,0.9816910933333333,0.8872663533333333,0.9456512333333333,0.9065313633333334,0.9513423200000001,0.97389782,0.9680538599999999,0.99649472,0.37936008,0.9859606900000001,0.9847442333333333,0.98787463,0.9651737466666667,0.8300513199999999,0.9734897166666666,0.9946904,0.9727618866666666,0.9494086933333333,0.98910099,0.94723009,0.9960679533333333,0.95377078,0.8899938666666666,0.9785898999999999,0.9594937,0.9516393533333334,0.9778835899999999,0.9295074933333334,0.8835499966666666,0.98423682,0.9724116466666667,0.7577521766666667,0.9802381033333333,0.9541911933333335,0.8599336000000001,0.9442333999999999,0.3866087,0.9441018900000001,0.9529052866666667,0.9323264966666667,0.9547664666666668,0.9547006166666666,0.8296105300000001,0.9538895666666667,0.92241465,0.88826588,0.29174497,0.9399443,0.9812686533333334,0.88895043,0.9497361966666666,0.7989628666666667,0.9553571766666668,0.9059804866666666,0.66922358,0.7553014466666667,0.9028959099999999,0.9685571533333333,0.6912854333333334,0.9736610466666668,0.9386306166666666,0.9138793266666667,0.64699757,0.9036920466666666,0.8793542766666667,0.9494544133333332,0.94589998,0.9510684833333333,0.54814191,0.7144593533333333,0.9132911766666667,0.9297326,0.9920422666666666,0.9192284599999999,0.8845779833333335,0.98173941,0.9114803866666666,0.9069911999999999,0.9889649,0.87996365,0.8144924333333333,0.96350006,0.8879540833333334,0.9784277833333334,0.9280521199999999,0.7816184333333333,0.9580800766666666,0.9663309866666667,0.9138992566666667,0.9318359266666666,0.9973351566666667,0.9440505433333333,0.9686419666666667,0.9320386333333334,0.9520598366666667,0.8745045466666667,0.9127331766666668,0.9244304766666667,0.9315888333333332,0.9688355566666665,0.8915286933333334,0.99915495,0.9961130800000001,0.8316673466666668,0.9576300466666666,0.9654182633333334,0.9354548233333334,0.7532696666666666,0.63096713,0.8216853833333334,0.56503148,0.9931554,0.8956452,0.9209576666666667,0.9453749,0.7127546333333333,0.8402365333333334,0.8522664833333332,0.7851223266666666,0.9265003433333333,0.99813863,0.8635198166666666,0.36524518,0.6859314,0.34584681500000003,0.9311583750000001,0.6966447666666667,0.45707948499999995,0.280589895,0.55711725,0.8150796233333333,0.7639155766666667,0.53829804,0.9525852466666667,0.8924728200000001,0.8350971333333334,0.8905815166666667,0.9175657199999999,0.9523543800000001,0.9889703566666667,0.9371858533333333,0.87300639,0.43878665,0.9585643566666667,0.8898604,0.4871632000000001,0.9516504,0.40612329,0.9674604533333334,0.27205811999999996,0.9647842566666668,0.8749257633333333,0.8335058933333332,0.8876385566666668,0.6303670433333334,0.8530636033333333,0.9687825533333333,0.9621513500000001,0.9886183966666667,0.9877026,0.9872294666666667,0.9983670666666667,0.9914014999999999,0.6018310666666666,0.8947260466666668,0.9864175,0.9895660833333334,0.9948509433333333,0.99816849,0.958759,0.79210031,0.9990040033333334,0.9883591800000001,0.9386509333333333,0.9911007166666668,0.9805838866666666,0.9148319533333332,0.9951515299999999,0.6467439333333332,0.9467476,0.5282034766666667,0.98730807,0.7215072100000001,0.96883361,0.28579324333333334,0.9196949866666667,0.8457565833333334,0.8762456633333334,0.8317779066666667,0.991444,0.9974120333333333,0.99312457,0.9498962333333334,0.468940305,0.8522810966666666,0.9094114000000001,0.8276330333333334,0.7804700666666666,0.8256942733333333,0.9270126433333333,0.7113022399999999,0.90159238,0.9589081,0.8375962100000001,0.9152409333333335,0.93130335,0.77217205,0.65156919,0.7167575433333333,0.9399695566666667,0.9740394766666668,0.9225277166666667,0.7452256199999999,0.75289745,0.8254552500000001,0.54342185,0.9694044333333333,0.489160935,0.3500304166666666,0.9051477333333334,0.8094316533333333,0.83993784,0.9530326499999999,0.9632957000000001,0.9829502433333334,0.9226214,0.6583465199999999,0.8024298666666666,0.5322784500000001,0.8534806466666667,0.9688893333333333,0.8797197433333332,0.6864727233333333,0.88425936,0.9265880333333333,0.6762142466666666,0.8315819866666666,0.8638963333333334,0.60248225,0.8812761333333333,0.9377719166666667,0.8882755566666667,0.3373665266666667,0.8646200266666667,0.78008366,0.9532339,0.27415151,0.94245293,0.9217940666666666,0.8726527900000001,0.9428974,0.45702344333333333,0.9649861333333334,0.9695591566666666,0.9710295666666666,0.841098,0.8440294833333333,0.90366761,0.7629786766666666,0.9349858366666667,0.9799049866666666,0.8757755566666666,0.8029181166666666,0.9666453966666667,0.7437008833333333,0.9478760733333335,0.85058268,0.9396446466666667,0.8079178100000001,0.8488392666666668,0.8136137466666667,0.9260476566666668,0.8535379333333334,0.9742717766666665,0.5219284000000001,0.9798276333333332,0.9836507666666666,0.9956393,0.9516586866666666,0.9100055766666667,0.6571628533333334,0.88234095,0.8256842,0.58117966,0.9698819100000001,0.9822379133333333,0.98335151,0.9618510666666666,0.8711799066666667,0.5473847633333334,0.9746364466666666,0.9665327466666667,0.9289264866666667,0.9931661433333333,0.9336490866666667,0.9744674833333332,0.9972589300000001,0.9669912866666667,0.9923549633333334,0.9940874366666667,0.9744499766666667,0.9955549933333333,0.99672348,0.9676834666666667,0.99383994,0.9893938366666667,0.9949801633333334,0.98944449,0.9691295333333333,0.9848603300000001,0.9816654233333333,0.4747978033333333,0.9795083433333334,0.9772412533333332,0.9894950433333333,0.9950823099999999,0.9967411899999999,0.9730920633333332,0.9384622566666666,0.9717411866666666,0.9785715233333333,0.9874563,0.88830507,0.9587522533333334,0.9725308833333334,0.9521803000000001,0.91246029,0.9468401333333333,0.9905925766666667,0.9901961766666666,0.8286663166666667,0.836617175,0.8311064533333333,0.9743763466666667,0.9820384999999999,0.9978210133333333,0.9918285333333333,0.9871642666666668,0.99826668,0.9951690566666667,0.96739732,0.9748187033333333,0.9767320833333333,0.8355127866666666,0.9952450066666666,0.9978257666666667,0.9977216999999999,0.97448625,0.9960881366666667,0.98803741,0.9569300566666666,0.99121329,0.9279378,0.9905105633333333,0.8898968333333332,0.9646681766666667,0.9091137266666666,0.95133412,0.9653722833333332,0.6728744133333334,0.9503689,0.9841801800000001,0.9647802200000001,0.7318038033333334,0.9950548133333333,0.9766195,0.9148839866666667,0.372439405,0.9849229466666666,0.84237486,0.88376661,0.9852832133333335,0.9328672,0.9569736666666667,0.9972269666666667,0.9971835466666666,0.9910064966666666,0.9940674366666666,0.94769924,0.9993274,0.9767130700000001,0.99154844,0.9255097,0.9659896466666668,0.9985239,0.95193031,0.9971623466666667,0.8885986533333333,0.9983531533333334,0.91912015,0.714694175,0.7940699233333333,0.565003165,0.8456388,0.6642498933333334,0.6196154733333333,0.4585171,0.5594913866666666,0.618800415,0.5852608666666667,0.7915440466666667,0.5429835233333333,0.48466531333333335,0.5306197466666667,0.93339673,0.7840202199999999,0.8677400666666667,0.8487822533333333,0.5109510033333333,0.95475552,0.9310310866666667,0.92310294,0.9438282166666667,0.8213439466666667,0.6309511333333333],\"type\":\"violin\"},{\"box\":{\"visible\":true},\"fillcolor\":\"rgb(30,30,30)\",\"line\":{\"color\":\"black\"},\"marker\":{\"color\":\"black\",\"size\":2},\"meanline\":{\"visible\":false},\"name\":\"input\",\"points\":\"all\",\"showlegend\":false,\"spanmode\":\"hard\",\"x\":[6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\"y\":[0.20267743,0.491453,0.947898,0.7382399,0.35095435,0.48994333,0.33787772,0.600013,0.4860671,0.342758,0.21667495,0.20402044,0.7524124,0.5934801,0.89888173,0.44481012,0.18785635,0.96511334,0.367742,0.39534575,0.5913922,0.594552,0.30300704,0.41551122,0.32988554,0.9728396,0.9346419,0.48993865,0.5548768,0.5977243,0.7240188,0.34569022,0.19995084,0.23058301,0.22635956,0.24550134,0.26345646,0.9018077,0.3991933,0.24768268,0.57169765,0.95077175,0.34849888,0.27421746,0.19621755,0.3773671,0.28708088,0.29600063,0.8612168,0.40213212,0.64151245,0.95904464,0.53724605,0.544946,0.6016806,0.260122,0.23171522,0.80391324,0.57229716,0.22961769,0.60817236,0.7033464,0.64710563,0.34610328,0.24511856,0.943178,0.8840954,0.62835413,0.7913389,0.9569598,0.31280878,0.36730653,0.6487205,0.31320146,0.36710367,0.3742392,0.5249672,0.42708805,0.33726647,0.24053384,0.3361613,0.4531253,0.53476596,0.30402303,0.67286503,0.31002164,0.3401498,0.30173486,0.29406187,0.35907426,0.5169036,0.35470173,0.27951518,0.28178376,0.18218814,0.23899446,0.26390857,0.9858342,0.1448916,0.671476,0.2596539,0.91410506,0.7937936,0.23915,0.28347695,0.26859564,0.37994027,0.38606182,0.9765127,0.35921714,0.33096257,0.36831543,0.19124348,0.22079928,0.80286825,0.19078875,0.4333434,0.1964838,0.25969514,0.19453916,0.22528438,0.254135,0.8756371,0.5248199,0.23989592,0.22104639,0.17644735,0.1520848,0.25843892,0.3465234,0.2717214,0.9863593,0.8732699,0.2482945,0.3200427,0.64093727,0.98786944,0.3109513,0.8277504,0.18957956,0.48557302,0.59824616,0.22501752,0.1974464,0.9721816,0.39344442,0.8876769,0.16991457,0.14765099,0.27922705,0.29085577,0.84775835,0.5643362,0.23004423,0.4039174,0.9860253,0.9063318,0.46304917,0.96572757,0.98762053,0.8898808,0.3462481,0.6984864,0.94004023,0.96554935,0.8806923,0.4710289,0.23223981,0.48019093,0.9809183,0.9962845,0.5729693,0.9692096,0.9865328,0.8136837,0.42112857,0.9669822,0.69547975,0.8746798,0.87982804,0.5030772,0.9389754,0.2604238,0.9873592,0.69069344,0.7731994,0.7563018,0.7635013,0.97176874,0.9870684,0.9965545,0.98390526,0.825229,0.98821056,0.9213433,0.5709991,0.2118311,0.95118856,0.7116536,0.5261169,0.98130566,0.98269296,0.4462938,0.9689048,0.59434557,0.97237194,0.7956224,0.95461273,0.4467043,0.52938914,0.9908765,0.9457951,0.9858139,0.9624426,0.14915769,0.26369503,0.16502772,0.84539205,0.2550716,0.14796706,0.6355483,0.59897316,0.2012647,0.19518857,0.2241842,0.24241984,0.768382,0.40917125,0.32929638,0.93764794,0.29587662,0.2507237,0.45028797,0.95108426,0.20471649,0.9688825,0.9447836,0.9515634,0.19331843,0.545321,0.24841215,0.7060318,0.35660186,0.70254177,0.94174504,0.40914175,0.997928,0.3750056,0.50720274,0.3068703,0.94523,0.7368219,0.6055341,0.61624026,0.78272116,0.7712641,0.8410996,0.93939346,0.21063598,0.32694596,0.36300963,0.8018261,0.52251595,0.827315,0.68078107,0.81784475,0.8509621,0.8059869,0.60772955,0.9475426,0.9659928,0.20103412999999998,0.3703994,0.17789426,0.34369242,0.9677577,0.15349516,0.95476973,0.9683211,0.2544245,0.20712213,0.2933815,0.18683699,0.76068497,0.4194622,0.5474375,0.48791906,0.75163156,0.33403578,0.74901325,0.2055008,0.21608073,0.5161094,0.85106635,0.23565647,0.25177282,0.94021153,0.2225581,0.91461957,0.70696026,0.2105201,0.2984312,0.4340176,0.47703132,0.34965807,0.24133657,0.4036668,0.96804184,0.24049875,0.5546419,0.9836512,0.24747112,0.3321841,0.58631426,0.4651687,0.8273652,0.3508404,0.23762219,0.6282512,0.44733942,0.46082053,0.39270636,0.6144872,0.30999973,0.69880027,0.17606589,0.7839679,0.34911913,0.16670696,0.22619446,0.47547075,0.50202775,0.42362478,0.3680724,0.23852886,0.79220283,0.93928343,0.37223613,0.20352957,0.24327803,0.2514515,0.8809837,0.28198645,0.26337203,0.9658066,0.90214276,0.90208274,0.72060776,0.47654834,0.41390383,0.7566997,0.51283026,0.91449064,0.20036198,0.26821905,0.28450516,0.40028852,0.23221971,0.99875295,0.23267724,0.429285,0.21228185,0.9974245,0.19097447,0.28859925,0.8796645,0.639261,0.25504926,0.9659872,0.41164035,0.75755686,0.62292165,0.99222577,0.20972581,0.44542724,0.2261819,0.92861855,0.9987876,0.91533464,0.3302278,0.5194542,0.19852297,0.99916613,0.99846005,0.99935347,0.5045,0.59282494,0.59185714,0.56691575,0.377352,0.4414935,0.3237074,0.30593184,0.5828933,0.27100676,0.99768627,0.60264194,0.58907145,0.41952562,0.35319826,0.3399685,0.25480473,0.28754824,0.5733097,0.94025266,0.5984913,0.8503455,0.87857723,0.3975613,0.8945726,0.90900034,0.798053,0.48432282,0.34372517,0.53364533,0.47207725,0.16691317,0.3689796,0.4946968,0.23537041,0.24345335,0.28770038,0.25164366,0.40383005,0.26719356,0.53325367,0.29484773,0.28330603,0.17655067,0.38148928,0.28844115,0.3686358,0.53948665,0.95766324,0.4167823,0.98195213,0.5455404,0.6268983,0.9878001,0.98342323,0.32091874,0.47009724,0.717814,0.59276044,0.362195,0.40545583,0.49097687,0.7088569,0.28655043,0.43269715,0.5392796,0.7119678,0.63290197,0.80573744,0.23661445,0.21611816,0.43037915,0.520281,0.41458133,0.37972838,0.2646445,0.55208504,0.6360209,0.38024205,0.49006572,0.47061747,0.5141119,0.25354457,0.2872164,0.3676971,0.43861777,0.38252264,0.3429026,0.25784937,0.32932544,0.26606587,0.3689593,0.5614997,0.41831622,0.3398038,0.41294965,0.3471288,0.22884582,0.5706069,0.469959,0.24103571,0.29650214,0.4793413,0.51073754,0.47415683,0.33400595,0.46380785,0.5328644,0.42836574,0.57424444,0.34791365,0.33671328,0.401313,0.3528203,0.38121784,0.54627734,0.50947225,0.3492057,0.98757577,0.67099535,0.9988165,0.9978157,0.9182689,0.9569037,0.29492524,0.38828328,0.9284369,0.9548402,0.18274978,0.9825926,0.8383138,0.17776266,0.56109506,0.31694776,0.5566225,0.2524998,0.98736763,0.4309132,0.26572555,0.8218636,0.44616106,0.9269771,0.83062154,0.36863047,0.8669826,0.4636543,0.56453663,0.44327945,0.4352943,0.34274393,0.57491076,0.37417638,0.5710074,0.7082305,0.27071694,0.7198524,0.63606,0.6194477,0.5584503,0.33573368,0.31770006,0.20288947,0.29039848],\"type\":\"violin\"},{\"box\":{\"visible\":true},\"fillcolor\":\"rgb(30,176,75)\",\"line\":{\"color\":\"black\"},\"marker\":{\"color\":\"black\",\"size\":2},\"meanline\":{\"visible\":false},\"name\":\"rna_seq\",\"points\":\"all\",\"showlegend\":false,\"spanmode\":\"hard\",\"x\":[7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7],\"y\":[0.99672225,0.996363035,0.999006925,0.99042238,0.998451985,0.99966055,0.9393781999999999,0.9960745200000001,0.99848433,0.981477325,0.9969292750000001,0.9835654,0.9876294999999999,0.9988704500000001,0.98693705,0.8869906350000001,0.9728432,0.97823547,0.9991039500000001,0.9922924900000001,0.9985035099999999,0.959392275,0.92037808,0.97043105,0.986622225,0.99930236,0.99964325,0.99921463,0.99194095,0.994881825,0.9938101500000001,0.998786215,0.99673935,0.9605468500000001,0.993976765,0.9706110800000001,0.99157566,0.99904685,0.95506,0.9991405,0.97176412,0.9979516150000001,0.989444625,0.9976241,0.9947442,0.9762265999999999,0.997187,0.9983396499999999,0.997372865,0.993212625,0.9977876999999999,0.9947657999999999,0.9987825699999999,0.98936415,0.9960424,0.9927483850000001,0.99874195,0.9900811199999999,0.9855689400000001,0.9763476200000001,0.99955032,0.94244534,0.995750425,0.998910925,0.997669625,0.9920793,0.99848282,0.98408768,0.998229425,0.93919088,0.9376848,0.98981545,0.95685665,0.9959872750000001,0.9987288999999999,0.990801225,0.99627455,0.9767413,0.9984259,0.9925965,0.9987270500000001,0.9887075750000001,0.98366049,0.9987017149999999,0.99707255,0.998723215,0.989988715,0.98155886,0.99709606,0.987392075,0.979064065,0.98888934,0.662248265,0.99771005,0.998597415,0.9904936,0.959049015,0.9984472200000001,0.95792557,0.9958644999999999,0.9978889550000001,0.9701148500000001,0.99987065,0.9706549950000001,0.99937579,0.9996308,0.998010085,0.978678275,0.996915735,0.92238058,0.98613368,0.9888241849999999,0.9003148999999999,0.9899435999999999,0.994655175,0.88656165,0.98789627,0.6885337,0.99796483,0.7911663,0.9980373499999999,0.9963902250000001,0.9975024,0.7245395,0.89678182,0.67187287,0.99077034,0.970015855,0.99864656,0.99878568,0.99630262,0.995058805,0.99979013,0.9872195500000001,0.9913127,0.99398264,0.98540616,0.98485595,0.9985319,0.96423849,0.9991417,0.9988146,0.9959222000000001,0.995039665,0.9990181,0.99114815,0.99662936,0.9815294649999999,0.9509864450000001,0.9718064500000001,0.9987977,0.99721853,0.994505915,0.99714965,0.8959865,0.9955461,0.955397,0.9988323,0.9966644499999999,0.99934523,0.91039795,0.99822542,0.9899957,0.9777792000000001,0.9946759,0.880163195,0.9534832,0.96276935,0.994456205,0.9887211,0.9004292199999999,0.98752815,0.99965227,0.992851,0.993282495,0.981568235,0.9925079,0.9988542499999999,0.9989432133333334,0.99568993,0.9910736499999999,0.99964557,0.96232438,0.9933911799999999,0.996286,0.997759575,0.986252225,0.998161215,0.99375312,0.9921607,0.9971672,0.9982650799999999,0.9976,0.99208005,0.9994802700000001,0.9677103,0.9896358000000001,0.99885862,0.9981736,0.9981121399999999,0.9980679800000001,0.99497265,0.999084965,0.9970593,0.9918214249999999,0.99792412,0.9953315,0.9926217500000001,0.99568835,0.9974729,0.99129993,0.9967174,0.996578065,0.996107845,0.95985505,0.99783415,0.997734755,0.990079765,0.9967817750000001,0.998515815,0.99003565,0.99758988,0.99760425,0.998472005,0.98666195,0.99726945,0.996034225,0.9991589000000001,0.9978001,0.9966926,0.9996178,0.99804213,0.9995289199999999,0.99091782,0.996771675,0.9989281,0.995143,0.994238935,0.99871697,0.99558675,0.98982742,0.997929455,0.997729585,0.9974604,0.99005788,0.9968315649999999,0.998367035,0.99865297,0.99716925,0.9953527799999999,0.996697755,0.99029693,0.996375,0.99483469,0.9955677700000001,0.9943649,0.997915085,0.9795072499999999,0.99913525,0.99351293,0.99879028,0.9975815699999999,0.9929879,0.99936634,0.98738675,0.99531795,0.992098275,0.9980855,0.9890480749999999,0.9959667999999999,0.996397515,0.99619535,0.9946196,0.99518775,0.9979331499999999,0.998301025,0.9939681,0.998917735,0.9877681,0.9977542500000001,0.99130255,0.997546645,0.99646759,0.9988558700000001,0.990601085,0.99446165,0.9996898000000001,0.992701975,0.997559575,0.9988524,0.9900423,0.99533498,0.989120625,0.99531865,0.9949972149999999,0.998277115,0.99452403,0.99671569,0.99914675,0.99866395,0.9933105849999999,0.99873203,0.989659275,0.996669665,0.9966578500000001,0.997186305,0.994142635,0.9943133,0.9995023000000001,0.99438373,0.99881025,0.97983396,0.992331025,0.9958619200000001,0.98458607,0.9973867,0.99671368,0.99719095,0.997353725,0.9978434,0.993131605,0.99635945,0.99283052,0.98787728,0.9981994,0.99632875,0.9957571,0.99882598,0.993366215,0.99901952,0.725459015,0.99908882,0.9921301899999999,0.9560019,0.99951112,0.9969361999999999,0.99285215,0.9981534000000001,0.9956293700000001,0.982193055,0.994985,0.99479825,0.9953198,0.98578595,0.99899205,0.98439895,0.994724385,0.99891995,0.99378994,0.98823898,0.9979392300000001,0.999383065,0.994402865,0.99706425,0.9977540250000001,0.9912222500000001,0.9976704750000001,0.99824365,0.99314927,0.997062685,0.997084795,0.99245472,0.990512915,0.996410675,0.9974056250000001,0.99401162,0.9976775,0.99580625,0.9972613,0.9884306,0.997884725,0.99912263,0.99505025,0.9964063999999999,0.9906614499999999,0.9914274000000001,0.9936944,0.9969097499999999,0.99467463,0.9929264849999999,0.9970627,0.9881640199999999,0.9991622,0.995116425,0.9919778,0.99691485,0.9964276700000001,0.99248132,0.9952166,0.9994154,0.999472275,0.991579875,0.99820983,0.9864624,0.9920823000000001,0.9771519,0.999817575,0.9691557,0.9922469700000001,0.9928796200000001,0.99105275,0.3162173,0.6982071,0.991602565,0.9932569200000001,0.981051675,0.999176775,0.9994787,0.9995038350000001,0.99939112,0.8589453,0.9993494,0.998965925,0.999471585,0.9995568850000001,0.9989015450000001,0.9995518000000001,0.980443865,0.99691258,0.9991086499999999,0.9506395750000001,0.9981027,0.9991685699999999,0.977979665,0.9999832,0.9956121149999999,0.980134625,0.57324189,0.99288362,0.9995207500000001,0.998966815,0.99632115,0.990680775,0.87367968,0.9613891299999999,0.766779185,0.893868385,0.998730785,0.98459797,0.98211815,0.97912741,0.998325855,0.9549704999999999,0.9948351200000001,0.94943615,0.9523132,0.990407365,0.99638357,0.804727215,0.9913437249999999,0.99381038,0.96977227,0.90172585,0.9991252500000001,0.99660007,0.9864614700000001,0.9724711699999999,0.97335465,0.9874633500000001,0.87205815,0.98961512,0.99869963,0.9747967,0.640791215,0.9995086,0.9939464,0.9113823999999999,0.97295192,0.6255205,0.997426965,0.97253995,0.992147,0.65778195,0.96347776,0.638537775,0.842039185,0.98101132,0.98404013,0.96756418,0.9934145249999999,0.97382422,0.89707445,0.924369,0.99955431,0.99952782,0.9981998999999999,0.9577884999999999,0.9995007124999999,0.9977652625,0.9997494825,0.99364622,0.9961557249999999,0.9166804500000001,0.996458625,0.99521663,0.9928747499999999,0.9869360125,0.9890199725,0.974704215,0.991300465,0.9921349349999999,0.9859228225,0.99579615,0.98745456,0.97652125,0.9929782225,0.9877101,0.97856595,0.880109655,0.99523815,0.9785488499999999,0.7558787,0.80666152,0.891047955,0.9964205500000001,0.997977965,0.9981379850000001,0.9966074,0.9985678,0.94934338,0.998584245,0.9988668549999999,0.96623077,0.995708775,0.9949602500000001,0.9937386850000001,0.9973369000000001,0.9803526499999999,0.99782495,0.996249765,0.997766685,0.9850996999999999,0.9984132,0.9964180149999999,0.99788305,0.9960693700000001,0.99820465,0.99533843,0.9961345349999999,0.9968941,0.99507295,0.9934327350000001,0.9984562,0.993755755,0.9902409999999999,0.9943472,0.99797653,0.9943697149999999,0.99533703,0.99667105,0.994962465,0.987841465,0.98584202,0.993732955,0.992409475,0.994309305,0.993700535,0.9971021200000001,0.99860423,0.9986115499999999,0.996098,0.996060935,0.99798845,0.996277225,0.98208064,0.9966602,0.988973135,0.996123925,0.9966196350000001,0.9993121333333334,0.99912878,0.9988269000000001,0.99767575,0.998412845,0.99883363,0.99899455,0.99875757,0.997838585,0.9986222650000001,0.99909546,0.998894535,0.997599085,0.9961532,0.9989179349999999,0.99839285,0.9987212700000001,0.99917049,0.9980976500000001,0.998475,0.9981524049999999,0.998625665,0.999255715,0.9988060599999999,0.9985711500000001,0.997839175,0.9992696750000001,0.99843682,0.998977985,0.998962175,0.99731613,0.99815472,0.9986210233333334,0.99898818,0.99877315,0.99886895,0.997958985,0.9985606499999999,0.998578545,0.99878068,0.998377725,0.9986029599999999,0.998653825,0.9980941400000001,0.9991090499999999,0.998871595,0.998113225,0.9979116,0.99860695,0.9988414800000001,0.998228315,0.99867987,0.99883755,0.998543835,0.99862902,0.9986239,0.9988379000000001,0.9989139149999999,0.9985644499999999,0.99860317,0.998798785,0.998615415,0.9982506250000001,0.9986184149999999,0.9985569000000001,0.9990429000000001,0.999106485,0.998219465,0.9984709350000001,0.99875054,0.998620275,0.998636095,0.999042,0.99912305,0.9988901,0.998109085,0.9983688749999999,0.998433925,0.998817085,0.9988786000000001,0.99817657,0.99877375,0.99883795,0.998227975,0.996234975,0.998485835,0.9978207699999999,0.9987973,0.9986620500000001,0.998680225,0.9983891,0.998373655,0.99866862,0.99917815,0.9982174699999999,0.99822833,0.9973893,0.9989709,0.99878093,0.9985927,0.99798812,0.998244515,0.99906045,0.9978744500000001,0.99866208,0.996637765,0.998724685,0.997601785,0.99863282,0.99886192,0.99861129,0.998624815,0.9986313400000001,0.9990508300000001,0.9979306450000001,0.9988695700000001,0.9988477,0.996557295,0.998472455,0.9980292049999999,0.99823083,0.99867482,0.9982853549999999,0.9986814749999999,0.9987947349999999,0.998129625,0.998707285,0.998864,0.99853775,0.9988543,0.998273425,0.9990873950000001,0.9987735,0.99729882,0.99844105,0.996656465,0.998976915,0.99839645,0.99826152,0.9983162999999999,0.99796642,0.998887675,0.99885045,0.9991470650000001,0.9983026500000001,0.9981185699999999,0.99841015,0.9982696,0.9983639799999999,0.99760145,0.99874005,0.999114135,0.998824805,0.9991973999999999,0.99812935,0.9984074000000001,0.9985596649999999,0.999110185,0.99905658,0.998646825,0.9983316849999999,0.9984435566666666,0.99796346,0.9987424,0.998975665,0.9982488,0.997283175,0.9970707000000001,0.9990318,0.99846613,0.9981655300000001,0.998847885,0.9969912000000001,0.9982137,0.99852845,0.99836505,0.9981180199999999,0.9981675,0.997609335,0.998771635,0.9989048,0.9989338,0.998900425,0.9988868200000001,0.9990989,0.99766872,0.99874745,0.9988192499999999,0.9984982,0.9993536199999999,0.9986783,0.99878372,0.9983711533333334,0.9985888,0.997500745,0.997759915,0.9986135,0.99787877,0.99867005,0.99723637,0.99886248,0.9977254499999999,0.9989712,0.977912165,0.99658322,0.998768975,0.99895332,0.99726695,0.9986974900000001,0.998840155,0.99901035,0.998783185,0.998288525,0.9984518,0.998591885,0.998773715,0.99869205,0.9977491,0.99896626,0.9985361100000001,0.99808438,0.99894525,0.9989393799999999,0.9989287099999999,0.99882083,0.9986314000000001,0.99835457,0.9980502250000001,0.9990281000000001,0.998115035,0.9980868,0.99829762,0.931936215,0.99742248,0.9985668766666667,0.9985048000000001,0.9987922,0.9984825599999999,0.9982996,0.998782095,0.997969745,0.9988358,0.9975976,0.99899715,0.99732429,0.998557025,0.997826285,0.99861038,0.99753707,0.998177675,0.9982255,0.9988634750000001,0.9988582850000001,0.99746685,0.9979009000000001,0.998538725,0.9977802499999999,0.99860595,0.99869145,0.998786095,0.99840092,0.998595875,0.99866555,0.998750575,0.9988497199999999,0.9984994,0.9978257,0.9982749200000001,0.998909185,0.998155275,0.9982146199999999,0.9987901,0.99765318,0.99879965,0.9982896,0.99857818,0.99849,0.998332215,0.997792,0.9981868,0.99808991,0.9984137,0.99765138,0.9960804000000001,0.9979423199999999,0.9993262500000001,0.998711235,0.9990416449999999,0.99862308,0.997123185,0.998526065,0.998454425,0.99850653,0.9984936799999999,0.9989095,0.998696,0.99920398,0.9986166000000001,0.9984751999999999,0.9985691999999999,0.9976184,0.9976138299999999,0.9990588300000001,0.9986648333333333,0.9981701000000001,0.9985890199999999,0.998916025,0.99892565,0.9987264499999999,0.998984725,0.9976637349999999,0.997811075,0.9988217699999999,0.99844453,0.9990043033333333,0.99911075,0.9991576,0.9990763066666667,0.99886233,0.998767,0.99860997,0.9983482699999999,0.9984677500000001,0.9979451500000001,0.99903846,0.99868974,0.99871405,0.997294455,0.99853598,0.99709355,0.9989806,0.99849678,0.998594165,0.998216725,0.9978511,0.9984108,0.998324575,0.99830755,0.998584565,0.9991931199999999,0.9987184,0.9990621,0.99851833,0.998358015,0.99853242,0.9977657799999999,0.9983196999999999,0.99903935,0.99914055,0.9981207000000001,0.9990833,0.9980483,0.9987771750000001,0.9985786533333334,0.998605785,0.9990711999999999,0.9989955500000001,0.99872122,0.9985438,0.99925157,0.9980563,0.9982522149999999,0.9991717149999999,0.99880931,0.99917285,0.9987996766666667,0.99817729,0.9984728866666667,0.99774312,0.998537985,0.9988106299999999,0.99907615,0.998970975,0.998526165,0.9986162000000001,0.9988561499999999,0.9992156,0.998694435,0.998538045,0.9977239499999999,0.998787575,0.9977399250000001,0.9989033,0.998918185,0.996829185,0.99873995,0.998869675,0.99704548,0.9993769766666666,0.998024225,0.9988946999999999,0.998938145,0.9984165666666667,0.9986570866666667,0.99898285,0.9979326,0.9979848,0.9972645333333334,0.99871015,0.998511925,0.998955925,0.9985788449999999,0.9988101,0.9992205000000001,0.99901995,0.9991644749999999,0.9988337249999999,0.999070065,0.9989036466666666,0.99836375,0.99866475,0.998740125,0.9988284000000001,0.997657875,0.99797994,0.99882242,0.99768425,0.9986787500000001,0.9985775800000001,0.9983305650000001,0.9988769799999999,0.998704075,0.99848169,0.99857085,0.9991157500000001,0.999058125,0.99873138,0.99867454,0.9986169949999999,0.99835315,0.99886535,0.998956205,0.99864315,0.998667895,0.9988510500000001,0.9985628,0.9977048500000001,0.99884217,0.99874302,0.99770565,0.998765965,0.9983229,0.99843293,0.9990010300000001,0.99942273,0.99907038,0.999265835,0.99923715,0.9992162,0.9987018249999999,0.9980264,0.9982998949999999,0.99871337,0.997567715,0.99837248,0.9979235,0.999235985,0.99787973,0.99876735,0.9987944049999999,0.9992105,0.9982433500000001,0.998792785,0.9988938199999999,0.99756065,0.9991301349999999,0.99900455,0.99796375,0.9983814099999999,0.99874388,0.99860413,0.99789587,0.998313775,0.99844227,0.999080575,0.99870593,0.99930955,0.99762443,0.99895878,0.9992412500000001,0.99837725,0.9983665349999999,0.99851885,0.998543795,0.9987828999999999,0.9981520500000001,0.9982587249999999,0.9983888700000001,0.9987347799999999,0.9986952,0.99900778,0.9983940499999999,0.9984628,0.9988582500000001,0.998813715,0.99856342,0.99859705,0.99845647,0.9985377,0.9972864349999999,0.9988960499999999,0.998925075,0.998877535,0.9987264,0.99902415,0.998304275,0.9987076500000001,0.9990714,0.99800736,0.9987710999999999,0.9988576666666668,0.998697195,0.9989979,0.9984014649999999,0.99910375,0.9990234,0.9987765,0.9993727800000001,0.9989865149999999,0.9985691,0.998791725,0.99883168,0.99752985,0.99899328,0.99739501,0.9987356949999999,0.998886675,0.998213115,0.9987418,0.99926417,0.99898188,0.9988651,0.998310655,0.99868375,0.9990854499999999,0.9989780450000001,0.99808683,0.9986579250000001,0.998489875,0.99741127,0.998886715,0.983664635,0.99887405,0.9985988,0.9989558,0.99777255,0.9980576566666667,0.99862435,0.998123025,0.9985102699999999,0.9987535,0.9984222300000001,0.99879445,0.99856605,0.998193125,0.99886688,0.9988166300000001,0.9985572500000001,0.9988366333333333,0.99920565,0.992071095,0.98331453,0.99347898,0.99130092,0.99403888,0.99274394,0.98531535,0.9978138000000001,0.97474153,0.99811288,0.99699117,0.9840889,0.99641377,0.98293902,0.99900537,0.99936545,0.97810782,0.9991922950000001,0.9992975,0.999488375,0.9985735499999999,0.99710625,0.9438565999999999,0.993217425,0.934842685,0.9193434,0.9982173625,0.9582771,0.991152235,0.9993519775,0.999346925,0.9994574775,0.9992496200000001,0.99928071,0.9996587575,0.9750408075,0.74845022,0.9994857975,0.9995848766666667,0.99311065,0.9869892600000001,0.954824,0.99480032,0.992395025,0.823389565,0.9667413300000001,0.990783125,0.9477638,0.71866925,0.99663715,0.9946583250000001,0.99442082,0.99329803,0.52209365,0.99600082,0.5093521,0.88413712,0.9991007000000001,0.99600697,0.9987349249999999,0.99675965,0.988238225,0.99920988,0.998547495,0.99871065,0.9971382049999999,0.9979337500000001,0.997655585,0.9948231700000001,0.9934817549999999,0.9978519,0.993622825,0.998919675,0.9964045,0.9964463400000001,0.70073137,0.9888387750000001,0.9938806,0.99820525,0.996235125,0.99580712,0.998215175,0.948006675,0.76673114,0.99688365,0.8895031,0.9921012499999999,0.9950514500000001,0.9887629,0.9951379600000001,0.992810215,0.9963476499999999,0.9978743999999999,0.9977498,0.99792625,0.9448311,0.967645535,0.99344813,0.98300225,0.99453708,0.59626537,0.99877065],\"type\":\"violin\"},{\"box\":{\"visible\":true},\"fillcolor\":\"rgb(81,89,168)\",\"line\":{\"color\":\"black\"},\"marker\":{\"color\":\"black\",\"size\":2},\"meanline\":{\"visible\":false},\"name\":\"wgbs\",\"points\":\"all\",\"showlegend\":false,\"spanmode\":\"hard\",\"x\":[8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8],\"y\":[0.19903432,0.165404265,0.22667138,0.2628587,0.41611962999999996,0.45564456,0.454311295,0.19306402,0.19462242000000002,0.29476875,0.256769455,0.638127475,0.86894855,0.464552805,0.32345254,0.61538497,0.20553573,0.27522275,0.38073256,0.241435475,0.19721866500000002,0.202380915,0.204102895,0.17104160499999999,0.16924144,0.43914279,0.306232825,0.13137161,0.652080575,0.5223996049999999,0.3434802,0.20726645,0.22878241500000002,0.24800539000000002,0.2961484,0.97181875,0.26673121,0.6412411650000001,0.390174,0.16103201,0.17845982,0.33664364,0.404729155,0.5270522,0.20866126000000002,0.16202569,0.748307125,0.21860956999999998,0.25818178,0.291770665,0.5194371,0.233773515,0.30276248,0.18510664999999998,0.47461110500000003,0.71020624,0.80112877,0.599476465,0.8445151,0.6455820999999999,0.564922275,0.50055315,0.68394407,0.5058768,0.43586224500000004,0.7106880550000001,0.688439785,0.9187488500000001,0.633855515,0.6446094,0.5528511700000001,0.75769728,0.5268564250000001,0.5149834,0.47345368,0.230821365,0.9919583249999999,0.21901702,0.30028856,0.40754328500000003,0.259018365,0.22847144,0.21768362000000002,0.24517760500000002,0.508195525,0.5184811300000001,0.19177929500000002,0.584140925,0.14412555,0.44461578,0.72858145,0.23085972,0.494116605,0.62495382,0.550979165,0.14503685,0.23117521000000002,0.72335109,0.24138491,0.199795485,0.20675759,0.24007704,0.227736335,0.28361425,0.300809325,0.19829940000000001,0.24939462499999998,0.33252423,0.77654833,0.98087318,0.319797905,0.2891387,0.651064885,0.197345015,0.6353670499999999,0.309826045,0.32030228000000005,0.30885039000000003,0.21640747999999999,0.36130359999999995,0.38216344999999996,0.23839365,0.18788451,0.16745423,0.973090355,0.8728722,0.54305056,0.567891725,0.98912052,0.64753412,0.59653785,0.186406445,0.190788805,0.35962992,0.31268114999999996,0.94825737,0.55712172,0.25678095,0.30387606,0.8575058,0.6337250000000001,0.6305206,0.47854653,0.6791866,0.61009935,0.24329785999999998,0.89972688,0.27925305,0.905055625,0.833073325,0.14969298,0.6225703,0.628311215,0.5341136,0.670332215,0.9144734,0.41647349499999997,0.45344458499999996,0.66742039,0.61515657,0.125591755,0.17846838,0.29019329,0.19872146499999999,0.87748775,0.19811223500000003,0.962345435,0.9771994500000001,0.19359088,0.5923591,0.42681646500000003,0.57181865,0.70512555,0.33537093,0.37724002,0.6657482800000001,0.44039989999999996,0.616430925,0.5872831199999999,0.4226598,0.42679448499999995,0.444464175,0.483384255,0.16211973,0.214211665,0.916810585,0.8786183000000001,0.923084185,0.9278917200000001,0.922129235,0.5200380099999999,0.8153520999999999,0.91823554,0.75301425,0.7366731,0.65968845,0.83804245,0.941668535,0.83415222,0.9503455000000001,0.77833007,0.945591955,0.858515615,0.866621525,0.88981888,0.81910422,0.49821227999999995,0.7828563500000001,0.6606753000000001,0.7739256999999999,0.22531211499999998,0.17968756000000002,0.5778172,0.316560745,0.54246975,0.373235715,0.5466195899999999,0.9875065199999999,0.47013262,0.31596465500000004,0.5076080000000001,0.40315066,0.26302148999999997,0.456501485,0.60567777,0.24056694,0.7024435499999999,0.197453295,0.33661382500000003,0.4557478,0.267944065,0.47667453,0.525991025,0.240785365,0.419995675,0.40082954,0.29158201,0.45312958000000003,0.26970373999999997,0.46059517,0.35970682,0.218906975,0.6147826000000001,0.94406108,0.65617435,0.309907005,0.57810123,0.3259672,0.42014689,0.5205668999999999,0.916673315,0.82246112,0.905839235,0.98475322,0.9202274,0.8702375,0.942344325,0.95532555,0.6680207650000001,0.86243175,0.962142175,0.9303581000000001,0.6080720500000001,0.412737875,0.380107935,0.138208045,0.426866605,0.38454139,0.1620007,0.365407885,0.34153206999999997,0.9863893,0.990073055,0.930091145,0.98452502,0.97173647,0.99129218,0.9793445700000001,0.9983635200000001,0.9897866,0.9750352250000001,0.915274665,0.85386142,0.712766125,0.6164477150000001,0.44169406,0.880230665,0.467495785,0.5061809349999999,0.66348635,0.88393268,0.3246392,0.19941331,0.22443995,0.8099651,0.480901025,0.8203460899999999,0.86421822,0.7013202000000001,0.3335852,0.874856675,0.8579869600000001,0.779224675,0.7253229000000001,0.5213584,0.455171985,0.78338833,0.87986358,0.63546122,0.41010636,0.42967821500000003,0.577331305,0.89271715,0.85872568,0.8161473,0.8388926999999999,0.879403295,0.86115735,0.8729599699999999,0.721468225,0.816149,0.88526938,0.841221005,0.8241287749999999,0.651602275,0.82349622,0.29275255,0.5358076,0.981446465,0.9952581199999999,0.9800211,0.90602878,0.926796535,0.5574140000000001,0.98580035,0.97421802,0.9722697149999999,0.920000975,0.97907923,0.9660086649999999,0.3500019,0.952306315,0.9635838999999999,0.693541425,0.41495335,0.5669300749999999,0.31073440500000005,0.577483885,0.56282373,0.652439235,0.23631859,0.66928035,0.5224541149999999,0.7378287699999999,0.67090142,0.6031093249999999,0.22913751,0.5473898349999999,0.63081265,0.239471145,0.6700744000000001,0.43992627,0.23332471500000002,0.985589375,0.9893796450000001,0.96784526,0.981577245,0.9200713,0.65082067,0.560652825,0.890765995,0.7916887,0.80066465,0.817094035,0.485505955,0.9520887149999999,0.92359402,0.93014575,0.83747758,0.74609305,0.845816165,0.939941475,0.30644396,0.61078525,0.25056581,0.29558262,0.30253256500000003,0.657063475,0.39957980000000004,0.25470331,0.9641650900000001,0.9511769800000001,0.8783422000000001,0.75480333,0.9425174000000001,0.9730847499999999,0.64590835,0.65650443,0.7396631499999999,0.862981965,0.16620125000000002,0.860288505],\"type\":\"violin\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"tickvals\":[0,1,2,3,4,5,6,7,8],\"ticktext\":[\"h3k27ac\",\"h3k27me3\",\"h3k36me3\",\"h3k4me1\",\"h3k4me3\",\"h3k9me3\",\"input\",\"rna_seq\",\"wgbs\"],\"title\":{\"text\":\"assay_epiclass\"}},\"yaxis\":{\"range\":[0,1.001],\"title\":{\"text\":\"Avg. prediction score (majority class)\"}},\"title\":{\"text\":\"Sample ontology - Prediction score distribution (EpiRR majority vote)\"},\"width\":900,\"height\":600},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('c4f66091-c8ce-4bf3-8c86-271d3568bf35');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"sample_ontology_pred_score\"\n",
    "this_logdir = base_fig_dir\n",
    "\n",
    "# pred_score_violin_alt(ct_concat_pred, this_logdir, f\"{name}_by_output_class\", use_aggregate_vote=False, group_by_column=CELL_TYPE, min_y=0)\n",
    "# pred_score_violin_alt(ct_concat_pred, this_logdir, f\"{name}_by_assay\", use_aggregate_vote=True, group_by_column=ASSAY, min_y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise KeyboardInterrupt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced features sets NN metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regulatory regions NN trainings data download\n",
    "\n",
    "~~~bash\n",
    "# Download phase\n",
    "paper_dir=\"${HOME}/Projects/epiclass/output/paper/data\"\n",
    "cd ${paper_dir}/training_results/dfreeze_v2\n",
    "\n",
    "base_path=\"/lustre07/scratch/rabyj/epilap-logs/epiatlas-dfreeze-v2.1\"\n",
    "rsync --info=progress2 -av --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"validation_confusion*\" --exclude \"*.md5\" --exclude \"full*\" narval:${base_path}/hg38_regulatory_regions_n* .\n",
    "\n",
    "# Cleanup phase\n",
    "# Remove files related to failed experiements\n",
    "# Step 1: Find files and extract numbers\n",
    "find hg38_regulatory_regions_n* -type f -name \"*.e\" -exec grep -l 'has non-string label of type' {} + | \\\n",
    "grep -oE \"job[0-9]+\" | grep -oE \"[0-9]+\" > failure_jobid.txt\n",
    "\n",
    "# Step 2: Use extracted numbers to find and echo all matching filenames\n",
    "cat failure_jobid.txt | xargs -I% sh -c 'find . -type f -name \"*%*\" -delete'\n",
    "rm failure_jobid.txt\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_all_feature_set_metrics(\n",
    "    parent_folder: Path,\n",
    "    merge_assays: bool,\n",
    ") -> Dict[str, Dict[str, Dict[str, Dict[str, float]]]]:\n",
    "    \"\"\"Obtain all metrics for all feature sets.\n",
    "\n",
    "    Args:\n",
    "        parent_folder (Path): The parent folder containing all feature set folders.\n",
    "                              Needs to be parent of feature set folders.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, float]]]: A dictionary containing all metrics for all feature sets.\n",
    "            Format: {feature_set: {task_name: {split_name: metric_dict}}}\n",
    "    \"\"\"\n",
    "    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]] = {}\n",
    "    for folder in parent_folder.iterdir():\n",
    "        if not folder.is_dir():\n",
    "            continue\n",
    "        feature_set = folder.name\n",
    "        try:\n",
    "            split_results_metrics = general_split_metrics(\n",
    "                folder, merge_assays=merge_assays, return_type=\"metrics\"\n",
    "            )\n",
    "        except ValueError as err:\n",
    "            raise ValueError(f\"Problem with {feature_set}\") from err\n",
    "        inverted_dict = split_results_handler.invert_metrics_dict(split_results_metrics)  # type: ignore\n",
    "        all_metrics[feature_set] = inverted_dict\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_all_feature_set_results(\n",
    "    parent_folder: Path,\n",
    "    merge_assays: bool,\n",
    ") -> Dict[str, Dict[str, Dict[str, pd.DataFrame]]]:\n",
    "    \"\"\"Obtain all metrics for all feature sets.\n",
    "\n",
    "    Args:\n",
    "        parent_folder (Path): The parent folder containing all feature set folders.\n",
    "                              Needs to be parent of feature set folders.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, pd.DataFrame]]]: A dictionary containing all results for all feature sets.\n",
    "            Format: {feature_set: {task_name: {split_name: results_dataframe}}}\n",
    "    \"\"\"\n",
    "    all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]] = {}\n",
    "    for folder in parent_folder.iterdir():\n",
    "        if not folder.is_dir():\n",
    "            continue\n",
    "        feature_set = folder.name\n",
    "        try:\n",
    "            split_results = general_split_metrics(\n",
    "                folder, merge_assays=merge_assays, return_type=\"split_results\"\n",
    "            )\n",
    "        except ValueError as err:\n",
    "            raise ValueError(f\"Problem with {feature_set}\") from err\n",
    "        all_results[feature_set] = split_results  # type: ignore\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_sizes_from_metadata() -> Dict[str, int]:\n",
    "    \"\"\"Get input sizes for models using certain feature sets using comet-ml run metadata file.\"\"\"\n",
    "    run_metadata = RUN_METADATA.copy(deep=True)\n",
    "\n",
    "    # Filter out epigeec_filter_1.4.5 runs, wrong input sizes.\n",
    "    run_metadata = run_metadata[run_metadata[\"startTimeMillis\"] > 1706943404420]\n",
    "\n",
    "    run_metadata[\"feature_set\"] = run_metadata[\"Name\"].str.extract(\n",
    "        pat=r\"(^hg38_1[0]{0,2}kb_.*_none).*$\"\n",
    "    )\n",
    "\n",
    "    input_sizes_count = run_metadata.groupby([\"feature_set\", \"input_size\"]).size()\n",
    "    accepted_input_sizes = {idx[0]: int(idx[1]) for idx in input_sizes_count.index}\n",
    "\n",
    "    assert len(input_sizes_count) == len(accepted_input_sizes)\n",
    "    accepted_input_sizes.update({\"hg38_100kb_all_none\": 30321})\n",
    "\n",
    "    return accepted_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def filter_feature_set(feature_set_data:Dict[str, Dict], include_names: List[str], exclude_names: List[str]) -> Dict[str, Dict]:\n",
    "    \"\"\"bleg\"\"\"\n",
    "    filtered_set_data = copy.deepcopy(feature_set_data)\n",
    "    for name, data_dict in list(filtered_set_data.items()):\n",
    "        for task_name in list(data_dict.keys()):\n",
    "            if not any(label in task_name for label in include_names):\n",
    "                del filtered_set_data[name][task_name]\n",
    "                continue\n",
    "            if any(label in task_name for label in exclude_names):\n",
    "                del filtered_set_data[name][task_name]\n",
    "    return filtered_set_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "input_sizes = extract_input_sizes_from_output_files(gen_data_dir)  # type: ignore\n",
    "input_sizes: Dict[str, int] = {k: v.pop() for k, v in input_sizes.items() if len(v) == 1}  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flagship_order = [\n",
    "#     \"hg38_10mb_all_none_1mb_coord\",\n",
    "#     \"hg38_100kb_random_n316_none\",\n",
    "#     \"hg38_1mb_all_none\",\n",
    "#     \"hg38_100kb_random_n3044_none\",\n",
    "#     \"hg38_100kb_all_none\",\n",
    "#     \"hg38_gene_regions_100kb_coord_n19864\",\n",
    "#     \"hg38_regulatory_regions_n30321\",\n",
    "#     \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n",
    "#     \"hg38_10kb_random_n30321_none\",\n",
    "#     \"hg38_1kb_random_n30321_none\",\n",
    "#     \"hg38_10kb_all_none\",\n",
    "#     \"hg38_regulatory_regions_n303114\",\n",
    "#     \"hg38_cpg_topvar_200bp_10kb_coord_n300k\",\n",
    "#     \"hg38_1kb_random_n303114_none\",\n",
    "# ]\n",
    "\n",
    "flagship_order = [\n",
    "    \"hg38_regulatory_regions_n30321\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n",
    "    \"hg38_gene_regions_100kb_coord_n19864\",\n",
    "    \"hg38_100kb_all_none\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = obtain_all_feature_set_metrics(gen_data_dir, merge_assays=True)\n",
    "flagship_metrics = {k: all_metrics[k] for k in flagship_order if k in all_metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_colors = {\n",
    "    \"100kb\": px.colors.qualitative.Safe[0],\n",
    "    \"10kb\": px.colors.qualitative.Safe[1],\n",
    "    \"1kb\": px.colors.qualitative.Safe[2],\n",
    "    \"regulatory\": px.colors.qualitative.Safe[3],\n",
    "    \"gene\": px.colors.qualitative.Safe[4],\n",
    "    \"cpg\": px.colors.qualitative.Safe[5],\n",
    "    \"1mb\": px.colors.qualitative.Safe[6],\n",
    "    \"5mb\": px.colors.qualitative.Safe[7],\n",
    "    \"10mb\": px.colors.qualitative.Safe[8],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_feature_set_metrics(\n",
    "    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n",
    "    input_sizes: Dict[str, int],\n",
    "    logdir: Path,\n",
    "    sort_by_input_size: bool = False,\n",
    "    name: str | None = None,\n",
    "    y_range: Tuple[float, float]|None = None\n",
    ") -> None:\n",
    "    \"\"\"Graph the metrics for all feature sets.\n",
    "\n",
    "    Args:\n",
    "        all_metrics (Dict[str, Dict[str, Dict[str, Dict[str, float]]]): A dictionary containing all metrics for all feature sets.\n",
    "            Format: {feature_set: {task_name: {split_name: metric_dict}}}\n",
    "        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n",
    "        name (str|None): The name of the figure.\n",
    "    \"\"\"\n",
    "    reference_hdf5_type = \"hg38_100kb_all_none\"\n",
    "    metadata_categories = list(all_metrics[reference_hdf5_type].keys())\n",
    "    print(metadata_categories)\n",
    "\n",
    "    non_standard_names = {ASSAY: f\"{ASSAY}_11c\", SEX: f\"{SEX}_w-mixed\"}\n",
    "    non_standard_assay_task_names = [\"hg38_100kb_all_none\"]\n",
    "    non_standard_sex_task_name = [\n",
    "        \"hg38_100kb_all_none\",\n",
    "        \"hg38_regulatory_regions_n30321\",\n",
    "        \"hg38_regulatory_regions_n303114\",\n",
    "    ]\n",
    "\n",
    "    for i in range(len(metadata_categories)):\n",
    "        category_idx = i\n",
    "        category_fig = make_subplots(\n",
    "            rows=1,\n",
    "            cols=2,\n",
    "            shared_yaxes=True,\n",
    "            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n",
    "            x_title=\"Feature set\",\n",
    "            y_title=\"Metric value\",\n",
    "        )\n",
    "\n",
    "        trace_names = []\n",
    "        order = list(all_metrics.keys())\n",
    "        if sort_by_input_size:\n",
    "            order = sorted(\n",
    "                all_metrics.keys(),\n",
    "                key=lambda x: input_sizes[x],\n",
    "            )\n",
    "        for feature_set_name in order:\n",
    "            # print(feature_set_name)\n",
    "            tasks_dicts = all_metrics[feature_set_name]\n",
    "            meta_categories = copy.deepcopy(metadata_categories)\n",
    "\n",
    "            if feature_set_name not in input_sizes:\n",
    "                print(f\"Skipping {feature_set_name}, no input size found.\")\n",
    "                continue\n",
    "\n",
    "            task_name = meta_categories[category_idx]\n",
    "            if \"split\" in task_name:\n",
    "                raise ValueError(\"Split in task name. Wrong metrics dict.\")\n",
    "\n",
    "            try:\n",
    "                task_dict = tasks_dicts[task_name]\n",
    "            except KeyError as err:\n",
    "                if SEX in str(err) and feature_set_name in non_standard_sex_task_name:\n",
    "                    task_dict = tasks_dicts[non_standard_names[SEX]]\n",
    "                elif (\n",
    "                    ASSAY in str(err)\n",
    "                    and feature_set_name in non_standard_assay_task_names\n",
    "                ):\n",
    "                    task_dict = tasks_dicts[non_standard_names[ASSAY]]\n",
    "                else:\n",
    "                    print(\"Skipping\", feature_set_name, task_name)\n",
    "                    continue\n",
    "\n",
    "            input_size = input_sizes[feature_set_name]\n",
    "\n",
    "            feature_set_name = feature_set_name.replace(\"_none\", \"\")\n",
    "            feature_set_name = feature_set_name.replace(\"hg38_\", \"\")\n",
    "\n",
    "            resolution = feature_set_name.split(\"_\")[0]\n",
    "\n",
    "            trace_name = f\"{input_size}|{feature_set_name}\"\n",
    "            trace_names.append(trace_name)\n",
    "\n",
    "            # Accuracy\n",
    "            metric = \"Accuracy\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"outliers\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=resolution_colors[resolution],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                    legendgroup=resolution,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "            metric = \"F1_macro\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"outliers\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=resolution_colors[resolution],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                    legendgroup=resolution,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=2,\n",
    "            )\n",
    "\n",
    "        # category_fig.update_yaxes(range=[0.65, 1.001])\n",
    "        title = f\"Neural network performance - {metadata_categories[category_idx]}\"\n",
    "        if name is not None:\n",
    "            title += f\" - {name}\"\n",
    "        category_fig.update_layout(\n",
    "            width=1500,\n",
    "            height=1500,\n",
    "            title=title,\n",
    "        )\n",
    "\n",
    "        if logdir.name == \"all\":\n",
    "            category_fig.update_xaxes(\n",
    "                categoryorder=\"array\",\n",
    "                categoryarray=sorted(trace_names, key=lambda x: int(x.split(\"|\")[0])),\n",
    "            )\n",
    "\n",
    "        # dummy scatters for resolution colors\n",
    "        relevant_resolutions = [\n",
    "            resolution\n",
    "            for resolution in resolution_colors\n",
    "            if any(resolution in name for name in trace_names)\n",
    "        ]\n",
    "        for resolution in relevant_resolutions:\n",
    "            color = resolution_colors[resolution]\n",
    "            category_fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[None],\n",
    "                    y=[None],\n",
    "                    mode=\"markers\",\n",
    "                    name=resolution,\n",
    "                    marker=dict(color=color, size=5),\n",
    "                    showlegend=True,\n",
    "                    legendgroup=resolution,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        category_fig.update_layout(legend=dict(itemsizing=\"constant\"))\n",
    "        if y_range:\n",
    "            category_fig.update_yaxes(range=y_range)\n",
    "\n",
    "        # Save figure\n",
    "        base_name = f\"feature_set_metrics_{metadata_categories[category_idx]}\"\n",
    "        if name is not None:\n",
    "            base_name = base_name + f\"_{name}\"\n",
    "        category_fig.write_html(logdir / f\"{base_name}.html\")\n",
    "        category_fig.write_image(logdir / f\"{base_name}.svg\")\n",
    "        category_fig.write_image(logdir / f\"{base_name}.png\")\n",
    "\n",
    "        category_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task selection, sample ontology and assay_epiclass\n",
    "include_names = [\"assay\", \"sample_ontology\"]\n",
    "exclude_names = [\"7c\", \"chip\", \"ct\"]\n",
    "flagship_metrics = filter_feature_set(\n",
    "    flagship_metrics,\n",
    "    include_names,\n",
    "    exclude_names\n",
    ")\n",
    "\n",
    "flagship_metrics[\"hg38_100kb_all_none\"][ASSAY] = flagship_metrics[\"hg38_100kb_all_none\"][f\"{ASSAY}_11c\"]\n",
    "del flagship_metrics[\"hg38_100kb_all_none\"][f\"{ASSAY}_11c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = (\n",
    "    base_fig_dir\n",
    "    / \"fig2_EpiAtlas_other\"\n",
    "    / \"fig2--reduced_feature_sets\"\n",
    "    / \"flagship_selection_2\"\n",
    "\n",
    ")\n",
    "logdir.mkdir(parents=False, exist_ok=True)\n",
    "print(logdir)\n",
    "y_min=0.9\n",
    "graph_feature_set_metrics(\n",
    "    all_metrics=flagship_metrics,\n",
    "    input_sizes=input_sizes,\n",
    "    logdir=logdir,\n",
    "    sort_by_input_size=False,\n",
    "    y_range=[y_min,1.001],\n",
    "    name = f\"y_min{y_min}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_metric_sets_per_assay(\n",
    "    all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]], verbose: bool = False\n",
    ") -> Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]:\n",
    "    \"\"\"Prepare metric sets per assay.\n",
    "\n",
    "    Args:\n",
    "        all_results (Dict[str, Dict[str, Dict[str, pd.DataFrame]]]): A dictionary containing all results for all feature sets.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, Dict[str, float]]]]: A dictionary containing all metrics per assay for all feature sets.\n",
    "            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Loading metadata.\")\n",
    "    metadata = metadata_handler.load_metadata(\"v2\")\n",
    "    metadata.convert_classes(ASSAY, ASSAY_MERGE_DICT)\n",
    "    md5_per_assay = metadata.md5_per_class(ASSAY)\n",
    "    md5_per_assay = {k: set(v) for k, v in md5_per_assay.items()}\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Getting results per assay.\")\n",
    "    results_per_assay = {}\n",
    "    for assay_label in ASSAY_ORDER:\n",
    "        if verbose:\n",
    "            print(assay_label)\n",
    "        results_per_assay[assay_label] = {}\n",
    "        for feature_set, task_dict in all_results.items():\n",
    "            if verbose:\n",
    "                print(feature_set)\n",
    "            results_per_assay[assay_label][feature_set] = {}\n",
    "            for task_name, split_dict in task_dict.items():\n",
    "                # if not CELL_TYPE in task_name:\n",
    "                #     continue\n",
    "                if verbose:\n",
    "                    print(task_name)\n",
    "                results_per_assay[assay_label][feature_set][task_name] = {}\n",
    "\n",
    "                # Only keep the relevant assay\n",
    "                for split_name, split_df in split_dict.items():\n",
    "                    if verbose:\n",
    "                        print(split_name)\n",
    "                    assay_df = split_df[split_df.index.isin(md5_per_assay[assay_label])]\n",
    "                    results_per_assay[assay_label][feature_set][task_name][\n",
    "                        split_name\n",
    "                    ] = assay_df\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Finished getting results per assay. Now computing metrics.\")\n",
    "    metrics_per_assay = {}\n",
    "    for assay_label in ASSAY_ORDER:\n",
    "        if verbose:\n",
    "            print(assay_label)\n",
    "        metrics_per_assay[assay_label] = {}\n",
    "        for feature_set, task_dict in results_per_assay[assay_label].items():\n",
    "            if verbose:\n",
    "                print(feature_set)\n",
    "            assay_metrics = split_results_handler.compute_split_metrics(\n",
    "                task_dict, concat_first_level=True\n",
    "            )\n",
    "            inverted_dict = split_results_handler.invert_metrics_dict(assay_metrics)\n",
    "            metrics_per_assay[assay_label][feature_set] = inverted_dict\n",
    "\n",
    "    return metrics_per_assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results = obtain_all_feature_set_results(gen_data_dir, merge_assays=True)\n",
    "# flagship_results = {k: all_results[k] for k in flagship_order if k in all_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter flagship results\n",
    "# flagship_results = filter_feature_set(flagship_results, include_names, exclude_names)\n",
    "# flagship_results[\"hg38_100kb_all_none\"][ASSAY] = flagship_results[\"hg38_100kb_all_none\"][f\"{ASSAY}_11c\"]\n",
    "# del flagship_results[\"hg38_100kb_all_none\"][f\"{ASSAY}_11c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When computing metrics per assay for cell type, some assays can be missing certain cell types, rendering AUC computation impossible.\n",
    "# flagship_metrics_per_assay = prepare_metric_sets_per_assay(\n",
    "#     flagship_results, verbose=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = (\n",
    "    base_fig_dir\n",
    "    / \"fig2_EpiAtlas_other\"\n",
    "    / \"fig2--reduced_feature_sets\"\n",
    "    / \"flagship_selection_1\"\n",
    "    / \"results_per_assay\"\n",
    ")\n",
    "# for assay in ASSAY_ORDER:\n",
    "#     assay_metrics = flagship_metrics_per_assay[assay]\n",
    "#     graph_feature_set_metrics(\n",
    "#         all_metrics=assay_metrics,\n",
    "#         input_sizes=input_sizes,\n",
    "#         logdir=logdir,\n",
    "#         sort_by_input_size=False,\n",
    "#         name=f\"{assay}_only\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_feature_set_metrics_per_assay(\n",
    "    all_metrics_per_assay: Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]],\n",
    "    input_sizes: Dict[str, int],\n",
    "    logdir: Path,\n",
    "    sort_by_input_size: bool = False,\n",
    "    name: str | None = None,\n",
    "    y_range: Tuple[float, float] | None = None\n",
    ") -> None:\n",
    "    \"\"\"Graph the metrics for all feature sets, per assay, with separate plots for accuracy and F1-score.\n",
    "\n",
    "    Args:\n",
    "        all_metrics_per_assay (Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]): A dictionary containing all metrics per assay for all feature sets.\n",
    "            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n",
    "        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n",
    "        logdir (Path): The directory where the figures will be saved.\n",
    "        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n",
    "        name (str|None): The name of the figure.\n",
    "        y_range (Tuple[float, float]|None): The y-axis range for the plots.\n",
    "    \"\"\"\n",
    "    reference_assay = next(iter(all_metrics_per_assay))\n",
    "    reference_feature_set = next(iter(all_metrics_per_assay[reference_assay]))\n",
    "    metadata_categories = list(all_metrics_per_assay[reference_assay][reference_feature_set].keys())\n",
    "    print(metadata_categories)\n",
    "\n",
    "    for category_idx, category in enumerate(metadata_categories):\n",
    "        for metric, metric_name in [(\"Accuracy\", \"Accuracy\"), (\"F1_macro\", \"F1-score (macro)\")]:\n",
    "            fig = go.Figure()\n",
    "\n",
    "            feature_sets = list(all_metrics_per_assay[reference_assay].keys())\n",
    "            unique_feature_sets = set(feature_sets)\n",
    "            for assay in ASSAY_ORDER:\n",
    "                if set(all_metrics_per_assay[assay].keys()) != unique_feature_sets:\n",
    "                    raise ValueError(\"Different feature sets through assays.\")\n",
    "\n",
    "            feature_set_order = feature_sets\n",
    "            if sort_by_input_size:\n",
    "                feature_set_order = sorted(feature_set_order, key=lambda x: input_sizes[x])\n",
    "\n",
    "            x_positions = {assay: i for i, assay in enumerate(ASSAY_ORDER)}\n",
    "            x_offset = len(feature_set_order) * 0.1 / 2  # Half the total width of feature set group\n",
    "\n",
    "            for i, feature_set_name in enumerate(feature_set_order):\n",
    "                resolution = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\").split(\"_\")[0]\n",
    "                color = resolution_colors[resolution]\n",
    "                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n",
    "\n",
    "                for assay in ASSAY_ORDER:\n",
    "                    if feature_set_name not in all_metrics_per_assay[assay]:\n",
    "                        continue\n",
    "\n",
    "                    tasks_dicts = all_metrics_per_assay[assay][feature_set_name]\n",
    "\n",
    "                    if feature_set_name not in input_sizes:\n",
    "                        print(f\"Skipping {feature_set_name}, no input size found.\")\n",
    "                        continue\n",
    "\n",
    "                    task_name = category\n",
    "                    if \"split\" in task_name:\n",
    "                        raise ValueError(\"Split in task name. Wrong metrics dict.\")\n",
    "\n",
    "                    try:\n",
    "                        task_dict = tasks_dicts[task_name]\n",
    "                    except KeyError:\n",
    "                        print(f\"Skipping {feature_set_name}, {task_name} for assay {assay}\")\n",
    "                        continue\n",
    "\n",
    "                    y_vals = [task_dict[split][metric] for split in task_dict]\n",
    "                    hovertext = [\n",
    "                        f\"{assay} - {display_name} - {split}: {metrics_dict[metric]:.4f}\"\n",
    "                        for split, metrics_dict in task_dict.items()\n",
    "                    ]\n",
    "\n",
    "                    x_position = x_positions[assay] + (i - len(feature_set_order)/2 + 0.5) * 0.1\n",
    "\n",
    "                    fig.add_trace(\n",
    "                        go.Box(\n",
    "                            x=[x_position] * len(y_vals),\n",
    "                            y=y_vals,\n",
    "                            name=f\"{assay}|{display_name}\",\n",
    "                            boxmean=True,\n",
    "                            boxpoints=\"outliers\",\n",
    "                            marker=dict(size=3, color=\"black\"),\n",
    "                            line=dict(width=1, color=\"black\"),\n",
    "                            fillcolor=color,\n",
    "                            hovertemplate=\"%{text}\",\n",
    "                            text=hovertext,\n",
    "                            showlegend=False,\n",
    "                            legendgroup=display_name,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            # Add dummy traces for the legend\n",
    "            for feature_set_name in feature_set_order:\n",
    "                resolution = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\").split(\"_\")[0]\n",
    "                color = resolution_colors[resolution]\n",
    "                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=[None],\n",
    "                        y=[None],\n",
    "                        mode='markers',\n",
    "                        marker=dict(size=10, color=color),\n",
    "                        showlegend=True,\n",
    "                        name=display_name,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            title = f\"Neural network performance - {category} - {metric_name} (per assay)\"\n",
    "            if name is not None:\n",
    "                title += f\" - {name}\"\n",
    "            fig.update_layout(\n",
    "                width=1500,\n",
    "                height=1000,\n",
    "                title=title,\n",
    "                xaxis_title=\"Assay\",\n",
    "                yaxis_title=metric_name,\n",
    "            )\n",
    "\n",
    "            fig.update_xaxes(\n",
    "                tickmode='array',\n",
    "                tickvals=list(x_positions.values()),\n",
    "                ticktext=list(x_positions.keys()),\n",
    "                title=\"Assay\"\n",
    "            )\n",
    "\n",
    "            fig.update_layout(\n",
    "                legend=dict(\n",
    "                    title=\"Feature Sets\",\n",
    "                    itemsizing=\"constant\",\n",
    "                    traceorder=\"normal\"\n",
    "                )\n",
    "            )\n",
    "            if y_range:\n",
    "                fig.update_yaxes(range=y_range)\n",
    "\n",
    "            base_name = f\"feature_set_metrics_{category}_{metric}_{metric_name}_per_assay\"\n",
    "            if name is not None:\n",
    "                base_name = base_name + f\"_{name}\"\n",
    "            fig.write_html(logdir / f\"{base_name}.html\")\n",
    "            fig.write_image(logdir / f\"{base_name}.svg\")\n",
    "            fig.write_image(logdir / f\"{base_name}.png\")\n",
    "\n",
    "            fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for y_min in [0.2, 0.96]:\n",
    "#     graph_feature_set_metrics_per_assay(\n",
    "#         all_metrics_per_assay=flagship_metrics_per_assay,\n",
    "#         input_sizes=input_sizes,\n",
    "#         logdir=logdir,\n",
    "#         sort_by_input_size=False,\n",
    "#         y_range=[y_min, 1.001],\n",
    "#         name=f\"y_min{y_min}\"\n",
    "#     )\n",
    "\n",
    "# y_min = 0.96\n",
    "# graph_feature_set_metrics_per_assay(\n",
    "#     all_metrics_per_assay=flagship_metrics_per_assay,\n",
    "#     input_sizes=input_sizes,\n",
    "#     logdir=logdir,\n",
    "#     sort_by_input_size=False,\n",
    "#     y_range=[y_min, 1.001],\n",
    "#     name=f\"y_min{y_min}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of zeroing blacklisted regions, and winzorizing input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data download\n",
    "\n",
    "~~~bash\n",
    "paper_dir=\"${HOME}/Projects/epiclass/output/paper/data\"\n",
    "cd ${paper_dir}/training_results/2023-01-epiatlas-freeze\n",
    "\n",
    "base_path=\"/lustre07/scratch/rabyj/epilap-logs/2023-01-epiatlas-freeze\"\n",
    "rsync --info=progress2 -a --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"validation_confusion*\" --exclude \"*.md5\" --exclude \"full*\" narval:${base_path}/hg38_100kb_all_none_0blklst* .\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLKLST_CATEGORIES = [\n",
    "    \"assay_epiclass\",\n",
    "    \"harmonized_biomaterial_type\",\n",
    "    \"harmonized_donor_sex\",\n",
    "    \"harmonized_sample_ontology_intermediate\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check oversampling\n",
    "\n",
    "Make sure oversampling is same in all training runs used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_2023_runs_oversampling():\n",
    "    \"\"\"Check if oversampling is on for all 2023 training runs used for blacklisted/winzorized metrics.\"\"\"\n",
    "    data_dir = base_data_dir / \"2023-01-epiatlas-freeze\"\n",
    "    for folder in data_dir.iterdir():\n",
    "        for category in BLKLST_CATEGORIES:\n",
    "            category_parent_folder = folder / f\"{category}_1l_3000n\"\n",
    "\n",
    "            if not category_parent_folder.exists():\n",
    "                raise FileNotFoundError(\"Cannot find: {category_parent_folder}\")\n",
    "\n",
    "            print(f\"Processing {category_parent_folder}\")\n",
    "\n",
    "            check_for_oversampling(category_parent_folder, verbose=False)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify_2023_runs_oversampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verify_2023_runs_oversampling result:\n",
    "Oversampling uniform across hdf5 types, but unsure across metadata categories.\n",
    "  - harmonized_biomaterial_type: On\n",
    "  - harmonized_sample_ontology_intermediate: On\n",
    "  - harmonized_donor_sex: Unknown, very probably On.\n",
    "    All nan values, but used human_longer.json hparams, which is the same as with the other runs that have oversampling on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blklst_split_metrics(\n",
    "    verbose: bool = False,\n",
    ") -> Dict[str, Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"Compute metrics on relevant categories and runs.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, float]]]: A dictionary containing all metrics for all blklst related feature sets.\n",
    "            Format: {feature_set: {task_name: {split_name: metric_dict}}}\n",
    "    \"\"\"\n",
    "    data_dir = base_data_dir / \"training_results\" / \"2023-01-epiatlas-freeze\"\n",
    "    feature_set_metrics_dict = {}\n",
    "    for folder in data_dir.iterdir():\n",
    "        if folder.is_file():\n",
    "            continue\n",
    "        feature_set_name = folder.name\n",
    "\n",
    "        tasks_dict = {}\n",
    "        for category in BLKLST_CATEGORIES:\n",
    "            category_parent_folder = folder / f\"{category}_1l_3000n\"\n",
    "\n",
    "            if not category_parent_folder.exists():\n",
    "                raise FileNotFoundError(\"Cannot find: {category_parent_folder}\")\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Processing {category_parent_folder}\")\n",
    "\n",
    "            for task_folder in category_parent_folder.iterdir():\n",
    "                if task_folder.is_file():\n",
    "                    continue\n",
    "                split_results = split_results_handler.read_split_results(task_folder)\n",
    "                general_name = f\"{category_parent_folder.name}-{task_folder.name}\"\n",
    "                tasks_dict[general_name] = split_results\n",
    "\n",
    "        feature_set_metrics = split_results_handler.compute_split_metrics(\n",
    "            tasks_dict, concat_first_level=True\n",
    "        )\n",
    "        feature_set_metrics_dict[\n",
    "            feature_set_name\n",
    "        ] = split_results_handler.invert_metrics_dict(feature_set_metrics)\n",
    "\n",
    "    return feature_set_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_set_metrics_dict = get_blklst_split_metrics(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blklst_graphs(\n",
    "    feature_set_metrics_dict: Dict[str, Dict[str, Dict[str, float]]], logdir: Path\n",
    ") -> None:\n",
    "    \"\"\"Create boxplots for blacklisted related feature sets.\n",
    "\n",
    "    Args:\n",
    "        feature_set_metrics_dict (Dict[str, Dict[str, Dict[str, float]]]): The dictionary containing all metrics for all blklst related feature sets.\n",
    "            format: {feature_set: {task_name: {split_name: metric_dict}}}\n",
    "    \"\"\"\n",
    "    # Assume names exist in all feature sets\n",
    "    task_names = list(feature_set_metrics_dict.values())[0].keys()\n",
    "\n",
    "    traces_names_dict = {\n",
    "        \"hg38_100kb_all_none\": \"observed\",\n",
    "        \"hg38_100kb_all_none_0blklst\": \"0blklst\",\n",
    "        \"hg38_100kb_all_none_0blklst_winsorized\": \"0blklst_winsorized\",\n",
    "    }\n",
    "\n",
    "    for task_name in task_names:\n",
    "        category_fig = make_subplots(\n",
    "            rows=1,\n",
    "            cols=2,\n",
    "            shared_yaxes=False,\n",
    "            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n",
    "            x_title=\"Feature set\",\n",
    "            y_title=\"Metric value\",\n",
    "            horizontal_spacing=0.03,\n",
    "        )\n",
    "        for feature_set_name, tasks_dicts in feature_set_metrics_dict.items():\n",
    "            task_dict = tasks_dicts[task_name]\n",
    "            trace_name = traces_names_dict[feature_set_name]\n",
    "\n",
    "            # Accuracy\n",
    "            metric = \"Accuracy\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "            metric = \"F1_macro\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=2,\n",
    "            )\n",
    "\n",
    "        category_fig.update_xaxes(\n",
    "            categoryorder=\"array\",\n",
    "            categoryarray=list(traces_names_dict.values()),\n",
    "        )\n",
    "        category_fig.update_yaxes(range=[0.85, 1.001])\n",
    "\n",
    "        task_name = task_name.replace(\"_1l_3000n-10fold\", \"\")\n",
    "        category_fig.update_layout(\n",
    "            title=f\"Neural network performance - {task_name} - 100kb\",\n",
    "        )\n",
    "\n",
    "        # Save figure\n",
    "        base_name = f\"metrics_{task_name}\"\n",
    "        category_fig.write_html(logdir / f\"{base_name}.html\")\n",
    "        category_fig.write_image(logdir / f\"{base_name}.svg\")\n",
    "        category_fig.write_image(logdir / f\"{base_name}.png\")\n",
    "\n",
    "        category_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--blklst_and_winsorized\" / \"y0.85\"\n",
    "logdir.mkdir(parents=False, exist_ok=True)\n",
    "# create_blklst_graphs(feature_set_metrics_dict, logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusions matrices per assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\n",
    "    \"assay\",\n",
    "    \"track_type\",\n",
    "    \"groups\",\n",
    "    \"disease\",\n",
    "    \"paired\",\n",
    "    \"life\",\n",
    "    \"sex\",\n",
    "    \"project\",\n",
    "]\n",
    "exclude_names = [\"chip-seq\", \"7c\"]\n",
    "\n",
    "hdf5_type = \"hg38_10mb_all_none_1mb_coord\"\n",
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "# all_split_results = general_split_metrics(\n",
    "#     results_dir,\n",
    "#     merge_assays=True,\n",
    "#     exclude_categories=exclude_categories,\n",
    "#     exclude_names=exclude_names,\n",
    "#     return_type=\"split_results\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix_per_assay(\n",
    "    results_df: Dict[str, Dict[str, pd.DataFrame]], metadata: Metadata, logdir: Path\n",
    "):\n",
    "    \"\"\"Create confusion matrices for each assay. Exclusive to cell type results.\n",
    "\n",
    "    Args:\n",
    "        results_df (pd.DataFrame): The DataFrame containing concatenated prediction results (no metadata).\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "    \"\"\"\n",
    "    merged_results = split_results_handler.concatenate_split_results(\n",
    "        results_df, concat_first_level=True\n",
    "    )\n",
    "    ct_results = merged_results[CELL_TYPE]\n",
    "    labels = ct_results.columns[2:].to_list()\n",
    "\n",
    "    augmented_ct_results = metadata_handler.join_metadata(ct_results, metadata)\n",
    "    augmented_ct_results[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    for group in augmented_ct_results.groupby(ASSAY):\n",
    "        assay = group[0]\n",
    "        assay_results = group[1]\n",
    "        pred = assay_results[\"Predicted class\"]\n",
    "        true = assay_results[\"True class\"]\n",
    "        conf_matrix = sk_cm(true, pred, labels=labels)\n",
    "        acc = accuracy_score(true, pred)\n",
    "        matrix_writer = ConfusionMatrixWriter(labels, conf_matrix)\n",
    "        matrix_writer.to_all_formats(logdir=logdir, name=f\"{assay}_acc{acc*100:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to make confusion matrices for each assay, and also rna/wgb merged\n",
    "base_conf_matrix_dir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"confusion_matrix_per_assay\"\n",
    "if not base_conf_matrix_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_conf_matrix_dir} does not exist.\")\n",
    "\n",
    "conf_matrix_dir = base_conf_matrix_dir / hdf5_type / f\"{CELL_TYPE}_1l_3000n\"\n",
    "if not conf_matrix_dir.exists():\n",
    "    conf_matrix_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# conf_matrix_per_assay(\n",
    "#     results_df=all_split_results, # type: ignore\n",
    "#     metadata=metadata_v2,\n",
    "#     logdir=conf_matrix_dir\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
