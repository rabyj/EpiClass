{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create figures (fig2) destined for the paper.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches, duplicate-code, unreachable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.core.metadata import Metadata\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    TemporaryLogFilter,\n",
    "    create_mislabel_corrector,\n",
    "    extract_experiment_keys_from_output_files,\n",
    "    extract_input_sizes_from_output_files,\n",
    "    merge_similar_assays,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE7_ASSAYS = ASSAY_ORDER[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map\n",
    "sex_colors = IHECColorMap.sex_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sex, color in list(sex_colors.items()):\n",
    "    if sex in [\"male\", \"female\", \"mixed\"]:\n",
    "        new_label = f\"{sex}_pale\"\n",
    "        new_color = color.replace(\"rgb\", \"rgba\").replace(\")\", \",0.5)\")\n",
    "        sex_colors[new_label] = new_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()\n",
    "metadata_handler = MetadataHandler(paper_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_v2 = metadata_handler.load_metadata(\"v2\")\n",
    "metadata_v2_df = metadata_handler.load_metadata_df(\"v2\", merge_assays=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_metadata_path = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"all_results_cometml_filtered_oversampling-fixed.csv\"\n",
    ")\n",
    "RUN_METADATA = pd.read_csv(parameters_metadata_path, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig 2 - EpiClass results on EpiAtlas other metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN performance across metadata categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if oversampling is uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_oversampling(parent_dir: Path, verbose: bool = False):\n",
    "    \"\"\"Check for oversampling status in the results, using \"output_job*.o\" files.\n",
    "    Args:\n",
    "        parent_dir (Path): Parent directory of the results. (classifier type level, e.g. assay_epiclass_1l_3000n)\n",
    "\n",
    "    \"\"\"\n",
    "    # Identify experiments\n",
    "    exp_keys_dict = extract_experiment_keys_from_output_files(parent_dir)\n",
    "\n",
    "    # Filter metadata to only include experiments in the results\n",
    "    all_exp_keys = set()\n",
    "    for exp_keys in exp_keys_dict.values():\n",
    "        all_exp_keys.update(exp_keys)\n",
    "\n",
    "    df = RUN_METADATA[RUN_METADATA[\"experimentKey\"].isin(all_exp_keys)]\n",
    "    df[\"general_name\"] = df[\"Name\"].str.replace(r\"[_-]?split\\d+$\", \"\", regex=True)\n",
    "    # print(df[[\"general_name\"] + [f\"run_arg_{i}\" for i in range(5)]].value_counts())\n",
    "\n",
    "    # Check oversampling values, ignore nan\n",
    "    df_na = df[df[\"hparams/oversampling\"].isna()]\n",
    "    df = df[df[\"hparams/oversampling\"].notna()]\n",
    "    if not (df[\"hparams/oversampling\"] == \"true\").all():\n",
    "        err_df = df.groupby([\"general_name\", \"hparams/oversampling\"]).agg(\"size\")\n",
    "        print(\n",
    "            \"Not all experiments have oversampling:\\n%s\",\n",
    "            err_df,\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Checked {len(exp_keys_dict)} folders and found {len(df)} oversampling values.\"\n",
    "    )\n",
    "    if len(df_na) != 0:\n",
    "        print(\n",
    "            \"Could not read oversampling value of all visited experiments. Values missing in:\"\n",
    "        )\n",
    "        print(df_na[[\"general_name\"] + [f\"run_arg_{i}\" for i in range(5)]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_for_oversampling(base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_10kb_all_none\" / \"assay_epiclass_1l_3000n\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read/correct results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=dangerous-default-value\n",
    "\n",
    "\n",
    "def fig2_a(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    exclude_categories: List[str] | None = None,\n",
    "    y_range: List[float] | None = None,\n",
    "    sort_by_acc: bool = False,\n",
    "    metric_names: List[str] = [\"Accuracy\", \"F1_macro\"],\n",
    "    show_plot: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Render box plots of metrics per classifier and split, each in its own subplot.\n",
    "\n",
    "    This function generates a figure with subplots, each representing a different\n",
    "    metric. Each subplot contains box plots for each classifier, ordered by accuracy.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A nested dictionary with structure {split: {classifier: {metric: score}}}.\n",
    "        logdir: The directory path to save the output plots.\n",
    "        name: The base name for the output plot files.\n",
    "        exclude_categories: Task categories to exclude from the plot.\n",
    "        y_range: The y-axis range for the plots.\n",
    "        sort_by_acc: Whether to sort the classifiers by accuracy.\n",
    "        metrics: The metrics to include in the plot.\n",
    "    \"\"\"\n",
    "    # Exclude some categories\n",
    "    classifier_names = list(split_metrics[\"split0\"].keys())\n",
    "    if exclude_categories is not None:\n",
    "        for category in exclude_categories:\n",
    "            classifier_names = [c for c in classifier_names if category not in c]\n",
    "\n",
    "    available_metrics = list(split_metrics[\"split0\"][classifier_names[0]].keys())\n",
    "    if any(metric not in available_metrics for metric in metric_names):\n",
    "        raise ValueError(f\"Invalid metric. Metrics need to be in {available_metrics}\")\n",
    "\n",
    "    # Sort classifiers by accuracy\n",
    "    if sort_by_acc:\n",
    "        mean_acc = {}\n",
    "        for classifier in classifier_names:\n",
    "            mean_acc[classifier] = np.mean(\n",
    "                [split_metrics[split][classifier][\"Accuracy\"] for split in split_metrics]\n",
    "            )\n",
    "        classifier_names = sorted(\n",
    "            classifier_names, key=lambda x: mean_acc[x], reverse=True\n",
    "        )\n",
    "\n",
    "    # Create subplots, one column for each metric\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(metric_names),\n",
    "        subplot_titles=metric_names,\n",
    "        horizontal_spacing=0.03,\n",
    "    )\n",
    "\n",
    "    color_group = px.colors.qualitative.Plotly\n",
    "    colors = {\n",
    "        classifier: color_group[i % len(color_group)]\n",
    "        for i, classifier in enumerate(classifier_names)\n",
    "    }\n",
    "\n",
    "    # point_pos = -1.35\n",
    "    point_pos = 0\n",
    "    for i, metric in enumerate(metric_names):\n",
    "        for classifier_name in classifier_names:\n",
    "            values = [\n",
    "                split_metrics[split][classifier_name][metric] for split in split_metrics\n",
    "            ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=values,\n",
    "                    name=classifier_name,\n",
    "                    fillcolor=colors[classifier_name],\n",
    "                    line=dict(color=\"black\", width=1.5),\n",
    "                    marker=dict(size=3, color=\"white\", line_width=1),\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    pointpos=point_pos,\n",
    "                    showlegend=i == 0,  # Only show legend in the first subplot\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in zip(split_metrics, values)\n",
    "                    ],\n",
    "                    legendgroup=classifier_name,\n",
    "                    width=0.5,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=i + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Neural network classification - Metric distribution for 10-fold cross-validation\",\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode=\"group\",\n",
    "        height=1200 * 0.8,\n",
    "        width=1750 * 0.8,\n",
    "    )\n",
    "\n",
    "    # Acc, F1\n",
    "    # range_acc = [0.86, 1.001]\n",
    "    # fig.update_layout(yaxis=dict(range=range_acc))\n",
    "    # fig.update_layout(yaxis2=dict(range=range_acc))\n",
    "    fig.update_layout(yaxis=dict(range=[0.88, 1.001]))\n",
    "    fig.update_layout(yaxis2=dict(range=[0.80, 1.001]))\n",
    "\n",
    "    # AUC\n",
    "    range_auc = [0.986, 1.0001]\n",
    "    fig.update_layout(yaxis3=dict(range=range_auc))\n",
    "    fig.update_layout(yaxis4=dict(range=range_auc))\n",
    "\n",
    "    if y_range is not None:\n",
    "        fig.update_yaxes(range=y_range)\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    if show_plot:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\"track_type\", \"groups\", \"disease\", \"no-mixed\"]\n",
    "# exclude_categories = [\"track_type\", \"group\"]\n",
    "exclude_names = [\"chip-seq\", \"7c\", \"16ct\"]\n",
    "\n",
    "hdf5_type = \"hg38_100kb_all_none\"\n",
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "\n",
    "\n",
    "mislabel_correction = True\n",
    "if mislabel_correction:\n",
    "    mislabel_corrector = create_mislabel_corrector(paper_dir)\n",
    "else:\n",
    "    mislabel_corrector = None\n",
    "\n",
    "split_results_metrics, all_split_results = split_results_handler.general_split_metrics(\n",
    "    results_dir,\n",
    "    merge_assays=True,\n",
    "    exclude_categories=exclude_categories,\n",
    "    exclude_names=exclude_names,\n",
    "    return_type=\"both\",\n",
    "    oversampled_only=True,\n",
    "    mislabel_corrections=mislabel_corrector,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate pre-correction vs post-correction sex/life_stage metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_names = [\"no-mixed\", \"chip-seq-only\"]\n",
    "# include_categories = [SEX, LIFE_STAGE]\n",
    "# v1_1_split_results_metrics = split_results_handler.general_split_metrics(\n",
    "#     results_dir,\n",
    "#     merge_assays=True,\n",
    "#     include_categories=include_categories,\n",
    "#     exclude_names=exclude_names,\n",
    "#     return_type=\"metrics\",\n",
    "#     mislabel_corrections=None\n",
    "# )\n",
    "\n",
    "# mislabels_corrector = create_mislabel_corrector(paper_dir)\n",
    "# v1_2_split_results_metrics = split_results_handler.general_split_metrics(\n",
    "#     results_dir,\n",
    "#     merge_assays=True,\n",
    "#     include_categories=include_categories,\n",
    "#     exclude_names=exclude_names,\n",
    "#     return_type=\"metrics\",\n",
    "#     mislabel_corrections=mislabels_corrector\n",
    "# )\n",
    "\n",
    "# for classifier in v1_1_split_results_metrics[\"split0\"].keys():\n",
    "#     print(f\"Classifier {classifier}\")\n",
    "#     avg_acc_v1_1 = np.mean([v1_1_split_results_metrics[split][classifier][\"Accuracy\"] for split in v1_1_split_results_metrics.keys()])\n",
    "#     avg_acc_v1_2 = np.mean([v1_2_split_results_metrics[split][classifier][\"Accuracy\"] for split in v1_2_split_results_metrics.keys()])\n",
    "#     print(f\"V1.1: Avg Acc={avg_acc_v1_1:0.4f}\")\n",
    "#     print(f\"V1.2: Avg Acc={avg_acc_v1_2:0.4f}\")\n",
    "#     avg_f1_v1_1 = np.mean([v1_1_split_results_metrics[split][classifier][\"F1_macro\"] for split in v1_1_split_results_metrics.keys()])\n",
    "#     avg_f1_v1_2 = np.mean([v1_2_split_results_metrics[split][classifier][\"F1_macro\"] for split in v1_2_split_results_metrics.keys()])\n",
    "#     print(f\"V1.1: Avg F1={avg_f1_v1_1:0.4f}\")\n",
    "#     print(f\"V1.2: Avg F1={avg_f1_v1_2:0.4f}\")\n",
    "#     for split in v1_1_split_results_metrics.keys():\n",
    "#         print(f\"Split {split}\")\n",
    "#         print(f\"V1.1: Acc={v1_1_split_results_metrics[split][classifier]['Accuracy']:0.4f}, F1={v1_1_split_results_metrics[split][classifier]['F1_macro']:0.4f}\")\n",
    "#         print(f\"V1.2: Acc={v1_2_split_results_metrics[split][classifier]['Accuracy']:0.4f}, F1={v1_2_split_results_metrics[split][classifier]['F1_macro']:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\"\n",
    "fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "if mislabel_correction:\n",
    "    this_fig_logdir = fig_logdir / \"post_mislabel_correction\"\n",
    "    this_fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "else:\n",
    "    this_fig_logdir = fig_logdir / \"no_mislabel_correction\"\n",
    "    this_fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "metrics_full = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "metrics_AUC = [\"AUC_micro\", \"AUC_macro\"]\n",
    "metrics_acc_F1 = [\"Accuracy\", \"F1_macro\"]\n",
    "exclude_categories = [\"sex_no-mixed\", \"disease\"]\n",
    "y_range_AUC = [0.986, 1.0001]\n",
    "y_range_acc = [0.88, 1.001]\n",
    "\n",
    "for name, metrics, y_range in zip(\n",
    "    [\"full\", \"acc_F1\", \"AUC\"],\n",
    "    [metrics_full, metrics_acc_F1, metrics_AUC],\n",
    "    [None, y_range_acc, y_range_AUC],\n",
    "):\n",
    "    fig_name = f\"{hdf5_type}_perf_across_categories_{name}\"\n",
    "    continue\n",
    "    fig2_a(\n",
    "        split_results_metrics,  # type: ignore\n",
    "        this_fig_logdir,\n",
    "        fig_name,\n",
    "        sort_by_acc=True,\n",
    "        metric_names=metrics,\n",
    "        exclude_categories=exclude_categories,\n",
    "        show_plot=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_imbalance(\n",
    "    all_split_results: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute class imbalance for each task and split.\n",
    "\n",
    "    Args:\n",
    "        all_split_results: A dictionary with structure {task_name: {split_name: split_results_df}}.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the following columns:\n",
    "            - avg(balance_ratio): The average balance ratio for each task.\n",
    "            - n: The number of classes for each task (used for the average).\n",
    "    \"\"\"\n",
    "    # combine md5 lists\n",
    "    task_md5s = {\n",
    "        classifier_task: [split_df.index for split_df in split_results.values()]\n",
    "        for classifier_task, split_results in all_split_results.items()\n",
    "    }\n",
    "    task_md5s = {\n",
    "        classifier_task: [list(split_md5s) for split_md5s in md5s]\n",
    "        for classifier_task, md5s in task_md5s.items()\n",
    "    }\n",
    "    task_md5s = {\n",
    "        classifier_task: list(itertools.chain(*md5s))\n",
    "        for classifier_task, md5s in task_md5s.items()\n",
    "    }\n",
    "\n",
    "    # get metadata\n",
    "    metadata_df = metadata_handler.load_metadata_df(\"v2-encode\")\n",
    "\n",
    "    label_counts = {}\n",
    "    for classifier_task, md5s in task_md5s.items():\n",
    "        try:\n",
    "            label_counts[classifier_task] = metadata_df.loc[md5s][\n",
    "                classifier_task\n",
    "            ].value_counts()\n",
    "        except KeyError as e:\n",
    "            category_name = classifier_task.rsplit(\"_\", maxsplit=1)[0]\n",
    "            try:\n",
    "                label_counts[classifier_task] = metadata_df.loc[md5s][\n",
    "                    category_name\n",
    "                ].value_counts()\n",
    "            except KeyError as e:\n",
    "                raise e\n",
    "\n",
    "    # Compute average class ratio vs majority class\n",
    "    # class_ratios = {}\n",
    "    # for classifier_task, counts in label_counts.items():\n",
    "    #     class_ratios[classifier_task] = (np.mean(counts / max(counts)), len(counts))\n",
    "\n",
    "    # Compute Shannon Entropy\n",
    "    class_balance = {}\n",
    "    for classifier_task, counts in label_counts.items():\n",
    "        total_count = counts.sum()\n",
    "        k = len(counts)\n",
    "        p_x = counts / total_count  # class proportions\n",
    "        p_x = p_x.values\n",
    "        shannon_entropy = -np.sum(p_x * np.log2(p_x))\n",
    "        balance = shannon_entropy / np.log2(k)\n",
    "        class_balance[classifier_task] = (balance, k)\n",
    "\n",
    "    df_class_balance = pd.DataFrame.from_dict(\n",
    "        class_balance, orient=\"index\", columns=[\"Normalized Shannon Entropy\", \"k\"]\n",
    "    ).sort_index()\n",
    "\n",
    "    return df_class_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_type = \"hg38_100kb_all_none\"\n",
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "# all_split_results = split_results_handler.general_split_metrics(\n",
    "#     results_dir,\n",
    "#     exclude_categories=None,\n",
    "#     exclude_names=None,\n",
    "#     merge_assays=True,\n",
    "#     return_type=\"split_results\",\n",
    "# )\n",
    "\n",
    "# fig_logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\"\n",
    "# df_class_balance = compute_class_imbalance(all_split_results)\n",
    "# df_class_balance.to_csv(fig_logdir / \"class_balance_Shannon.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN performance per assay across metadata categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_performance_per_assay_across_categories(\n",
    "    all_split_results: Dict[str, Dict[str, pd.DataFrame]],\n",
    "    logdir: Path | None = None,\n",
    "    name: str | None = None,\n",
    "    title_end: str = \"\",\n",
    "    exclude_categories: List[str] | None = None,\n",
    "    y_range: None | List[float] = None,\n",
    "    verbose: bool = False,\n",
    "    min_predScore: float | None = None,\n",
    "):\n",
    "    \"\"\"Create a box plot of assay accuracy for each classifier.\"\"\"\n",
    "    all_split_results = copy.deepcopy(all_split_results)\n",
    "\n",
    "    # Exclude some categories\n",
    "    classifier_names = list(all_split_results.keys())\n",
    "    if exclude_categories is not None:\n",
    "        for category in exclude_categories:\n",
    "            classifier_names = [c for c in classifier_names if category not in c]\n",
    "\n",
    "    metadata_df = MetadataHandler(paper_dir).load_metadata_df(\"v2-encode\")\n",
    "\n",
    "    # One graph per metadata category\n",
    "    for task_name in classifier_names:\n",
    "        if verbose:\n",
    "            print(f\"Processing {task_name}\")\n",
    "        split_results = all_split_results[task_name]\n",
    "\n",
    "        # Merge similar assays\n",
    "        if ASSAY in task_name:\n",
    "            for split_name in split_results:\n",
    "                try:\n",
    "                    split_results[split_name] = merge_similar_assays(\n",
    "                        split_results[split_name]\n",
    "                    )\n",
    "                except ValueError as e:\n",
    "                    print(f\"Skipping {task_name} assay merging: {e}\")\n",
    "                    break\n",
    "\n",
    "        # Filter by min_predScore\n",
    "        if min_predScore is not None:\n",
    "            for split_name in split_results:\n",
    "                df = split_results[split_name]\n",
    "                df = split_results_handler.add_max_pred(df)\n",
    "                df = df[df[\"Max pred\"] >= min_predScore]\n",
    "                split_results[split_name] = df\n",
    "\n",
    "        # files per assay\n",
    "        files_per_assay = Counter()\n",
    "        for df in split_results.values():\n",
    "            split_meta = metadata_df.loc[df.index]\n",
    "            files_per_assay.update(split_meta[ASSAY].replace(ASSAY_MERGE_DICT).values)\n",
    "\n",
    "        assay_acc_df = split_results_handler.compute_acc_per_assay(\n",
    "            split_results, metadata_df\n",
    "        )\n",
    "\n",
    "        fig = go.Figure()\n",
    "        for assay in ASSAY_ORDER:\n",
    "            try:\n",
    "                assay_accuracies = assay_acc_df[assay]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=assay_accuracies.values,\n",
    "                    name=f\"{assay} (N={files_per_assay[assay]})\",\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=True,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=assay_colors[assay],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in assay_accuracies.items()\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # if \"sample_ontology\" in task_name:\n",
    "        #     yrange = [0.59, 1.001]\n",
    "        # elif ASSAY in task_name:\n",
    "        #     yrange = [0.985, 1.001]\n",
    "        # else:\n",
    "        yrange = [assay_acc_df.min(), 1.001]  # type: ignore\n",
    "\n",
    "        if y_range is not None:\n",
    "            yrange = y_range\n",
    "\n",
    "        fig.update_yaxes(range=yrange)\n",
    "\n",
    "        title_text = f\"NN classification - {task_name}\"\n",
    "        if min_predScore is not None:\n",
    "            title_text += f\" (minPredScore={min_predScore})\"\n",
    "        if title_end:\n",
    "            title_text += f\" - {title_end}\"\n",
    "        fig.update_layout(\n",
    "            title_text=title_text,\n",
    "            yaxis_title=\"Accuracy\",\n",
    "            xaxis_title=\"Assay\",\n",
    "            width=1000,\n",
    "            height=700,\n",
    "        )\n",
    "\n",
    "        # Save figure\n",
    "        if logdir:\n",
    "            this_name = f\"perf_per_assay_{task_name}\"\n",
    "            if name:\n",
    "                this_name = name + f\"_{task_name}\"\n",
    "            if min_predScore is not None:\n",
    "                this_name += f\"_minPredScore_{min_predScore}\"\n",
    "\n",
    "            fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "            fig.write_image(logdir / f\"{this_name}.png\")\n",
    "            fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\"track_type\", \"groups\", \"disease\"]\n",
    "exclude_names = [\"chip-seq\", \"7c\", \"no-mixed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 303114\n",
    "# # N = 30321\n",
    "# hdf5_type = f\"hg38_regulatory_regions_n{N}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_types = []\n",
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "# exclude_names = [\"random\", \"global\", \"cpg\", \"gene\", \"regulatory\", \"100kb\", \"10kb\"]\n",
    "exclude_names = [\n",
    "    \"global\",\n",
    "    \"cpg\",\n",
    "    \"gene\",\n",
    "    \"regulatory\",\n",
    "    \"coord\",\n",
    "    \"10kb\",\n",
    "    \"1kb\",\n",
    "    \"1mb\",\n",
    "    \"4510\",\n",
    "    \"118\",\n",
    "    \"all_none\",\n",
    "]\n",
    "for folder in results_dir.iterdir():\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    if any(label in folder.name for label in exclude_names):\n",
    "        continue\n",
    "\n",
    "    hdf5_types.append(folder.name)\n",
    "\n",
    "for hdf5_type in hdf5_types:\n",
    "    print(hdf5_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf5_type_1mb = \"hg38_1mb_all_none\"\n",
    "# hdf5_type_gene = \"hg38_gene_regions_100kb_coord_n19864\"\n",
    "# hdf5_type_cpg_300k = \"hg38_cpg_regions_10kb_coord_n300k\"\n",
    "# hdf5_type_cpg_30k = \"hg38_cpg_regions_10kb_coord_n30k\"\n",
    "# hdf5_types = [hdf5_type_1mb, hdf5_type_gene, hdf5_type_cpg_300k, hdf5_type_cpg_30k]\n",
    "# hdf5_types = [\"hg38_100kb_all_none\"]\n",
    "hdf5_types = [\"hg38_100kb_all_none\"]\n",
    "\n",
    "for hdf5_type in hdf5_types:\n",
    "    results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "    if not results_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "\n",
    "    all_split_results = split_results_handler.general_split_metrics(\n",
    "        results_dir,\n",
    "        merge_assays=True,\n",
    "        exclude_categories=exclude_categories\n",
    "        + [\"disease\", \"cancer\", \"life\", \"sex\", \"end\", \"bio\", \"project\"],\n",
    "        exclude_names=exclude_names + [\"11c\", \"chip\"],\n",
    "        return_type=\"split_results\",\n",
    "        # verbose=True,\n",
    "    )\n",
    "\n",
    "    logdir = (\n",
    "        base_fig_dir\n",
    "        / \"fig2_EpiAtlas_other\"\n",
    "        / \"fig2--NN_perf_across_categories\"\n",
    "        / \"per_assay\"\n",
    "        / hdf5_type\n",
    "    )\n",
    "    logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "    # min_y = 0.65\n",
    "    # fig_name = f\"perf_per_assay_Y_{min_y:.2f}\"\n",
    "    # NN_performance_per_assay_across_categories(\n",
    "    #     all_split_results,  # type: ignore\n",
    "    #     logdir,\n",
    "    #     fig_name,\n",
    "    #     title_end=hdf5_type.replace(\"hg38_\", \"\"),\n",
    "    #     # exclude_categories=[CELL_TYPE],\n",
    "    #     y_range=[min_y, 1.001],\n",
    "    # )\n",
    "\n",
    "    # min_y = 0.3\n",
    "    # fig_name = f\"perf_per_assay_Y_{min_y:.2f}\"\n",
    "    # NN_performance_per_assay_across_categories(\n",
    "    #     all_split_results, # type: ignore\n",
    "    #     logdir,\n",
    "    #     fig_name,\n",
    "    #     title_end=hdf5_type.replace(\"hg38_\", \"\"),\n",
    "    #     exclude_categories=[ASSAY],\n",
    "    #     y_range=[min_y, 1.001],\n",
    "    # )\n",
    "\n",
    "    # min_y = 0.65\n",
    "    # fig_name = f\"perf_per_assay_Y_{min_y:.2f}\"\n",
    "    # NN_performance_per_assay_across_categories(\n",
    "    #     all_split_results, # type: ignore\n",
    "    #     logdir,\n",
    "    #     fig_name,\n",
    "    #     title_end=hdf5_type.replace(\"hg38_\", \"\"),\n",
    "    #     y_range=[min_y, 1.001],\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_fig_dir = (\n",
    "    base_fig_dir\n",
    "    / \"fig2_EpiAtlas_other\"\n",
    "    / \"fig2--NN_perf_across_categories\"\n",
    "    / \"per_assay\"\n",
    "    / f\"{hdf5_type}\"\n",
    "    / \"custom_cell_types\"\n",
    ")\n",
    "this_fig_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_categories = [\n",
    "    \"cell_type_martin\",\n",
    "    \"cell_type_PE\",\n",
    "    \"harmonized_sample_ontology_intermediate\",\n",
    "]\n",
    "exclude_names = [\"FC\", \"only\", \"chip\", \"unique\", \"27ct\", \"16ct\"]\n",
    "\n",
    "hdf5_type = \"hg38_100kb_all_none\"\n",
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "\n",
    "special_cell_type_split_results = split_results_handler.general_split_metrics(\n",
    "    results_dir,\n",
    "    merge_assays=True,\n",
    "    include_categories=include_categories,\n",
    "    exclude_names=exclude_names,\n",
    "    return_type=\"split_results\",\n",
    "    oversampled_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(special_cell_type_split_results) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detail content in a separate filea\n",
    "concat_results: Dict[str, pd.DataFrame] = split_results_handler.concatenate_split_results(\n",
    "    special_cell_type_split_results, concat_first_level=True  # type: ignore\n",
    ")\n",
    "assert len(concat_results) == 4\n",
    "\n",
    "for task_name, df in list(concat_results.items()):\n",
    "    concat_results[task_name] = split_results_handler.add_max_pred(df).copy()  # type: ignore\n",
    "    df_w_meta = df.merge(\n",
    "        metadata_v2_df[[ASSAY, CELL_TYPE, \"uuid\", \"EpiRR\"]],\n",
    "        left_index=True,\n",
    "        right_on=\"md5sum\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    df_w_meta.set_index(\"md5sum\", drop=True, inplace=True)\n",
    "    df_w_meta.to_csv(this_fig_dir / f\"{task_name}_merged_10fold_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_name, df in concat_results.items():\n",
    "    df_w_meta = metadata_handler.join_metadata(df, metadata_v2)  # type: ignore\n",
    "    groupby = [ASSAY, \"True class\"]\n",
    "    df_grouped = df_w_meta.groupby(groupby).agg(\"size\").unstack(fill_value=0)\n",
    "    df_grouped.to_csv(this_fig_dir / f\"{task_name}_assay_ct_pair_counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_y = 0\n",
    "fig_name = f\"perf_per_assay_Y_{min_y:.2f}\"\n",
    "\n",
    "for min_predScore in [0, 0.6]:\n",
    "    continue\n",
    "    NN_performance_per_assay_across_categories(\n",
    "        all_split_results=special_cell_type_split_results,  # type: ignore\n",
    "        logdir=this_fig_dir,\n",
    "        name=fig_name,\n",
    "        y_range=[min_y, 1.001],\n",
    "        min_predScore=min_predScore,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_y = 0\n",
    "fig_name = f\"perf_per_assay_Y_{min_y:.2f}_merge-3Tcell-2progenitor\"\n",
    "merging_dict = {\n",
    "    \"T-Cell CD4 Helper\": \"T-Cell\",\n",
    "    \"T-Cell Naive CD4 Helper\": \"T-Cell\",\n",
    "    \"T-Cell Others\": \"T-Cell\",\n",
    "    \"Progenitor Mixed\": \"Stem Cell+ Mixed\",\n",
    "    \"Stem Cell Mixed\": \"Stem Cell+ Mixed\",\n",
    "}\n",
    "\n",
    "special_cell_type_split_results: Dict = special_cell_type_split_results  # type: ignore\n",
    "special_cell_type_split_results = {\n",
    "    name: df for name, df in special_cell_type_split_results.items() if \"PE\" in name\n",
    "}\n",
    "for task_name, split_dfs in special_cell_type_split_results.items():\n",
    "    for split_name, df in split_dfs.items():\n",
    "        df: pd.DataFrame = df.copy()\n",
    "        df[[\"Predicted class\", \"True class\"]] = df[\n",
    "            [\"Predicted class\", \"True class\"]\n",
    "        ].replace(merging_dict)\n",
    "\n",
    "        # merge columns\n",
    "        df.loc[:, \"T-Cell\"] = df.loc[:, df.columns.str.contains(\"T-Cell\")].mean(axis=1)\n",
    "        df.loc[:, \"Stem Cell+ Mixed\"] = df.loc[\n",
    "            :, df.columns.str.contains(r\"Stem Cell Mixed|Progenitor Mixed\", regex=True)\n",
    "        ].mean(axis=1)\n",
    "        colomns_to_drop = df.columns[\n",
    "            df.columns.str.contains(\n",
    "                r\"T-Cell.+|Progenitor Mixed|Stem Cell Mixed\", regex=True\n",
    "            )\n",
    "        ]\n",
    "        df = df.drop(columns=colomns_to_drop)\n",
    "\n",
    "        special_cell_type_split_results[task_name][split_name] = df  # type: ignore\n",
    "\n",
    "for min_predScore in [0, 0.6]:\n",
    "    continue\n",
    "    NN_performance_per_assay_across_categories(\n",
    "        all_split_results=special_cell_type_split_results,  # type: ignore\n",
    "        logdir=this_fig_dir,\n",
    "        name=fig_name,\n",
    "        title_end=\"(merge-3Tcell-2progenitor)\",\n",
    "        y_range=[min_y, 1.001],\n",
    "        min_predScore=min_predScore,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assay & CT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSAY_AND_CT = \"assay_epiclass_harmonized_sample_ontology_intermediate_16ct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "concat_results: Dict[str, pd.DataFrame] = split_results_handler.concatenate_split_results(  # type: ignore\n",
    "    all_split_results, concat_first_level=True\n",
    ")\n",
    "assay_ct_results: pd.DataFrame = concat_results[ASSAY_AND_CT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(\n",
    "    df: pd.DataFrame, name1: str, name2: str, new_name: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge columns starting with \"name1_\" and \"name2_\" in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame\n",
    "    name1 (str): First prefix to match\n",
    "    name2 (str): Second prefix to match\n",
    "    new_name (str): Prefix for the new merged columns\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with merged columns\n",
    "    \"\"\"\n",
    "    # Get all column names\n",
    "    columns = df.columns\n",
    "\n",
    "    # Find columns starting with 'name1_' and 'name2_'\n",
    "    cols1 = [col for col in columns if col.startswith(f\"{name1}_\")]\n",
    "    cols2 = [col for col in columns if col.startswith(f\"{name2}_\")]\n",
    "\n",
    "    # Create a dictionary to store new column names and their corresponding columns to merge\n",
    "    merge_dict = {}\n",
    "\n",
    "    for col1 in cols1:\n",
    "        suffix = col1.split(f\"{name1}_\")[1]\n",
    "        col2 = f\"{name2}_{suffix}\"\n",
    "\n",
    "        if col2 in cols2:\n",
    "            new_col_name = f\"{new_name}_{suffix}\"\n",
    "            merge_dict[new_col_name] = [col1, col2]\n",
    "\n",
    "    # Merge columns\n",
    "    for new_col, cols_to_merge in merge_dict.items():\n",
    "        df[new_col] = df[cols_to_merge].sum(axis=1)\n",
    "\n",
    "    # Drop original columns\n",
    "    df = df.drop(columns=[col for sublist in merge_dict.values() for col in sublist])\n",
    "\n",
    "    # Adjust values in other columns, change name1 and name2 for new_name\n",
    "    for col in [\"True class\", \"Predicted class\"]:\n",
    "        df[col] = df[col].apply(lambda x: x.replace(name1, new_name))\n",
    "        df[col] = df[col].apply(lambda x: x.replace(name2, new_name))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_ct_results = merge_columns(\n",
    "    assay_ct_results, name1=\"wgbs-standard\", name2=\"wgbs-pbat\", new_name=\"wgbs\"\n",
    ")\n",
    "assay_ct_results = merge_columns(\n",
    "    assay_ct_results, name1=\"rna_seq\", name2=\"mrna_seq\", new_name=\"rna\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_ct_results[\"Max pred\"] = assay_ct_results[assay_ct_results.columns[2:]].max(axis=1)\n",
    "classes = sorted(set(assay_ct_results[\"True class\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_pred in [0, 0.6, 0.8]:\n",
    "    continue\n",
    "    df = assay_ct_results[assay_ct_results[\"Max pred\"] > min_pred]\n",
    "    conf_matrix = sk_cm(\n",
    "        df[\"True class\"], df[\"Predicted class\"], normalize=None, labels=classes\n",
    "    )\n",
    "    mat_writer = ConfusionMatrixWriter(labels=classes, confusion_matrix=conf_matrix)\n",
    "    mat_writer.to_all_formats(\n",
    "        logdir, name=f\"{ASSAY_AND_CT}_confusion_matrix_min_pred_{min_pred:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to create a comprehensive confusion matrix report\n",
    "\n",
    "# y_true = assay_ct_results[\"True class\"].values\n",
    "# y_pred = assay_ct_results[\"Predicted class\"].values\n",
    "# classes = sorted(set(y_true) | set(y_pred))\n",
    "# from pycm import ConfusionMatrix\n",
    "# cm = ConfusionMatrix(y_true, y_pred, classes=list(classes))\n",
    "# output_name = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\" / f\"{ASSAY_AND_CT}_confusion_matrix\"\n",
    "# cm.save_html(str(output_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN performance per assay, scatterplot\n",
    "\n",
    "model_X split_n vs model_Y split_n for all n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_performance_scatterplot(\n",
    "    all_split_results: Dict[str, Dict[str, pd.DataFrame]],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    label_category: str,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    For the two given classification tasks split results (need to be from same category),\n",
    "    create a scatter plot of split performance per assay, split for split.\n",
    "\n",
    "    Args:\n",
    "        all_split_results: A dictionary with structure {task_name: {split_name: split_results_df}}.\n",
    "        logdir (Path): The directory path to save the output plots.\n",
    "        name (str): The base name for the output plot files.\n",
    "        label_category (str): category used for labels, used for title and axis labels.\n",
    "        verbose (bool): Print more information.\n",
    "    \"\"\"\n",
    "    all_split_results = copy.deepcopy(all_split_results)\n",
    "    metadata_df = MetadataHandler(paper_dir).load_metadata_df(\n",
    "        \"v2-encode\", merge_assays=True\n",
    "    )\n",
    "\n",
    "    for task_name_1, task_name_2 in itertools.combinations(all_split_results.keys(), 2):\n",
    "        if verbose:\n",
    "            print(task_name_1, task_name_2)\n",
    "        split_results_1 = all_split_results[task_name_1]\n",
    "        split_results_2 = all_split_results[task_name_2]\n",
    "\n",
    "        if ASSAY in task_name_1:\n",
    "            for split_name in split_results_1:\n",
    "                split_results_1[split_name] = merge_similar_assays(\n",
    "                    split_results_1[split_name]\n",
    "                )\n",
    "                split_results_2[split_name] = merge_similar_assays(\n",
    "                    split_results_2[split_name]\n",
    "                )\n",
    "\n",
    "        if split_results_1[\"split0\"].shape != split_results_2[\"split0\"].shape:\n",
    "            raise ValueError(\n",
    "                f\"Split results for {task_name_1} and {task_name_2} do not have the same shape: {split_results_1['split0'].shape} != {split_results_2['split0'].shape}\"\n",
    "            )\n",
    "        assay_acc_df_1 = split_results_handler.compute_acc_per_assay(\n",
    "            split_results_1, metadata_df\n",
    "        )\n",
    "        assay_acc_df_2 = split_results_handler.compute_acc_per_assay(\n",
    "            split_results_2, metadata_df\n",
    "        )\n",
    "\n",
    "        fig = go.Figure()\n",
    "        min_x = 1\n",
    "        min_y = 1\n",
    "        for assay in ASSAY_ORDER:\n",
    "            if verbose:\n",
    "                print(assay)\n",
    "            try:\n",
    "                assay_accuracies_1 = assay_acc_df_1[assay]\n",
    "                assay_accuracies_2 = assay_acc_df_2[assay]\n",
    "            except KeyError as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{task_name_1}: {assay_accuracies_1}\")\n",
    "                print(f\"{task_name_2}: {assay_accuracies_2}\")\n",
    "\n",
    "            hovertext = [\n",
    "                f\"{split}: ({assay_accuracies_1[split]:.4f},{assay_accuracies_2[split]:.4f})\"\n",
    "                for split in assay_accuracies_1.keys()\n",
    "            ]\n",
    "\n",
    "            x_gt_y = sum(assay_accuracies_1 > assay_accuracies_2)\n",
    "            y_gt_x = sum(assay_accuracies_1 < assay_accuracies_2)\n",
    "            trace_name = f\"{assay} ({y_gt_x},{x_gt_y})\"\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=assay_accuracies_1.values,\n",
    "                    y=assay_accuracies_2.values,\n",
    "                    mode=\"markers\",\n",
    "                    name=trace_name,\n",
    "                    marker=dict(size=5, color=assay_colors[assay]),\n",
    "                    text=hovertext,\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "            min_x = min(min_x, *assay_accuracies_1.values)\n",
    "            min_y = min(min_y, *assay_accuracies_2.values)\n",
    "\n",
    "        # diagonal line\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[0, 1],\n",
    "                y=[0, 1],\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        range_x = 1 - min_x\n",
    "        range_y = 1 - min_y\n",
    "        fig.update_xaxes(range=[min_x - 0.01 * range_x, 1 + 0.01 * range_x])\n",
    "        fig.update_yaxes(range=[min_y - 0.01 * range_y, 1 + 0.01 * range_y])\n",
    "\n",
    "        x_name = task_name_1.replace(f\"_{label_category}\", \"\")\n",
    "        y_name = task_name_2.replace(f\"_{label_category}\", \"\")\n",
    "        fig.update_layout(\n",
    "            title_text=f\"Neural network classification - {label_category} - 10-fold cross-validation\",\n",
    "            xaxis_title=f\"{x_name} accuracy\",\n",
    "            yaxis_title=f\"{y_name} accuracy\",\n",
    "        )\n",
    "\n",
    "        fig.update_layout(legend_title_text=\"Assay: (y>x, x>y)\")\n",
    "\n",
    "        # Save figure\n",
    "        this_name = f\"{name}-{label_category}-{x_name}_VS_{y_name}\"\n",
    "        this_name = this_name.replace(ASSAY, \"assay\")\n",
    "        this_name = this_name.replace(CELL_TYPE, \"sample_ontology\")\n",
    "        fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "        fig.write_image(logdir / f\"{this_name}.png\")\n",
    "        fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "        # fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\"track_type\", \"groups\", \"disease\"]\n",
    "exclude_names = [\"chip-seq\", \"7c\", \"no-mixed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 303114\n",
    "N2 = 30321\n",
    "hdf5_type_reg1 = f\"hg38_regulatory_regions_n{N1}\"\n",
    "hdf5_type_reg2 = f\"hg38_regulatory_regions_n{N2}\"\n",
    "hdf5_type_100kb = \"hg38_100kb_all_none\"\n",
    "hdf5_type_10kb = \"hg38_10kb_all_none\"\n",
    "hdf5_type_gene = \"hg38_gene_regions_100kb_coord_n19864\"\n",
    "hdf5_type_1mb = \"hg38_1mb_all_none\"\n",
    "hdf5_type_5mb = \"hg38_5mb_all_none_1mb_coord\"\n",
    "hdf5_type_10mb = \"hg38_10mb_all_none_1mb_coord\"\n",
    "hdf5_type_100kb_n316 = \"hg38_100kb_random_n316_none\"\n",
    "hdf5_type_100kb_n3044 = \"hg38_100kb_random_n3044_none\"\n",
    "\n",
    "scatter_fig_results = {}\n",
    "for hdf5_type in [\n",
    "    hdf5_type_reg1,\n",
    "    hdf5_type_reg2,\n",
    "    hdf5_type_100kb,\n",
    "    hdf5_type_10kb,\n",
    "    hdf5_type_gene,\n",
    "    hdf5_type_1mb,\n",
    "    hdf5_type_5mb,\n",
    "    hdf5_type_10mb,\n",
    "    hdf5_type_100kb_n316,\n",
    "    hdf5_type_100kb_n3044,\n",
    "]:\n",
    "    results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "    if not results_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "\n",
    "    continue\n",
    "    all_split_results = split_results_handler.general_split_metrics(\n",
    "        results_dir,\n",
    "        merge_assays=True,\n",
    "        exclude_categories=exclude_categories,\n",
    "        exclude_names=exclude_names,\n",
    "        return_type=\"split_results\",\n",
    "    )\n",
    "\n",
    "    scatter_fig_results.update(\n",
    "        {\n",
    "            f\"{hdf5_type}_{task_name}\": split_results\n",
    "            for task_name, split_results in all_split_results.items()  # type: ignore\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter_fig_results[\"hg38_regulatory_regions_n30321_harmonized_sample_ontology_intermediate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_category in [ASSAY, CELL_TYPE]:\n",
    "    continue\n",
    "    results = {k: v for k, v in scatter_fig_results.items() if label_category in k}\n",
    "    pairwise_performance_scatterplot(\n",
    "        results,\n",
    "        logdir=base_fig_dir / \"flagship\" / \"pairwise_scatterplot_acc\" / label_category,\n",
    "        name=\"acc_per_assay\",\n",
    "        label_category=label_category,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track type effect on NN performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "assay_parent_dir = parent_dir / \"assay_epiclass_1l_3000n\" / \"11c\"\n",
    "ct_parent_dir = parent_dir / \"harmonized_sample_ontology_intermediate_1l_3000n\"\n",
    "\n",
    "assay_results = {\n",
    "    folder.name: split_results_handler.read_split_results(folder)\n",
    "    for folder in assay_parent_dir.iterdir()\n",
    "    if \"chip\" not in folder.name\n",
    "}\n",
    "ct_results = {\n",
    "    folder.name: split_results_handler.read_split_results(folder)\n",
    "    for folder in ct_parent_dir.iterdir()\n",
    "    if \"l1\" not in folder.name\n",
    "}\n",
    "\n",
    "_ = assay_results.pop(\"10fold-oversampling\")\n",
    "_ = ct_results.pop(\"10fold-oversampling\")\n",
    "_ = ct_results.pop(\"10fold-oversampling_chip-seq-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected_assay_results = copy.deepcopy(assay_results)\n",
    "# for task_name, split_dfs in list(corrected_assay_results.items()):\n",
    "#     for split_name in split_dfs:\n",
    "#         split_dfs[split_name] = merge_similar_assays(split_dfs[split_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assay_metrics = split_results_handler.compute_split_metrics(\n",
    "#     corrected_assay_results, concat_first_level=True\n",
    "# )\n",
    "# ct_metrics = split_results_handler.compute_split_metrics(\n",
    "#     ct_results, concat_first_level=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\"\n",
    "name = f\"{ASSAY}_global_track_type_effect\"\n",
    "# fig2_a(\n",
    "#     assay_metrics,\n",
    "#     logdir,\n",
    "#     name,\n",
    "#     exclude_categories=None,\n",
    "#     y_range=[0.99, 1.0001],\n",
    "#     sort_by_acc=False,\n",
    "# )\n",
    "\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\"\n",
    "name = f\"{CELL_TYPE}_global_track_type_effect\"\n",
    "# fig2_a(\n",
    "#     ct_metrics,\n",
    "#     logdir,\n",
    "#     name,\n",
    "#     exclude_categories=None,\n",
    "#     y_range=[0.91, 1.001],\n",
    "#     sort_by_acc=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"{ASSAY}_global_track_type_effect_per_assay\"\n",
    "# NN_performance_per_assay_across_categories(\n",
    "#     corrected_assay_results, logdir, name, exclude_categories=None, y_range=[0.96, 1.001]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_keep_core_assays(\n",
    "    results_dfs: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Exclude non core-assays from split results. Also exclude input.\"\"\"\n",
    "    accepted_assays = ASSAY_ORDER[0:-3]\n",
    "    new_results = copy.deepcopy(results_dfs)\n",
    "    for task_name, split_dfs in list(new_results.items()):\n",
    "        for split_name in split_dfs:\n",
    "            df = split_dfs[split_name]\n",
    "            if ASSAY not in df.columns:\n",
    "                merged_df = df.merge(\n",
    "                    metadata_v2_df, how=\"left\", left_index=True, right_index=True\n",
    "                )\n",
    "                df = df[merged_df[ASSAY].isin(accepted_assays)]\n",
    "                new_results[task_name][split_name] = df  # type: ignore\n",
    "    return new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recompute metrics considering only histones\n",
    "# for result_df, category_name, y_range in zip(\n",
    "#     [corrected_assay_results, ct_results],\n",
    "#     [ASSAY, CELL_TYPE],\n",
    "#     [[0.85, 1.001], [0.91, 1.001]],\n",
    "# ):\n",
    "#     print(category_name)\n",
    "#     name = f\"{category_name}_core6c_track_type_effect\"\n",
    "\n",
    "#     core_result_df = only_keep_core_assays(result_df)\n",
    "#     metrics = split_results_handler.compute_split_metrics(\n",
    "#         core_result_df, concat_first_level=True\n",
    "#     )\n",
    "\n",
    "#     fig2_a(metrics, logdir, name, exclude_categories=None, y_range=y_range)\n",
    "\n",
    "#     if category_name == ASSAY:\n",
    "#         name = f\"{ASSAY}_core6_track_type_effect_per_assay\"\n",
    "#         NN_performance_per_assay_across_categories(\n",
    "#             core_result_df, logdir, name, exclude_categories=None, y_range=[0.97, 1.001]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex chrY z-score distribution vs predictions\n",
    "\n",
    "Violin plot of average z-score on chrY per sex, black dots for pred same class and red for pred different class.  \n",
    "\n",
    "- Do the split male female violin per assay (only FC, merge 2xwgbs and 2xrna, no rna unique_raw). \n",
    "- Use scatter for points on each side, agree same color as violin, disagree other.\n",
    "- Point labels: uuid, epirr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute chrY coverage z-score VS assay distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_chrY_zscore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, epirr_mislabels = create_mislabel_corrector(paper_dir)\n",
    "sex_mislabels = epirr_mislabels[SEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chrY_zscores(version: str):\n",
    "    \"\"\"Compute z-scores for chrY coverage data, per assay distribution.\n",
    "\n",
    "    Excludes raw, pval, and Unique_raw tracks.\n",
    "    \"\"\"\n",
    "    # Get chrY coverage data\n",
    "    chrY_coverage_dir = base_data_dir / \"chrY_coverage\"\n",
    "    if not chrY_coverage_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {chrY_coverage_dir} does not exist.\")\n",
    "    chrY_coverage_df = pd.read_csv(chrY_coverage_dir / \"chrXY_coverage_all.csv\", header=0)\n",
    "\n",
    "    # Filter out md5s not in metadata version\n",
    "    metadata = MetadataHandler(paper_dir).load_metadata(version)\n",
    "    md5s = set(metadata.md5s)\n",
    "    chrY_coverage_df = chrY_coverage_df[chrY_coverage_df[\"filename\"].isin(md5s)]\n",
    "\n",
    "    # Make sure all values are non-zero\n",
    "    assert (chrY_coverage_df[\"chrY\"] != 0).all()\n",
    "\n",
    "    # These tracks are excluded from z-score computation\n",
    "    metadata.remove_category_subsets(\"track_type\", [\"raw\", \"pval\", \"Unique_raw\"])\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n",
    "    metadata_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # Merge with metadata\n",
    "    chrY_coverage_df = chrY_coverage_df.merge(\n",
    "        metadata_df[[\"md5sum\", ASSAY]], left_on=\"filename\", right_on=\"md5sum\"\n",
    "    )\n",
    "\n",
    "    # Compute z-score per assay\n",
    "    chrY_dists = chrY_coverage_df.groupby(ASSAY).agg({\"chrY\": [\"mean\", \"std\", \"count\"]})\n",
    "\n",
    "    output_dir = chrY_coverage_dir / f\"dfreeze_{version}_stats\"\n",
    "    output_dir.mkdir(parents=False, exist_ok=True)\n",
    "    chrY_dists.to_csv(output_dir / \"chrY_coverage_stats_assay_w_track_exclusion.csv\")\n",
    "\n",
    "    # Compute z-score per assay group, merge back into the dataframe, save results\n",
    "    metric_name = \"chrY_zscore_vs_assay\"\n",
    "    groupby_df = chrY_coverage_df.groupby(ASSAY)\n",
    "    for _, group in groupby_df:\n",
    "        group[\"chrY_zscore\"] = zscore(group[\"chrY\"])\n",
    "        chrY_coverage_df.loc[group.index, metric_name] = group[\"chrY_zscore\"]\n",
    "\n",
    "    output_cols = [\"filename\", \"chrY\", metric_name, ASSAY]\n",
    "    chrY_coverage_df[output_cols].to_csv(\n",
    "        output_dir / \"chrY_coverage_zscore_vs_assay.csv\", index=False\n",
    "    )\n",
    "\n",
    "\n",
    "# compute_chrY_zscores(\"v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot z-scores according to sex\n",
    "\n",
    "main Fig: chrY per EpiRR (excluding WGBS): only boxplot with all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_label = \"chrY_zscore_vs_assay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fig_2B_data(version: str, prediction_data_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Prepare data for figure 2b.\"\"\"\n",
    "    # Load metadata\n",
    "    metadata = MetadataHandler(paper_dir).load_metadata(version)\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n",
    "\n",
    "    # Load z-score data\n",
    "    zscore_dir = base_data_dir / \"chrY_coverage\" / f\"dfreeze_{version}_stats\"\n",
    "    zscore_df = pd.read_csv(zscore_dir / \"chrY_coverage_zscore_vs_assay.csv\", header=0)\n",
    "\n",
    "    # Load NN predictions\n",
    "    split_results = split_results_handler.read_split_results(prediction_data_dir)\n",
    "    pred_df = split_results_handler.concatenate_split_results(\n",
    "        {\"sex\": split_results}, concat_first_level=True\n",
    "    )[\"sex\"]\n",
    "\n",
    "    # Merge all\n",
    "    zscore_df = zscore_df.merge(\n",
    "        metadata_df, left_on=\"filename\", right_on=\"md5sum\", suffixes=(\"\", \"_DROP\")\n",
    "    )\n",
    "    zscore_df = zscore_df.merge(\n",
    "        pred_df, left_on=\"filename\", right_index=True, suffixes=(\"\", \"_DROP\")\n",
    "    )\n",
    "    zscore_df.drop(\n",
    "        columns=[col for col in zscore_df.columns if col.endswith(\"_DROP\")], inplace=True\n",
    "    )\n",
    "\n",
    "    zscore_df[\"Max pred\"] = zscore_df[[\"female\", \"male\", \"mixed\"]].max(axis=1)\n",
    "    try:\n",
    "        zscore_df.set_index(\"filename\", inplace=True)\n",
    "    except ValueError:\n",
    "        zscore_df.set_index(\"md5sum\", inplace=True)\n",
    "    return zscore_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sex_zscore_all_tracks(sex_mislabels: Dict[str, str]) -> pd.DataFrame:\n",
    "    \"\"\"Prepare data for complete chrY coverage supp. figure\"\"\"\n",
    "    version = \"v2\"\n",
    "    metadata_df = MetadataHandler(paper_dir).load_metadata_df(version, merge_assays=False)\n",
    "\n",
    "    # Load z-score data\n",
    "    zscore_dir = base_data_dir / \"chrY_coverage\" / f\"dfreeze_{version}_stats\"\n",
    "    zscore_df = pd.read_csv(zscore_dir / \"chrY_coverage_zscores.csv\")\n",
    "\n",
    "    # Merge zscore/metadata\n",
    "    zscore_df = zscore_df.merge(\n",
    "        metadata_df,\n",
    "        left_on=\"filename\",\n",
    "        right_index=True,\n",
    "        how=\"right\",\n",
    "        suffixes=(\"\", \"_DROP\"),\n",
    "    )\n",
    "    zscore_df.drop(\n",
    "        columns=[col for col in zscore_df.columns if col.endswith(\"_DROP\")], inplace=True\n",
    "    )\n",
    "\n",
    "    metadata_df[SEX] = [\n",
    "        sex if epirr not in sex_mislabels else sex_mislabels[epirr]\n",
    "        for (sex, epirr) in metadata_df[[SEX, \"epirr_id_without_version\"]].values\n",
    "    ]\n",
    "    return zscore_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_zscore_assay_tracks(zscore_df: pd.DataFrame, logdir: Path, name: str) -> None:\n",
    "    \"\"\"Create supp fig.\n",
    "\n",
    "    Args:\n",
    "        zscore_df: The dataframe with z-score data.\n",
    "    \"\"\"\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "    pair_counts = zscore_df[[ASSAY, \"track_type\"]].value_counts()\n",
    "    pair_counts.sort_index(inplace=True)\n",
    "    pair_counts = pair_counts.to_dict()\n",
    "\n",
    "    j = 0\n",
    "    subplot_positions = {}\n",
    "    for i, (assay, track_type) in enumerate(pair_counts):\n",
    "        if \"h3\" in assay:\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            subplot_positions[assay, track_type] = (row + 1, col + 1)\n",
    "        elif \"input\" in assay:\n",
    "            subplot_positions[assay, track_type] = (8, 3)\n",
    "        elif \"wgb\" in assay:\n",
    "            col = (j % 2) + 1\n",
    "            subplot_positions[assay, track_type] = (8, col)\n",
    "            j += 1\n",
    "        elif \"rna\" in assay:\n",
    "            col = (j % 3) + 1\n",
    "            subplot_positions[assay, track_type] = (7, col)\n",
    "            j += 1\n",
    "\n",
    "    reverse_subplot_positions = {v: k for k, v in subplot_positions.items()}\n",
    "\n",
    "    subplot_titles = []\n",
    "    for i, j in itertools.product(range(8), range(3)):\n",
    "        assay, track_type = reverse_subplot_positions[(i + 1, j + 1)]\n",
    "        size = pair_counts[assay, track_type]\n",
    "        subplot_titles.append(f\"{assay}, {track_type}<br>(n={size})\")\n",
    "\n",
    "    x_title = \"(Assay, Track type, Sex) z-score distributions\"\n",
    "    fig = make_subplots(\n",
    "        cols=3,\n",
    "        rows=8,\n",
    "        shared_yaxes=True,\n",
    "        x_title=x_title,\n",
    "        y_title=\"z-score\",\n",
    "        subplot_titles=subplot_titles,\n",
    "        horizontal_spacing=0.01,\n",
    "        vertical_spacing=0.0275,\n",
    "    )\n",
    "\n",
    "    metric_label = \"chrY_zscore_vs_assay_track\"\n",
    "    for i, (assay, track_type) in enumerate(pair_counts):\n",
    "        sub_df = zscore_df[\n",
    "            (zscore_df[ASSAY] == assay) & (zscore_df[\"track_type\"] == track_type)\n",
    "        ].copy()\n",
    "\n",
    "        sub_df.reset_index(inplace=True, drop=False)\n",
    "        y_values = sub_df[metric_label]\n",
    "\n",
    "        hovertext = [\n",
    "            f\"({epirr}, {md5sum}): z-score={z_score:.3f}\"\n",
    "            for epirr, md5sum, z_score in zip(\n",
    "                sub_df[\"EpiRR\"],\n",
    "                sub_df[\"filename\"],\n",
    "                sub_df[metric_label],\n",
    "            )\n",
    "        ]\n",
    "        hovertext = np.array(hovertext)\n",
    "\n",
    "        female_idx = np.argwhere((sub_df[SEX] == \"female\").values).flatten()\n",
    "        male_idx = np.argwhere((sub_df[SEX] == \"male\").values).flatten()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=f\"{assay} - {track_type} - female\",\n",
    "                y=y_values.loc[female_idx],\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[female_idx],\n",
    "                marker=dict(\n",
    "                    size=2,\n",
    "                    color=sex_colors[\"female\"],\n",
    "                    line=dict(width=0.5, color=\"black\"),\n",
    "                ),\n",
    "                pointpos=-1.4,\n",
    "                fillcolor=sex_colors[\"female\"],\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                showlegend=False,\n",
    "                legendgroup=\"Female\",\n",
    "            ),\n",
    "            row=subplot_positions[assay, track_type][0],\n",
    "            col=subplot_positions[assay, track_type][1],\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=f\"{assay} - {track_type} - male\",\n",
    "                y=y_values[male_idx],\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[male_idx],\n",
    "                marker=dict(\n",
    "                    size=2, color=sex_colors[\"male\"], line=dict(width=0.5, color=\"black\")\n",
    "                ),\n",
    "                pointpos=-1.4,\n",
    "                fillcolor=sex_colors[\"male\"],\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                showlegend=False,\n",
    "                legendgroup=\"Male\",\n",
    "            ),\n",
    "            row=subplot_positions[assay, track_type][0],\n",
    "            col=subplot_positions[assay, track_type][1],\n",
    "        )\n",
    "\n",
    "    # Add a dummy scatter plot for legend\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Female\",\n",
    "            marker=dict(color=sex_colors[\"female\"], size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Female\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Male\",\n",
    "            marker=dict(color=sex_colors[\"male\"], size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Male\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(range=[-1.5, 3])\n",
    "    title = \"z-score(mean chrY coverage per file) distribution per (assay, track type)\"\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        width=1000,\n",
    "        height=1500,\n",
    "        boxgap=0.05,\n",
    "    )\n",
    "    fig.update_annotations(font_size=12)\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_df = prepare_sex_zscore_all_tracks(sex_mislabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_zscore_assay_tracks(zscore_df, logdir, \"zscore_assay_tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_B(zscore_df: pd.DataFrame, logdir: Path, name: str) -> None:\n",
    "    \"\"\"Create figure 2B.\n",
    "\n",
    "    Args:\n",
    "        zscore_df: The dataframe with z-score data.\n",
    "    \"\"\"\n",
    "    assay_sizes = zscore_df[ASSAY].value_counts()\n",
    "    assays = sorted(assay_sizes.index)\n",
    "\n",
    "    # x_title = \"Assay+Sex z-score distributions - Male/Female classification disagreement separate\"\n",
    "    x_title = \"Assay+Sex z-score distributions\"\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(assays),\n",
    "        shared_yaxes=True,\n",
    "        x_title=x_title,\n",
    "        y_title=\"z-score\",\n",
    "        horizontal_spacing=0.02,\n",
    "        subplot_titles=[\n",
    "            f\"{assay_label} ({assay_sizes[assay_label]})\" for assay_label in assays\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    for i, assay_label in enumerate(sorted(assays)):\n",
    "        sub_df = zscore_df[zscore_df[ASSAY] == assay_label]\n",
    "\n",
    "        y_values = sub_df[metric_label]\n",
    "        hovertext = [\n",
    "            f\"{epirr}: z-score={z_score:.3f}, pred={pred:.3f}\"\n",
    "            for epirr, pred, z_score in zip(\n",
    "                sub_df[\"EpiRR\"],\n",
    "                sub_df[\"Max pred\"],\n",
    "                sub_df[metric_label],\n",
    "            )\n",
    "        ]\n",
    "        hovertext = np.array(hovertext)\n",
    "\n",
    "        female_idx = np.argwhere((sub_df[\"True class\"] == \"female\").values).flatten()\n",
    "        male_idx = np.argwhere((sub_df[\"True class\"] == \"male\").values).flatten()\n",
    "\n",
    "        # predicted_as_female_idx = np.argwhere(\n",
    "        #     (\n",
    "        #         (sub_df[\"Predicted class\"] == \"female\") & (sub_df[\"True class\"] == \"male\")\n",
    "        #     ).values\n",
    "        # ).flatten()\n",
    "        # predicted_as_male_idx = np.argwhere(\n",
    "        #     (\n",
    "        #         (sub_df[\"Predicted class\"] == \"male\") & (sub_df[\"True class\"] == \"female\")\n",
    "        #     ).values\n",
    "        # ).flatten()\n",
    "\n",
    "        # fig.add_trace(\n",
    "        #     go.Violin(\n",
    "        #         name=\"\",\n",
    "        #         x0=i,\n",
    "        #         y=y_values[female_idx],\n",
    "        #         box_visible=True,\n",
    "        #         meanline_visible=True,\n",
    "        #         points=\"all\",\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[female_idx],\n",
    "        #         side=\"negative\",\n",
    "        #         line_color=sex_colors[\"male\"],\n",
    "        #         spanmode=\"hard\",\n",
    "        #         showlegend=False,\n",
    "        #         marker=dict(size=1),\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "        # fig.add_trace(\n",
    "        #     go.Violin(\n",
    "        #         name=\"\",\n",
    "        #         x0=i,\n",
    "        #         y=y_values[male_idx],\n",
    "        #         box_visible=True,\n",
    "        #         meanline_visible=True,\n",
    "        #         points=\"all\",\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[male_idx],\n",
    "        #         side=\"positive\",\n",
    "        #         line_color=sex_colors[\"male\"],\n",
    "        #         spanmode=\"hard\",\n",
    "        #         showlegend=False,\n",
    "        #         marker=dict(size=1),\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=assay_label,\n",
    "                y=y_values[female_idx],\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[female_idx],\n",
    "                marker=dict(\n",
    "                    size=2,\n",
    "                    color=sex_colors[\"female\"],\n",
    "                    line=dict(width=0.5, color=\"black\"),\n",
    "                ),\n",
    "                fillcolor=sex_colors[\"female\"],\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                showlegend=False,\n",
    "                legendgroup=\"Female\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=assay_label,\n",
    "                y=y_values[male_idx],\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[male_idx],\n",
    "                marker=dict(\n",
    "                    size=2, color=sex_colors[\"male\"], line=dict(width=0.5, color=\"black\")\n",
    "                ),\n",
    "                fillcolor=sex_colors[\"male\"],\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                showlegend=False,\n",
    "                legendgroup=\"Male\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "        # temp_y_values = y_values[predicted_as_female_idx]\n",
    "        # temp_size = 1 + 5 * sub_df[\"Max pred\"].values[predicted_as_female_idx]\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         name=\"\",\n",
    "        #         x=[i - 0.2] * len(temp_y_values),\n",
    "        #         y=temp_y_values,\n",
    "        #         mode=\"markers\",\n",
    "        #         marker=dict(color=sex_colors[\"female\"], size=temp_size),\n",
    "        #         showlegend=False,\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[predicted_as_female_idx],\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "        # temp_y_values = y_values[predicted_as_male_idx]\n",
    "        # temp_size = 1 + 5 * sub_df[\"Max pred\"].values[predicted_as_male_idx]\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         name=\"\",\n",
    "        #         x=[i - 0.25] * len(temp_y_values),\n",
    "        #         y=temp_y_values,\n",
    "        #         mode=\"markers\",\n",
    "        #         marker=dict(color=sex_colors[\"male\"], size=temp_size),\n",
    "        #         showlegend=False,\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[predicted_as_male_idx],\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "    # Add a dummy scatter plot for legend\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Female\",\n",
    "            marker=dict(color=sex_colors[\"female\"], size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Female\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Male\",\n",
    "            marker=dict(color=sex_colors[\"male\"], size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Male\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(range=[-1.5, 3])\n",
    "    title = \"z-score(mean chrY coverage per file) distribution per assay\"\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        width=3000,\n",
    "        height=1000,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_chrY_zscore\"\n",
    "logdir.mkdir(parents=False, exist_ok=True)\n",
    "name = \"fig2--sex_chrY_zscore_only_box\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v2\"\n",
    "pred_data_dir = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / f\"dfreeze_{version}\"\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / f\"{SEX}_1l_3000n\"\n",
    "    / \"w-mixed\"\n",
    "    / \"10fold-oversampling\"\n",
    ")\n",
    "if not pred_data_dir:\n",
    "    raise FileNotFoundError(f\"Directory {pred_data_dir} does not exist.\")\n",
    "zscore_df = prepare_fig_2B_data(version, pred_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No pval/raw\n",
    "display(zscore_df[\"track_type\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2_B(zscore_df, logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot z-score according to sex, merge assays except wgbs (1 violin plot, 1 point = 1 epirr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_B_merged_assays(\n",
    "    zscore_df: pd.DataFrame,\n",
    "    sex_mislabels: Dict[str, str],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    min_pred: float | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Create figure 2B.\n",
    "\n",
    "    Args:\n",
    "        zscore_df (pd.DataFrame): The dataframe with z-score data.\n",
    "        sex_mislabels (Dict[str, str]): {EpiRR_no-v: corrected_sex_label}\n",
    "        logdir (Path): The directory path to save the output plots.\n",
    "        name (str): The base name for the output plot files.\n",
    "        min_pred (float|None): Minimum prediction value to include in the plot. Used on average EpiRR 'Max pred' values.\n",
    "    \"\"\"\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "    zscore_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # wgbs reverses male/female chrY tendency, so removed here\n",
    "    zscore_df = zscore_df[zscore_df[ASSAY] != \"wgbs\"]  # type: ignore\n",
    "\n",
    "    # Average chrY z-score values\n",
    "    mean_chrY_values_df = zscore_df.groupby([\"EpiRR\", SEX]).agg(\n",
    "        {metric_label: \"mean\", \"Max pred\": \"mean\"}\n",
    "    )\n",
    "    mean_chrY_values_df.reset_index(inplace=True)\n",
    "    if not mean_chrY_values_df[\"EpiRR\"].is_unique:\n",
    "        raise ValueError(\"EpiRR is not unique.\")\n",
    "\n",
    "    # Filter out low prediction values\n",
    "    if min_pred is not None:\n",
    "        mean_chrY_values_df = mean_chrY_values_df[\n",
    "            mean_chrY_values_df[\"Max pred\"] > min_pred\n",
    "        ]\n",
    "\n",
    "    mean_chrY_values_df.reset_index(drop=True, inplace=True)\n",
    "    chrY_values = mean_chrY_values_df[metric_label]\n",
    "    female_idx = np.argwhere((mean_chrY_values_df[SEX] == \"female\").values).flatten()  # type: ignore\n",
    "    male_idx = np.argwhere((mean_chrY_values_df[SEX] == \"male\").values).flatten()  # type: ignore\n",
    "\n",
    "    # Mislabels\n",
    "    binary_mislabels = set(\n",
    "        epirr_no_v\n",
    "        for epirr_no_v, label in sex_mislabels.items()\n",
    "        if label in [\"male\", \"female\"]\n",
    "    )\n",
    "    epirr_no_v = mean_chrY_values_df[\"EpiRR\"].str.extract(pat=r\"(\\w+\\d+).\\d+\")[0]\n",
    "    mislabels_idx = np.argwhere(\n",
    "        epirr_no_v.isin(binary_mislabels).values  # type: ignore\n",
    "    ).flatten()\n",
    "\n",
    "    mislabel_color_dict = {\"female\": sex_colors[\"male\"], \"male\": sex_colors[\"female\"]}\n",
    "    mislabel_colors = [mislabel_color_dict[mean_chrY_values_df[SEX][i]] for i in mislabels_idx]  # type: ignore\n",
    "\n",
    "    # Hovertext\n",
    "    hovertext = [\n",
    "        f\"{epirr}: <z-score>={z_score:.3f}\"\n",
    "        for epirr, z_score in zip(\n",
    "            mean_chrY_values_df[\"EpiRR\"],\n",
    "            mean_chrY_values_df[metric_label],\n",
    "        )\n",
    "    ]\n",
    "    hovertext = np.array(hovertext)\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            name=\"Female\",\n",
    "            y=chrY_values[female_idx],  # type: ignore\n",
    "            boxmean=True,\n",
    "            boxpoints=\"all\",\n",
    "            pointpos=0,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[female_idx],\n",
    "            marker=dict(size=1, color=\"black\"),\n",
    "            line=dict(width=1, color=\"black\"),\n",
    "            fillcolor=sex_colors[\"female\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            name=\"Male\",\n",
    "            y=chrY_values[male_idx],  # type: ignore\n",
    "            boxmean=True,\n",
    "            boxpoints=\"all\",\n",
    "            pointpos=0,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[male_idx],\n",
    "            marker=dict(size=1, color=\"black\"),\n",
    "            line=dict(width=1, color=\"black\"),\n",
    "            fillcolor=sex_colors[\"male\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Mislabel\",\n",
    "            x=np.zeros(len(mislabels_idx)),\n",
    "            y=chrY_values[mislabels_idx],  # type: ignore\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=4, color=mislabel_colors, line=dict(width=1, color=\"black\")),\n",
    "            showlegend=False,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[mislabels_idx],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(range=[-1.5, 3])\n",
    "    title = \"z-score(mean chrY coverage per file) distribution - z-scores averaged over assays\"\n",
    "    if min_pred is not None:\n",
    "        title += f\"<br>avg_maxPred>{min_pred}\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(text=title, x=0.5),\n",
    "        xaxis_title=SEX,\n",
    "        yaxis_title=\"Average z-score\",\n",
    "        width=750,\n",
    "        height=750,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    this_name = f\"{name}_n{mean_chrY_values_df.shape[0]}\"\n",
    "    fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "    fig.write_image(logdir / f\"{this_name}.png\")\n",
    "    fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pred = None\n",
    "name = \"fig2--sex_chrY_zscore_merged_assays\"\n",
    "if min_pred is not None:\n",
    "    name = f\"fig2--sex_chrY_zscore_merged_assays_avg_maxPred>{min_pred}\"\n",
    "\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_chrY_zscore\"\n",
    "# fig2_B_merged_assays(zscore_df, sex_mislabels, logdir, name, min_pred=min_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Male/Female cluster separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_assays_separation_distance(\n",
    "    zscore_df: pd.DataFrame, logdir: Path, name: str\n",
    ") -> None:\n",
    "    \"\"\"Complement to figure 2B, showing separation distance (mean, median)\n",
    "    between male/female zscore clusters, for ChIP-seq (core7). Grouped by EpiRR.\n",
    "\n",
    "    Args:\n",
    "        zscore_df (pd.DataFrame): The dataframe with z-score data.\n",
    "        logdir (Path): The directory path to save the output plots.\n",
    "        name (str): The base name for the output plot files.\n",
    "    \"\"\"\n",
    "    # --- Preprocessing ---\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "    zscore_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    zscore_df = zscore_df[zscore_df[ASSAY].isin(CORE7_ASSAYS)]  # type: ignore\n",
    "\n",
    "    # Average chrY z-score values\n",
    "    mean_chrY_values_df = zscore_df.groupby([\"EpiRR\", SEX]).agg(\n",
    "        {metric_label: \"mean\", \"Max pred\": \"mean\"}\n",
    "    )\n",
    "    mean_chrY_values_df.reset_index(inplace=True)\n",
    "    if not mean_chrY_values_df[\"EpiRR\"].is_unique:\n",
    "        raise ValueError(\"EpiRR is not unique.\")\n",
    "\n",
    "    mean_chrY_values_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    distances = {\"mean\": [], \"median\": []}\n",
    "    min_preds = np.arange(0, 1.0, 0.01)\n",
    "    sample_count = []\n",
    "    for min_pred in min_preds:\n",
    "        subset_chrY_values_df = mean_chrY_values_df[\n",
    "            mean_chrY_values_df[\"Max pred\"] > min_pred\n",
    "        ]\n",
    "        sample_count.append(subset_chrY_values_df.shape[0])\n",
    "\n",
    "        # Compute separation distances\n",
    "        chrY_vals_female = subset_chrY_values_df[subset_chrY_values_df[SEX] == \"female\"][\n",
    "            metric_label\n",
    "        ]\n",
    "        chrY_vals_male = subset_chrY_values_df[subset_chrY_values_df[SEX] == \"male\"][\n",
    "            metric_label\n",
    "        ]\n",
    "\n",
    "        if not chrY_vals_female.empty and not chrY_vals_male.empty:\n",
    "            mean_distance = np.abs(chrY_vals_female.mean() - chrY_vals_male.mean())\n",
    "            median_distance = np.abs(chrY_vals_female.median() - chrY_vals_male.median())\n",
    "\n",
    "            distances[\"mean\"].append(mean_distance)\n",
    "            distances[\"median\"].append(median_distance)\n",
    "        else:\n",
    "            distances[\"mean\"].append(np.nan)\n",
    "            distances[\"median\"].append(np.nan)\n",
    "\n",
    "    # Plotting the results\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for mean and median distances\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=min_preds,\n",
    "            y=distances[\"mean\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Mean Distance (left)\",\n",
    "            line=dict(color=\"blue\"),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=min_preds,\n",
    "            y=distances[\"median\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Median Distance (left)\",\n",
    "            line=dict(color=\"green\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add trace for number of files\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=min_preds,\n",
    "            y=np.array(sample_count) / max(sample_count),\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Proportion of samples (right)\",\n",
    "            line=dict(color=\"red\"),\n",
    "            yaxis=\"y2\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(range=[0.499, 1.0])\n",
    "\n",
    "    # Update layout for secondary y-axis\n",
    "    fig.update_layout(\n",
    "        title=\"Separation Distance of chrY z-scores male/female clusters - ChIP-Seq\",\n",
    "        xaxis_title=\"Average Prediction Score minimum threshold\",\n",
    "        yaxis_title=\"Z-score Distance\",\n",
    "        yaxis2=dict(title=\"Proportion of samples\", overlaying=\"y\", side=\"right\"),\n",
    "        yaxis2_range=[0, 1.001],\n",
    "        legend=dict(\n",
    "            x=1.08,\n",
    "        ),\n",
    "    )\n",
    "    # Save the plot\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separation_distance_per_assay(\n",
    "    zscore_df: pd.DataFrame, logdir: Path, name_prefix: str\n",
    ") -> None:\n",
    "    \"\"\"Generates separation distance plots (mean, median) between male/female\n",
    "    zscore clusters for each assay individually.\n",
    "\n",
    "    Args:\n",
    "        zscore_df (pd.DataFrame): The dataframe with z-score data, including\n",
    "                                   columns for Assay, Sex, the metric (e.g.,\n",
    "                                   'chrY_Zscore'), and 'Max pred'.\n",
    "        logdir (Path): The directory path to save the output plots.\n",
    "        name_prefix (str): The base prefix for the output plot filenames.\n",
    "                           Assay name will be appended.\n",
    "    \"\"\"\n",
    "    # --- Preprocessing ---\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "    zscore_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    unique_assays = zscore_df[ASSAY].unique()\n",
    "\n",
    "    print(f\"Processing assays: {', '.join(unique_assays)}\")\n",
    "\n",
    "    # Define prediction score thresholds\n",
    "    min_preds = np.arange(0, 1.0, 0.01)\n",
    "\n",
    "    # --- Loop through each assay ---\n",
    "    for assay_name in unique_assays:\n",
    "        assay_df = zscore_df[zscore_df[ASSAY] == assay_name]\n",
    "\n",
    "        sex_counts = assay_df[SEX].value_counts()\n",
    "        if (\n",
    "            \"female\" not in sex_counts\n",
    "            or \"male\" not in sex_counts\n",
    "            or sex_counts[\"female\"] == 0\n",
    "            or sex_counts[\"male\"] == 0\n",
    "        ):\n",
    "            print(f\"Skipping assay '{assay_name}': Missing data for one or both sexes.\")\n",
    "            continue\n",
    "\n",
    "        # --- Calculate distances for the current assay ---\n",
    "        distances = {\"mean\": [], \"median\": []}\n",
    "        sample_count = []\n",
    "\n",
    "        for min_pred in min_preds:\n",
    "            # Subset data based on prediction threshold for the current assay\n",
    "            subset_assay_df = assay_df[assay_df[\"Max pred\"] > min_pred]\n",
    "            sample_count.append(subset_assay_df.shape[0])\n",
    "\n",
    "            # Compute separation distances within the subset\n",
    "            chrY_vals_female = subset_assay_df[subset_assay_df[SEX] == \"female\"][\n",
    "                metric_label\n",
    "            ]\n",
    "            chrY_vals_male = subset_assay_df[subset_assay_df[SEX] == \"male\"][metric_label]\n",
    "\n",
    "            # Ensure both male and female data exist at this threshold\n",
    "            if not chrY_vals_female.empty and not chrY_vals_male.empty:\n",
    "                mean_distance = np.abs(chrY_vals_female.mean() - chrY_vals_male.mean())\n",
    "                median_distance = np.abs(\n",
    "                    chrY_vals_female.median() - chrY_vals_male.median()\n",
    "                )\n",
    "\n",
    "                distances[\"mean\"].append(mean_distance)\n",
    "                distances[\"median\"].append(median_distance)\n",
    "            else:\n",
    "                # Append NaN if one group is empty (cannot calculate distance)\n",
    "                distances[\"mean\"].append(np.nan)\n",
    "                distances[\"median\"].append(np.nan)\n",
    "\n",
    "        # --- Plotting the results for the current assay ---\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add traces for mean and median distances\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=min_preds,\n",
    "                y=distances[\"mean\"],\n",
    "                mode=\"lines+markers\",\n",
    "                name=\"Mean Distance (left)\",\n",
    "                line=dict(color=\"blue\"),\n",
    "                hovertemplate=\"Threshold: %{x:.2f}<br>Mean Distance: %{y:.3f}<extra></extra>\",  # Improved hover\n",
    "            )\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=min_preds,\n",
    "                y=distances[\"median\"],\n",
    "                mode=\"lines+markers\",\n",
    "                name=\"Median Distance (left)\",\n",
    "                line=dict(color=\"green\"),\n",
    "                hovertemplate=\"Threshold: %{x:.2f}<br>Median Distance: %{y:.3f}<extra></extra>\",  # Improved hover\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add trace for number of files (samples)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=min_preds,\n",
    "                y=np.array(sample_count) / max(sample_count),\n",
    "                mode=\"lines+markers\",\n",
    "                name=\"Number of Samples (right)\",\n",
    "                line=dict(color=\"red\"),\n",
    "                yaxis=\"y2\",\n",
    "                hovertemplate=\"Threshold: %{x:.2f}<br>Samples: %{y}<extra></extra>\",  # Improved hover\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Update layout for secondary y-axis and add assay name to title\n",
    "        plot_title = (\n",
    "            f\"Separation Distance ({assay_name}) - chrY z-scores Male/Female Clusters\"\n",
    "        )\n",
    "        fig.update_layout(\n",
    "            title=plot_title,\n",
    "            xaxis_title=\"Prediction Score minimum threshold ('Max pred')\",\n",
    "            yaxis_title=\"Z-score Distance\",\n",
    "            yaxis2=dict(\n",
    "                title=r\"% of Samples\",\n",
    "                overlaying=\"y\",\n",
    "                side=\"right\",\n",
    "                range=[0, 1.01],\n",
    "            ),\n",
    "            legend=dict(x=1.05, y=1, xanchor=\"left\", yanchor=\"top\"),\n",
    "            hovermode=\"x unified\",  # Show hover data for all traces at a given x\n",
    "        )\n",
    "        fig.update_xaxes(range=[0.49, 1])\n",
    "\n",
    "        # --- Save the plot for the current assay ---\n",
    "        # logdir.mkdir(parents=True, exist_ok=True) # Ensure log directory exists\n",
    "        # base_filename = f\"{name_prefix}_{assay_name.replace(' ', '_').replace('/', '-')}\" # Sanitize assay name for filename\n",
    "        # fig.write_image(logdir / f\"{base_filename}.svg\")\n",
    "        # fig.write_image(logdir / f\"{base_filename}.png\")\n",
    "        # fig.write_html(logdir / f\"{base_filename}.html\")\n",
    "\n",
    "        fig.show()  # Optionally display plot during script execution\n",
    "\n",
    "    print(\"Finished generating plots per assay.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separation_distance_subplots(\n",
    "    zscore_df: pd.DataFrame, logdir: Path, name: str\n",
    ") -> None:\n",
    "    \"\"\"Generates a single figure with a grid of subplots showing separation\n",
    "    distance (mean, median) between male/female zscore clusters for each\n",
    "    assay individually.\n",
    "\n",
    "    Args:\n",
    "        zscore_df (pd.DataFrame): The dataframe with z-score data, including\n",
    "                                   columns for Assay, Sex, the metric (e.g.,\n",
    "                                   'chrY_Zscore'), and 'Max pred'.\n",
    "        logdir (Path): The directory path to save the output plot.\n",
    "        name (str): The base name for the output plot file.\n",
    "    \"\"\"\n",
    "    # --- Preprocessing ---\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "    zscore_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # --- Filter assays and prepare for subplots ---\n",
    "    all_assays = sorted(zscore_df[ASSAY].unique())  # Sort for consistent order\n",
    "    valid_assays_info = []  # Store tuples of (assay_name, total_samples)\n",
    "\n",
    "    for assay_name in all_assays:\n",
    "        assay_df = zscore_df[zscore_df[ASSAY] == assay_name]\n",
    "        sex_counts = assay_df[SEX].value_counts()\n",
    "        total_samples = assay_df.shape[0]\n",
    "\n",
    "        if (\n",
    "            \"female\" in sex_counts\n",
    "            and \"male\" in sex_counts\n",
    "            and sex_counts[\"female\"] > 0\n",
    "            and sex_counts[\"male\"] > 0\n",
    "        ):\n",
    "            valid_assays_info.append((assay_name, total_samples))\n",
    "        else:\n",
    "            print(\n",
    "                f\"Skipping assay '{assay_name}': Missing data for one or both sexes ({sex_counts.to_dict()}).\"\n",
    "            )\n",
    "\n",
    "    num_assays = len(valid_assays_info)\n",
    "    if num_assays == 0:\n",
    "        print(\"No valid assays found to plot.\")\n",
    "        return\n",
    "\n",
    "    # Calculate grid size\n",
    "    cols = math.ceil(math.sqrt(num_assays))\n",
    "    rows = math.ceil(num_assays / cols)\n",
    "\n",
    "    # Prepare subplot titles\n",
    "    subplot_titles = [f\"{name} (N={count})\" for name, count in valid_assays_info]\n",
    "\n",
    "    # Define prediction score thresholds\n",
    "    min_preds = np.arange(\n",
    "        0, 1.0, 0.01\n",
    "    )  # Or use a specific range like [0.5, 1.0] if desired\n",
    "\n",
    "    # --- Initialize Subplot Figure ---\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=subplot_titles,\n",
    "        shared_xaxes=True,  # Share threshold axis\n",
    "        specs=[[{\"secondary_y\": True}] * cols] * rows,\n",
    "        x_title=\"Prediction Score minimum threshold\",\n",
    "        y_title=\"Z-score Distance\",\n",
    "        vertical_spacing=0.1,\n",
    "        horizontal_spacing=0.1,\n",
    "    )\n",
    "\n",
    "    print(f\"Creating {rows}x{cols} subplot grid for {num_assays} assays...\")\n",
    "\n",
    "    # --- Loop through valid assays and add traces ---\n",
    "    for i, (assay_name, total_samples) in enumerate(valid_assays_info):\n",
    "        print(f\"  Processing subplot for assay: {assay_name}\")\n",
    "        assay_df = zscore_df[zscore_df[ASSAY] == assay_name]\n",
    "\n",
    "        # Calculate current subplot row/col (1-based index)\n",
    "        current_row = i // cols + 1\n",
    "        current_col = i % cols + 1\n",
    "\n",
    "        # --- Calculate distances for the current assay ---\n",
    "        distances = {\"mean\": [], \"median\": []}\n",
    "        sample_count_raw = []  # Store raw counts before normalization\n",
    "\n",
    "        for min_pred in min_preds:\n",
    "            subset_assay_df = assay_df[assay_df[\"Max pred\"] > min_pred]\n",
    "            sample_count_raw.append(subset_assay_df.shape[0])\n",
    "\n",
    "            chrY_vals_female = subset_assay_df[subset_assay_df[SEX] == \"female\"][\n",
    "                metric_label\n",
    "            ]\n",
    "            chrY_vals_male = subset_assay_df[subset_assay_df[SEX] == \"male\"][metric_label]\n",
    "\n",
    "            if not chrY_vals_female.empty and not chrY_vals_male.empty:\n",
    "                mean_distance = np.abs(chrY_vals_female.mean() - chrY_vals_male.mean())\n",
    "                median_distance = np.abs(\n",
    "                    chrY_vals_female.median() - chrY_vals_male.median()\n",
    "                )\n",
    "                distances[\"mean\"].append(mean_distance)\n",
    "                distances[\"median\"].append(median_distance)\n",
    "            else:\n",
    "                distances[\"mean\"].append(np.nan)\n",
    "                distances[\"median\"].append(np.nan)\n",
    "\n",
    "        # Normalize sample count for this assay (relative to its max count)\n",
    "        max_count_assay = (\n",
    "            max(sample_count_raw) if sample_count_raw else 1\n",
    "        )  # Avoid division by zero\n",
    "        sample_count_percent = [count / max_count_assay for count in sample_count_raw]\n",
    "\n",
    "        # --- Add traces to the current subplot ---\n",
    "        # Show legend only for the first subplot's traces\n",
    "        show_legend = i == 0\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=min_preds,\n",
    "                y=distances[\"mean\"],\n",
    "                mode=\"lines+markers\",\n",
    "                name=\"Mean Distance (left axis)\",\n",
    "                legendgroup=\"mean\",  # Group traces for legend toggling\n",
    "                showlegend=show_legend,\n",
    "                line=dict(color=\"blue\"),\n",
    "                hovertemplate=\"Mean Dist: %{y:.3f}<extra></extra>\",\n",
    "            ),\n",
    "            row=current_row,\n",
    "            col=current_col,\n",
    "            secondary_y=False,\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=min_preds,\n",
    "                y=distances[\"median\"],\n",
    "                mode=\"lines+markers\",\n",
    "                name=\"Median Distance (left axis)\",\n",
    "                legendgroup=\"median\",\n",
    "                showlegend=show_legend,\n",
    "                line=dict(color=\"green\"),\n",
    "                hovertemplate=\"Median Dist: %{y:.3f}<extra></extra>\",\n",
    "            ),\n",
    "            row=current_row,\n",
    "            col=current_col,\n",
    "            secondary_y=False,\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=min_preds,\n",
    "                y=sample_count_percent,\n",
    "                mode=\"lines+markers\",\n",
    "                name=\"% Samples (right axis)\",\n",
    "                legendgroup=\"samples\",\n",
    "                showlegend=show_legend,\n",
    "                line=dict(color=\"red\"),\n",
    "                hovertemplate=\"% Samples: %{y:.1%}<extra></extra>\",\n",
    "            ),\n",
    "            row=current_row,\n",
    "            col=current_col,\n",
    "            secondary_y=True,  # Link to secondary axis\n",
    "        )\n",
    "\n",
    "    # --- Final Figure Layout Updates ---\n",
    "    fig.update_xaxes(range=[0.45, 1])\n",
    "    fig.update_yaxes(range=[-0.01, 1.01], secondary_y=True)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Separation Distance - chrY z-scores Male/Female Clusters (per Assay)\",\n",
    "        height=max(500, rows * 250),  # Adjust height based on rows\n",
    "        width=max(800, cols * 350),  # Adjust width based on columns\n",
    "        legend_title_text=\"Metrics\",\n",
    "        legend=dict(\n",
    "            traceorder=\"grouped\",  # Group legend items by name\n",
    "            x=1.02,\n",
    "            y=1,\n",
    "            xanchor=\"left\",\n",
    "            yanchor=\"top\",\n",
    "        ),\n",
    "        hovermode=\"x unified\",  # Show hover data for all traces of a subplot at a given x\n",
    "    )\n",
    "\n",
    "    # Manually add the \"% Samples\" annotation, otherwise it deletes all others.\n",
    "    current_annotations = list(fig[\"layout\"][\"annotations\"])  # type: ignore\n",
    "    current_annotations.append(\n",
    "        go.layout.Annotation(  # type: ignore\n",
    "            text=\"% Samples\",\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            x=1,  # Adjust x position relative to figure edge\n",
    "            y=0.5,  # Center vertically\n",
    "            showarrow=False,\n",
    "            textangle=90,\n",
    "            font=dict(size=14),  # Or match make_subplots title size if different\n",
    "            xanchor=\"center\",\n",
    "            yanchor=\"middle\",\n",
    "        )\n",
    "    )\n",
    "    fig[\"layout\"][\"annotations\"] = tuple(current_annotations)  # type: ignore\n",
    "\n",
    "    # Save figure\n",
    "    if logdir:\n",
    "        this_name = f\"{name}\"\n",
    "        fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "        fig.write_image(logdir / f\"{this_name}.png\")\n",
    "        fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "    fig.show()  # Optionally display plot during script execution\n",
    "\n",
    "    print(\"Finished generating subplot figure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_median_position_subplots(\n",
    "    zscore_df: pd.DataFrame, logdir: Path, name: str\n",
    ") -> None:\n",
    "    \"\"\"Generates a single figure with a grid of subplots showing the median z-score\n",
    "    position (with Q1-Q3 confidence bands) for male/female clusters for each\n",
    "    assay individually.\n",
    "\n",
    "    Args:\n",
    "        zscore_df (pd.DataFrame): The dataframe with z-score data.\n",
    "        logdir (Path): The directory path to save the output plot.\n",
    "        name (str): The base name for the output plot file.\n",
    "    \"\"\"\n",
    "    # --- Preprocessing ---\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "    zscore_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # --- Define colors ---\n",
    "    color_female = sex_colors[\"female\"]\n",
    "    fill_female = sex_colors[\"female_pale\"]\n",
    "    color_male = sex_colors[\"male\"]\n",
    "    fill_male = sex_colors[\"male_pale\"]\n",
    "    color_samples = \"rgb(214, 39, 40)\"  # Red\n",
    "\n",
    "    # --- Filter assays and prepare for subplots (same as before) ---\n",
    "    all_assays = sorted(zscore_df[ASSAY].unique())\n",
    "    valid_assays_info = []\n",
    "    for assay_name in all_assays:\n",
    "        assay_df = zscore_df[zscore_df[ASSAY] == assay_name]\n",
    "        sex_counts = assay_df[SEX].value_counts()\n",
    "        total_samples = assay_df.shape[0]\n",
    "        if (\n",
    "            \"female\" in sex_counts\n",
    "            and \"male\" in sex_counts\n",
    "            and sex_counts[\"female\"] > 0\n",
    "            and sex_counts[\"male\"] > 0\n",
    "        ):\n",
    "            valid_assays_info.append((assay_name, total_samples))\n",
    "        else:\n",
    "            print(\n",
    "                f\"Skipping assay '{assay_name}': Missing data ({sex_counts.to_dict()}).\"\n",
    "            )\n",
    "\n",
    "    num_assays = len(valid_assays_info)\n",
    "    if num_assays == 0:\n",
    "        raise ValueError(\"No valid assays found to plot.\")\n",
    "\n",
    "    cols = math.ceil(math.sqrt(num_assays))\n",
    "    rows = math.ceil(num_assays / cols)\n",
    "    subplot_titles = [f\"{name} (N={count})\" for name, count in valid_assays_info]\n",
    "    min_preds = np.arange(0, 1.0, 0.01)\n",
    "\n",
    "    # --- Initialize Subplot Figure ---\n",
    "    axis_font_size = 16\n",
    "    fig = make_subplots(\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        subplot_titles=subplot_titles,\n",
    "        shared_xaxes=True,\n",
    "        shared_yaxes=True,  # Share primary y-axis\n",
    "        specs=[[{\"secondary_y\": True}] * cols] * rows,\n",
    "        x_title=\"Prediction Score minimum threshold\",\n",
    "        y_title=\"Median Z-score Position\",\n",
    "        vertical_spacing=0.05,\n",
    "        horizontal_spacing=0.01,\n",
    "    )\n",
    "\n",
    "    print(f\"Creating {rows}x{cols} subplot grid for {num_assays} assays...\")\n",
    "\n",
    "    # --- Keep track of overall min/max for primary axis range ---\n",
    "    global_min_primary_y = float(\"inf\")\n",
    "    global_max_primary_y = float(\"-inf\")\n",
    "\n",
    "    # --- Loop through valid assays and add traces ---\n",
    "    for i, (assay_name, total_samples) in enumerate(valid_assays_info):\n",
    "        assay_df = zscore_df[zscore_df[ASSAY] == assay_name]\n",
    "        current_row = i // cols + 1\n",
    "        current_col = i % cols + 1\n",
    "\n",
    "        # --- Calculate positions and quartiles for the current assay ---\n",
    "        female_stats = {\"median\": [], \"q1\": [], \"q3\": []}\n",
    "        male_stats = {\"median\": [], \"q1\": [], \"q3\": []}\n",
    "        sample_count_raw = []\n",
    "\n",
    "        for min_pred in min_preds:\n",
    "            subset_assay_df = assay_df[assay_df[\"Max pred\"] > min_pred]\n",
    "            sample_count_raw.append(subset_assay_df.shape[0])\n",
    "\n",
    "            chrY_vals_female = subset_assay_df[subset_assay_df[SEX] == \"female\"][\n",
    "                metric_label\n",
    "            ]\n",
    "            chrY_vals_male = subset_assay_df[subset_assay_df[SEX] == \"male\"][metric_label]\n",
    "\n",
    "            # Calculate Female Stats\n",
    "            if (\n",
    "                not chrY_vals_female.empty and len(chrY_vals_female) >= 3\n",
    "            ):  # Require at least 3 points\n",
    "                q1, med, q3 = np.percentile(chrY_vals_female, [25, 50, 75])\n",
    "                female_stats[\"q1\"].append(q1)\n",
    "                female_stats[\"median\"].append(med)\n",
    "                female_stats[\"q3\"].append(q3)\n",
    "                # Update global min/max based on Q1 and Q3\n",
    "                global_min_primary_y = min(global_min_primary_y, q1)\n",
    "                global_max_primary_y = max(global_max_primary_y, q3)\n",
    "            elif not chrY_vals_female.empty:  # Not enough points: ignore\n",
    "                female_stats[\"q1\"].append(np.nan)\n",
    "                female_stats[\"median\"].append(np.nan)\n",
    "                female_stats[\"q3\"].append(np.nan)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"No data found for assay '{assay_name}' and min_pred={min_pred}\"\n",
    "                )\n",
    "\n",
    "            # Calculate Male Stats\n",
    "            if not chrY_vals_male.empty and len(chrY_vals_male) >= 3:\n",
    "                q1, med, q3 = np.percentile(chrY_vals_male, [25, 50, 75])\n",
    "                male_stats[\"q1\"].append(q1)\n",
    "                male_stats[\"median\"].append(med)\n",
    "                male_stats[\"q3\"].append(q3)\n",
    "                global_min_primary_y = min(global_min_primary_y, q1)\n",
    "                global_max_primary_y = max(global_max_primary_y, q3)\n",
    "            elif not chrY_vals_male.empty:  # Not enough points: ignore\n",
    "                male_stats[\"q1\"].append(np.nan)\n",
    "                male_stats[\"median\"].append(np.nan)\n",
    "                male_stats[\"q3\"].append(np.nan)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"No data found for assay '{assay_name}' and min_pred={min_pred}\"\n",
    "                )\n",
    "        # --- END of calculation ---\n",
    "\n",
    "        max_count_assay = max(sample_count_raw) if sample_count_raw else 1\n",
    "        sample_count_percent = [count / max_count_assay for count in sample_count_raw]\n",
    "\n",
    "        show_legend = i == 0  # Show legend only for first subplot\n",
    "\n",
    "        # --- **** ADD TRACES WITH CONFIDENCE BANDS **** ---\n",
    "        # Add traces in order: Upper bound, Lower bound (with fill), Median line\n",
    "\n",
    "        # -- Female Traces --\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=min_preds,\n",
    "                y=female_stats[\"q3\"],\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0),\n",
    "                showlegend=False,\n",
    "                name=\"Female Q3\",\n",
    "                legendgroup=\"female\",\n",
    "            ),\n",
    "            row=current_row,\n",
    "            col=current_col,\n",
    "            secondary_y=False,\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=min_preds,\n",
    "                y=female_stats[\"q1\"],\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0),\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=fill_female,\n",
    "                showlegend=False,\n",
    "                name=\"Female Q1\",\n",
    "                legendgroup=\"female\",\n",
    "            ),\n",
    "            row=current_row,\n",
    "            col=current_col,\n",
    "            secondary_y=False,\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=min_preds,\n",
    "                y=female_stats[\"median\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Female Median\",\n",
    "                legendgroup=\"female\",\n",
    "                showlegend=show_legend,\n",
    "                line=dict(color=color_female, width=4),\n",
    "            ),\n",
    "            row=current_row,\n",
    "            col=current_col,\n",
    "            secondary_y=False,\n",
    "        )\n",
    "\n",
    "        # -- Male Traces --\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=min_preds,\n",
    "                y=male_stats[\"q3\"],\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0),\n",
    "                showlegend=False,\n",
    "                name=\"Male Q3\",\n",
    "                legendgroup=\"male\",\n",
    "            ),\n",
    "            row=current_row,\n",
    "            col=current_col,\n",
    "            secondary_y=False,\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=min_preds,\n",
    "                y=male_stats[\"q1\"],\n",
    "                mode=\"lines\",\n",
    "                line=dict(width=0),\n",
    "                fill=\"tonexty\",\n",
    "                fillcolor=fill_male,\n",
    "                showlegend=False,\n",
    "                name=\"Male Q1\",\n",
    "                legendgroup=\"male\",\n",
    "            ),\n",
    "            row=current_row,\n",
    "            col=current_col,\n",
    "            secondary_y=False,\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=min_preds,\n",
    "                y=male_stats[\"median\"],\n",
    "                mode=\"lines\",\n",
    "                name=\"Male Median\",\n",
    "                legendgroup=\"male\",\n",
    "                showlegend=show_legend,\n",
    "                line=dict(color=color_male, width=4),\n",
    "            ),\n",
    "            row=current_row,\n",
    "            col=current_col,\n",
    "            secondary_y=False,\n",
    "        )\n",
    "\n",
    "        # -- Sample Count Trace --\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=min_preds,\n",
    "                y=sample_count_percent,\n",
    "                mode=\"lines+markers\",\n",
    "                name=\"% Samples\",\n",
    "                legendgroup=\"samples\",\n",
    "                showlegend=show_legend,\n",
    "                line=dict(color=color_samples, width=2),\n",
    "                marker_size=4,\n",
    "            ),\n",
    "            row=current_row,\n",
    "            col=current_col,\n",
    "            secondary_y=True,  # Link to secondary axis\n",
    "        )\n",
    "        # --- **** END of trace modification **** ---\n",
    "\n",
    "        # --- **** Hide secondary y-axis ticks for inner columns **** ---\n",
    "        if current_col < cols:\n",
    "            fig.update_yaxes(\n",
    "                showticklabels=False, row=current_row, col=current_col, secondary_y=True\n",
    "            )\n",
    "\n",
    "    # --- Final Figure Layout Updates ---\n",
    "\n",
    "    # Set shared axis ranges globally\n",
    "    fig.update_xaxes(range=[0.49, 1.0])\n",
    "\n",
    "    # **** ADJUST PRIMARY Y-AXIS RANGE ****\n",
    "    # Add buffer to min/max, handle case where no data found\n",
    "    y_range_buffer = (\n",
    "        (global_max_primary_y - global_min_primary_y) * 0.05\n",
    "        if global_max_primary_y > global_min_primary_y\n",
    "        else 0.5\n",
    "    )\n",
    "    primary_y_range = (\n",
    "        [global_min_primary_y - y_range_buffer, global_max_primary_y + y_range_buffer]\n",
    "        if num_assays > 0\n",
    "        else [-1, 1]\n",
    "    )\n",
    "    fig.update_yaxes(range=primary_y_range, secondary_y=False)\n",
    "\n",
    "    # Set secondary y-axis range globally\n",
    "    fig.update_yaxes(range=[-0.01, 1.01], secondary_y=True)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Median Z-score Position (Q1-Q3 Bands) - Male/Female Clusters (per Assay)\",\n",
    "        height=max(500, rows * 250),\n",
    "        width=max(800, cols * 350 + 100),\n",
    "        legend_title_text=\"Cluster\",\n",
    "        legend=dict(traceorder=\"grouped\", x=1.02, y=1, xanchor=\"left\", yanchor=\"top\"),\n",
    "        margin=dict(l=80, r=100, t=100, b=80),\n",
    "    )\n",
    "\n",
    "    # Manually add the \"% Samples\" annotation, otherwise it deletes all others.\n",
    "    current_annotations = list(fig[\"layout\"][\"annotations\"])  # type: ignore\n",
    "    current_annotations.append(\n",
    "        go.layout.Annotation(  # type: ignore\n",
    "            text=\"% Samples\",\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            x=1,  # Adjust x position relative to figure edge\n",
    "            y=0.5,  # Center vertically\n",
    "            showarrow=False,\n",
    "            textangle=90,\n",
    "            font=dict(\n",
    "                size=axis_font_size\n",
    "            ),  # Or match make_subplots title size if different\n",
    "            xanchor=\"center\",\n",
    "            yanchor=\"middle\",\n",
    "        )\n",
    "    )\n",
    "    fig[\"layout\"][\"annotations\"] = tuple(current_annotations)  # type: ignore\n",
    "\n",
    "    # Save figure\n",
    "    if logdir:\n",
    "        this_name = f\"{name}\"\n",
    "        fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "        fig.write_image(logdir / f\"{this_name}.png\")\n",
    "        fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = (\n",
    "    base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_chrY_zscore\" / \"cluster_separation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"sex_chrY_zscore_merged_assays_distance\"\n",
    "merged_assays_separation_distance(zscore_df, logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"sex_chrY_zscore_cluster_distance_per_assay\"\n",
    "# plot_separation_distance_subplots(zscore_df, logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"sex_chrY_zscore_cluster_median_per_assay\"\n",
    "# plot_median_position_subplots(zscore_df, logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex: prediction score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_score_violin(\n",
    "    results_df: pd.DataFrame, logdir: Path, name: str, min_y: float | None = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with violin plots and associated scatter plots for each class.\n",
    "    Red scatter points, indicating a mismatch, appear on top and have a larger size.\n",
    "\n",
    "    Args:\n",
    "        results_df (pd.DataFrame): The DataFrame containing the neural network results, including metadata.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "        name (str): The name of the figure.\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Class ordering\n",
    "    class_labels = (\n",
    "        results_df[\"True class\"].unique().tolist()\n",
    "        + results_df[\"Predicted class\"].unique().tolist()\n",
    "    )\n",
    "    class_labels_sorted = sorted(set(class_labels))\n",
    "    class_index = {label: i for i, label in enumerate(class_labels_sorted)}\n",
    "\n",
    "    for label in class_labels_sorted:\n",
    "        df = results_df[results_df[\"True class\"] == label]\n",
    "\n",
    "        # Majority vote, mean prediction score\n",
    "        groupby_epirr = df.groupby([\"EpiRR\", \"Predicted class\"])[\"Max pred\"].aggregate(\n",
    "            [\"size\", \"mean\"]\n",
    "        )\n",
    "\n",
    "        groupby_epirr = groupby_epirr.reset_index().sort_values(\n",
    "            [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "        )\n",
    "        groupby_epirr = groupby_epirr.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "        assert groupby_epirr[\"EpiRR\"].is_unique\n",
    "\n",
    "        mean_pred = groupby_epirr[\"mean\"]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[class_index[label]] * len(mean_pred),\n",
    "                y=mean_pred,\n",
    "                name=label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=False,\n",
    "                fillcolor=sex_colors[label],\n",
    "                line_color=\"black\",\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Prepare data for scatter plots\n",
    "        match_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] == label\n",
    "        ]\n",
    "        mismatch_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] != label\n",
    "        ]\n",
    "\n",
    "        jitter_match = np.random.uniform(-0.01, 0.01, len(match_pred))\n",
    "\n",
    "        # Add scatter plots for matches in black\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[class_index[label]] * len(match_pred) + jitter_match,\n",
    "                y=match_pred,\n",
    "                mode=\"markers\",\n",
    "                name=label,\n",
    "                marker=dict(\n",
    "                    color=\"black\",\n",
    "                    size=2,  # Standard size for matches\n",
    "                ),\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.2f}\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] == label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "                legendgroup=\"match\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add scatter plots for mismatches in red, with larger size\n",
    "        mismatch_info = groupby_epirr[groupby_epirr[\"Predicted class\"] != label]\n",
    "        strong_mismatch = mismatch_info[mismatch_info[\"mean\"] > 0.9]\n",
    "        display(strong_mismatch)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[class_index[label]] * len(mismatch_pred),\n",
    "                y=mismatch_pred,\n",
    "                mode=\"markers\",\n",
    "                name=label,\n",
    "                marker=dict(\n",
    "                    color=\"red\",\n",
    "                    size=5,  # Larger size for mismatches\n",
    "                ),\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.3f} (n={row[1]['size']})\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] != label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "                legendgroup=\"mismatch\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - black points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Match\",\n",
    "            marker=dict(color=\"black\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"match\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - red points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Mismatch\",\n",
    "            marker=dict(color=\"red\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"mismatch\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tickvals = list(class_index.values())\n",
    "    ticktext = list(class_index.keys())\n",
    "    fig.update_xaxes(tickvals=tickvals, ticktext=ticktext)\n",
    "\n",
    "    if min_y is None:\n",
    "        min_y = min(results_df[\"Max pred\"])\n",
    "\n",
    "    fig.update_yaxes(range=[min_y, 1.001])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Prediction score distribution class\",\n",
    "        yaxis_title=\"Avg. prediction score (majority class)\",\n",
    "        xaxis_title=\"Expected class label\",\n",
    "        width=750,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            title_text=\"Legend\",\n",
    "            itemsizing=\"constant\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    # fig.write_html(logdir / f\"{name}.html\")\n",
    "    # fig.write_image(logdir / f\"{name}.svg\")\n",
    "    # fig.write_image(logdir / f\"{name}.png\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_df = zscore_df\n",
    "sex_df = split_results_handler.add_max_pred(sex_df)\n",
    "sex_df = metadata_handler.join_metadata(sex_df, metadata_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"fig2--sex_pred_score_post_correction\"\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_pred_score\"\n",
    "# pred_score_violin(sex_df, logdir, name, use_aggregate_vote=False, min_y=0.485)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample ontology prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_score_violin_alt(\n",
    "    results_df: pd.DataFrame,\n",
    "    name: str,\n",
    "    min_y: float | None = None,\n",
    "    use_aggregate_vote: bool = True,\n",
    "    group_by_column: str = \"True class\",\n",
    "    logdir: Path | None = None,\n",
    "    title: str | None = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with violin plots and associated scatter plots for each group.\n",
    "    Args:\n",
    "        results_df (pd.DataFrame): The DataFrame containing the neural network results, including metadata.\n",
    "        name (str): The name of the figure.\n",
    "        min_y (float, optional): Minimum y-axis value. If None, calculated from the data.\n",
    "        use_aggregate_vote (bool, optional): If True, use EpiRR for aggregation. If False, use md5sum without aggregation.\n",
    "        group_by_column (str, optional): The column name to use for grouping traces. Defaults to \"True class\".\n",
    "        logdir (Path, optional): The directory where the figure will be saved.\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Group ordering\n",
    "    group_labels = results_df[group_by_column].unique().tolist()\n",
    "    group_labels_sorted = sorted(set(group_labels))\n",
    "    group_index = {label: i for i, label in enumerate(group_labels_sorted)}\n",
    "\n",
    "    for label in group_labels_sorted:\n",
    "        df = results_df[results_df[group_by_column] == label]\n",
    "\n",
    "        if use_aggregate_vote:\n",
    "            # Majority vote, mean prediction score\n",
    "            groupby = df.groupby([\"EpiRR\", \"Predicted class\"])[\"Max pred\"].aggregate(\n",
    "                [\"size\", \"mean\"]\n",
    "            )\n",
    "            groupby = groupby.reset_index().sort_values(\n",
    "                [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "            )\n",
    "            groupby = groupby.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "            assert groupby[\"EpiRR\"].is_unique\n",
    "            mean_pred = groupby[\"mean\"]\n",
    "            # pylint: disable=unused-variable\n",
    "            hover_text = [\n",
    "                f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.2f}, n={row[1]['size']}\"\n",
    "                for row in groupby.iterrows()\n",
    "            ]\n",
    "        else:\n",
    "            # Use md5sum without aggregation\n",
    "            mean_pred = df[\"Max pred\"]\n",
    "            # pylint: disable=unused-variable\n",
    "            hover_text = [\n",
    "                f\"md5sum: {row['md5sum']}, Pred class: {row['Predicted class']}, Pred: {row['Max pred']:.2f}\"\n",
    "                for _, row in df.iterrows()\n",
    "            ]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[group_index[label]] * len(mean_pred),\n",
    "                y=mean_pred,\n",
    "                name=label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=False,\n",
    "                points=\"all\",\n",
    "                fillcolor=assay_colors[label],\n",
    "                line_color=\"black\",\n",
    "                showlegend=False,\n",
    "                marker=dict(\n",
    "                    color=\"black\",\n",
    "                    size=2,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # # Prepare data for scatter plots\n",
    "        # jitter = np.random.uniform(-0.01, 0.01, len(mean_pred))\n",
    "\n",
    "        # # Add scatter plots\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         x=[group_index[label]] * len(mean_pred) + jitter,\n",
    "        #         y=mean_pred,\n",
    "        #         mode=\"markers\",\n",
    "        #         name=label,\n",
    "        #         marker=dict(\n",
    "        #             color=\"black\",\n",
    "        #             size=2,\n",
    "        #         ),\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hover_text,\n",
    "        #         showlegend=False,\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "    tickvals = list(group_index.values())\n",
    "    ticktext = list(group_index.keys())\n",
    "    fig.update_xaxes(tickvals=tickvals, ticktext=ticktext)\n",
    "\n",
    "    if min_y is None:\n",
    "        min_y = min(results_df[\"Max pred\"])\n",
    "\n",
    "    fig.update_yaxes(range=[min_y, 1.001])\n",
    "\n",
    "    base_title = \"Prediction score distribution\"\n",
    "    if use_aggregate_vote:\n",
    "        base_title += \" (EpiRR majority vote)\"\n",
    "        y_axis_title = \"Avg. prediction score (majority class)\"\n",
    "        filename = f\"{name}_epirr\"\n",
    "    else:\n",
    "        base_title += \" (per file)\"\n",
    "        y_axis_title = \"Prediction score\"\n",
    "        filename = name\n",
    "\n",
    "    if title:\n",
    "        base_title = f\"{base_title} - {title}\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=base_title,\n",
    "        yaxis_title=y_axis_title,\n",
    "        xaxis_title=group_by_column,\n",
    "        width=len(group_index) * 100,\n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    if logdir:\n",
    "        fig.write_html(logdir / f\"{filename}.html\")\n",
    "        fig.write_image(logdir / f\"{filename}.svg\")\n",
    "        fig.write_image(logdir / f\"{filename}.png\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_split_dfs = all_split_results[CELL_TYPE]  # type: ignore\n",
    "ct_concat_pred: pd.DataFrame = split_results_handler.concatenate_split_results(\n",
    "    {CELL_TYPE: ct_split_dfs}, concat_first_level=True  # type: ignore\n",
    ")[CELL_TYPE]\n",
    "ct_concat_pred = split_results_handler.add_max_pred(ct_concat_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_concat_pred = metadata_handler.join_metadata(ct_concat_pred, metadata_v2)\n",
    "ct_concat_pred[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"sample_ontology_pred_score\"\n",
    "this_logdir = base_fig_dir\n",
    "\n",
    "# pred_score_violin_alt(ct_concat_pred, this_logdir, f\"{name}_by_output_class\", use_aggregate_vote=False, group_by_column=CELL_TYPE, min_y=0)\n",
    "# pred_score_violin_alt(\n",
    "#     results_df=ct_concat_pred,\n",
    "#     name=f\"{name}_by_output_class\",\n",
    "#     use_aggregate_vote=False,\n",
    "#     group_by_column=ASSAY,\n",
    "#     min_y=0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced features sets NN metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regulatory regions NN trainings data download\n",
    "\n",
    "~~~bash\n",
    "# Download phase\n",
    "paper_dir=\"${HOME}/Projects/epiclass/output/paper/data\"\n",
    "cd ${paper_dir}/training_results/dfreeze_v2\n",
    "\n",
    "base_path=\"/lustre07/scratch/rabyj/epilap-logs/epiatlas-dfreeze-v2.1\"\n",
    "rsync --info=progress2 -av --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"validation_confusion*\" --exclude \"*.md5\" --exclude \"full*\" narval:${base_path}/hg38_regulatory_regions_n* .\n",
    "\n",
    "# Cleanup phase\n",
    "# Remove files related to failed experiements\n",
    "# Step 1: Find files and extract numbers\n",
    "find hg38_regulatory_regions_n* -type f -name \"*.e\" -exec grep -l 'has non-string label of type' {} + | \\\n",
    "grep -oE \"job[0-9]+\" | grep -oE \"[0-9]+\" > failure_jobid.txt\n",
    "\n",
    "# Step 2: Use extracted numbers to find and echo all matching filenames\n",
    "cat failure_jobid.txt | xargs -I% sh -c 'find . -type f -name \"*%*\" -delete'\n",
    "rm failure_jobid.txt\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_sizes_from_metadata() -> Dict[str, int]:\n",
    "    \"\"\"Get input sizes for models using certain feature sets using comet-ml run metadata file.\"\"\"\n",
    "    run_metadata = RUN_METADATA.copy(deep=True)\n",
    "\n",
    "    # Filter out epigeec_filter_1.4.5 runs, wrong input sizes.\n",
    "    run_metadata = run_metadata[run_metadata[\"startTimeMillis\"] > 1706943404420]\n",
    "\n",
    "    run_metadata[\"feature_set\"] = run_metadata[\"Name\"].str.extract(\n",
    "        pat=r\"(^hg38_1[0]{0,2}kb_.*_none).*$\"\n",
    "    )\n",
    "\n",
    "    input_sizes_count = run_metadata.groupby([\"feature_set\", \"input_size\"]).size()\n",
    "    accepted_input_sizes = {idx[0]: int(idx[1]) for idx in input_sizes_count.index}\n",
    "\n",
    "    assert len(input_sizes_count) == len(accepted_input_sizes)\n",
    "    accepted_input_sizes.update({\"hg38_100kb_all_none\": 30321})\n",
    "\n",
    "    return accepted_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_feature_set(\n",
    "    feature_set_data: Dict[str, Dict], include_names: List[str], exclude_names: List[str]\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Filters the feature set data based on inclusion and exclusion criteria.\n",
    "\n",
    "    This function creates a deep copy of the input `feature_set_data`,\n",
    "    and then iterates through each feature set (identified by `name`)\n",
    "    and its corresponding task data (`data_dict`).\n",
    "    It removes task names that do not match the `include_names` list or match any of the names in the `exclude_names` list.\n",
    "    The filtering is applied at the task name level within each feature set.\n",
    "\n",
    "    Args:\n",
    "        feature_set_data (Dict[str, Dict]): The original feature set data to be filtered.\n",
    "            The outer dictionary's keys represent feature set names, and the inner dictionaries represent task names and their associated data.\n",
    "        include_names (List[str]): A list of strings. Task names must contain at least one of these strings to be retained.\n",
    "        exclude_names (List[str]): A list of strings. Task names containing any of these strings will be removed.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict]: A new dictionary containing only the filtered feature set data.\n",
    "\n",
    "    Example:\n",
    "        feature_set_data = {\n",
    "            'set1': {'taskA': {...}, 'taskB': {...}},\n",
    "            'set2': {'taskC': {...}, 'taskD': {...}}\n",
    "        }\n",
    "        include_names = ['A', 'C']\n",
    "        exclude_names = ['D']\n",
    "\n",
    "        filtered = filter_feature_set(feature_set_data, include_names, exclude_names)\n",
    "        # Output: {'set1': {'taskA': {...}}, 'set2': {'taskC': {...}}}\n",
    "    \"\"\"\n",
    "    filtered_set_data = copy.deepcopy(feature_set_data)\n",
    "    for name, data_dict in list(filtered_set_data.items()):\n",
    "        for task_name in list(data_dict.keys()):\n",
    "            if not any(label in task_name for label in include_names):\n",
    "                del filtered_set_data[name][task_name]\n",
    "                continue\n",
    "            if any(label in task_name for label in exclude_names):\n",
    "                del filtered_set_data[name][task_name]\n",
    "    return filtered_set_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "input_sizes = extract_input_sizes_from_output_files(gen_data_dir)  # type: ignore\n",
    "input_sizes: Dict[str, int] = {k: v.pop() for k, v in input_sizes.items() if len(v) == 1}  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagship_order_4cat = [\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n",
    "    \"hg38_regulatory_regions_n30321\",\n",
    "    \"hg38_gene_regions_100kb_coord_n19864\",\n",
    "    \"hg38_100kb_all_none\",\n",
    "]\n",
    "flagship_order_7cat = [\n",
    "    \"hg38_regulatory_regions_n30321\",\n",
    "    \"hg38_regulatory_regions_n303114\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n300k\",\n",
    "    \"hg38_gene_regions_100kb_coord_n19864\",\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_10kb_all_none\",\n",
    "]\n",
    "suppFig1_14cat = [\n",
    "    \"hg38_10mb_all_none_1mb_coord\",\n",
    "    \"hg38_100kb_random_n316_none\",\n",
    "    \"hg38_1mb_all_none\",\n",
    "    \"hg38_100kb_random_n3044_none\",\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_gene_regions_100kb_coord_n19864\",\n",
    "    \"hg38_10kb_random_n30321_none\",\n",
    "    \"hg38_regulatory_regions_n30321\",\n",
    "    \"hg38_1kb_random_n30321_none\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n",
    "    \"hg38_10kb_all_none\",\n",
    "    \"hg38_regulatory_regions_n303114\",\n",
    "    \"hg38_1kb_random_n303114_none\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n300k\",\n",
    "]\n",
    "\n",
    "different_nature_sets = [\n",
    "    \"hg38_regulatory_regions_n30321\",\n",
    "    \"hg38_regulatory_regions_n303114\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n",
    "    \"hg38_cpg_topvar_200bp_10kb_coord_n300k\",\n",
    "    \"hg38_cpg_topvar_2bp_10kb_coord_n30k\",\n",
    "    \"hg38_cpg_topvar_2bp_10kb_coord_n300k\",\n",
    "    \"hg38_gene_regions_100kb_coord_n19864\",\n",
    "    \"hg38_100kb_all_none\",\n",
    "    \"hg38_10kb_all_none\",\n",
    "    \"hg38_10kb_random_n30321_none\",\n",
    "    \"hg38_1kb_random_n30321_none\",\n",
    "    \"hg38_1kb_random_n303114_none\",\n",
    "]\n",
    "\n",
    "all_feature_sets = None\n",
    "\n",
    "metric_orders_map = {\n",
    "    \"flagship_selection_4cat\": flagship_order_4cat,\n",
    "    \"flagship_selection_7cat\": flagship_order_7cat,\n",
    "    \"suppFig1_14cat\": suppFig1_14cat,\n",
    "    \"different_nature_sets\": different_nature_sets,\n",
    "    \"all_feature_sets\": all_feature_sets,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_selection_name = \"different_nature_sets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_sets_metrics = split_results_handler.obtain_all_feature_set_data(\n",
    "    parent_folder=gen_data_dir,\n",
    "    merge_assays=True,\n",
    "    return_type=\"metrics\",\n",
    "    exclude_names=[\"27ct\", \"16ct\"],\n",
    "    include_sets=metric_orders_map[set_selection_name],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order results\n",
    "if set_selection_name:\n",
    "    selected_feature_sets_metrics = {\n",
    "        name: selected_feature_sets_metrics[name]\n",
    "        for name in metric_orders_map[set_selection_name]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_colors = {\n",
    "    \"100kb\": px.colors.qualitative.Safe[0],\n",
    "    \"10kb\": px.colors.qualitative.Safe[1],\n",
    "    \"1kb\": px.colors.qualitative.Safe[2],\n",
    "    \"regulatory\": px.colors.qualitative.Safe[3],\n",
    "    \"gene\": px.colors.qualitative.Safe[4],\n",
    "    \"cpg\": px.colors.qualitative.Safe[5],\n",
    "    \"1mb\": px.colors.qualitative.Safe[6],\n",
    "    \"5mb\": px.colors.qualitative.Safe[7],\n",
    "    \"10mb\": px.colors.qualitative.Safe[8],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_feature_set_metrics(\n",
    "    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n",
    "    input_sizes: Dict[str, int],\n",
    "    logdir: Path,\n",
    "    sort_by_input_size: bool = False,\n",
    "    name: str | None = None,\n",
    "    y_range: Tuple[float, float] | None = None,\n",
    "    boxpoints: str = \"all\",\n",
    ") -> None:\n",
    "    \"\"\"Graph the metrics for all feature sets.\n",
    "\n",
    "    Args:\n",
    "        all_metrics (Dict[str, Dict[str, Dict[str, Dict[str, float]]]): A dictionary containing all metrics for all feature sets.\n",
    "            Format: {feature_set: {task_name: {split_name: metric_dict}}}\n",
    "        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n",
    "        name (str|None): The name of the figure.\n",
    "        y_range (Tuple[float, float]|None): The y-axis range for the figure.\n",
    "        boxpoints (str): The type of boxpoints to display. Can be \"all\" or \"outliers\". Defaults to \"all\".\n",
    "    \"\"\"\n",
    "    if boxpoints not in [\"all\", \"outliers\"]:\n",
    "        raise ValueError(\"Invalid boxpoints value.\")\n",
    "\n",
    "    reference_hdf5_type = \"hg38_100kb_all_none\"\n",
    "    metadata_categories = list(all_metrics[reference_hdf5_type].keys())\n",
    "    print(metadata_categories)\n",
    "\n",
    "    non_standard_names = {ASSAY: f\"{ASSAY}_11c\", SEX: f\"{SEX}_w-mixed\"}\n",
    "    non_standard_assay_task_names = [\"hg38_100kb_all_none\"]\n",
    "    non_standard_sex_task_name = [\n",
    "        \"hg38_100kb_all_none\",\n",
    "        \"hg38_regulatory_regions_n30321\",\n",
    "        \"hg38_regulatory_regions_n303114\",\n",
    "    ]\n",
    "    used_resolutions = set()\n",
    "    for i in range(len(metadata_categories)):\n",
    "        category_idx = i\n",
    "        category_fig = make_subplots(\n",
    "            rows=1,\n",
    "            cols=2,\n",
    "            shared_yaxes=True,\n",
    "            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n",
    "            x_title=\"Feature set\",\n",
    "            y_title=\"Metric value\",\n",
    "        )\n",
    "\n",
    "        trace_names = []\n",
    "        order = list(all_metrics.keys())\n",
    "        if sort_by_input_size:\n",
    "            order = sorted(\n",
    "                all_metrics.keys(),\n",
    "                key=lambda x: input_sizes[x],\n",
    "            )\n",
    "        for feature_set_name in order:\n",
    "            # print(feature_set_name)\n",
    "            tasks_dicts = all_metrics[feature_set_name]\n",
    "            meta_categories = copy.deepcopy(metadata_categories)\n",
    "\n",
    "            if feature_set_name not in input_sizes:\n",
    "                print(f\"Skipping {feature_set_name}, no input size found.\")\n",
    "                continue\n",
    "\n",
    "            task_name = meta_categories[category_idx]\n",
    "            if \"split\" in task_name:\n",
    "                raise ValueError(\"Split in task name. Wrong metrics dict.\")\n",
    "\n",
    "            try:\n",
    "                task_dict = tasks_dicts[task_name]\n",
    "            except KeyError as err:\n",
    "                if SEX in str(err) and feature_set_name in non_standard_sex_task_name:\n",
    "                    task_dict = tasks_dicts[non_standard_names[SEX]]\n",
    "                elif (\n",
    "                    ASSAY in str(err)\n",
    "                    and feature_set_name in non_standard_assay_task_names\n",
    "                ):\n",
    "                    task_dict = tasks_dicts[non_standard_names[ASSAY]]\n",
    "                else:\n",
    "                    print(\"Skipping\", feature_set_name, task_name)\n",
    "                    continue\n",
    "\n",
    "            input_size = input_sizes[feature_set_name]\n",
    "\n",
    "            feature_set_name = feature_set_name.replace(\"_none\", \"\")\n",
    "            feature_set_name = feature_set_name.replace(\"hg38_\", \"\")\n",
    "\n",
    "            resolution = feature_set_name.split(\"_\")[0]\n",
    "            used_resolutions.add(resolution)\n",
    "\n",
    "            trace_name = f\"{input_size}|{feature_set_name}\"\n",
    "            trace_names.append(trace_name)\n",
    "\n",
    "            # Accuracy\n",
    "            metric = \"Accuracy\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=boxpoints,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=resolution_colors[resolution],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                    legendgroup=resolution,\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "            metric = \"F1_macro\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=boxpoints,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=resolution_colors[resolution],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                    legendgroup=resolution,\n",
    "                    showlegend=False,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=2,\n",
    "            )\n",
    "\n",
    "        title = f\"Neural network performance - {metadata_categories[category_idx]}\"\n",
    "        if name is not None:\n",
    "            title += f\" - {name}\"\n",
    "        category_fig.update_layout(\n",
    "            width=1500,\n",
    "            height=1500,\n",
    "            title=title,\n",
    "        )\n",
    "\n",
    "        if logdir.name == \"all\":\n",
    "            category_fig.update_xaxes(\n",
    "                categoryorder=\"array\",\n",
    "                categoryarray=sorted(trace_names, key=lambda x: int(x.split(\"|\")[0])),\n",
    "            )\n",
    "\n",
    "        # dummy scatters for resolution colors\n",
    "        for resolution in used_resolutions:\n",
    "            color = resolution_colors[resolution]\n",
    "            category_fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[None],\n",
    "                    y=[None],\n",
    "                    mode=\"markers\",\n",
    "                    name=resolution,\n",
    "                    marker=dict(color=color, size=5),\n",
    "                    showlegend=True,\n",
    "                    legendgroup=resolution,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        category_fig.update_layout(legend=dict(itemsizing=\"constant\"))\n",
    "        if y_range:\n",
    "            category_fig.update_yaxes(range=y_range)\n",
    "\n",
    "        # Save figure\n",
    "        base_name = f\"feature_set_metrics_{metadata_categories[category_idx]}\"\n",
    "        base_name = base_name.replace(CELL_TYPE, \"cell_type\")\n",
    "        if name is not None:\n",
    "            base_name = base_name + f\"_{name}\"\n",
    "        category_fig.write_html(logdir / f\"{base_name}.html\")\n",
    "        category_fig.write_image(logdir / f\"{base_name}.svg\")\n",
    "        category_fig.write_image(logdir / f\"{base_name}.png\")\n",
    "\n",
    "        category_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task selection, sample ontology and assay_epiclass\n",
    "include_names = [\"sample_ontology\", \"assay_epiclass\"]\n",
    "exclude_names = [\"7c\", \"chip\", \"ct\"]\n",
    "\n",
    "selected_metrics = filter_feature_set(\n",
    "    selected_feature_sets_metrics, include_names, exclude_names\n",
    ")\n",
    "try:\n",
    "    selected_metrics[\"hg38_100kb_all_none\"][ASSAY] = selected_metrics[\n",
    "        \"hg38_100kb_all_none\"\n",
    "    ][f\"{ASSAY}_11c\"]\n",
    "    del selected_metrics[\"hg38_100kb_all_none\"][f\"{ASSAY}_11c\"]\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_metrics[\"hg38_100kb_all_none\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = (\n",
    "    base_fig_dir\n",
    "    / \"fig2_EpiAtlas_other\"\n",
    "    / \"fig2--reduced_feature_sets\"\n",
    "    / set_selection_name\n",
    ")\n",
    "logdir.mkdir(parents=False, exist_ok=True)\n",
    "logdir = logdir / \"global_average\"\n",
    "logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "print(logdir)\n",
    "\n",
    "for boxpoints in [\"all\", \"outliers\"]:\n",
    "    this_logdir = logdir / f\"{boxpoints}_points\"\n",
    "    this_logdir.mkdir(parents=False, exist_ok=True)\n",
    "    for y_min in [0, 0.1, 0.3, 0.7, 0.95]:\n",
    "        # continue\n",
    "        graph_feature_set_metrics(\n",
    "            all_metrics=selected_metrics,\n",
    "            input_sizes=input_sizes,\n",
    "            logdir=this_logdir,\n",
    "            sort_by_input_size=False,\n",
    "            y_range=(y_min, 1.001),\n",
    "            name=f\"y_min{y_min}\",\n",
    "            boxpoints=boxpoints,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_metric_sets_per_assay(\n",
    "    all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]], verbose: bool = False\n",
    ") -> Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]:\n",
    "    \"\"\"Prepare metric sets per assay.\n",
    "\n",
    "    Args:\n",
    "        all_results (Dict[str, Dict[str, Dict[str, pd.DataFrame]]]): A dictionary containing all results for all feature sets.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, Dict[str, float]]]]: A dictionary containing all metrics per assay for all feature sets.\n",
    "            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Loading metadata.\")\n",
    "    metadata = metadata_handler.load_metadata(\"v2\")\n",
    "    metadata.convert_classes(ASSAY, ASSAY_MERGE_DICT)\n",
    "    md5_per_assay = metadata.md5_per_class(ASSAY)\n",
    "    md5_per_assay = {k: set(v) for k, v in md5_per_assay.items()}\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Getting results per assay.\")\n",
    "    results_per_assay = {}\n",
    "    for assay_label in ASSAY_ORDER:\n",
    "        if verbose:\n",
    "            print(assay_label)\n",
    "        results_per_assay[assay_label] = {}\n",
    "        for feature_set, task_dict in all_results.items():\n",
    "            if verbose:\n",
    "                print(feature_set)\n",
    "            results_per_assay[assay_label][feature_set] = {}\n",
    "            for task_name, split_dict in task_dict.items():\n",
    "                # if not CELL_TYPE in task_name:\n",
    "                #     continue\n",
    "                if verbose:\n",
    "                    print(task_name)\n",
    "                results_per_assay[assay_label][feature_set][task_name] = {}\n",
    "\n",
    "                # Only keep the relevant assay\n",
    "                for split_name, split_df in split_dict.items():\n",
    "                    if verbose:\n",
    "                        print(split_name)\n",
    "                    assay_df = split_df[\n",
    "                        split_df.index.isin(md5_per_assay[assay_label])\n",
    "                    ].copy()\n",
    "                    results_per_assay[assay_label][feature_set][task_name][\n",
    "                        split_name\n",
    "                    ] = assay_df\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Finished getting results per assay. Now computing metrics.\")\n",
    "    metrics_per_assay = {}\n",
    "    for assay_label in ASSAY_ORDER:\n",
    "        if verbose:\n",
    "            print(assay_label)\n",
    "        metrics_per_assay[assay_label] = {}\n",
    "        for feature_set, task_dict in results_per_assay[assay_label].items():\n",
    "            if verbose:\n",
    "                print(feature_set)\n",
    "            assay_metrics = split_results_handler.compute_split_metrics(\n",
    "                task_dict, concat_first_level=True\n",
    "            )\n",
    "            inverted_dict = split_results_handler.invert_metrics_dict(assay_metrics)\n",
    "            metrics_per_assay[assay_label][feature_set] = inverted_dict\n",
    "\n",
    "    return metrics_per_assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_results = split_results_handler.obtain_all_feature_set_data(\n",
    "    parent_folder=gen_data_dir,\n",
    "    merge_assays=True,\n",
    "    return_type=\"split_results\",\n",
    "    include_sets=metric_orders_map[set_selection_name],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order results\n",
    "if set_selection_name:\n",
    "    selected_results = {\n",
    "        name: selected_results[name] for name in metric_orders_map[set_selection_name]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter results further\n",
    "selected_results = filter_feature_set(selected_results, include_names, exclude_names)\n",
    "\n",
    "try:\n",
    "    selected_results[\"hg38_100kb_all_none\"][ASSAY] = selected_results[\n",
    "        \"hg38_100kb_all_none\"\n",
    "    ][f\"{ASSAY}_11c\"]\n",
    "    del selected_results[\"hg38_100kb_all_none\"][f\"{ASSAY}_11c\"]\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity check file counts\n",
    "\n",
    "Verify that each task contains the same file count across feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_assay_files(metrics_per_assay) -> Dict[str, int]:\n",
    "    \"\"\"Compute the number of files per assay. Assume each task is the cell type.\"\"\"\n",
    "    feature_set_dict = defaultdict(int)\n",
    "    for assay, feature_set_metrics in metrics_per_assay.items():\n",
    "        for feature_set, task_metrics in feature_set_metrics.items():\n",
    "            for task_name, split_metrics in task_metrics.items():\n",
    "                for _, metrics in split_metrics.items():\n",
    "                    feature_set_dict[(feature_set, task_name, assay)] += metrics[\"count\"]\n",
    "\n",
    "    count_sets = {assay: set() for assay in ASSAY_ORDER}\n",
    "    for (feature_set, task_name, assay), count in feature_set_dict.items():\n",
    "        count_sets[assay].add(count)\n",
    "\n",
    "    task_names = set()\n",
    "    for (feature_set, task_name, assay), count in feature_set_dict.items():\n",
    "        task_names.add(task_name)\n",
    "\n",
    "    if len(task_names) != 1:\n",
    "        raise ValueError(f\"Expecting all tasks to be the same, but found {task_names}\")\n",
    "\n",
    "    for assay in ASSAY_ORDER:\n",
    "        if len(count_sets[assay]) != 1:\n",
    "            # Probably caused by min_PredScore threshold\n",
    "            raise ValueError(\n",
    "                f\"{assay} filecount has {len(count_sets[assay])} different values.\"\n",
    "            )\n",
    "\n",
    "    return {assay: count_sets[assay].pop() for assay in ASSAY_ORDER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logdir = (\n",
    "#     base_fig_dir\n",
    "#     / \"fig2_EpiAtlas_other\"\n",
    "#     / \"fig2--reduced_feature_sets\"\n",
    "#     / set_selection_name\n",
    "# )\n",
    "# logdir.mkdir(parents=False, exist_ok=True)\n",
    "# logdir = logdir / \"results_per_assay\"\n",
    "# logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "# y_min = 0.1\n",
    "# for assay in ASSAY_ORDER:\n",
    "#     assay_metrics = metrics_per_assay[assay]\n",
    "#     graph_feature_set_metrics(\n",
    "#         all_metrics=assay_metrics,\n",
    "#         input_sizes=input_sizes,\n",
    "#         logdir=logdir,\n",
    "#         sort_by_input_size=False,\n",
    "#         name=f\"{assay}_only_y_min{y_min}\",\n",
    "#         y_range=(y_min, 1.001),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute/graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_feature_set_metrics_per_assay(\n",
    "    all_metrics_per_assay: Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]],\n",
    "    input_sizes: Dict[str, int],\n",
    "    logdir: Path,\n",
    "    sort_by_input_size: bool = False,\n",
    "    name: str | None = None,\n",
    "    y_range: Tuple[float, float] | None = None,\n",
    "    boxpoints: str = \"all\",\n",
    "    spacing_multiplier: float = 0.6,\n",
    ") -> None:\n",
    "    \"\"\"Graph the metrics for all feature sets, per assay, with separate plots for accuracy and F1-score.\n",
    "\n",
    "    Args:\n",
    "        all_metrics_per_assay (Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]): A dictionary containing all metrics per assay for all feature sets.\n",
    "            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n",
    "        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n",
    "        logdir (Path): The directory where the figures will be saved.\n",
    "        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n",
    "        name (str|None): The name of the figure.\n",
    "        y_range (Tuple[float, float]|None): The y-axis range for the plots.\n",
    "        boxpoints (str): The type of points to display in the box plots. Defaults to \"all\".\n",
    "    \"\"\"\n",
    "    if boxpoints not in [\"all\", \"outliers\"]:\n",
    "        raise ValueError(\"Invalid boxpoints value.\")\n",
    "\n",
    "    fig_assay_order = [\n",
    "        \"rna_seq\",\n",
    "        \"h3k27ac\",\n",
    "        \"h3k4me1\",\n",
    "        \"h3k4me3\",\n",
    "        \"h3k36me3\",\n",
    "        \"h3k27me3\",\n",
    "        \"h3k9me3\",\n",
    "        \"input\",\n",
    "        \"wgbs\",\n",
    "    ]\n",
    "    try:\n",
    "        assay_counts = count_assay_files(all_metrics_per_assay)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        assay_counts = None\n",
    "\n",
    "    reference_assay = next(iter(all_metrics_per_assay))\n",
    "    reference_feature_set = next(iter(all_metrics_per_assay[reference_assay]))\n",
    "    metadata_categories = list(\n",
    "        all_metrics_per_assay[reference_assay][reference_feature_set].keys()\n",
    "    )\n",
    "\n",
    "    for _, category in enumerate(metadata_categories):\n",
    "        for metric, metric_name in [\n",
    "            (\"Accuracy\", \"Accuracy\"),\n",
    "            (\"F1_macro\", \"F1-score (macro)\"),\n",
    "        ]:\n",
    "            fig = go.Figure()\n",
    "\n",
    "            feature_sets = list(all_metrics_per_assay[reference_assay].keys())\n",
    "            unique_feature_sets = set(feature_sets)\n",
    "            for assay in fig_assay_order:\n",
    "                if set(all_metrics_per_assay[assay].keys()) != unique_feature_sets:\n",
    "                    raise ValueError(\"Different feature sets through assays.\")\n",
    "\n",
    "            feature_set_order = feature_sets\n",
    "            if sort_by_input_size:\n",
    "                feature_set_order = sorted(\n",
    "                    feature_set_order, key=lambda x: input_sizes[x]\n",
    "                )\n",
    "\n",
    "            # Modify the spacing between assay groups by using a larger multiplier\n",
    "            x_positions = {\n",
    "                assay: i * spacing_multiplier for i, assay in enumerate(fig_assay_order)\n",
    "            }\n",
    "\n",
    "            for i, feature_set_name in enumerate(feature_set_order):\n",
    "                resolution = (\n",
    "                    feature_set_name.replace(\"_none\", \"\")\n",
    "                    .replace(\"hg38_\", \"\")\n",
    "                    .split(\"_\")[0]\n",
    "                )\n",
    "                color = resolution_colors[resolution]\n",
    "                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n",
    "\n",
    "                for assay in fig_assay_order:\n",
    "                    if feature_set_name not in all_metrics_per_assay[assay]:\n",
    "                        continue\n",
    "\n",
    "                    tasks_dicts = all_metrics_per_assay[assay][feature_set_name]\n",
    "\n",
    "                    if feature_set_name not in input_sizes:\n",
    "                        print(f\"Skipping {feature_set_name}, no input size found.\")\n",
    "                        continue\n",
    "\n",
    "                    task_name = category\n",
    "                    if \"split\" in task_name:\n",
    "                        raise ValueError(\"Split in task name. Wrong metrics dict.\")\n",
    "\n",
    "                    try:\n",
    "                        task_dict = tasks_dicts[task_name]\n",
    "                    except KeyError:\n",
    "                        print(\n",
    "                            f\"Skipping {feature_set_name}, {task_name} for assay {assay}\"\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    y_vals = [task_dict[split][metric] for split in task_dict]\n",
    "                    hovertext = [\n",
    "                        f\"{assay} - {display_name} - {split}: {metrics_dict[metric]:.4f}\"\n",
    "                        for split, metrics_dict in task_dict.items()\n",
    "                    ]\n",
    "\n",
    "                    x_position = (\n",
    "                        x_positions[assay] + (i - len(feature_set_order) / 2 + 0.5) * 0.1\n",
    "                    )\n",
    "\n",
    "                    fig.add_trace(\n",
    "                        go.Box(\n",
    "                            x=[x_position] * len(y_vals),\n",
    "                            y=y_vals,\n",
    "                            name=f\"{assay}|{display_name}\",\n",
    "                            boxmean=True,\n",
    "                            boxpoints=boxpoints,\n",
    "                            marker=dict(size=3, color=\"black\"),\n",
    "                            line=dict(width=1, color=\"black\"),\n",
    "                            fillcolor=color,\n",
    "                            hovertemplate=\"%{text}\",\n",
    "                            text=hovertext,\n",
    "                            showlegend=False,\n",
    "                            legendgroup=display_name,\n",
    "                        )\n",
    "                    )\n",
    "                    # separate box groups\n",
    "                    # fig.add_vline(x=x_positions[assay]+1, line_width=1, line_color=\"black\")\n",
    "\n",
    "            # Add dummy traces for the legend\n",
    "            for feature_set_name in feature_set_order:\n",
    "                resolution = (\n",
    "                    feature_set_name.replace(\"_none\", \"\")\n",
    "                    .replace(\"hg38_\", \"\")\n",
    "                    .split(\"_\")[0]\n",
    "                )\n",
    "                color = resolution_colors[resolution]\n",
    "                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        name=display_name,\n",
    "                        x=[None],\n",
    "                        y=[None],\n",
    "                        mode=\"markers\",\n",
    "                        marker=dict(size=10, color=color),\n",
    "                        showlegend=True,\n",
    "                        legendgroup=display_name,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            title = f\"Neural network performance - {category} - {metric_name} (per assay)\"\n",
    "            if name is not None:\n",
    "                title += f\" - {name}\"\n",
    "            fig.update_layout(\n",
    "                width=1500,\n",
    "                height=1000,\n",
    "                title=title,\n",
    "                xaxis_title=\"Assay\",\n",
    "                yaxis_title=metric_name,\n",
    "            )\n",
    "\n",
    "            ticktext = list(x_positions.keys())\n",
    "            if assay_counts:\n",
    "                ticktext = [f\"{assay} (N={assay_counts[assay]})\" for assay in ticktext]\n",
    "            fig.update_xaxes(\n",
    "                tickmode=\"array\",\n",
    "                tickvals=list(x_positions.values()),\n",
    "                ticktext=ticktext,\n",
    "                title=\"Assay\",\n",
    "            )\n",
    "\n",
    "            fig.update_layout(\n",
    "                legend=dict(\n",
    "                    title=\"Feature Sets\", itemsizing=\"constant\", traceorder=\"normal\"\n",
    "                )\n",
    "            )\n",
    "            if y_range:\n",
    "                fig.update_yaxes(range=y_range)\n",
    "\n",
    "            base_name = f\"feature_set_metrics_{category}_{metric}_per_assay\"\n",
    "            if name is not None:\n",
    "                base_name = base_name + f\"_{name}\"\n",
    "            fig.write_html(logdir / f\"{base_name}.html\")\n",
    "            fig.write_image(logdir / f\"{base_name}.svg\")\n",
    "            fig.write_image(logdir / f\"{base_name}.png\")\n",
    "\n",
    "            fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuppressSpecificWarning(logging.Filter):\n",
    "    \"\"\"Suppress specific warnings.\"\"\"\n",
    "\n",
    "    def filter(self, record):\n",
    "        return \"Cannot compute ROC AUC\" not in record.getMessage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = (\n",
    "    base_fig_dir\n",
    "    / \"fig2_EpiAtlas_other\"\n",
    "    / \"fig2--reduced_feature_sets\"\n",
    "    / set_selection_name\n",
    ")\n",
    "logdir.mkdir(parents=False, exist_ok=True)\n",
    "logdir = logdir / \"results_per_assay\"\n",
    "logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "min_pred_score_filecounts = defaultdict(int)\n",
    "for min_predScore in [0, 0.6, 0.8]:\n",
    "    filtered_results = copy.deepcopy(selected_results)\n",
    "    for feature_set in filtered_results.keys():\n",
    "        for task_name in filtered_results[feature_set].keys():\n",
    "            for split, split_df in filtered_results[feature_set][task_name].items():\n",
    "                split_df = split_results_handler.add_max_pred(split_df)\n",
    "                filtered_split = split_df[split_df[\"Max pred\"] > min_predScore]\n",
    "                filtered_split = filtered_split.drop(\"Max pred\", axis=1)\n",
    "                filtered_results[feature_set][task_name][split] = filtered_split\n",
    "\n",
    "    # When computing metrics per assay for cell type, some assays can be missing certain cell types, rendering AUC computation impossible.\n",
    "    # This happens since the stratified split is stratified by cell type and not assay\n",
    "    with TemporaryLogFilter(SuppressSpecificWarning()):\n",
    "        metrics_per_assay = prepare_metric_sets_per_assay(filtered_results, verbose=False)\n",
    "\n",
    "    # Compute the number of files per assay\n",
    "    for assay, feature_sets in metrics_per_assay.items():\n",
    "        for feature_set, feature_set_metrics in feature_sets.items():\n",
    "            classifiers_N = split_results_handler.extract_count_from_metrics(\n",
    "                feature_set_metrics\n",
    "            )\n",
    "            for task_name, count in classifiers_N.items():\n",
    "                min_pred_score_filecounts[\n",
    "                    (feature_set, task_name, min_predScore, assay)\n",
    "                ] = count\n",
    "\n",
    "    for boxpoints in [\"all\", \"outliers\"]:\n",
    "        this_logdir = logdir / f\"{boxpoints}_points\"\n",
    "        this_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "        for y_min in [0, 0.3]:\n",
    "            # continue\n",
    "            graph_feature_set_metrics_per_assay(\n",
    "                all_metrics_per_assay=metrics_per_assay,\n",
    "                input_sizes=input_sizes,\n",
    "                logdir=this_logdir,\n",
    "                sort_by_input_size=False,\n",
    "                y_range=(y_min, 1.001),\n",
    "                name=f\"y_min={y_min} - min_predScore={min_predScore:.2f}\",\n",
    "                boxpoints=boxpoints,\n",
    "                spacing_multiplier=2,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_counts_records = [\n",
    "    (*keys, count) for keys, count in min_pred_score_filecounts.items()\n",
    "]\n",
    "file_counts_df = pd.DataFrame.from_records(\n",
    "    file_counts_records,\n",
    "    columns=[\"feature_set\", \"task_name\", \"min_predScore\", \"assay\", \"filecount\"],\n",
    ")\n",
    "file_counts_df.sort_values(\n",
    "    by=[\"feature_set\", \"task_name\", \"min_predScore\", \"assay\"], inplace=True\n",
    ")\n",
    "# file_counts_df.to_csv(logdir / \"file_counts_per_assay.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat results\n",
    "concat_feature_set_results = copy.deepcopy(selected_results)\n",
    "for feature_set, feature_set_results in list(concat_feature_set_results.items()):\n",
    "    concat_results: Dict[str, pd.DataFrame] = split_results_handler.concatenate_split_results(  # type: ignore\n",
    "        split_dfs=feature_set_results,\n",
    "        concat_first_level=True,\n",
    "    )\n",
    "    for task_name, df in list(concat_results.items()):\n",
    "        df = split_results_handler.add_max_pred(df)\n",
    "        df = metadata_handler.join_metadata(df, metadata_v2)\n",
    "        concat_results[task_name] = df\n",
    "\n",
    "    assert concat_results[CELL_TYPE].shape[0] == 16379\n",
    "\n",
    "    concat_feature_set_results[feature_set] = concat_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(concat_feature_set_results.keys())\n",
    "# print(concat_feature_set_results[\"hg38_100kb_all_none\"].keys())\n",
    "# print(concat_feature_set_results[\"hg38_100kb_all_none\"][CELL_TYPE].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = (\n",
    "    base_fig_dir\n",
    "    / \"fig2_EpiAtlas_other\"\n",
    "    / \"fig2--reduced_feature_sets\"\n",
    "    / set_selection_name\n",
    "    / \"prediction_scores\"\n",
    ")\n",
    "if not logdir.exists():\n",
    "    raise ValueError(\n",
    "        f\"Logdir {logdir} does not exist. Are you sure you want to compute this for this set selection?\"\n",
    "    )\n",
    "\n",
    "for feature_set_name, feature_set_results in concat_feature_set_results.items():\n",
    "    for task_name, task_results in feature_set_results.items():\n",
    "        if task_name != CELL_TYPE:\n",
    "            continue\n",
    "        pred_score_violin_alt(\n",
    "            results_df=task_results,\n",
    "            name=f\"{feature_set_name}-{task_name}-by_assay-per_file\",\n",
    "            use_aggregate_vote=False,\n",
    "            group_by_column=ASSAY,\n",
    "            min_y=0,\n",
    "            title=f\"{feature_set_name.replace('hg38_', '')} - {task_name}\",\n",
    "            logdir=logdir,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of zeroing blacklisted regions, and winzorizing input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data download\n",
    "\n",
    "~~~bash\n",
    "paper_dir=\"${HOME}/Projects/epiclass/output/paper/data\"\n",
    "cd ${paper_dir}/training_results/2023-01-epiatlas-freeze\n",
    "\n",
    "base_path=\"/lustre07/scratch/rabyj/epilap-logs/2023-01-epiatlas-freeze\"\n",
    "rsync --info=progress2 -a --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"validation_confusion*\" --exclude \"*.md5\" --exclude \"full*\" narval:${base_path}/hg38_100kb_all_none_0blklst* .\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLKLST_CATEGORIES = [\n",
    "    \"assay_epiclass\",\n",
    "    \"harmonized_biomaterial_type\",\n",
    "    \"harmonized_donor_sex\",\n",
    "    \"harmonized_sample_ontology_intermediate\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check oversampling\n",
    "\n",
    "Make sure oversampling is same in all training runs used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_2023_runs_oversampling():\n",
    "    \"\"\"Check if oversampling is on for all 2023 training runs used for blacklisted/winzorized metrics.\"\"\"\n",
    "    data_dir = base_data_dir / \"2023-01-epiatlas-freeze\"\n",
    "    for folder in data_dir.iterdir():\n",
    "        for category in BLKLST_CATEGORIES:\n",
    "            category_parent_folder = folder / f\"{category}_1l_3000n\"\n",
    "\n",
    "            if not category_parent_folder.exists():\n",
    "                raise FileNotFoundError(\"Cannot find: {category_parent_folder}\")\n",
    "\n",
    "            print(f\"Processing {category_parent_folder}\")\n",
    "\n",
    "            check_for_oversampling(category_parent_folder, verbose=False)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify_2023_runs_oversampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verify_2023_runs_oversampling result:\n",
    "Oversampling uniform across hdf5 types, but unsure across metadata categories.\n",
    "  - harmonized_biomaterial_type: On\n",
    "  - harmonized_sample_ontology_intermediate: On\n",
    "  - harmonized_donor_sex: Unknown, very probably On.\n",
    "    All nan values, but used human_longer.json hparams, which is the same as with the other runs that have oversampling on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blklst_split_metrics(\n",
    "    verbose: bool = False,\n",
    ") -> Dict[str, Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"Compute metrics on relevant categories and runs.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, float]]]: A dictionary containing all metrics for all blklst related feature sets.\n",
    "            Format: {feature_set: {task_name: {split_name: metric_dict}}}\n",
    "    \"\"\"\n",
    "    data_dir = base_data_dir / \"training_results\" / \"2023-01-epiatlas-freeze\"\n",
    "    feature_set_metrics_dict = {}\n",
    "    for folder in data_dir.iterdir():\n",
    "        if folder.is_file():\n",
    "            continue\n",
    "        feature_set_name = folder.name\n",
    "\n",
    "        tasks_dict = {}\n",
    "        for category in BLKLST_CATEGORIES:\n",
    "            category_parent_folder = folder / f\"{category}_1l_3000n\"\n",
    "\n",
    "            if not category_parent_folder.exists():\n",
    "                raise FileNotFoundError(\"Cannot find: {category_parent_folder}\")\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Processing {category_parent_folder}\")\n",
    "\n",
    "            for task_folder in category_parent_folder.iterdir():\n",
    "                if task_folder.is_file():\n",
    "                    continue\n",
    "                split_results = split_results_handler.read_split_results(task_folder)\n",
    "                general_name = f\"{category_parent_folder.name}-{task_folder.name}\"\n",
    "                tasks_dict[general_name] = split_results\n",
    "\n",
    "        feature_set_metrics = split_results_handler.compute_split_metrics(\n",
    "            tasks_dict, concat_first_level=True\n",
    "        )\n",
    "        feature_set_metrics_dict[\n",
    "            feature_set_name\n",
    "        ] = split_results_handler.invert_metrics_dict(feature_set_metrics)\n",
    "\n",
    "    return feature_set_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_set_metrics_dict = get_blklst_split_metrics(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blklst_graphs(\n",
    "    feature_set_metrics_dict: Dict[str, Dict[str, Dict[str, float]]], logdir: Path\n",
    ") -> None:\n",
    "    \"\"\"Create boxplots for blacklisted related feature sets.\n",
    "\n",
    "    Args:\n",
    "        feature_set_metrics_dict (Dict[str, Dict[str, Dict[str, float]]]): The dictionary containing all metrics for all blklst related feature sets.\n",
    "            format: {feature_set: {task_name: {split_name: metric_dict}}}\n",
    "    \"\"\"\n",
    "    # Assume names exist in all feature sets\n",
    "    task_names = list(feature_set_metrics_dict.values())[0].keys()\n",
    "\n",
    "    traces_names_dict = {\n",
    "        \"hg38_100kb_all_none\": \"observed\",\n",
    "        \"hg38_100kb_all_none_0blklst\": \"0blklst\",\n",
    "        \"hg38_100kb_all_none_0blklst_winsorized\": \"0blklst_winsorized\",\n",
    "    }\n",
    "\n",
    "    for task_name in task_names:\n",
    "        category_fig = make_subplots(\n",
    "            rows=1,\n",
    "            cols=2,\n",
    "            shared_yaxes=False,\n",
    "            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n",
    "            x_title=\"Feature set\",\n",
    "            y_title=\"Metric value\",\n",
    "            horizontal_spacing=0.03,\n",
    "        )\n",
    "        for feature_set_name, tasks_dicts in feature_set_metrics_dict.items():\n",
    "            task_dict = tasks_dicts[task_name]\n",
    "            trace_name = traces_names_dict[feature_set_name]\n",
    "\n",
    "            # Accuracy\n",
    "            metric = \"Accuracy\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "            metric = \"F1_macro\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=2,\n",
    "            )\n",
    "\n",
    "        category_fig.update_xaxes(\n",
    "            categoryorder=\"array\",\n",
    "            categoryarray=list(traces_names_dict.values()),\n",
    "        )\n",
    "        category_fig.update_yaxes(range=[0.85, 1.001])\n",
    "\n",
    "        task_name = task_name.replace(\"_1l_3000n-10fold\", \"\")\n",
    "        category_fig.update_layout(\n",
    "            title=f\"Neural network performance - {task_name} - 100kb\",\n",
    "        )\n",
    "\n",
    "        # Save figure\n",
    "        base_name = f\"metrics_{task_name}\"\n",
    "        category_fig.write_html(logdir / f\"{base_name}.html\")\n",
    "        category_fig.write_image(logdir / f\"{base_name}.svg\")\n",
    "        category_fig.write_image(logdir / f\"{base_name}.png\")\n",
    "\n",
    "        category_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--blklst_and_winsorized\" / \"y0.85\"\n",
    "logdir.mkdir(parents=False, exist_ok=True)\n",
    "# create_blklst_graphs(feature_set_metrics_dict, logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusions matrices per assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix_per_assay(\n",
    "    results_df: Dict[str, Dict[str, pd.DataFrame]],\n",
    "    metadata: Metadata,\n",
    "    logdir: Path,\n",
    "    min_pred: float = 0,\n",
    ") -> None:\n",
    "    \"\"\"Create confusion matrices for each assay. Exclusive to cell type results.\n",
    "\n",
    "    Args:\n",
    "        results_df (pd.DataFrame): The DataFrame containing concatenated prediction results (no metadata).\n",
    "        metadata (Metadata): The metadata object containing the cell type information.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "    \"\"\"\n",
    "    merged_results: Dict = split_results_handler.concatenate_split_results(\n",
    "        results_df, concat_first_level=True\n",
    "    )  # type: ignore\n",
    "    ct_results = merged_results[CELL_TYPE]\n",
    "    labels = ct_results.columns[2:].to_list()\n",
    "\n",
    "    augmented_ct_results = split_results_handler.add_max_pred(ct_results)\n",
    "    augmented_ct_results = augmented_ct_results[\n",
    "        augmented_ct_results[\"Max pred\"] >= min_pred\n",
    "    ]\n",
    "\n",
    "    augmented_ct_results = metadata_handler.join_metadata(ct_results, metadata)\n",
    "    augmented_ct_results[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # global conf matrix\n",
    "    pred = augmented_ct_results[\"Predicted class\"]\n",
    "    true = augmented_ct_results[\"True class\"]\n",
    "    conf_matrix = sk_cm(true, pred, labels=labels)\n",
    "    acc = accuracy_score(true, pred)\n",
    "    matrix_writer = ConfusionMatrixWriter(labels, conf_matrix)\n",
    "    matrix_writer.to_all_formats(logdir=logdir, name=f\"global_acc{acc*100:.4}\")\n",
    "\n",
    "    # per assay\n",
    "    for group in augmented_ct_results.groupby(ASSAY):\n",
    "        continue\n",
    "        assay = group[0]\n",
    "        assay_results = group[1]\n",
    "        pred = assay_results[\"Predicted class\"]\n",
    "        true = assay_results[\"True class\"]\n",
    "        conf_matrix = sk_cm(true, pred, labels=labels)\n",
    "        acc = accuracy_score(true, pred)\n",
    "        matrix_writer = ConfusionMatrixWriter(labels, conf_matrix)\n",
    "        matrix_writer.to_all_formats(logdir=logdir, name=f\"{assay}_acc{acc*100:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\n",
    "    \"assay\",\n",
    "    \"track_type\",\n",
    "    \"groups\",\n",
    "    \"disease\",\n",
    "    \"paired\",\n",
    "    \"life\",\n",
    "    \"sex\",\n",
    "    \"project\",\n",
    "    \"biomaterial\",\n",
    "    \"cancer\",\n",
    "]\n",
    "exclude_names = [\"chip-seq\", \"7c\"]\n",
    "\n",
    "hdf5_type = \"hg38_10mb_all_none_1mb_coord\"\n",
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "# all_split_results = split_results_handler.general_split_metrics(\n",
    "#     results_dir,\n",
    "#     merge_assays=True,\n",
    "#     exclude_categories=exclude_categories,\n",
    "#     exclude_names=exclude_names,\n",
    "#     return_type=\"split_results\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to make confusion matrices for each assay, and also rna/wgb merged\n",
    "base_conf_matrix_dir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"confusion_matrix_per_assay\"\n",
    "if not base_conf_matrix_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_conf_matrix_dir} does not exist.\")\n",
    "\n",
    "\n",
    "for hdf5_type in flagship_order_7cat:\n",
    "    continue\n",
    "    print(hdf5_type)\n",
    "    results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "    if not results_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "\n",
    "    conf_matrix_dir = base_conf_matrix_dir / hdf5_type / f\"{CELL_TYPE}_1l_3000n\"\n",
    "    conf_matrix_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # if len(list(conf_matrix_dir.glob(\"*.png\"))) > 0:\n",
    "    #     print(f\"Skipping {hdf5_type}, already exists.\")\n",
    "    #     continue\n",
    "\n",
    "    all_split_results = split_results_handler.general_split_metrics(\n",
    "        results_dir,\n",
    "        merge_assays=True,\n",
    "        exclude_categories=exclude_categories,\n",
    "        exclude_names=exclude_names,\n",
    "        return_type=\"split_results\",\n",
    "    )\n",
    "\n",
    "    for min_pred in [0, 0.6]:\n",
    "        this_logdir = conf_matrix_dir / f\"min_pred{min_pred}\"\n",
    "        this_logdir.mkdir(parents=False, exist_ok=True)\n",
    "        conf_matrix_per_assay(\n",
    "            results_df=all_split_results,  # type: ignore\n",
    "            metadata=metadata_v2,\n",
    "            logdir=this_logdir,\n",
    "            min_pred=min_pred,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
