{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create figures destined for the paper.\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    CELL_TYPE,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    merge_similar_assays,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig 2 - EpiClass results on EpiAtlas other metadata\n",
    "\n",
    "For all sub-figures 2+3 use v1.1 of sample metadata (called v2.1 internally)\n",
    "\n",
    "A) Histogram of performance (accuracy and F1 scores) for each category (using metadata v1)  \n",
    "B) Violin plot of average z-score on chrY per sex, black dots for pred same class and red for pred different class.  \n",
    "- Do the split male female violin per assay (only FC, merge 2xwgbs and 2xrna, no rna unique_raw). \n",
    "- Use scatter for points on each side, agree same color as violin, disagree other.\n",
    "- Point labels: uuid, epirr  \n",
    "\n",
    "C) ---  \n",
    "D) ---  \n",
    "E) SHAP cell-types GO  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all training runs were done with oversampling on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_results_dir = base_data_dir / \"dfreeze_v1\"\n",
    "if not v1_results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {v1_results_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_oversampling(base_data_dir: Path):\n",
    "    \"\"\"Check for oversampling status in the results.\n",
    "\n",
    "    Returns a ValeError if not all experiments have oversampling.\n",
    "    \"\"\"\n",
    "    # Identify experiments\n",
    "    exp_key_line = \"The current experiment key is\"\n",
    "    exp_keys_dict = defaultdict(list)\n",
    "    for category in v1_results_dir.iterdir():\n",
    "        for stdout_file in category.glob(\"*/output_job*.o\"):\n",
    "            with open(stdout_file, \"r\", encoding=\"utf8\") as f:\n",
    "                lines = [l.rstrip() for l in f if exp_key_line in l]\n",
    "            exp_keys = [l.split(exp_key_line)[1].strip() for l in lines]\n",
    "            exp_keys_dict[category.name].extend(exp_keys)\n",
    "\n",
    "    # Get all hparam values\n",
    "    gen_run_metadata = (\n",
    "        base_data_dir / \"all_results_cometml_filtered_oversampling-fixed.csv\"\n",
    "    )\n",
    "    run_metadata = pd.read_csv(gen_run_metadata, header=0)\n",
    "\n",
    "    # Check oversampling values\n",
    "    all_exp_keys = set()\n",
    "    for exp_keys in exp_keys_dict.values():\n",
    "        all_exp_keys.update(exp_keys)\n",
    "\n",
    "    df = run_metadata[run_metadata[\"experimentKey\"].isin(all_exp_keys)]\n",
    "    if not df[\"hparams/oversampling\"].all():\n",
    "        raise ValueError(\"Not all experiments have oversampling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_for_oversampling(base_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of performance (accuracy and F1 scores) for each category (using metadata v1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_a_content() -> Dict[str, Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"Create the content data for figure 2a. (get metrics for each task)\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, float]]] A metrics dictionary with the following structure:\n",
    "            {split_name: {task_name: metrics_dict}}\n",
    "    \"\"\"\n",
    "    metadata_v1 = MetadataHandler(paper_dir).load_metadata(\"v1\")\n",
    "    all_md5s = set(metadata_v1.md5s)\n",
    "\n",
    "    # Get the data\n",
    "    results_dir = base_data_dir / \"dfreeze_v1\"\n",
    "    split_results_handler = SplitResultsHandler()\n",
    "    split_results = split_results_handler.gather_split_results_across_categories(\n",
    "        results_dir\n",
    "    )\n",
    "\n",
    "    # Verify all md5sums are part of metadata v1\n",
    "    concat_results = split_results_handler.concatenate_split_results(\n",
    "        split_results, concat_first_level=True\n",
    "    )\n",
    "    for task_name, task_results in concat_results.items():\n",
    "        task_md5s = set(task_results.index)\n",
    "        if not task_md5s.issubset(all_md5s):\n",
    "            problematic_md5s = task_md5s - all_md5s\n",
    "            raise ValueError(\n",
    "                f\"Some md5s not in metadata v1 for {task_name}: {problematic_md5s}\"\n",
    "            )\n",
    "\n",
    "    split_results_metrics = split_results_handler.compute_split_metrics(\n",
    "        split_results, concat_first_level=True\n",
    "    )\n",
    "    return split_results_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_a(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    ") -> None:\n",
    "    \"\"\"Render box plots of metrics per classifier and split, each in its own subplot.\n",
    "\n",
    "    This function generates a figure with subplots, each representing a different\n",
    "    metric. Each subplot contains box plots for each classifier, ordered by accuracy.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A nested dictionary with structure {split: {classifier: {metric: score}}}.\n",
    "        logdir: The directory path to save the output plots.\n",
    "        name: The base name for the output plot files.\n",
    "    \"\"\"\n",
    "    metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "    classifier_names = list(next(iter(split_metrics.values())).keys())\n",
    "\n",
    "    # sort classifiers by accuracy\n",
    "    mean_acc = {}\n",
    "    for classifier in classifier_names:\n",
    "        mean_acc[classifier] = np.mean(\n",
    "            [split_metrics[split][classifier][\"Accuracy\"] for split in split_metrics]\n",
    "        )\n",
    "    classifier_names = sorted(classifier_names, key=lambda x: mean_acc[x], reverse=True)\n",
    "\n",
    "    # Create subplots, one row for each metric\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(metrics),\n",
    "        subplot_titles=metrics,\n",
    "        horizontal_spacing=0.03,\n",
    "    )\n",
    "\n",
    "    colors = {\n",
    "        classifier: px.colors.qualitative.Alphabet[i]\n",
    "        for i, classifier in enumerate(classifier_names)\n",
    "    }\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        for classifier_name in classifier_names:\n",
    "            values = [\n",
    "                split_metrics[split][classifier_name][metric] for split in split_metrics\n",
    "            ]\n",
    "\n",
    "            label_name = classifier_name\n",
    "            if classifier_name == \"random_10fold\":\n",
    "                label_name = \"random23c_10fold\"\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=values,\n",
    "                    name=label_name,\n",
    "                    fillcolor=colors[classifier_name],\n",
    "                    line=dict(color=\"black\", width=1),\n",
    "                    marker=dict(size=2),\n",
    "                    marker_color=colors[classifier_name],\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",  # or \"outliers\" to show only outliers\n",
    "                    pointpos=-1.4,\n",
    "                    showlegend=i == 0,  # Only show legend in the first subplot\n",
    "                    width=0.5,\n",
    "                    hoverinfo=\"text\",\n",
    "                    hovertext=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in zip(split_metrics, values)\n",
    "                    ],\n",
    "                    legendgroup=classifier_name,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=i + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Neural network classification - Metric distribution for 10-fold cross-validation\",\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode=\"group\",\n",
    "        height=1000,\n",
    "        width=1750,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_logdir = base_fig_dir / \"fig2\" / \"fig2_A\"\n",
    "fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "fig_name = \"fig2_A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_split_metrics = fig2_a_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split_name, split_metrics in all_split_metrics.items():\n",
    "#     print(split_name)\n",
    "#     for task_name, task_metrics in split_metrics.items():\n",
    "#         print(task_name)\n",
    "#         print(task_metrics)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2_a(all_split_metrics, fig_logdir, fig_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
