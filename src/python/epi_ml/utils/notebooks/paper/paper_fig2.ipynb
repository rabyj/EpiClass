{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create figures (fig2) destined for the paper.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Set, Tuple\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    extract_experiment_keys_from_output_files,\n",
    "    extract_input_sizes_from_output_files,\n",
    "    extract_node_jobs_from_error_files,\n",
    "    merge_similar_assays,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map\n",
    "sex_colors = IHECColorMap.sex_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()\n",
    "metadata_handler = MetadataHandler(paper_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_v2_df = metadata_handler.load_metadata_df(\"v2\", merge_assays=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_metadata_path = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"all_results_cometml_filtered_oversampling-fixed.csv\"\n",
    ")\n",
    "RUN_METADATA = pd.read_csv(parameters_metadata_path, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig 2 - EpiClass results on EpiAtlas other metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network performance across metadata categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if oversampling is uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_oversampling(parent_dir: Path, verbose: bool = False):\n",
    "    \"\"\"Check for oversampling status in the results, using \"output_job*.o\" files.\n",
    "    Args:\n",
    "        parent_dir (Path): Parent directory of the results. (classifier type level, e.g. assay_epiclass_1l_3000n)\n",
    "\n",
    "    \"\"\"\n",
    "    # Identify experiments\n",
    "    exp_keys_dict = extract_experiment_keys_from_output_files(parent_dir)\n",
    "\n",
    "    # Filter metadata to only include experiments in the results\n",
    "    all_exp_keys = set()\n",
    "    for exp_keys in exp_keys_dict.values():\n",
    "        all_exp_keys.update(exp_keys)\n",
    "\n",
    "    df = RUN_METADATA[RUN_METADATA[\"experimentKey\"].isin(all_exp_keys)]\n",
    "    df[\"general_name\"] = df[\"Name\"].str.replace(r\"[_-]?split\\d+$\", \"\", regex=True)\n",
    "    # print(df[[\"general_name\"] + [f\"run_arg_{i}\" for i in range(5)]].value_counts())\n",
    "\n",
    "    # Check oversampling values, ignore nan\n",
    "    df_na = df[df[\"hparams/oversampling\"].isna()]\n",
    "    df = df[df[\"hparams/oversampling\"].notna()]\n",
    "    if not (df[\"hparams/oversampling\"] == \"true\").all():\n",
    "        err_df = df.groupby([\"general_name\", \"hparams/oversampling\"]).agg(\"size\")\n",
    "        print(\n",
    "            \"Not all experiments have oversampling:\\n%s\",\n",
    "            err_df,\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Checked {len(exp_keys_dict)} folders and found {len(df)} oversampling values.\"\n",
    "    )\n",
    "    if len(df_na) != 0:\n",
    "        print(\n",
    "            \"Could not read oversampling value of all visited experiments. Values missing in:\"\n",
    "        )\n",
    "        print(df_na[[\"general_name\"] + [f\"run_arg_{i}\" for i in range(5)]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_for_oversampling(base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_10kb_all_none\" / \"assay_epiclass_1l_3000n\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_dir = (\n",
    "    base_data_dir\n",
    "    / \"dfreeze_v2\"\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / \"assay_epiclass_1l_3000n\"\n",
    "    / \"11c\"\n",
    ")\n",
    "ct_dir = (\n",
    "    base_data_dir\n",
    "    / \"dfreeze_v2\"\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / \"harmonized_sample_ontology_intermediate_1l_3000n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mislabel_corrector():\n",
    "    \"\"\"Obtain information necessary to correct sex and life_stage mislabels.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: {md5sum: EpiRR_no-v}\n",
    "        Dict[str, Dict[str, str]]: {label_category: {EpiRR_no-v: corrected_label}}\n",
    "    \"\"\"\n",
    "    epirr_no_v = \"EpiRR_no-v\"\n",
    "    # Associate epirrs to md5sums\n",
    "    metadata = MetadataHandler(paper_dir).load_metadata(\"v2\")\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n",
    "    md5sum_to_epirr = metadata_df.set_index(\"md5sum\")[epirr_no_v].to_dict()\n",
    "\n",
    "    # Load mislabels\n",
    "    epirr_to_corrections = {}\n",
    "    metadata_dir = base_data_dir / \"metadata\" / \"official\" / \"BadQual-mislabels\"\n",
    "\n",
    "    sex_mislabeled = pd.read_csv(metadata_dir / \"official_Sex_mislabeled.csv\")\n",
    "    epirr_to_corrections[SEX] = sex_mislabeled.set_index(epirr_no_v)[\n",
    "        \"EpiClass_pred_Sex\"\n",
    "    ].to_dict()\n",
    "\n",
    "    life_stage_mislabeled = pd.read_csv(\n",
    "        metadata_dir / \"official_Life_stage_mislabeled.csv\"\n",
    "    )\n",
    "    epirr_to_corrections[LIFE_STAGE] = life_stage_mislabeled.set_index(epirr_no_v)[\n",
    "        \"EpiClass_pred_Life_stage\"\n",
    "    ].to_dict()\n",
    "\n",
    "    return md5sum_to_epirr, epirr_to_corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_split_metrics(\n",
    "    results_dir: Path,\n",
    "    merge_assays: bool,\n",
    "    exclude_categories: List[str] | None = None,\n",
    "    exclude_names: List[str] | None = None,\n",
    ") -> Tuple[Dict[str, Dict[str, Dict[str, float]]], Dict[str, Dict[str, pd.DataFrame]]]:\n",
    "    \"\"\"Create the content data for figure 2a. (get metrics for each task)\n",
    "\n",
    "    Currently only using oversampled runs.\n",
    "\n",
    "    Args:\n",
    "        results_dir (Path): Directory containing the results. Needs to be parent over category folders.\n",
    "        merge_assays (bool): Merge similar assays (rna-seq x2, wgbs x2)\n",
    "        exclude_categories (List[str]): Task categories to exclude (first level directory names).\n",
    "        exclude_names (List[str]): Names of folders to exclude (ex: 7c or no-mix).\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, float]]] A metrics dictionary with the following structure:\n",
    "            {split_name: {task_name: metrics_dict}}\n",
    "        Dict[str, Dict[str, pd.DataFrame]] A split results dictionary with the following structure:\n",
    "            {task_name: {split_name: split_results_df}}\n",
    "    \"\"\"\n",
    "    all_split_results = {}\n",
    "    split_results_handler = SplitResultsHandler()\n",
    "\n",
    "    md5sum_to_epirr, epirr_to_corrections = create_mislabel_corrector()\n",
    "\n",
    "    for parent, _, _ in os.walk(results_dir):\n",
    "        # Looking for oversampling only results\n",
    "        parent = Path(parent)\n",
    "        if parent.name != \"10fold-oversampling\":\n",
    "            continue\n",
    "\n",
    "        # Get the category\n",
    "        relpath = parent.relative_to(results_dir)\n",
    "        category = relpath.parts[0].rstrip(\"_1l_3000n\")\n",
    "        if exclude_categories is not None:\n",
    "            if any(exclude_str in category for exclude_str in exclude_categories):\n",
    "                continue\n",
    "\n",
    "        # Get the rest of the name, ignore certain runs\n",
    "        rest_of_name = list(relpath.parts[1:])\n",
    "        rest_of_name.remove(\"10fold-oversampling\")\n",
    "\n",
    "        if len(rest_of_name) > 1:\n",
    "            raise ValueError(\n",
    "                f\"Too many parts in the name: {rest_of_name}. Path: {relpath}\"\n",
    "            )\n",
    "        if rest_of_name:\n",
    "            rest_of_name = rest_of_name[0]\n",
    "\n",
    "        if exclude_names is not None:\n",
    "            if any(name in rest_of_name for name in exclude_names):\n",
    "                continue\n",
    "\n",
    "        full_task_name = category\n",
    "        if rest_of_name:\n",
    "            full_task_name += f\"_{rest_of_name}\"\n",
    "\n",
    "        # Get the split results\n",
    "        split_results = split_results_handler.read_split_results(parent)\n",
    "        if not split_results:\n",
    "            raise ValueError(f\"No split results found in {parent}\")\n",
    "\n",
    "        if \"sex\" in full_task_name or \"life_stage\" in full_task_name:\n",
    "            corrections = epirr_to_corrections[category]\n",
    "            for split_name in split_results:\n",
    "                split_result_df = split_results[split_name]\n",
    "                current_true_class = split_result_df[\"True class\"].to_dict()\n",
    "                new_true_class = {\n",
    "                    k: corrections.get(md5sum_to_epirr[k], v)\n",
    "                    for k, v in current_true_class.items()\n",
    "                }\n",
    "                split_result_df[\"True class\"] = new_true_class.values()\n",
    "\n",
    "                split_results[split_name] = split_result_df\n",
    "\n",
    "        if (\"assay\" in full_task_name) and (\"11c\" in full_task_name) and merge_assays:\n",
    "            for split_name in split_results:\n",
    "                split_result_df = merge_similar_assays(split_results[split_name])\n",
    "                split_results[split_name] = split_result_df\n",
    "\n",
    "        all_split_results[full_task_name] = split_results\n",
    "\n",
    "    try:\n",
    "        split_results_metrics = split_results_handler.compute_split_metrics(\n",
    "            all_split_results, concat_first_level=True\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        logging.error(\"KeyError: %s\", e)\n",
    "        logging.error(\"all_split_results: %s\", all_split_results)\n",
    "        logging.error(\"check folder: %s\", results_dir)\n",
    "        raise e\n",
    "    return split_results_metrics, all_split_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=dangerous-default-value\n",
    "def fig2_a(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    exclude_categories: List[str] | None = None,\n",
    "    y_range: List[float] | None = None,\n",
    "    sort_by_acc: bool = False,\n",
    "    metric_names: List[str] = [\"Accuracy\", \"F1_macro\"],\n",
    "    show_plot: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Render box plots of metrics per classifier and split, each in its own subplot.\n",
    "\n",
    "    This function generates a figure with subplots, each representing a different\n",
    "    metric. Each subplot contains box plots for each classifier, ordered by accuracy.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A nested dictionary with structure {split: {classifier: {metric: score}}}.\n",
    "        logdir: The directory path to save the output plots.\n",
    "        name: The base name for the output plot files.\n",
    "        exclude_categories: Task categories to exclude from the plot.\n",
    "        y_range: The y-axis range for the plots.\n",
    "        sort_by_acc: Whether to sort the classifiers by accuracy.\n",
    "        metrics: The metrics to include in the plot.\n",
    "    \"\"\"\n",
    "    # Exclude some categories\n",
    "    classifier_names = list(split_metrics[\"split0\"].keys())\n",
    "    if exclude_categories is not None:\n",
    "        for category in exclude_categories:\n",
    "            classifier_names = [c for c in classifier_names if category not in c]\n",
    "\n",
    "    available_metrics = list(split_metrics[\"split0\"][classifier_names[0]].keys())\n",
    "    if any(metric not in available_metrics for metric in metric_names):\n",
    "        raise ValueError(f\"Invalid metric. Metrics need to be in {available_metrics}\")\n",
    "\n",
    "    # Sort classifiers by accuracy\n",
    "    if sort_by_acc:\n",
    "        mean_acc = {}\n",
    "        for classifier in classifier_names:\n",
    "            mean_acc[classifier] = np.mean(\n",
    "                [split_metrics[split][classifier][\"Accuracy\"] for split in split_metrics]\n",
    "            )\n",
    "        classifier_names = sorted(\n",
    "            classifier_names, key=lambda x: mean_acc[x], reverse=True\n",
    "        )\n",
    "\n",
    "    # Create subplots, one column for each metric\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(metric_names),\n",
    "        subplot_titles=metric_names,\n",
    "        horizontal_spacing=0.03,\n",
    "    )\n",
    "\n",
    "    color_group = px.colors.qualitative.Plotly\n",
    "    colors = {\n",
    "        classifier: color_group[i % len(color_group)]\n",
    "        for i, classifier in enumerate(classifier_names)\n",
    "    }\n",
    "\n",
    "    # point_pos = -1.35\n",
    "    point_pos = 0\n",
    "    for i, metric in enumerate(metric_names):\n",
    "        for classifier_name in classifier_names:\n",
    "            values = [\n",
    "                split_metrics[split][classifier_name][metric] for split in split_metrics\n",
    "            ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=values,\n",
    "                    name=classifier_name,\n",
    "                    fillcolor=colors[classifier_name],\n",
    "                    line=dict(color=\"black\", width=1.5),\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    pointpos=point_pos,\n",
    "                    showlegend=i == 0,  # Only show legend in the first subplot\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in zip(split_metrics, values)\n",
    "                    ],\n",
    "                    legendgroup=classifier_name,\n",
    "                    width=0.5,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=i + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Neural network classification - Metric distribution for 10-fold cross-validation\",\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode=\"group\",\n",
    "        height=1200 * 0.8,\n",
    "        width=1750 * 0.8,\n",
    "    )\n",
    "\n",
    "    # Acc, F1\n",
    "    range_acc = [0.86, 1.001]\n",
    "    fig.update_layout(yaxis=dict(range=range_acc))\n",
    "    fig.update_layout(yaxis2=dict(range=range_acc))\n",
    "\n",
    "    # AUC\n",
    "    range_auc = [0.986, 1.0001]\n",
    "    fig.update_layout(yaxis3=dict(range=range_auc))\n",
    "    fig.update_layout(yaxis4=dict(range=range_auc))\n",
    "\n",
    "    if y_range is not None:\n",
    "        fig.update_yaxes(range=y_range)\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    if show_plot:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_imbalance(\n",
    "    all_split_results: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute class imbalance for each task and split.\n",
    "\n",
    "    Args:\n",
    "        all_split_results: A dictionary with structure {task_name: {split_name: split_results_df}}.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the following columns:\n",
    "            - avg(balance_ratio): The average balance ratio for each task.\n",
    "            - n: The number of classes for each task (used for the average).\n",
    "    \"\"\"\n",
    "    # combine md5 lists\n",
    "    task_md5s = {\n",
    "        classifier_task: [split_df.index for split_df in split_results.values()]\n",
    "        for classifier_task, split_results in all_split_results.items()\n",
    "    }\n",
    "    task_md5s = {\n",
    "        classifier_task: [list(split_md5s) for split_md5s in md5s]\n",
    "        for classifier_task, md5s in task_md5s.items()\n",
    "    }\n",
    "    task_md5s = {\n",
    "        classifier_task: list(itertools.chain(*md5s))\n",
    "        for classifier_task, md5s in task_md5s.items()\n",
    "    }\n",
    "\n",
    "    # get metadata\n",
    "    metadata_df = metadata_handler.load_metadata_df(\"v2-encode\")\n",
    "\n",
    "    label_counts = {}\n",
    "    for classifier_task, md5s in task_md5s.items():\n",
    "        try:\n",
    "            label_counts[classifier_task] = metadata_df.loc[md5s][\n",
    "                classifier_task\n",
    "            ].value_counts()\n",
    "        except KeyError as e:\n",
    "            category_name = classifier_task.rsplit(\"_\", maxsplit=1)[0]\n",
    "            try:\n",
    "                label_counts[classifier_task] = metadata_df.loc[md5s][\n",
    "                    category_name\n",
    "                ].value_counts()\n",
    "            except KeyError as e:\n",
    "                raise e\n",
    "\n",
    "    # Compute average class ratio vs majority class\n",
    "    # class_ratios = {}\n",
    "    # for classifier_task, counts in label_counts.items():\n",
    "    #     class_ratios[classifier_task] = (np.mean(counts / max(counts)), len(counts))\n",
    "\n",
    "    # Compute Shannon Entropy\n",
    "    class_balance = {}\n",
    "    for classifier_task, counts in label_counts.items():\n",
    "        total_count = counts.sum()\n",
    "        k = len(counts)\n",
    "        p_x = counts / total_count  # class proportions\n",
    "        p_x = p_x.values\n",
    "        shannon_entropy = -np.sum(p_x * np.log2(p_x))\n",
    "        balance = shannon_entropy / np.log2(k)\n",
    "        class_balance[classifier_task] = (balance, k)\n",
    "\n",
    "    df_class_balance = pd.DataFrame.from_dict(\n",
    "        class_balance, orient=\"index\", columns=[\"Normalized Shannon Entropy\", \"k\"]\n",
    "    ).sort_index()\n",
    "\n",
    "    return df_class_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdf5_type = \"hg38_100kb_all_none\"\n",
    "# results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "# if not results_dir.exists():\n",
    "#     raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "# _, all_split_results = general_split_metrics(\n",
    "# results_dir, exclude_categories=None, exclude_names=exclude_names, merge_assays=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\"\n",
    "# df_class_balance = compute_class_imbalance(all_split_results)\n",
    "# df_class_balance.to_csv(fig_logdir / \"class_balance_Shannon.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph performance per metadata category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\"track_type\", \"groups\"]\n",
    "exclude_names = [\"chip-seq\", \"7c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_type = \"hg38_100kb_all_none\"\n",
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "split_results_metrics, all_split_results = general_split_metrics(\n",
    "    results_dir,\n",
    "    merge_assays=True,\n",
    "    exclude_categories=exclude_categories,\n",
    "    exclude_names=exclude_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\"\n",
    "fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "fig_name = f\"{hdf5_type}_perf_across_categories_full_internal\"\n",
    "\n",
    "metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_micro\"]\n",
    "# fig2_a(\n",
    "#     split_results_metrics,\n",
    "#     fig_logdir,\n",
    "#     fig_name,\n",
    "#     sort_by_acc=True,\n",
    "#     metric_names=metrics,\n",
    "#     exclude_categories=None,\n",
    "#     show_plot=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\"\n",
    "fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "metrics_full = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "metrics_AUC = [\"AUC_micro\", \"AUC_macro\"]\n",
    "metrics_acc_F1 = [\"Accuracy\", \"F1_macro\"]\n",
    "exclude_categories = [\"sex_no-mixed\", \"disease\"]\n",
    "y_range_AUC = [0.986, 1.0001]\n",
    "y_range_acc = [0.86, 1.001]\n",
    "\n",
    "for name, metrics, y_range in zip(\n",
    "    [\"full\", \"acc_F1\", \"AUC\"],\n",
    "    [metrics_full, metrics_acc_F1, metrics_AUC],\n",
    "    [None, y_range_acc, y_range_AUC],\n",
    "):\n",
    "    fig_name = f\"{hdf5_type}_perf_across_categories_{name}\"\n",
    "    # fig2_a(\n",
    "    #     split_results_metrics,\n",
    "    #     fig_logdir,\n",
    "    #     fig_name,\n",
    "    #     sort_by_acc=True,\n",
    "    #     metric_names=metrics,\n",
    "    #     exclude_categories=exclude_categories,\n",
    "    #     show_plot=False,\n",
    "    #     y_range=y_range,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--NN_perf_across_categories\"\n",
    "fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "metrics_acc_F1 = [\"Accuracy\", \"F1_macro\"]\n",
    "fig_name = f\"{hdf5_type}_perf_across_categories_acc_F1\"\n",
    "# fig2_a(\n",
    "#     split_results_metrics,\n",
    "#     fig_logdir,\n",
    "#     fig_name,\n",
    "#     sort_by_acc=True,\n",
    "#     metric_names=metrics_acc_F1,\n",
    "#     exclude_categories=exclude_categories,\n",
    "#     show_plot=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network performance per assay across metadata categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_performance_per_assay_across_categories(\n",
    "    all_split_results: Dict[str, Dict[str, pd.DataFrame]],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    title_end: str = \"\",\n",
    "    exclude_categories: List[str] | None = None,\n",
    "    y_range: None | List[float] = None,\n",
    "):\n",
    "    \"\"\"Create a box plot of assay accuracy for each classifier.\"\"\"\n",
    "    all_split_results = copy.deepcopy(all_split_results)\n",
    "\n",
    "    # Exclude some categories\n",
    "    classifier_names = list(all_split_results.keys())\n",
    "    if exclude_categories is not None:\n",
    "        for category in exclude_categories:\n",
    "            classifier_names = [c for c in classifier_names if category not in c]\n",
    "\n",
    "    metadata_df = MetadataHandler(paper_dir).load_metadata_df(\"v2-encode\")\n",
    "\n",
    "    # One graph per metadata category\n",
    "    for task_name in classifier_names:\n",
    "        split_results = all_split_results[task_name]\n",
    "        if ASSAY in task_name:\n",
    "            for split_name in split_results:\n",
    "                split_results[split_name] = merge_similar_assays(\n",
    "                    split_results[split_name]\n",
    "                )\n",
    "\n",
    "        assay_acc_df = split_results_handler.compute_acc_per_assay(\n",
    "            split_results, metadata_df\n",
    "        )\n",
    "\n",
    "        fig = go.Figure()\n",
    "        for assay in ASSAY_ORDER:\n",
    "            try:\n",
    "                assay_accuracies = assay_acc_df[assay]\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=assay_accuracies.values,\n",
    "                    name=assay,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=True,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=assay_colors[assay],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in assay_accuracies.items()\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # if \"sample_ontology\" in task_name:\n",
    "        #     yrange = [0.59, 1.001]\n",
    "        # elif ASSAY in task_name:\n",
    "        #     yrange = [0.985, 1.001]\n",
    "        # else:\n",
    "        yrange = [assay_acc_df.min(), 1.001]  # type: ignore\n",
    "\n",
    "        if y_range is not None:\n",
    "            yrange = y_range\n",
    "\n",
    "        fig.update_yaxes(range=yrange)\n",
    "\n",
    "        title_text = f\"NN classification - {task_name}\"\n",
    "        if title_end:\n",
    "            title_text += f\" - {title_end}\"\n",
    "        fig.update_layout(\n",
    "            title_text=title_text,\n",
    "            yaxis_title=\"Accuracy\",\n",
    "            xaxis_title=\"Assay\",\n",
    "        )\n",
    "\n",
    "        # Save figure\n",
    "        this_name = name + f\"_{task_name}\"\n",
    "        fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "        fig.write_image(logdir / f\"{this_name}.png\")\n",
    "        fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\"track_type\", \"groups\", \"disease\"]\n",
    "exclude_names = [\"chip-seq\", \"7c\", \"no-mixed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 303114\n",
    "# N = 30321\n",
    "hdf5_type = f\"hg38_regulatory_regions_n{N}\"\n",
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "\n",
    "split_results_metrics, all_split_results = general_split_metrics(\n",
    "    results_dir,\n",
    "    merge_assays=True,\n",
    "    exclude_categories=exclude_categories,\n",
    "    exclude_names=exclude_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_split_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = (\n",
    "    base_fig_dir\n",
    "    / \"fig2_EpiAtlas_other\"\n",
    "    / \"fig2--NN_perf_across_categories\"\n",
    "    / \"per_assay\"\n",
    "    / hdf5_type\n",
    ")\n",
    "logdir.mkdir(parents=False, exist_ok=True)\n",
    "fig_name = \"perf_per_assay\"\n",
    "\n",
    "# exclude_categories = None\n",
    "# exclude_categories = [\"groups\", \"track_type\", \"harmonized\", \"project\", \"paired\"] # only assay_epiclass\n",
    "\n",
    "# NN_performance_per_assay_across_categories(all_split_results, logdir, fig_name, exclude_categories, y_range=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network performance per assay, scatterplot\n",
    "\n",
    "model_X split_n vs model_Y split_n for all n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_performance_scatterplot(\n",
    "    all_split_results: Dict[str, Dict[str, pd.DataFrame]],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    label_category: str,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    For the two given classification tasks split results (need to be from same category),\n",
    "    create a scatter plot of split performance per assay, split for split.\n",
    "\n",
    "    Args:\n",
    "        all_split_results: A dictionary with structure {task_name: {split_name: split_results_df}}.\n",
    "        logdir (Path): The directory path to save the output plots.\n",
    "        name (str): The base name for the output plot files.\n",
    "        label_category (str): category used for labels, used for title and axis labels.\n",
    "        verbose (bool): Print more information.\n",
    "    \"\"\"\n",
    "    all_split_results = copy.deepcopy(all_split_results)\n",
    "    metadata_df = MetadataHandler(paper_dir).load_metadata_df(\n",
    "        \"v2-encode\", merge_assays=True\n",
    "    )\n",
    "\n",
    "    for task_name_1, task_name_2 in itertools.combinations(all_split_results.keys(), 2):\n",
    "        if verbose:\n",
    "            print(task_name_1, task_name_2)\n",
    "        split_results_1 = all_split_results[task_name_1]\n",
    "        split_results_2 = all_split_results[task_name_2]\n",
    "\n",
    "        if ASSAY in task_name_1:\n",
    "            for split_name in split_results_1:\n",
    "                split_results_1[split_name] = merge_similar_assays(\n",
    "                    split_results_1[split_name]\n",
    "                )\n",
    "                split_results_2[split_name] = merge_similar_assays(\n",
    "                    split_results_2[split_name]\n",
    "                )\n",
    "\n",
    "        if split_results_1[\"split0\"].shape != split_results_2[\"split0\"].shape:\n",
    "            raise ValueError(\n",
    "                f\"Split results for {task_name_1} and {task_name_2} do not have the same shape: {split_results_1['split0'].shape} != {split_results_2['split0'].shape}\"\n",
    "            )\n",
    "        assay_acc_df_1 = split_results_handler.compute_acc_per_assay(\n",
    "            split_results_1, metadata_df\n",
    "        )\n",
    "        assay_acc_df_2 = split_results_handler.compute_acc_per_assay(\n",
    "            split_results_2, metadata_df\n",
    "        )\n",
    "\n",
    "        fig = go.Figure()\n",
    "        min_x = 1\n",
    "        min_y = 1\n",
    "        for assay in ASSAY_ORDER:\n",
    "            if verbose:\n",
    "                print(assay)\n",
    "            try:\n",
    "                assay_accuracies_1 = assay_acc_df_1[assay]\n",
    "                assay_accuracies_2 = assay_acc_df_2[assay]\n",
    "            except KeyError as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{task_name_1}: {assay_accuracies_1}\")\n",
    "                print(f\"{task_name_2}: {assay_accuracies_2}\")\n",
    "\n",
    "            hovertext = [\n",
    "                f\"{split}: ({assay_accuracies_1[split]:.4f},{assay_accuracies_2[split]:.4f})\"\n",
    "                for split in assay_accuracies_1.keys()\n",
    "            ]\n",
    "\n",
    "            x_gt_y = sum(assay_accuracies_1 > assay_accuracies_2)\n",
    "            y_gt_x = sum(assay_accuracies_1 < assay_accuracies_2)\n",
    "            trace_name = f\"{assay} ({y_gt_x},{x_gt_y})\"\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=assay_accuracies_1.values,\n",
    "                    y=assay_accuracies_2.values,\n",
    "                    mode=\"markers\",\n",
    "                    name=trace_name,\n",
    "                    marker=dict(size=5, color=assay_colors[assay]),\n",
    "                    text=hovertext,\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "            min_x = min(min_x, *assay_accuracies_1.values)\n",
    "            min_y = min(min_y, *assay_accuracies_2.values)\n",
    "\n",
    "        # diagonal line\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[0, 1],\n",
    "                y=[0, 1],\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        range_x = 1 - min_x\n",
    "        range_y = 1 - min_y\n",
    "        fig.update_xaxes(range=[min_x - 0.01 * range_x, 1 + 0.01 * range_x])\n",
    "        fig.update_yaxes(range=[min_y - 0.01 * range_y, 1 + 0.01 * range_y])\n",
    "\n",
    "        x_name = task_name_1.replace(f\"_{label_category}\", \"\")\n",
    "        y_name = task_name_2.replace(f\"_{label_category}\", \"\")\n",
    "        fig.update_layout(\n",
    "            title_text=f\"Neural network classification - {label_category} - 10-fold cross-validation\",\n",
    "            xaxis_title=f\"{x_name} accuracy\",\n",
    "            yaxis_title=f\"{y_name} accuracy\",\n",
    "        )\n",
    "\n",
    "        fig.update_layout(legend_title_text=\"Assay: (y>x, x>y)\")\n",
    "\n",
    "        # Save figure\n",
    "        this_name = f\"{name}-{label_category}-{x_name}_VS_{y_name}\"\n",
    "        this_name = this_name.replace(ASSAY, \"assay\")\n",
    "        this_name = this_name.replace(CELL_TYPE, \"sample_ontology\")\n",
    "        fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "        fig.write_image(logdir / f\"{this_name}.png\")\n",
    "        fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "        # fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\"track_type\", \"groups\", \"disease\"]\n",
    "exclude_names = [\"chip-seq\", \"7c\", \"no-mixed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = 303114\n",
    "N2 = 30321\n",
    "hdf5_type_reg1 = f\"hg38_regulatory_regions_n{N1}\"\n",
    "hdf5_type_reg2 = f\"hg38_regulatory_regions_n{N2}\"\n",
    "hdf5_type_100kb = \"hg38_100kb_all_none\"\n",
    "hdf5_type_10kb = \"hg38_10kb_all_none\"\n",
    "\n",
    "scatter_fig_results = {}\n",
    "for hdf5_type in [hdf5_type_reg1, hdf5_type_reg2, hdf5_type_100kb, hdf5_type_10kb]:\n",
    "    results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\n",
    "    if not results_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {results_dir} does not exist.\")\n",
    "\n",
    "    _, all_split_results = general_split_metrics(\n",
    "        results_dir,\n",
    "        merge_assays=True,\n",
    "        exclude_categories=exclude_categories,\n",
    "        exclude_names=exclude_names,\n",
    "    )\n",
    "\n",
    "    scatter_fig_results.update(\n",
    "        {\n",
    "            f\"{hdf5_type}_{task_name}\": split_results\n",
    "            for task_name, split_results in all_split_results.items()\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_category in [ASSAY, CELL_TYPE]:\n",
    "    results = {k: v for k, v in scatter_fig_results.items() if label_category in k}\n",
    "    pairwise_performance_scatterplot(\n",
    "        results,\n",
    "        logdir=base_fig_dir / \"flagship\" / \"pairwise_scatterplot_acc\" / label_category,\n",
    "        name=\"acc_per_assay\",\n",
    "        label_category=label_category,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track type effect on NN performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "assay_parent_dir = parent_dir / \"assay_epiclass_1l_3000n\" / \"11c\"\n",
    "ct_parent_dir = parent_dir / \"harmonized_sample_ontology_intermediate_1l_3000n\"\n",
    "\n",
    "assay_results = {\n",
    "    folder.name: split_results_handler.read_split_results(folder)\n",
    "    for folder in assay_parent_dir.iterdir()\n",
    "    if \"chip\" not in folder.name\n",
    "}\n",
    "ct_results = {\n",
    "    folder.name: split_results_handler.read_split_results(folder)\n",
    "    for folder in ct_parent_dir.iterdir()\n",
    "    if \"l1\" not in folder.name\n",
    "}\n",
    "\n",
    "_ = assay_results.pop(\"10fold-oversampling\")\n",
    "_ = ct_results.pop(\"10fold-oversampling\")\n",
    "_ = ct_results.pop(\"10fold-oversampling_chip-seq-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_assay_results = copy.deepcopy(assay_results)\n",
    "for task_name, split_dfs in list(corrected_assay_results.items()):\n",
    "    for split_name in split_dfs:\n",
    "        split_dfs[split_name] = merge_similar_assays(split_dfs[split_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_metrics = split_results_handler.compute_split_metrics(\n",
    "    corrected_assay_results, concat_first_level=True\n",
    ")\n",
    "ct_metrics = split_results_handler.compute_split_metrics(\n",
    "    ct_results, concat_first_level=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\"\n",
    "name = f\"{ASSAY}_global_track_type_effect\"\n",
    "# fig2_a(\n",
    "#     assay_metrics,\n",
    "#     logdir,\n",
    "#     name,\n",
    "#     exclude_categories=None,\n",
    "#     y_range=[0.99, 1.0001],\n",
    "#     sort_by_acc=False,\n",
    "# )\n",
    "\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\"\n",
    "name = f\"{CELL_TYPE}_global_track_type_effect\"\n",
    "# fig2_a(\n",
    "#     ct_metrics,\n",
    "#     logdir,\n",
    "#     name,\n",
    "#     exclude_categories=None,\n",
    "#     y_range=[0.91, 1.001],\n",
    "#     sort_by_acc=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"{ASSAY}_global_track_type_effect_per_assay\"\n",
    "# NN_performance_per_assay_across_categories(\n",
    "#     corrected_assay_results, logdir, name, exclude_categories=None, y_range=[0.96, 1.001]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_keep_core_assays(\n",
    "    results_dfs: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"Exclude non core-assays from split results. Also exclude input.\"\"\"\n",
    "    accepted_assays = ASSAY_ORDER[0:-3]\n",
    "    new_results = copy.deepcopy(results_dfs)\n",
    "    for task_name, split_dfs in list(new_results.items()):\n",
    "        for split_name in split_dfs:\n",
    "            df = split_dfs[split_name]\n",
    "            if ASSAY not in df.columns:\n",
    "                merged_df = df.merge(\n",
    "                    metadata_v2_df, how=\"left\", left_index=True, right_index=True\n",
    "                )\n",
    "                df = df[merged_df[ASSAY].isin(accepted_assays)]\n",
    "                new_results[task_name][split_name] = df\n",
    "    return new_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recompute metrics considering only histones\n",
    "# for result_df, category_name, y_range in zip(\n",
    "#     [corrected_assay_results, ct_results],\n",
    "#     [ASSAY, CELL_TYPE],\n",
    "#     [[0.85, 1.001], [0.91, 1.001]],\n",
    "# ):\n",
    "#     print(category_name)\n",
    "#     name = f\"{category_name}_core6c_track_type_effect\"\n",
    "\n",
    "#     core_result_df = only_keep_core_assays(result_df)\n",
    "#     metrics = split_results_handler.compute_split_metrics(\n",
    "#         core_result_df, concat_first_level=True\n",
    "#     )\n",
    "\n",
    "#     fig2_a(metrics, logdir, name, exclude_categories=None, y_range=y_range)\n",
    "\n",
    "#     if category_name == ASSAY:\n",
    "#         name = f\"{ASSAY}_core6_track_type_effect_per_assay\"\n",
    "#         NN_performance_per_assay_across_categories(\n",
    "#             core_result_df, logdir, name, exclude_categories=None, y_range=[0.97, 1.001]\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex chrY z-score distribution vs predictions\n",
    "\n",
    "Violin plot of average z-score on chrY per sex, black dots for pred same class and red for pred different class.  \n",
    "\n",
    "- Do the split male female violin per assay (only FC, merge 2xwgbs and 2xrna, no rna unique_raw). \n",
    "- Use scatter for points on each side, agree same color as violin, disagree other.\n",
    "- Point labels: uuid, epirr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute chrY coverage z-score VS assay distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chrY_zscores(version: str):\n",
    "    \"\"\"Compute z-scores for chrY coverage data, per assay distribution.\n",
    "\n",
    "    Excludes raw, pval, and Unique_raw tracks.\n",
    "    \"\"\"\n",
    "    # Get chrY coverage data\n",
    "    chrY_coverage_dir = base_data_dir / \"chrY_coverage\"\n",
    "    if not chrY_coverage_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {chrY_coverage_dir} does not exist.\")\n",
    "    chrY_coverage_df = pd.read_csv(chrY_coverage_dir / \"chrXY_coverage_all.csv\", header=0)\n",
    "\n",
    "    # Filter out md5s not in metadata version\n",
    "    metadata = MetadataHandler(paper_dir).load_metadata(version)\n",
    "    md5s = set(metadata.md5s)\n",
    "    chrY_coverage_df = chrY_coverage_df[chrY_coverage_df[\"filename\"].isin(md5s)]\n",
    "\n",
    "    # Make sure all values are non-zero\n",
    "    assert (chrY_coverage_df[\"chrY\"] != 0).all()\n",
    "\n",
    "    # These tracks are excluded from z-score computation\n",
    "    metadata.remove_category_subsets(\"track_type\", [\"raw\", \"pval\", \"Unique_raw\"])\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n",
    "    metadata_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # Merge with metadata\n",
    "    chrY_coverage_df = chrY_coverage_df.merge(\n",
    "        metadata_df[[\"md5sum\", ASSAY]], left_on=\"filename\", right_on=\"md5sum\"\n",
    "    )\n",
    "\n",
    "    # Compute z-score per assay\n",
    "    chrY_dists = chrY_coverage_df.groupby(ASSAY).agg({\"chrY\": [\"mean\", \"std\", \"count\"]})\n",
    "\n",
    "    output_dir = chrY_coverage_dir / f\"dfreeze_{version}_stats\"\n",
    "    output_dir.mkdir(parents=False, exist_ok=True)\n",
    "    chrY_dists.to_csv(output_dir / \"chrY_coverage_stats.csv\")\n",
    "\n",
    "    # Compute z-score per assay group, merge back into the dataframe, save results\n",
    "    metric_name = \"chrY_zscore_vs_assay\"\n",
    "    groupby_df = chrY_coverage_df.groupby(ASSAY)\n",
    "    for _, group in groupby_df:\n",
    "        group[\"chrY_zscore\"] = zscore(group[\"chrY\"])\n",
    "        chrY_coverage_df.loc[group.index, metric_name] = group[\"chrY_zscore\"]\n",
    "\n",
    "    output_cols = [\"filename\", \"chrY\", metric_name, ASSAY]\n",
    "    chrY_coverage_df[output_cols].to_csv(\n",
    "        output_dir / \"chrY_coverage_zscore_vs_assay.csv\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chrY_zscores(\"v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot z-scores according to sex\n",
    "\n",
    "main Fig: chrY per EpiRR (excluding WGBS): only boxplot with all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_label = \"chrY_zscore_vs_assay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fig_2B_data(version: str, prediction_data_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Prepare data for figure 2b.\"\"\"\n",
    "    # Load metadata\n",
    "    meta_cols = [\"md5sum\", \"EpiRR\", SEX]\n",
    "    metadata = MetadataHandler(paper_dir).load_metadata(version)\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n",
    "    metadata_df = metadata_df[meta_cols]\n",
    "\n",
    "    # Load z-score data\n",
    "    zscore_dir = base_data_dir / \"chrY_coverage\" / f\"dfreeze_{version}_stats\"\n",
    "    zscore_df = pd.read_csv(zscore_dir / \"chrY_coverage_zscore_vs_assay.csv\", header=0)\n",
    "\n",
    "    # Load NN predictions\n",
    "    split_results = split_results_handler.read_split_results(prediction_data_dir)\n",
    "    pred_df = split_results_handler.concatenate_split_results(\n",
    "        {\"sex\": split_results}, concat_first_level=True\n",
    "    )[\"sex\"]\n",
    "\n",
    "    # Merge all\n",
    "    zscore_df = zscore_df.merge(metadata_df, left_on=\"filename\", right_on=\"md5sum\")\n",
    "    zscore_df = zscore_df.merge(pred_df, left_on=\"filename\", right_index=True)\n",
    "    zscore_df[\"Max pred\"] = zscore_df[[\"female\", \"male\", \"mixed\"]].max(axis=1)\n",
    "    zscore_df.set_index(\"md5sum\", inplace=True)\n",
    "    return zscore_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_B(zscore_df: pd.DataFrame, logdir: Path, name: str) -> None:\n",
    "    \"\"\"Create figure 2B.\n",
    "\n",
    "    Args:\n",
    "        zscore_df: The dataframe with z-score data.\n",
    "    \"\"\"\n",
    "    assay_sizes = zscore_df[ASSAY].value_counts()\n",
    "    assays = sorted(assay_sizes.index)\n",
    "\n",
    "    # x_title = \"Assay+Sex z-score distributions - Male/Female classification disagreement separate\"\n",
    "    x_title = \"Assay+Sex z-score distributions\"\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(assays),\n",
    "        shared_yaxes=True,\n",
    "        x_title=x_title,\n",
    "        y_title=\"z-score\",\n",
    "        horizontal_spacing=0.02,\n",
    "        subplot_titles=[\n",
    "            f\"{assay_label} ({assay_sizes[assay_label]})\" for assay_label in assays\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    for i, assay_label in enumerate(sorted(assays)):\n",
    "        sub_df = zscore_df[zscore_df[ASSAY] == assay_label]\n",
    "\n",
    "        y_values = sub_df[metric_label]\n",
    "        hovertext = [\n",
    "            f\"{epirr}: z-score={z_score:.3f}, pred={pred:.3f}\"\n",
    "            for epirr, pred, z_score in zip(\n",
    "                sub_df[\"EpiRR\"],\n",
    "                sub_df[\"Max pred\"],\n",
    "                sub_df[metric_label],\n",
    "            )\n",
    "        ]\n",
    "        hovertext = np.array(hovertext)\n",
    "\n",
    "        female_idx = np.argwhere((sub_df[\"True class\"] == \"female\").values).flatten()\n",
    "        male_idx = np.argwhere((sub_df[\"True class\"] == \"male\").values).flatten()\n",
    "\n",
    "        # predicted_as_female_idx = np.argwhere(\n",
    "        #     (\n",
    "        #         (sub_df[\"Predicted class\"] == \"female\") & (sub_df[\"True class\"] == \"male\")\n",
    "        #     ).values\n",
    "        # ).flatten()\n",
    "        # predicted_as_male_idx = np.argwhere(\n",
    "        #     (\n",
    "        #         (sub_df[\"Predicted class\"] == \"male\") & (sub_df[\"True class\"] == \"female\")\n",
    "        #     ).values\n",
    "        # ).flatten()\n",
    "\n",
    "        # fig.add_trace(\n",
    "        #     go.Violin(\n",
    "        #         name=\"\",\n",
    "        #         x0=i,\n",
    "        #         y=y_values[female_idx],\n",
    "        #         box_visible=True,\n",
    "        #         meanline_visible=True,\n",
    "        #         points=\"all\",\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[female_idx],\n",
    "        #         side=\"negative\",\n",
    "        #         line_color=sex_colors[\"male\"],\n",
    "        #         spanmode=\"hard\",\n",
    "        #         showlegend=False,\n",
    "        #         marker=dict(size=1),\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "        # fig.add_trace(\n",
    "        #     go.Violin(\n",
    "        #         name=\"\",\n",
    "        #         x0=i,\n",
    "        #         y=y_values[male_idx],\n",
    "        #         box_visible=True,\n",
    "        #         meanline_visible=True,\n",
    "        #         points=\"all\",\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[male_idx],\n",
    "        #         side=\"positive\",\n",
    "        #         line_color=sex_colors[\"male\"],\n",
    "        #         spanmode=\"hard\",\n",
    "        #         showlegend=False,\n",
    "        #         marker=dict(size=1),\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=assay_label,\n",
    "                y=y_values[female_idx],\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[female_idx],\n",
    "                marker=dict(\n",
    "                    size=2,\n",
    "                    color=sex_colors[\"female\"],\n",
    "                    line=dict(width=0.5, color=\"black\"),\n",
    "                ),\n",
    "                fillcolor=sex_colors[\"female\"],\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                showlegend=False,\n",
    "                legendgroup=\"Female\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                name=assay_label,\n",
    "                y=y_values[male_idx],\n",
    "                boxmean=True,\n",
    "                boxpoints=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[male_idx],\n",
    "                marker=dict(\n",
    "                    size=2, color=sex_colors[\"male\"], line=dict(width=0.5, color=\"black\")\n",
    "                ),\n",
    "                fillcolor=sex_colors[\"male\"],\n",
    "                line=dict(width=1, color=\"black\"),\n",
    "                showlegend=False,\n",
    "                legendgroup=\"Male\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "        # temp_y_values = y_values[predicted_as_female_idx]\n",
    "        # temp_size = 1 + 5 * sub_df[\"Max pred\"].values[predicted_as_female_idx]\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         name=\"\",\n",
    "        #         x=[i - 0.2] * len(temp_y_values),\n",
    "        #         y=temp_y_values,\n",
    "        #         mode=\"markers\",\n",
    "        #         marker=dict(color=sex_colors[\"female\"], size=temp_size),\n",
    "        #         showlegend=False,\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[predicted_as_female_idx],\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "        # temp_y_values = y_values[predicted_as_male_idx]\n",
    "        # temp_size = 1 + 5 * sub_df[\"Max pred\"].values[predicted_as_male_idx]\n",
    "        # fig.add_trace(\n",
    "        #     go.Scatter(\n",
    "        #         name=\"\",\n",
    "        #         x=[i - 0.25] * len(temp_y_values),\n",
    "        #         y=temp_y_values,\n",
    "        #         mode=\"markers\",\n",
    "        #         marker=dict(color=sex_colors[\"male\"], size=temp_size),\n",
    "        #         showlegend=False,\n",
    "        #         hovertemplate=\"%{text}\",\n",
    "        #         text=hovertext[predicted_as_male_idx],\n",
    "        #     ),\n",
    "        #     row=1,\n",
    "        #     col=i + 1,\n",
    "        # )\n",
    "\n",
    "    # Add a dummy scatter plot for legend\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Female\",\n",
    "            marker=dict(color=sex_colors[\"female\"], size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Female\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Male\",\n",
    "            marker=dict(color=sex_colors[\"male\"], size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Male\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    fig.update_yaxes(range=[-1.5, 3])\n",
    "    title = \"z-score(mean chrY coverage per file) distribution per assay\"\n",
    "    fig.update_layout(\n",
    "        title_text=title,\n",
    "        width=3000,\n",
    "        height=1000,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_chrY_zscore\"\n",
    "logdir.mkdir(parents=False, exist_ok=True)\n",
    "name = \"fig2--sex_chrY_zscore_only_box\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v2\"\n",
    "pred_data_dir = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / f\"dfreeze_{version}\"\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / f\"{SEX}_1l_3000n\"\n",
    "    / \"w-mixed\"\n",
    "    / \"10fold-oversampling\"\n",
    ")\n",
    "if not pred_data_dir:\n",
    "    raise FileNotFoundError(f\"Directory {pred_data_dir} does not exist.\")\n",
    "zscore_df = prepare_fig_2B_data(version, pred_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig2_B(zscore_df, logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot z-score according to sex, merge assays except wgbs (1 violin plot, 1 point = 1 epirr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_B_merged_assays(\n",
    "    zscore_df: pd.DataFrame,\n",
    "    sex_mislabels: Dict[str, str],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    "    min_pred: float | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Create figure 2B.\n",
    "\n",
    "    Args:\n",
    "        zscore_df (pd.DataFrame): The dataframe with z-score data.\n",
    "        sex_mislabels (Dict[str, str]): {EpiRR_no-v: corrected_sex_label}\n",
    "        logdir (Path): The directory path to save the output plots.\n",
    "        name (str): The base name for the output plot files.\n",
    "        min_pred (float|None): Minimum prediction value to include in the plot. Used on average EpiRR 'Max pred' values.\n",
    "    \"\"\"\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "    zscore_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # wgbs reverses male/female chrY tendency, so removed here\n",
    "    zscore_df = zscore_df[zscore_df[ASSAY] != \"wgbs\"]\n",
    "\n",
    "    # Average chrY z-score values\n",
    "    mean_chrY_values_df = zscore_df.groupby([\"EpiRR\", SEX]).agg(\n",
    "        {metric_label: \"mean\", \"Max pred\": \"mean\"}\n",
    "    )\n",
    "    mean_chrY_values_df.reset_index(inplace=True)\n",
    "    if not mean_chrY_values_df[\"EpiRR\"].is_unique:\n",
    "        raise ValueError(\"EpiRR is not unique.\")\n",
    "\n",
    "    # Filter out low prediction values\n",
    "    if min_pred is not None:\n",
    "        mean_chrY_values_df = mean_chrY_values_df[\n",
    "            mean_chrY_values_df[\"Max pred\"] > min_pred\n",
    "        ]\n",
    "\n",
    "    mean_chrY_values_df.reset_index(drop=True, inplace=True)\n",
    "    chrY_values = mean_chrY_values_df[metric_label]\n",
    "    female_idx = np.argwhere((mean_chrY_values_df[SEX] == \"female\").values).flatten()  # type: ignore\n",
    "    male_idx = np.argwhere((mean_chrY_values_df[SEX] == \"male\").values).flatten()  # type: ignore\n",
    "\n",
    "    # Mislabels\n",
    "    binary_mislabels = set(\n",
    "        epirr_no_v\n",
    "        for epirr_no_v, label in sex_mislabels.items()\n",
    "        if label in [\"male\", \"female\"]\n",
    "    )\n",
    "    epirr_no_v = mean_chrY_values_df[\"EpiRR\"].str.extract(pat=r\"(\\w+\\d+).\\d+\")[0]\n",
    "    mislabels_idx = np.argwhere(\n",
    "        epirr_no_v.isin(binary_mislabels).values  # type: ignore\n",
    "    ).flatten()\n",
    "\n",
    "    mislabel_color_dict = {\"female\": sex_colors[\"male\"], \"male\": sex_colors[\"female\"]}\n",
    "    mislabel_colors = [mislabel_color_dict[mean_chrY_values_df[SEX][i]] for i in mislabels_idx]  # type: ignore\n",
    "\n",
    "    # Hovertext\n",
    "    hovertext = [\n",
    "        f\"{epirr}: <z-score>={z_score:.3f}\"\n",
    "        for epirr, z_score in zip(\n",
    "            mean_chrY_values_df[\"EpiRR\"],\n",
    "            mean_chrY_values_df[metric_label],\n",
    "        )\n",
    "    ]\n",
    "    hovertext = np.array(hovertext)\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            name=\"Female\",\n",
    "            y=chrY_values[female_idx],\n",
    "            boxmean=True,\n",
    "            boxpoints=\"all\",\n",
    "            pointpos=0,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[female_idx],\n",
    "            marker=dict(size=1, color=\"black\"),\n",
    "            line=dict(width=1, color=\"black\"),\n",
    "            fillcolor=sex_colors[\"female\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            name=\"Male\",\n",
    "            y=chrY_values[male_idx],\n",
    "            boxmean=True,\n",
    "            boxpoints=\"all\",\n",
    "            pointpos=0,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[male_idx],\n",
    "            marker=dict(size=1, color=\"black\"),\n",
    "            line=dict(width=1, color=\"black\"),\n",
    "            fillcolor=sex_colors[\"male\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            name=\"Mislabel\",\n",
    "            x=np.zeros(len(mislabels_idx)),\n",
    "            y=chrY_values[mislabels_idx],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=4, color=mislabel_colors, line=dict(width=1, color=\"black\")),\n",
    "            showlegend=False,\n",
    "            hovertemplate=\"%{text}\",\n",
    "            text=hovertext[mislabels_idx],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(range=[-1.5, 3])\n",
    "    title = \"z-score(mean chrY coverage per file) distribution - z-scores averaged over assays\"\n",
    "    if min_pred is not None:\n",
    "        title += f\"<br>avg_maxPred>{min_pred}\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(text=title, x=0.5),\n",
    "        xaxis_title=SEX,\n",
    "        yaxis_title=\"Average z-score\",\n",
    "        width=750,\n",
    "        height=750,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    this_name = f\"{name}_n{mean_chrY_values_df.shape[0]}\"\n",
    "    fig.write_image(logdir / f\"{this_name}.svg\")\n",
    "    fig.write_image(logdir / f\"{this_name}.png\")\n",
    "    fig.write_html(logdir / f\"{this_name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, epirr_mislabels = create_mislabel_corrector()\n",
    "sex_mislabels = epirr_mislabels[SEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pred = None\n",
    "name = \"fig2--sex_chrY_zscore_merged_assays\"\n",
    "if min_pred is not None:\n",
    "    name = f\"fig2--sex_chrY_zscore_merged_assays_avg_maxPred>{min_pred}\"\n",
    "\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_chrY_zscore\"\n",
    "# fig2_B_merged_assays(zscore_df, sex_mislabels, logdir, name, min_pred=min_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_assays_separation_distance(\n",
    "    zscore_df: pd.DataFrame, logdir: Path, name: str\n",
    ") -> None:\n",
    "    \"\"\"Complement to figure 2B, showing separation distance (mean, median)\n",
    "    between male/female zscore clusters.\n",
    "\n",
    "    Args:\n",
    "        zscore_df (pd.DataFrame): The dataframe with z-score data.\n",
    "        logdir (Path): The directory path to save the output plots.\n",
    "        name (str): The base name for the output plot files.\n",
    "    \"\"\"\n",
    "    zscore_df = zscore_df.copy(deep=True)\n",
    "    zscore_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # wgbs reverses male/female chrY tendency, so removed here\n",
    "    zscore_df = zscore_df[zscore_df[ASSAY] != \"wgbs\"]\n",
    "\n",
    "    # Average chrY z-score values\n",
    "    mean_chrY_values_df = zscore_df.groupby([\"EpiRR\", SEX]).agg(\n",
    "        {metric_label: \"mean\", \"Max pred\": \"mean\"}\n",
    "    )\n",
    "    mean_chrY_values_df.reset_index(inplace=True)\n",
    "    if not mean_chrY_values_df[\"EpiRR\"].is_unique:\n",
    "        raise ValueError(\"EpiRR is not unique.\")\n",
    "\n",
    "    mean_chrY_values_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    distances = {\"mean\": [], \"median\": []}\n",
    "    min_preds = np.arange(0, 1.0, 0.01)\n",
    "    sample_count = []\n",
    "    for min_pred in min_preds:\n",
    "        subset_chrY_values_df = mean_chrY_values_df[\n",
    "            mean_chrY_values_df[\"Max pred\"] > min_pred\n",
    "        ]\n",
    "        sample_count.append(subset_chrY_values_df.shape[0])\n",
    "\n",
    "        # Compute separation distances\n",
    "        chrY_vals_female = subset_chrY_values_df[subset_chrY_values_df[SEX] == \"female\"][\n",
    "            metric_label\n",
    "        ]\n",
    "        chrY_vals_male = subset_chrY_values_df[subset_chrY_values_df[SEX] == \"male\"][\n",
    "            metric_label\n",
    "        ]\n",
    "\n",
    "        if not chrY_vals_female.empty and not chrY_vals_male.empty:\n",
    "            mean_distance = np.abs(chrY_vals_female.mean() - chrY_vals_male.mean())\n",
    "            median_distance = np.abs(chrY_vals_female.median() - chrY_vals_male.median())\n",
    "\n",
    "            distances[\"mean\"].append(mean_distance)\n",
    "            distances[\"median\"].append(median_distance)\n",
    "        else:\n",
    "            distances[\"mean\"].append(np.nan)\n",
    "            distances[\"median\"].append(np.nan)\n",
    "\n",
    "    # Plotting the results\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for mean and median distances\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=min_preds,\n",
    "            y=distances[\"mean\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Mean Distance (left)\",\n",
    "            line=dict(color=\"blue\"),\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=min_preds,\n",
    "            y=distances[\"median\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Median Distance (left)\",\n",
    "            line=dict(color=\"green\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add trace for number of files\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=min_preds,\n",
    "            y=sample_count,\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"Number of Files (right)\",\n",
    "            line=dict(color=\"red\"),\n",
    "            yaxis=\"y2\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout for secondary y-axis\n",
    "    fig.update_layout(\n",
    "        title=\"Separation Distance of chrY z-scores male/female clusters\",\n",
    "        xaxis_title=\"Average Prediction Score minimum threshold\",\n",
    "        yaxis_title=\"Z-score Distance\",\n",
    "        yaxis2=dict(title=\"Number of Files\", overlaying=\"y\", side=\"right\"),\n",
    "        legend=dict(\n",
    "            x=1.08,\n",
    "        ),\n",
    "    )\n",
    "    # Save the plot\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"sex_chrY_zscore_merged_assays_distance\"\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_chrY_zscore\"\n",
    "merged_assays_separation_distance(zscore_df, logdir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex: prediction score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_score_violin(\n",
    "    results_df: pd.DataFrame, logdir: Path, name: str, min_y: float | None = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates a Plotly figure with violin plots and associated scatter plots for each class.\n",
    "    Red scatter points, indicating a mismatch, appear on top and have a larger size.\n",
    "\n",
    "    Args:\n",
    "        results_df (pd.DataFrame): The DataFrame containing the neural network results, including metadata.\n",
    "        logdir (Path): The directory where the figure will be saved.\n",
    "        name (str): The name of the figure.\n",
    "    Returns:\n",
    "        None: Displays the plotly figure.\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Class ordering\n",
    "    class_labels = (\n",
    "        results_df[\"True class\"].unique().tolist()\n",
    "        + results_df[\"Predicted class\"].unique().tolist()\n",
    "    )\n",
    "    class_labels_sorted = sorted(set(class_labels))\n",
    "    class_index = {label: i for i, label in enumerate(class_labels_sorted)}\n",
    "\n",
    "    for label in class_labels_sorted:\n",
    "        df = results_df[results_df[\"True class\"] == label]\n",
    "\n",
    "        # Majority vote, mean prediction score\n",
    "        groupby_epirr = df.groupby([\"EpiRR\", \"Predicted class\"])[\"Max pred\"].aggregate(\n",
    "            [\"size\", \"mean\"]\n",
    "        )\n",
    "\n",
    "        groupby_epirr = groupby_epirr.reset_index().sort_values(\n",
    "            [\"EpiRR\", \"size\"], ascending=[True, False]\n",
    "        )\n",
    "        groupby_epirr = groupby_epirr.drop_duplicates(subset=\"EpiRR\", keep=\"first\")\n",
    "        assert groupby_epirr[\"EpiRR\"].is_unique\n",
    "\n",
    "        mean_pred = groupby_epirr[\"mean\"]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                x=[class_index[label]] * len(mean_pred),\n",
    "                y=mean_pred,\n",
    "                name=label,\n",
    "                spanmode=\"hard\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=False,\n",
    "                fillcolor=sex_colors[label],\n",
    "                line_color=\"black\",\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Prepare data for scatter plots\n",
    "        match_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] == label\n",
    "        ]\n",
    "        mismatch_pred = [\n",
    "            mean_pred.iloc[i]\n",
    "            for i, row in enumerate(groupby_epirr.iterrows())\n",
    "            if row[1][\"Predicted class\"] != label\n",
    "        ]\n",
    "\n",
    "        jitter_match = np.random.uniform(-0.01, 0.01, len(match_pred))\n",
    "\n",
    "        # Add scatter plots for matches in black\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[class_index[label]] * len(match_pred) + jitter_match,\n",
    "                y=match_pred,\n",
    "                mode=\"markers\",\n",
    "                name=label,\n",
    "                marker=dict(\n",
    "                    color=\"black\",\n",
    "                    size=2,  # Standard size for matches\n",
    "                ),\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.2f}\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] == label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "                legendgroup=\"match\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add scatter plots for mismatches in red, with larger size\n",
    "        mismatch_info = groupby_epirr[groupby_epirr[\"Predicted class\"] != label]\n",
    "        strong_mismatch = mismatch_info[mismatch_info[\"mean\"] > 0.9]\n",
    "        display(strong_mismatch)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[class_index[label]] * len(mismatch_pred),\n",
    "                y=mismatch_pred,\n",
    "                mode=\"markers\",\n",
    "                name=label,\n",
    "                marker=dict(\n",
    "                    color=\"red\",\n",
    "                    size=5,  # Larger size for mismatches\n",
    "                ),\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=[\n",
    "                    f\"EpiRR: {row[1]['EpiRR']}, Pred class: {row[1]['Predicted class']}, Mean pred: {row[1]['mean']:.3f} (n={row[1]['size']})\"\n",
    "                    for row in groupby_epirr.iterrows()\n",
    "                    if row[1][\"Predicted class\"] != label\n",
    "                ],\n",
    "                showlegend=False,\n",
    "                legendgroup=\"mismatch\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - black points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Match\",\n",
    "            marker=dict(color=\"black\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"match\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add a dummy scatter plot for legend - red points\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Mismatch\",\n",
    "            marker=dict(color=\"red\", size=10),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"mismatch\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tickvals = list(class_index.values())\n",
    "    ticktext = list(class_index.keys())\n",
    "    fig.update_xaxes(tickvals=tickvals, ticktext=ticktext)\n",
    "\n",
    "    if min_y is None:\n",
    "        min_y = min(results_df[\"Max pred\"])\n",
    "\n",
    "    fig.update_yaxes(range=[min_y, 1.001])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Prediction score distribution class\",\n",
    "        yaxis_title=\"Avg. prediction score (majority class)\",\n",
    "        xaxis_title=\"Expected class label\",\n",
    "        width=750,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            title_text=\"Legend\",\n",
    "            itemsizing=\"constant\",\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    # fig.write_html(logdir / f\"{name}.html\")\n",
    "    # fig.write_image(logdir / f\"{name}.svg\")\n",
    "    # fig.write_image(logdir / f\"{name}.png\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sex_df = concatenated_results[\"harmonized_donor_sex_w-mixed\"]\n",
    "# sex_df = split_results_handler.add_max_pred(sex_df)\n",
    "# sex_df = metadata_handler.join_metadata(sex_df, metadata_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"fig2--sex_pred_score_post_correction\"\n",
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--sex_pred_score\"\n",
    "# pred_score_violin(sex_df, logdir, name, min_y=0.485)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced features sets NN metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regulatory regions NN trainings data download\n",
    "\n",
    "~~~bash\n",
    "# Download phase\n",
    "paper_dir=\"${HOME}/Projects/epiclass/output/paper/data\"\n",
    "cd ${paper_dir}/training_results/dfreeze_v2\n",
    "\n",
    "base_path=\"/lustre07/scratch/rabyj/epilap-logs/epiatlas-dfreeze-v2.1\"\n",
    "rsync --info=progress2 -av --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"validation_confusion*\" --exclude \"*.md5\" --exclude \"full*\" narval:${base_path}/hg38_regulatory_regions_n* .\n",
    "\n",
    "# Cleanup phase\n",
    "# Remove files related to failed experiements\n",
    "# Step 1: Find files and extract numbers\n",
    "find hg38_regulatory_regions_n* -type f -name \"*.e\" -exec grep -l 'has non-string label of type' {} + | \\\n",
    "grep -oE \"job[0-9]+\" | grep -oE \"[0-9]+\" > failure_jobid.txt\n",
    "\n",
    "# Step 2: Use extracted numbers to find and echo all matching filenames\n",
    "cat failure_jobid.txt | xargs -I% sh -c 'find . -type f -name \"*%*\" -delete'\n",
    "rm failure_jobid.txt\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_all_feature_set_metrics(\n",
    "    parent_folder: Path,\n",
    "    merge_assays: bool,\n",
    ") -> Dict[str, Dict[str, Dict[str, Dict[str, float]]]]:\n",
    "    \"\"\"Obtain all metrics for all feature sets.\n",
    "\n",
    "    Args:\n",
    "        parent_folder (Path): The parent folder containing all feature set folders.\n",
    "                              Needs to be parent of feature set folders.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, float]]]: A dictionary containing all metrics for all feature sets.\n",
    "            Format: {feature_set: {task_name: {split_name: metric_dict}}}\n",
    "    \"\"\"\n",
    "    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]] = {}\n",
    "    for folder in parent_folder.iterdir():\n",
    "        if not folder.is_dir():\n",
    "            continue\n",
    "        feature_set = folder.name\n",
    "        try:\n",
    "            split_results_metrics, _ = general_split_metrics(\n",
    "                folder, merge_assays=merge_assays\n",
    "            )\n",
    "        except ValueError as err:\n",
    "            raise ValueError(f\"Problem with {feature_set}\") from err\n",
    "        inverted_dict = split_results_handler.invert_metrics_dict(split_results_metrics)\n",
    "        all_metrics[feature_set] = inverted_dict\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_sizes_from_metadata() -> Dict[str, int]:\n",
    "    \"\"\"Get input sizes for models using certain feature sets using comet-ml run metadata file.\"\"\"\n",
    "    run_metadata = RUN_METADATA.copy(deep=True)\n",
    "\n",
    "    # Filter out epigeec_filter_1.4.5 runs, wrong input sizes.\n",
    "    run_metadata = run_metadata[run_metadata[\"startTimeMillis\"] > 1706943404420]\n",
    "\n",
    "    run_metadata[\"feature_set\"] = run_metadata[\"Name\"].str.extract(\n",
    "        pat=r\"(^hg38_1[0]{0,2}kb_.*_none).*$\"\n",
    "    )\n",
    "\n",
    "    input_sizes_count = run_metadata.groupby([\"feature_set\", \"input_size\"]).size()\n",
    "    accepted_input_sizes = {idx[0]: int(idx[1]) for idx in input_sizes_count.index}\n",
    "\n",
    "    assert len(input_sizes_count) == len(accepted_input_sizes)\n",
    "    accepted_input_sizes.update({\"hg38_100kb_all_none\": 30321})\n",
    "\n",
    "    return accepted_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "input_sizes = extract_input_sizes_from_output_files(gen_data_dir)  # type: ignore\n",
    "input_sizes: Dict[str, int] = {k: v.pop() for k, v in input_sizes.items() if len(v) == 1}  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = obtain_all_feature_set_metrics(gen_data_dir, merge_assays=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - contenant slmt 100kb_all + 10kb_all + 1kb_200k qui servira à la suppFig1  \n",
    "\n",
    "1.1 - les 2 regulatory (30k puis 300k) à la fin\n",
    "\n",
    "2 - enlever les 118 puis réordonner pour commencer par les 3 précédentes, suivies 4.5k_100kb et 4.5k_random, 45k_10kb et son random, puis  45k_1kb et son random; ce sera visuellement un peu moins intéressant mais bcp plus facile à décrire je crois  \n",
    "\n",
    "2.1 - v2 en intégrant aussi regulatory donc dans l'ordre : enlever les 118 puis réordonner : 100kb_all, suivies 4.5k_100kb et 4.5k_random, puis 10kb_all, 45k_10kb et son random, puis 1kb_200k, 45k_1kb et son random + regul_30k et 300k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(all_metrics[\"hg38_100kb_all_none\"].keys())\n",
    "print(all_metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_metrics = {\n",
    "    name: vals\n",
    "    for name, vals in all_metrics.items()\n",
    "    if any(label in name for label in [\"all_none\", \"200k\"])\n",
    "}\n",
    "v1_reg_metrics = {\n",
    "    name: vals\n",
    "    for name, vals in all_metrics.items()\n",
    "    if any(label in name for label in [\"all_none\", \"200k\", \"reg\"])\n",
    "}\n",
    "v2_metrics = {\n",
    "    **v1_metrics,\n",
    "    **{\n",
    "        name: vals\n",
    "        for name, vals in all_metrics.items()\n",
    "        if any(label in name for label in [\"global_tasks_union\", \"4510\", \"45k\"])\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(enumerate(v1_metrics.keys())))\n",
    "# print(list(enumerate(v1_reg_metrics.keys())))\n",
    "# print(list(enumerate(v2_metrics.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_v1_order = [1, 2, 0]\n",
    "desired_v1_reg_order = [1, 3, 0, 2, 4]\n",
    "desired_v2_order = [1, 5, 7, 2, 8, 4, 0, 6, 3]\n",
    "\n",
    "ordered_v1_metrics = {\n",
    "    list(v1_metrics.keys())[i]: list(v1_metrics.values())[i] for i in desired_v1_order\n",
    "}\n",
    "ordered_v1_reg_metrics = {\n",
    "    list(v1_reg_metrics.keys())[i]: list(v1_reg_metrics.values())[i]\n",
    "    for i in desired_v1_reg_order\n",
    "}\n",
    "ordered_v2_metrics = {\n",
    "    list(v2_metrics.keys())[i]: list(v2_metrics.values())[i] for i in desired_v2_order\n",
    "}\n",
    "ordered_v2_reg_metrics = {\n",
    "    **ordered_v2_metrics,\n",
    "    **{k: v for k, v in all_metrics.items() if \"reg\" in k},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ordered_v1_metrics.keys())\n",
    "# print(ordered_v1_reg_metrics.keys())\n",
    "# print(ordered_v2_metrics.keys())\n",
    "# print(ordered_v2_reg_metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_colors = {\n",
    "    \"100kb\": px.colors.qualitative.Safe[0],\n",
    "    \"10kb\": px.colors.qualitative.Safe[1],\n",
    "    \"1kb\": px.colors.qualitative.Safe[2],\n",
    "    \"regulatory\": px.colors.qualitative.Safe[3],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_colors = {\n",
    "    \"100kb\": px.colors.qualitative.Safe[0],\n",
    "    \"10kb\": px.colors.qualitative.Safe[1],\n",
    "    \"1kb\": px.colors.qualitative.Safe[2],\n",
    "    \"regulatory\": px.colors.qualitative.Safe[3],\n",
    "    \"gene\": px.colors.qualitative.Safe[4],\n",
    "    \"cpg\": px.colors.qualitative.Safe[5],\n",
    "    \"1mb\": px.colors.qualitative.Safe[6],\n",
    "    \"5mb\": px.colors.qualitative.Safe[7],\n",
    "    \"10mb\": px.colors.qualitative.Safe[8],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_feature_set_metrics(\n",
    "    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n",
    "    input_sizes: Dict[str, int],\n",
    "    logdir: Path,\n",
    "    sort_by_input_size: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Create a graph of all metrics for all feature sets.\"\"\"\n",
    "    reference_hdf5_type = \"hg38_1kb_global_tasks_union_UpResolution_1kb_sampled-200k_none\"\n",
    "    metadata_categories = list(all_metrics[reference_hdf5_type].keys())\n",
    "\n",
    "    non_standard_names = {ASSAY: f\"{ASSAY}_11c\", SEX: f\"{SEX}_w-mixed\"}\n",
    "    non_standard_assay_task_names = [\"hg38_100kb_all_none\"]\n",
    "    non_standard_sex_task_name = [\n",
    "        \"hg38_100kb_all_none\",\n",
    "        \"hg38_regulatory_regions_n30321\",\n",
    "        \"hg38_regulatory_regions_n303114\",\n",
    "    ]\n",
    "\n",
    "    for i in range(len(metadata_categories)):\n",
    "        category_idx = i\n",
    "        category_fig = make_subplots(\n",
    "            rows=1,\n",
    "            cols=2,\n",
    "            shared_yaxes=True,\n",
    "            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n",
    "            x_title=\"Feature set\",\n",
    "            y_title=\"Metric value\",\n",
    "        )\n",
    "\n",
    "        trace_names = []\n",
    "        order = list(all_metrics.keys())\n",
    "        if sort_by_input_size:\n",
    "            order = sorted(\n",
    "                all_metrics.keys(),\n",
    "                key=lambda x: input_sizes[x],\n",
    "            )\n",
    "        for feature_set_name in order:\n",
    "            tasks_dicts = all_metrics[feature_set_name]\n",
    "            meta_categories = copy.deepcopy(metadata_categories)\n",
    "\n",
    "            if feature_set_name not in input_sizes:\n",
    "                print(f\"Skipping {feature_set_name}, no input size found.\")\n",
    "                continue\n",
    "\n",
    "            task_name = meta_categories[category_idx]\n",
    "            try:\n",
    "                task_dict = tasks_dicts[task_name]\n",
    "            except KeyError as err:\n",
    "                if SEX in str(err) and feature_set_name in non_standard_sex_task_name:\n",
    "                    task_dict = tasks_dicts[non_standard_names[SEX]]\n",
    "                elif (\n",
    "                    ASSAY in str(err)\n",
    "                    and feature_set_name in non_standard_assay_task_names\n",
    "                ):\n",
    "                    task_dict = tasks_dicts[non_standard_names[ASSAY]]\n",
    "                else:\n",
    "                    print(\"Skipping\", feature_set_name, task_name)\n",
    "                    continue\n",
    "\n",
    "            input_size = input_sizes[feature_set_name]\n",
    "\n",
    "            feature_set_name = feature_set_name.replace(\"_none\", \"\")\n",
    "            feature_set_name = feature_set_name.replace(\"hg38_\", \"\")\n",
    "\n",
    "            resolution = feature_set_name.split(\"_\")[0]\n",
    "\n",
    "            trace_name = f\"{input_size}|{feature_set_name}\"\n",
    "            trace_names.append(trace_name)\n",
    "\n",
    "            # Accuracy\n",
    "            metric = \"Accuracy\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=resolution_colors[resolution],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                    legendgroup=resolution,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "            metric = \"F1_macro\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    fillcolor=resolution_colors[resolution],\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                    legendgroup=resolution,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=2,\n",
    "            )\n",
    "\n",
    "        # category_fig.update_yaxes(range=[0.65, 1.001])\n",
    "        category_fig.update_layout(\n",
    "            width=1500,\n",
    "            height=1500,\n",
    "            title=f\"Neural network performance - {metadata_categories[category_idx]}\",\n",
    "        )\n",
    "\n",
    "        if logdir.name == \"all\":\n",
    "            category_fig.update_xaxes(\n",
    "                categoryorder=\"array\",\n",
    "                categoryarray=sorted(trace_names, key=lambda x: int(x.split(\"|\")[0])),\n",
    "            )\n",
    "\n",
    "        # dummy scatters for resolution colors\n",
    "        relevant_resolutions = [\n",
    "            resolution\n",
    "            for resolution in resolution_colors\n",
    "            if any(resolution in name for name in trace_names)\n",
    "        ]\n",
    "        for resolution in relevant_resolutions:\n",
    "            color = resolution_colors[resolution]\n",
    "            category_fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[None],\n",
    "                    y=[None],\n",
    "                    mode=\"markers\",\n",
    "                    name=resolution,\n",
    "                    marker=dict(color=color, size=5),\n",
    "                    showlegend=True,\n",
    "                    legendgroup=resolution,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Save figure\n",
    "        base_name = f\"feature_set_metrics_{metadata_categories[category_idx]}\"\n",
    "        category_fig.write_html(logdir / f\"{base_name}.html\")\n",
    "        category_fig.write_image(logdir / f\"{base_name}.svg\")\n",
    "        category_fig.write_image(logdir / f\"{base_name}.png\")\n",
    "\n",
    "        # category_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder, metrics in zip(\n",
    "    [\"v1\", \"v1_reg\", \"v2\", \"v2_reg\"],\n",
    "    [\n",
    "        ordered_v1_metrics,\n",
    "        ordered_v1_reg_metrics,\n",
    "        ordered_v2_metrics,\n",
    "        ordered_v2_reg_metrics,\n",
    "    ],\n",
    "):\n",
    "    logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--reduced_feature_sets\" / folder\n",
    "    logdir.mkdir(parents=False, exist_ok=True)\n",
    "    graph_feature_set_metrics(metrics, input_sizes, logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of zeroing blacklisted regions, and winzorizing input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data download\n",
    "\n",
    "~~~bash\n",
    "paper_dir=\"${HOME}/Projects/epiclass/output/paper/data\"\n",
    "cd ${paper_dir}/training_results/2023-01-epiatlas-freeze\n",
    "\n",
    "base_path=\"/lustre07/scratch/rabyj/epilap-logs/2023-01-epiatlas-freeze\"\n",
    "rsync --info=progress2 -a --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"validation_confusion*\" --exclude \"*.md5\" narval:${base_path}/hg38_100kb_all_none_0blklst* .\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLKLST_CATEGORIES = [\n",
    "    \"assay_epiclass\",\n",
    "    \"harmonized_biomaterial_type\",\n",
    "    \"harmonized_donor_sex\",\n",
    "    \"harmonized_sample_ontology_intermediate\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check oversampling\n",
    "\n",
    "Make sure oversampling is same in all training runs used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_2023_runs_oversampling():\n",
    "    \"\"\"Check if oversampling is on for all 2023 training runs used for blacklisted/winzorized metrics.\"\"\"\n",
    "    data_dir = base_data_dir / \"2023-01-epiatlas-freeze\"\n",
    "    for folder in data_dir.iterdir():\n",
    "        for category in BLKLST_CATEGORIES:\n",
    "            category_parent_folder = folder / f\"{category}_1l_3000n\"\n",
    "\n",
    "            if not category_parent_folder.exists():\n",
    "                raise FileNotFoundError(\"Cannot find: {category_parent_folder}\")\n",
    "\n",
    "            print(f\"Processing {category_parent_folder}\")\n",
    "\n",
    "            check_for_oversampling(category_parent_folder, verbose=False)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify_2023_runs_oversampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verify_2023_runs_oversampling result:\n",
    "Oversampling uniform across hdf5 types, but unsure across metadata categories.\n",
    "  - harmonized_biomaterial_type: On\n",
    "  - harmonized_sample_ontology_intermediate: On\n",
    "  - harmonized_donor_sex: Unknown, very probably On.\n",
    "    All nan values, but used human_longer.json hparams, which is the same as with the other runs that have oversampling on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blklst_split_metrics(\n",
    "    verbose: bool = False,\n",
    ") -> Dict[str, Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"Compute metrics on relevant categories and runs.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, float]]]: A dictionary containing all metrics for all blklst related feature sets.\n",
    "            Format: {feature_set: {task_name: {split_name: metric_dict}}}\n",
    "    \"\"\"\n",
    "    data_dir = base_data_dir / \"training_results\" / \"2023-01-epiatlas-freeze\"\n",
    "    feature_set_metrics_dict = {}\n",
    "    for folder in data_dir.iterdir():\n",
    "        if folder.is_file():\n",
    "            continue\n",
    "        feature_set_name = folder.name\n",
    "\n",
    "        tasks_dict = {}\n",
    "        for category in BLKLST_CATEGORIES:\n",
    "            category_parent_folder = folder / f\"{category}_1l_3000n\"\n",
    "\n",
    "            if not category_parent_folder.exists():\n",
    "                raise FileNotFoundError(\"Cannot find: {category_parent_folder}\")\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Processing {category_parent_folder}\")\n",
    "\n",
    "            for task_folder in category_parent_folder.iterdir():\n",
    "                if task_folder.is_file():\n",
    "                    continue\n",
    "                split_results = split_results_handler.read_split_results(task_folder)\n",
    "                general_name = f\"{category_parent_folder.name}-{task_folder.name}\"\n",
    "                tasks_dict[general_name] = split_results\n",
    "\n",
    "        feature_set_metrics = split_results_handler.compute_split_metrics(\n",
    "            tasks_dict, concat_first_level=True\n",
    "        )\n",
    "        feature_set_metrics_dict[\n",
    "            feature_set_name\n",
    "        ] = split_results_handler.invert_metrics_dict(feature_set_metrics)\n",
    "\n",
    "    return feature_set_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_set_metrics_dict = get_blklst_split_metrics(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blklst_graphs(\n",
    "    feature_set_metrics_dict: Dict[str, Dict[str, Dict[str, float]]], logdir: Path\n",
    ") -> None:\n",
    "    \"\"\"Create boxplots for blacklisted related feature sets.\n",
    "\n",
    "    Args:\n",
    "        feature_set_metrics_dict (Dict[str, Dict[str, Dict[str, float]]]): The dictionary containing all metrics for all blklst related feature sets.\n",
    "            format: {feature_set: {task_name: {split_name: metric_dict}}}\n",
    "    \"\"\"\n",
    "    # Assume names exist in all feature sets\n",
    "    task_names = list(feature_set_metrics_dict.values())[0].keys()\n",
    "\n",
    "    traces_names_dict = {\n",
    "        \"hg38_100kb_all_none\": \"observed\",\n",
    "        \"hg38_100kb_all_none_0blklst\": \"0blklst\",\n",
    "        \"hg38_100kb_all_none_0blklst_winsorized\": \"0blklst_winsorized\",\n",
    "    }\n",
    "\n",
    "    for task_name in task_names:\n",
    "        category_fig = make_subplots(\n",
    "            rows=1,\n",
    "            cols=2,\n",
    "            shared_yaxes=False,\n",
    "            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n",
    "            x_title=\"Feature set\",\n",
    "            y_title=\"Metric value\",\n",
    "            horizontal_spacing=0.03,\n",
    "        )\n",
    "        for feature_set_name, tasks_dicts in feature_set_metrics_dict.items():\n",
    "            task_dict = tasks_dicts[task_name]\n",
    "            trace_name = traces_names_dict[feature_set_name]\n",
    "\n",
    "            # Accuracy\n",
    "            metric = \"Accuracy\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "            metric = \"F1_macro\"\n",
    "            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n",
    "            hovertext = [\n",
    "                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n",
    "                for split, metrics_dict in task_dict.items()\n",
    "            ]\n",
    "            category_fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=y_vals,\n",
    "                    name=trace_name,\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",\n",
    "                    showlegend=False,\n",
    "                    marker=dict(size=3, color=\"black\"),\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    text=hovertext,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=2,\n",
    "            )\n",
    "\n",
    "        category_fig.update_xaxes(\n",
    "            categoryorder=\"array\",\n",
    "            categoryarray=list(traces_names_dict.values()),\n",
    "        )\n",
    "        category_fig.update_yaxes(range=[0.85, 1.001])\n",
    "\n",
    "        task_name = task_name.replace(\"_1l_3000n-10fold\", \"\")\n",
    "        category_fig.update_layout(\n",
    "            title=f\"Neural network performance - {task_name} - 100kb\",\n",
    "        )\n",
    "\n",
    "        # Save figure\n",
    "        base_name = f\"metrics_{task_name}\"\n",
    "        category_fig.write_html(logdir / f\"{base_name}.html\")\n",
    "        category_fig.write_image(logdir / f\"{base_name}.svg\")\n",
    "        category_fig.write_image(logdir / f\"{base_name}.png\")\n",
    "\n",
    "        category_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"fig2_EpiAtlas_other\" / \"fig2--blklst_and_winsorized\" / \"y0.85\"\n",
    "logdir.mkdir(parents=False, exist_ok=True)\n",
    "# create_blklst_graphs(feature_set_metrics_dict, logdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
