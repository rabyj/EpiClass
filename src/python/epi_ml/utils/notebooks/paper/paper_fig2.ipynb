{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to create figures (fig2) destined for the paper.\n",
    "\n",
    "Please use dfreeze v2 for these. v1 is only for fig1.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    CELL_TYPE,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fig 2 - EpiClass results on EpiAtlas other metadata\n",
    "\n",
    "For following figures, use v1.1 of sample metadata (called v2.1 internally), i.e. dfreeze 2\n",
    "\n",
    "A) Histogram of performance (accuracy and F1 scores) for each category \n",
    "B) Violin plot of average z-score on chrY per sex, black dots for pred same class and red for pred different class.  \n",
    "- Do the split male female violin per assay (only FC, merge 2xwgbs and 2xrna, no rna unique_raw). \n",
    "- Use scatter for points on each side, agree same color as violin, disagree other.\n",
    "- Point labels: uuid, epirr\n",
    "\n",
    "C) ---  \n",
    "D) ---  \n",
    "E) --- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all training runs were done with oversampling on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_results_dir = base_data_dir / \"dfreeze_v1\"\n",
    "if not v1_results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {v1_results_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_oversampling(base_data_dir: Path):\n",
    "    \"\"\"Check for oversampling status in the results.\n",
    "\n",
    "    Returns a ValeError if not all experiments have oversampling.\n",
    "    \"\"\"\n",
    "    # Identify experiments\n",
    "    exp_key_line = \"The current experiment key is\"\n",
    "    exp_keys_dict = defaultdict(list)\n",
    "    for category in v1_results_dir.iterdir():\n",
    "        for stdout_file in category.glob(\"*/output_job*.o\"):\n",
    "            with open(stdout_file, \"r\", encoding=\"utf8\") as f:\n",
    "                lines = [l.rstrip() for l in f if exp_key_line in l]\n",
    "            exp_keys = [l.split(exp_key_line)[1].strip() for l in lines]\n",
    "            exp_keys_dict[category.name].extend(exp_keys)\n",
    "\n",
    "    # Get all hparam values\n",
    "    gen_run_metadata = (\n",
    "        base_data_dir / \"all_results_cometml_filtered_oversampling-fixed.csv\"\n",
    "    )\n",
    "    run_metadata = pd.read_csv(gen_run_metadata, header=0)\n",
    "\n",
    "    # Check oversampling values\n",
    "    all_exp_keys = set()\n",
    "    for exp_keys in exp_keys_dict.values():\n",
    "        all_exp_keys.update(exp_keys)\n",
    "\n",
    "    df = run_metadata[run_metadata[\"experimentKey\"].isin(all_exp_keys)]\n",
    "    if not df[\"hparams/oversampling\"].all():\n",
    "        raise ValueError(\"Not all experiments have oversampling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_for_oversampling(base_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of performance (accuracy and F1 scores) for each category (using metadata v1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_a_content(\n",
    "    exclude_categories: List[str], exclude_names: List[str]\n",
    ") -> Dict[str, Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"Create the content data for figure 2a. (get metrics for each task)\n",
    "\n",
    "    Currently only using oversampled runs.\n",
    "\n",
    "    Args:\n",
    "        exclude_categories (List[str]): Task categories to exclude (first level directory names).\n",
    "        exclude_names (List[str]): Names of folders to exclude (ex: 7c or no-mix).\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dict[str, float]]] A metrics dictionary with the following structure:\n",
    "            {split_name: {task_name: metrics_dict}}\n",
    "    \"\"\"\n",
    "    all_split_results = {}\n",
    "    split_results_handler = SplitResultsHandler()\n",
    "\n",
    "    # Get the data\n",
    "    results_dir = base_data_dir / \"dfreeze_v2\"\n",
    "    for parent, _, _ in os.walk(results_dir):\n",
    "        # Looking for oversampling only results\n",
    "        parent = Path(parent)\n",
    "        if parent.name != \"10fold-oversampling\":\n",
    "            continue\n",
    "\n",
    "        # Get the category\n",
    "        relpath = parent.relative_to(results_dir)\n",
    "        category = relpath.parts[0].rstrip(\"_1l_3000n\")\n",
    "        if category in exclude_categories:\n",
    "            continue\n",
    "\n",
    "        # Get the rest of the name, ignore certain runs\n",
    "        rest_of_name = list(relpath.parts[1:])\n",
    "        rest_of_name.remove(\"10fold-oversampling\")\n",
    "\n",
    "        if len(rest_of_name) > 1:\n",
    "            raise ValueError(f\"Too many parts in the name: {rest_of_name}\")\n",
    "        if rest_of_name:\n",
    "            rest_of_name = rest_of_name[0]\n",
    "\n",
    "        if any(name in rest_of_name for name in exclude_names):\n",
    "            continue\n",
    "\n",
    "        full_task_name = category\n",
    "        if rest_of_name:\n",
    "            full_task_name += f\"_{rest_of_name}\"\n",
    "\n",
    "        # Get the split results\n",
    "        split_results = split_results_handler.read_split_results(parent)\n",
    "        all_split_results[full_task_name] = split_results\n",
    "\n",
    "    split_results_metrics = split_results_handler.compute_split_metrics(\n",
    "        all_split_results, concat_first_level=True\n",
    "    )\n",
    "    return split_results_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_categories = [\"groups_second_level_name\", \"track_type\"]\n",
    "exclude_names = [\"chip-seq\", \"7c\"]\n",
    "\n",
    "fig2_a_content = fig2_a_content(exclude_categories, exclude_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_a(\n",
    "    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n",
    "    logdir: Path,\n",
    "    name: str,\n",
    ") -> None:\n",
    "    \"\"\"Render box plots of metrics per classifier and split, each in its own subplot.\n",
    "\n",
    "    This function generates a figure with subplots, each representing a different\n",
    "    metric. Each subplot contains box plots for each classifier, ordered by accuracy.\n",
    "\n",
    "    Args:\n",
    "        split_metrics: A nested dictionary with structure {split: {classifier: {metric: score}}}.\n",
    "        logdir: The directory path to save the output plots.\n",
    "        name: The base name for the output plot files.\n",
    "    \"\"\"\n",
    "    metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n",
    "    classifier_names = list(next(iter(split_metrics.values())).keys())\n",
    "\n",
    "    # sort classifiers by accuracy\n",
    "    mean_acc = {}\n",
    "    for classifier in classifier_names:\n",
    "        mean_acc[classifier] = np.mean(\n",
    "            [split_metrics[split][classifier][\"Accuracy\"] for split in split_metrics]\n",
    "        )\n",
    "    classifier_names = sorted(classifier_names, key=lambda x: mean_acc[x], reverse=True)\n",
    "\n",
    "    # Create subplots, one row for each metric\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(metrics),\n",
    "        subplot_titles=metrics,\n",
    "        horizontal_spacing=0.03,\n",
    "    )\n",
    "\n",
    "    colors = {\n",
    "        classifier: px.colors.qualitative.Alphabet[i]\n",
    "        for i, classifier in enumerate(classifier_names)\n",
    "    }\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        for classifier_name in classifier_names:\n",
    "            values = [\n",
    "                split_metrics[split][classifier_name][metric] for split in split_metrics\n",
    "            ]\n",
    "\n",
    "            label_name = classifier_name\n",
    "            if classifier_name == \"random_10fold\":\n",
    "                label_name = \"random23c_10fold\"\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    y=values,\n",
    "                    name=label_name,\n",
    "                    fillcolor=colors[classifier_name],\n",
    "                    line=dict(color=\"black\", width=1),\n",
    "                    marker=dict(size=2),\n",
    "                    marker_color=colors[classifier_name],\n",
    "                    boxmean=True,\n",
    "                    boxpoints=\"all\",  # or \"outliers\" to show only outliers\n",
    "                    pointpos=-1.4,\n",
    "                    showlegend=i == 0,  # Only show legend in the first subplot\n",
    "                    width=0.5,\n",
    "                    hoverinfo=\"text\",\n",
    "                    hovertext=[\n",
    "                        f\"{split}: {value:.4f}\"\n",
    "                        for split, value in zip(split_metrics, values)\n",
    "                    ],\n",
    "                    legendgroup=classifier_name,\n",
    "                ),\n",
    "                row=1,\n",
    "                col=i + 1,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Neural network classification - Metric distribution for 10-fold cross-validation\",\n",
    "        yaxis_title=\"Value\",\n",
    "        boxmode=\"group\",\n",
    "        height=1000,\n",
    "        width=1750,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_logdir = base_fig_dir / \"fig2\" / \"fig2_A\"\n",
    "fig_logdir.mkdir(parents=False, exist_ok=True)\n",
    "fig_name = \"fig2_A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split_name, split_metrics in all_split_metrics.items():\n",
    "#     print(split_name)\n",
    "#     for task_name, task_metrics in split_metrics.items():\n",
    "#         print(task_name)\n",
    "#         print(task_metrics)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2_a(fig2_a_content, fig_logdir, fig_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 2.B\n",
    "\n",
    "Violin plot of average z-score on chrY per sex, black dots for pred same class and red for pred different class.  \n",
    "\n",
    "- Do the split male female violin per assay (only FC, merge 2xwgbs and 2xrna, no rna unique_raw). \n",
    "- Use scatter for points on each side, agree same color as violin, disagree other.\n",
    "- Point labels: uuid, epirr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute chrY coverage z-score VS assay distribution, using dfreeze v1 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_chrY_zscores():\n",
    "    \"\"\"Compute z-scores for chrY coverage data, per assay distribution.\n",
    "    Use metadata v1.\n",
    "    \"\"\"\n",
    "    # Get chrY coverage data\n",
    "    chrY_coverage_dir = base_data_dir / \"chrY_coverage\"\n",
    "    if not chrY_coverage_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {chrY_coverage_dir} does not exist.\")\n",
    "    chrY_coverage_df = pd.read_csv(chrY_coverage_dir / \"chrXY_coverage_all.csv\", header=0)\n",
    "\n",
    "    # Filter out md5s not in metadata v1\n",
    "    metadata_v1 = MetadataHandler(paper_dir).load_metadata(\"v1\")\n",
    "    v1_md5s = set(metadata_v1.md5s)\n",
    "    chrY_coverage_df = chrY_coverage_df[chrY_coverage_df[\"filename\"].isin(v1_md5s)]\n",
    "\n",
    "    # Make sure all values are non-zero\n",
    "    assert (chrY_coverage_df[\"chrY\"] != 0).all()\n",
    "\n",
    "    # These tracks are excluded from z-score computation\n",
    "    metadata_v1.remove_category_subsets(\"track_type\", [\"raw\", \"pval\", \"Unique_raw\"])\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata_v1.datasets))\n",
    "    metadata_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "\n",
    "    # Merge with metadata\n",
    "    chrY_coverage_df = chrY_coverage_df.merge(\n",
    "        metadata_df[[\"md5sum\", ASSAY]], left_on=\"filename\", right_on=\"md5sum\"\n",
    "    )\n",
    "\n",
    "    # Compute z-score per assay\n",
    "    chrY_dists = chrY_coverage_df.groupby(ASSAY).agg({\"chrY\": [\"mean\", \"std\", \"count\"]})\n",
    "    chrY_dists.to_csv(chrY_coverage_dir / \"dfreeze_v1_stats\" / \"chrY_coverage_stats.csv\")\n",
    "\n",
    "    # Compute z-score per assay group, merge back into the dataframe, save results\n",
    "    metric_name = \"chrY_zscore_vs_assay\"\n",
    "    groupby_df = chrY_coverage_df.groupby(ASSAY)\n",
    "    for _, group in groupby_df:\n",
    "        group[\"chrY_zscore\"] = zscore(group[\"chrY\"])\n",
    "        chrY_coverage_df.loc[group.index, metric_name] = group[\"chrY_zscore\"]\n",
    "\n",
    "    output_cols = [\"filename\", \"chrY\", metric_name, ASSAY]\n",
    "    chrY_coverage_df[output_cols].to_csv(\n",
    "        chrY_coverage_dir / \"dfreeze_v1_stats\" / \"chrY_coverage_zscore_vs_assay.csv\",\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_chrY_zscores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot z-scores according to sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEX = \"harmonized_donor_sex\"\n",
    "metric_label = \"chrY_zscore_vs_assay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fig_2B_data() -> pd.DataFrame:\n",
    "    \"\"\"Prepare data for figure 2b.\"\"\"\n",
    "    # Load metadata\n",
    "    meta_cols = [\"md5sum\", \"EpiRR\"]\n",
    "    metadata_v1 = MetadataHandler(paper_dir).load_metadata(\"v1\")\n",
    "    metadata_df = pd.DataFrame.from_records(list(metadata_v1.datasets))\n",
    "    metadata_df = metadata_df[meta_cols]\n",
    "\n",
    "    # Load z-score data\n",
    "    zscore_dir = base_data_dir / \"chrY_coverage\" / \"dfreeze_v1_stats\"\n",
    "    zscore_df = pd.read_csv(zscore_dir / \"chrY_coverage_zscore_vs_assay.csv\", header=0)\n",
    "\n",
    "    # Load NN predictions\n",
    "    pred_data_dir = (\n",
    "        base_data_dir\n",
    "        / \"dfreeze_v2\"\n",
    "        / f\"{SEX}_1l_3000n\"\n",
    "        / \"w-mixed\"\n",
    "        / \"10fold-oversampling\"\n",
    "    )\n",
    "    pred_df = pd.read_csv(\n",
    "        pred_data_dir / \"full-10fold-validation_prediction.csv\", header=0, index_col=0\n",
    "    )\n",
    "\n",
    "    # Merge all\n",
    "    zscore_df = zscore_df.merge(metadata_df, left_on=\"filename\", right_on=\"md5sum\")\n",
    "    zscore_df = zscore_df.merge(pred_df, left_on=\"filename\", right_index=True)\n",
    "    zscore_df[\"Max pred\"] = zscore_df[[\"female\", \"male\"]].max(axis=1)\n",
    "    zscore_df.set_index(\"md5sum\", inplace=True)\n",
    "    return zscore_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig2_B(zscore_df: pd.DataFrame) -> None:\n",
    "    \"\"\"Create figure 2B.\n",
    "\n",
    "    Args:\n",
    "        zscore_df: The dataframe with z-score data.\n",
    "    \"\"\"\n",
    "    assay_sizes = zscore_df[ASSAY].value_counts()\n",
    "    assays = sorted(assay_sizes.index)\n",
    "\n",
    "    x_title = \"Assay+Sex z-score distributions - Male/Female classification disagreement separate\"\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=len(assays),\n",
    "        shared_yaxes=False,\n",
    "        x_title=x_title,\n",
    "        y_title=\"z-score\",\n",
    "        horizontal_spacing=0.02,\n",
    "        subplot_titles=[\n",
    "            f\"{assay_label} ({assay_sizes[assay_label]})\" for assay_label in assays\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    for i, assay_label in enumerate(sorted(assays)):\n",
    "        sub_df = zscore_df[zscore_df[ASSAY] == assay_label]\n",
    "\n",
    "        y_values = sub_df[metric_label]\n",
    "        hovertext = [\n",
    "            f\"{epirr}: z-score={z_score:.3f}, pred={pred:.3f}\"\n",
    "            for epirr, pred, z_score in zip(\n",
    "                sub_df[\"EpiRR\"],\n",
    "                sub_df[\"Max pred\"],\n",
    "                sub_df[metric_label],\n",
    "            )\n",
    "        ]\n",
    "        hovertext = np.array(hovertext)\n",
    "\n",
    "        female_idx = np.argwhere((sub_df[\"True class\"] == \"female\").values).flatten()\n",
    "        male_idx = np.argwhere((sub_df[\"True class\"] == \"male\").values).flatten()\n",
    "\n",
    "        predicted_as_female_idx = np.argwhere(\n",
    "            (\n",
    "                (sub_df[\"Predicted class\"] == \"female\") & (sub_df[\"True class\"] == \"male\")\n",
    "            ).values\n",
    "        ).flatten()\n",
    "        predicted_as_male_idx = np.argwhere(\n",
    "            (\n",
    "                (sub_df[\"Predicted class\"] == \"male\") & (sub_df[\"True class\"] == \"female\")\n",
    "            ).values\n",
    "        ).flatten()\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                name=\"\",\n",
    "                x0=i,\n",
    "                y=y_values[female_idx],\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[female_idx],\n",
    "                side=\"negative\",\n",
    "                line_color=\"red\",\n",
    "                spanmode=\"hard\",\n",
    "                showlegend=False,\n",
    "                marker=dict(size=1),\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                name=\"\",\n",
    "                x0=i,\n",
    "                y=y_values[male_idx],\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=\"all\",\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[male_idx],\n",
    "                side=\"positive\",\n",
    "                line_color=\"blue\",\n",
    "                spanmode=\"hard\",\n",
    "                showlegend=False,\n",
    "                marker=dict(size=1),\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "        temp_y_values = y_values[predicted_as_female_idx]\n",
    "        temp_size = 1 + 5 * sub_df[\"Max pred\"].values[predicted_as_female_idx]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=\"\",\n",
    "                x=[i - 0.2] * len(temp_y_values),\n",
    "                y=temp_y_values,\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=\"red\", size=temp_size),\n",
    "                showlegend=False,\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[predicted_as_female_idx],\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "        temp_y_values = y_values[predicted_as_male_idx]\n",
    "        temp_size = 1 + 5 * sub_df[\"Max pred\"].values[predicted_as_male_idx]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                name=\"\",\n",
    "                x=[i - 0.25] * len(temp_y_values),\n",
    "                y=temp_y_values,\n",
    "                mode=\"markers\",\n",
    "                marker=dict(color=\"blue\", size=temp_size),\n",
    "                showlegend=False,\n",
    "                hovertemplate=\"%{text}\",\n",
    "                text=hovertext[predicted_as_male_idx],\n",
    "            ),\n",
    "            row=1,\n",
    "            col=i + 1,\n",
    "        )\n",
    "\n",
    "    # Add a dummy scatter plot for legend\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Female\",\n",
    "            marker=dict(color=\"red\", size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Female\",\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode=\"markers\",\n",
    "            name=\"Male\",\n",
    "            marker=dict(color=\"blue\", size=20),\n",
    "            showlegend=True,\n",
    "            legendgroup=\"Male\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(showticklabels=False)\n",
    "    title = \"z-score(mean chrY coverage per file) distribution per assay\"\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{title}\",\n",
    "        width=3000,\n",
    "        height=1000,\n",
    "    )\n",
    "\n",
    "    # Save figure\n",
    "    logdir = base_fig_dir / \"fig2\" / \"fig2_B\"\n",
    "    logdir.mkdir(parents=False, exist_ok=True)\n",
    "    name = \"fig2_B\"\n",
    "\n",
    "    fig.write_image(logdir / f\"{name}.svg\")\n",
    "    fig.write_image(logdir / f\"{name}.png\")\n",
    "    fig.write_html(logdir / f\"{name}.html\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_df = prepare_fig_2B_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2_B(zscore_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
