{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to quantify bias present in metadata\n",
    "Q: Can you identify certain labels by using other metadata\n",
    "e.g. find cell type using project+assay+other\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display  # pylint: disable=unused-import\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    create_mislabel_corrector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "metadata_df = metadata_handler.load_metadata_df(\"v2\")\n",
    "metadata = metadata_handler.load_metadata(\"v2\")\n",
    "\n",
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate bias in input samples classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect observed average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_100kb_all_none\"\n",
    "\n",
    "exclusion = [\"cancer\", \"random\", \"track\", \"disease\", \"second\", \"end\"]\n",
    "exclude_names = [\"chip\", \"no-mixed\", \"ct\", \"7c\"]\n",
    "\n",
    "all_split_results = split_results_handler.general_split_metrics(\n",
    "    results_dir=results_dir,\n",
    "    exclude_categories=exclusion,\n",
    "    exclude_names=exclude_names,\n",
    "    merge_assays=True,\n",
    "    mislabel_corrections=create_mislabel_corrector(paper_dir),\n",
    "    return_type=\"split_results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_split_results: Dict[str, pd.DataFrame] = split_results_handler.concatenate_split_results(all_split_results, concat_first_level=True)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_name, df in list(concat_split_results.items()):\n",
    "    new_df = metadata_handler.join_metadata(df, metadata)\n",
    "    concat_split_results[cat_name] = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_input_acc = {}\n",
    "for cat_name, df in list(concat_split_results.items()):\n",
    "    # filtered_df = df[df[ASSAY] == \"input\"]\n",
    "    filtered_df = df\n",
    "    acc = (filtered_df[\"True class\"] == filtered_df[\"Predicted class\"]).sum() / len(\n",
    "        filtered_df\n",
    "    )\n",
    "    avg_input_acc[cat_name] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(avg_input_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_input_acc[SEX] = avg_input_acc[\"harmonized_donor_sex_w-mixed\"]\n",
    "concat_split_results[SEX] = concat_split_results[\"harmonized_donor_sex_w-mixed\"]\n",
    "\n",
    "avg_input_acc[ASSAY] = avg_input_acc[\"assay_epiclass_11c\"]\n",
    "concat_split_results[ASSAY] = concat_split_results[\"assay_epiclass_11c\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute max bias accuracy using metadata as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_input_bias_categories(target_category: str) -> List[List[str]]:\n",
    "    \"\"\"Define bias categories used for bias analysis.\n",
    "\n",
    "    Args:\n",
    "        target_category (str): Classification target category. Is excluded from input lists.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: List of bias categories.\n",
    "    \"\"\"\n",
    "    bias_categories_1 = [ASSAY, \"project\", \"harmonized_biomaterial_type\", CELL_TYPE]\n",
    "    bias_categories_2 = [\n",
    "        ASSAY,\n",
    "        \"project\",\n",
    "        \"harmonized_biomaterial_type\",\n",
    "        CELL_TYPE,\n",
    "        LIFE_STAGE,\n",
    "    ]\n",
    "    bias_categories_3 = [ASSAY, \"project\", \"harmonized_biomaterial_type\", CELL_TYPE, SEX]\n",
    "    bias_categories_4 = [\n",
    "        ASSAY,\n",
    "        \"project\",\n",
    "        \"harmonized_biomaterial_type\",\n",
    "        CELL_TYPE,\n",
    "        SEX,\n",
    "        LIFE_STAGE,\n",
    "    ]\n",
    "\n",
    "    all_bias_categories = [\n",
    "        bias_categories_1,\n",
    "        bias_categories_2,\n",
    "        bias_categories_3,\n",
    "        bias_categories_4,\n",
    "    ]\n",
    "    for bias_categories in all_bias_categories:\n",
    "        try:\n",
    "            bias_categories.remove(target_category)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return all_bias_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models() -> List:\n",
    "    \"\"\"Create models for bias analysis.\"\"\"\n",
    "    lr_model_1 = LogisticRegression(\n",
    "        solver=\"lbfgs\", max_iter=1000, multi_class=\"multinomial\", random_state=42\n",
    "    )\n",
    "    lr_model_2 = LogisticRegression(\n",
    "        solver=\"lbfgs\", max_iter=1000, multi_class=\"ovr\", random_state=42\n",
    "    )\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    svm_model = SVC(kernel=\"linear\", random_state=42)\n",
    "    svm_model_rbf = SVC(kernel=\"rbf\", random_state=42)\n",
    "    return [lr_model_1, lr_model_2, rf_model, svm_model, svm_model_rbf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_samples(\n",
    "    metadata_df: pd.DataFrame, target_category: str, verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Filter samples based on the output category to match the original training set.\"\"\"\n",
    "    df = metadata_df.copy(deep=True)\n",
    "\n",
    "    if \"md5sum\" not in df.columns:\n",
    "        df[\"md5sum\"] = df.index\n",
    "\n",
    "    df = df[df[\"md5sum\"].isin(concat_split_results[target_category][\"md5sum\"])]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Metadata shape:\", metadata_df.shape)\n",
    "        print(\"Filtered shape:\", df.shape)\n",
    "        display(df[target_category].value_counts())\n",
    "\n",
    "    return df  # type: ignore\n",
    "\n",
    "\n",
    "def find_max_bias(\n",
    "    metadata_df: pd.DataFrame, target_category: str, verbose: bool = True\n",
    ") -> Dict[Tuple[str, ...], float]:\n",
    "    \"\"\"Find the bias categories that provide the highest accuracy for the target category.\"\"\"\n",
    "\n",
    "    filtered_df = filter_samples(metadata_df, target_category)\n",
    "\n",
    "    max_bias_dict = {}\n",
    "    for bias_categories in define_input_bias_categories(target_category):\n",
    "        print(f\"Using bias categories: {bias_categories}\")\n",
    "        X = filtered_df[bias_categories]\n",
    "        y = filtered_df[target_category]\n",
    "\n",
    "        # one-hot encode the data\n",
    "        X_encoded = OneHotEncoder().fit_transform(X).toarray()  # type: ignore\n",
    "        y_encoded = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        max_acc = 0\n",
    "        for model in create_models():\n",
    "            scores = cross_val_score(\n",
    "                model, X_encoded, y_encoded, cv=10, scoring=\"accuracy\", n_jobs=-1\n",
    "            )\n",
    "            if verbose:\n",
    "                print(f\"Model: {model}\")\n",
    "                print(f\"Accuracy: {np.mean(scores):.2f} (+/- {np.std(scores):.2f})\")\n",
    "            if np.mean(scores) > max_acc:\n",
    "                max_acc = np.mean(scores)\n",
    "                max_bias_dict[tuple(bias_categories)] = max_acc\n",
    "\n",
    "    return max_bias_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_max_bias(\n",
    "    metadata_df: pd.DataFrame, target_categories: List[str], verbose: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Compute the max metadata bias for all target categories.\"\"\"\n",
    "    final_results: Dict[str, Any] = {}\n",
    "    for target_category in target_categories:\n",
    "        if verbose:\n",
    "            print(f\"Target category: {target_category}\")\n",
    "\n",
    "        max_bias_dict = find_max_bias(metadata_df, target_category)\n",
    "        max_bias_cats, max_bias_acc = max(max_bias_dict.items(), key=lambda x: x[1])\n",
    "        if verbose:\n",
    "            print(f\"Max bias categories: {max_bias_cats}\")\n",
    "            print(f\"Max bias acc: {max_bias_acc}\")\n",
    "\n",
    "        MLP_acc = avg_input_acc[target_category]\n",
    "\n",
    "        acc_to_compare = [\n",
    "            acc for cat, acc in avg_input_acc.items() if cat in max_bias_cats\n",
    "        ]\n",
    "        avg_MLP_acc = np.mean(acc_to_compare)\n",
    "        max_acc_with_bias = max_bias_acc * avg_MLP_acc\n",
    "\n",
    "        if verbose:\n",
    "            print(\"CLASSIFICATION ACCURACY\")\n",
    "            print(f\"Average {target_category} observed acc: {MLP_acc:.1%}\")\n",
    "            print(f\"Average MLP acc on bias categories: {avg_MLP_acc:.1%}\")\n",
    "            print(\n",
    "                f\"Max avg acc with bias from ({max_bias_cats}): {max_acc_with_bias:.1%}\"\n",
    "            )\n",
    "            print(f\"Not accounted for: {MLP_acc - max_acc_with_bias:.1%}\\n\")\n",
    "\n",
    "        final_results[target_category] = {\n",
    "            \"max_bias_cats\": max_bias_cats,\n",
    "            \"max_bias_acc\": max_bias_acc,\n",
    "            \"MLP_acc\": MLP_acc,\n",
    "            \"bias_avg_MLP_acc\": avg_MLP_acc,\n",
    "            \"max_bias_acc_corrected\": max_acc_with_bias,\n",
    "            \"acc_diff\": MLP_acc - max_acc_with_bias,\n",
    "        }\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_categories = [\"project\", \"harmonized_biomaterial_type\", CELL_TYPE, SEX, LIFE_STAGE]\n",
    "final_results = compute_all_max_bias(metadata_df, target_categories)\n",
    "\n",
    "final_results_df = pd.DataFrame.from_dict(final_results, orient=\"index\")\n",
    "final_results_df.to_csv(\"metadata_bias_analysis_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between results and metadata labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell type classification: check (input, ct) pairs for enrichment in any metadata category  \n",
    "&emsp;Use:  \n",
    "  - harmonized_donor_life_stage  \n",
    "  - harmonized_donor_sex  \n",
    "  - harmonized_sample_cancer_high  \n",
    "  - harmonized_biomaterial  \n",
    "  - paired_end  \n",
    "  - project  \n",
    "\n",
    "&emsp;find a 3rd factor metric, e.g. if any pair (assay, ct) subclass is very different from global dist, it can use that info as 3rd factor, and we're looking at assay specifically  \n",
    "  - one score per pair, pearson w accuracy vector (one vector per assay)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_results_cell_type = (\n",
    "    base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"hg38_10kb_all_none\"\n",
    ")\n",
    "if not path_results_cell_type.exists():\n",
    "    raise FileNotFoundError(f\"{path_results_cell_type} does not exist.\")\n",
    "\n",
    "# Load split results into one combined dataframe\n",
    "ct_split_dfs = split_results_handler.gather_split_results_across_categories(\n",
    "    path_results_cell_type\n",
    ")[\"harmonized_sample_ontology_intermediate_1l_3000n_10fold-oversampling\"]\n",
    "ct_full_df = pd.concat(ct_split_dfs.values(), axis=0)\n",
    "\n",
    "# Load metadata and join with split results\n",
    "ct_full_df = metadata_handler.join_metadata(ct_full_df, metadata)\n",
    "ct_full_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metadata_distribution(\n",
    "    df: pd.DataFrame, columns: List[str]\n",
    ") -> Dict[str, pd.Series]:\n",
    "    \"\"\"\n",
    "    Calculates the percentage of metadata labels within specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: A pandas DataFrame containing the data.\n",
    "        columns: A list of column names to analyze.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are column names and values are Series objects containing\n",
    "        the percentage of each unique label in the respective column.\n",
    "    \"\"\"\n",
    "    distribution = {}\n",
    "    nb_samples = len(df)\n",
    "    for column in columns:\n",
    "        # Count the occurrences of each unique value in the column\n",
    "        value_counts = df[column].value_counts(dropna=False)\n",
    "        # Calculate the percentages\n",
    "        percentages = (value_counts / nb_samples) * 100\n",
    "        # Store the results in the dictionary\n",
    "        distribution[column] = percentages\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_label_ratios(\n",
    "    target_distribution: Dict[str, pd.Series],\n",
    "    comparison_distributions: List[Dict[str, pd.Series]],\n",
    "    labels: List[str],\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compares label ratios of a target distribution against multiple comparison distributions,\n",
    "    calculating the difference in percentage points for each label within each metadata category.\n",
    "\n",
    "    Args:\n",
    "        target_distribution: A dictionary of Series representing the target distribution for comparison.\n",
    "        comparison_distributions: A list of dictionaries of Series, where each dictionary\n",
    "                                  represents a distribution (e.g., assay, cell type, global) for comparison.\n",
    "        labels: A list of labels corresponding to each distribution in `comparison_distributions`,\n",
    "                used for labeling the columns in the result.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of DataFrames, where each DataFrame shows the difference in percentage points\n",
    "        for each label in a metadata category between the target distribution and each of the\n",
    "        comparison distributions.\n",
    "    \"\"\"\n",
    "    comparison_results = {}\n",
    "    for category, target_series in target_distribution.items():\n",
    "        # Initialize a DataFrame to store comparison results for this category\n",
    "        comparison_df = pd.DataFrame()\n",
    "\n",
    "        for label, comparison_distribution in zip(labels, comparison_distributions):\n",
    "            # Ensure the comparison distribution series for this category exists and align target with comparison\n",
    "            comparison_series = comparison_distribution.get(\n",
    "                category, pd.Series(dtype=\"float64\")\n",
    "            )\n",
    "            aligned_target, aligned_comparison = target_series.align(\n",
    "                comparison_series, fill_value=0\n",
    "            )\n",
    "\n",
    "            # Calculate difference in percentage points\n",
    "            difference = aligned_target - aligned_comparison\n",
    "\n",
    "            # Store the results in the comparison DataFrame\n",
    "            comparison_df[f\"Difference_vs_{label}\"] = difference\n",
    "\n",
    "        comparison_results[category] = comparison_df\n",
    "\n",
    "    return comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_categories = [\n",
    "    \"harmonized_donor_life_stage\",\n",
    "    \"harmonized_donor_sex\",\n",
    "    \"harmonized_sample_disease_high\",\n",
    "    \"harmonized_biomaterial_type\",\n",
    "    \"paired_end\",\n",
    "    \"project\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_third_factor_correlation(\n",
    "    ct_full_df: pd.DataFrame,\n",
    "    metadata_categories: List[str],\n",
    "    save_full_details: bool = False,\n",
    "    metric: str = \"min\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the correlation between third factor influence and cell type classification accuracy for each assay.\n",
    "\n",
    "    This function operates on classification results to evaluate how a third factor, represented by metadata category distributions,\n",
    "    correlates with the accuracy of cell type classifications across assays. It involves comparing metadata distributions\n",
    "    within assay and cell type groups to a global distribution, and then correlating these comparisons with classification\n",
    "    accuracies.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with epigenomics data, including assays, cell types, and metadata for classification.\n",
    "        metadata_categories (List[str]): A list of metadata categories to analyze.\n",
    "        save_full_details (bool): Whether to save the full details of the comparison results to a CSV file.\n",
    "        metric (str): The metric to apply on \"Difference vs Global\" col for correlation calculation. (max, min, abs_max, abs_min, abs_sum)\n",
    "    \"\"\"\n",
    "    global_dist = calculate_metadata_distribution(ct_full_df, metadata_categories)\n",
    "    subclass_distributions = {}\n",
    "    comparison_results = (\n",
    "        {}\n",
    "    )  # Initialize a dict to hold comparison results for each subgroup\n",
    "\n",
    "    for group in ct_full_df.groupby(ASSAY):\n",
    "        label = group[0]\n",
    "        sub_df = group[1]\n",
    "        subclass_distributions[label] = calculate_metadata_distribution(\n",
    "            sub_df, metadata_categories\n",
    "        )\n",
    "\n",
    "    for group in ct_full_df.groupby(CELL_TYPE):\n",
    "        label = group[0]\n",
    "        sub_df = group[1]\n",
    "        subclass_distributions[label] = calculate_metadata_distribution(\n",
    "            sub_df, metadata_categories\n",
    "        )\n",
    "\n",
    "    # Loop through each group and compare to global\n",
    "    for group in ct_full_df.groupby([ASSAY, CELL_TYPE]):\n",
    "        assay, cell_type = group[0]  # type: ignore\n",
    "        sub_df = group[1]\n",
    "        pair_subclass_dist = calculate_metadata_distribution(sub_df, metadata_categories)\n",
    "        subclass_distributions[(assay, cell_type)] = pair_subclass_dist\n",
    "\n",
    "        assay_dist = subclass_distributions[assay]\n",
    "        cell_type_dist = subclass_distributions[cell_type]\n",
    "\n",
    "        comparisons_dists = [assay_dist, cell_type_dist, global_dist]\n",
    "        comparison_labels = [assay, cell_type, \"global\"]\n",
    "\n",
    "        comparison_results[(assay, cell_type)] = compare_label_ratios(\n",
    "            target_distribution=pair_subclass_dist,\n",
    "            comparison_distributions=comparisons_dists,\n",
    "            labels=comparison_labels,\n",
    "        )\n",
    "\n",
    "    pair_dfs = {}\n",
    "    pairs_3rd_factor = {}\n",
    "    for (assay, cell_type), comparisons in comparison_results.items():\n",
    "        # Initialize an empty list to collect DataFrames for concatenation\n",
    "        dfs_to_concat = []\n",
    "\n",
    "        for category, df_comparison in comparisons.items():\n",
    "            df_comparison.columns = [\n",
    "                \"Difference vs Assay\",\n",
    "                \"Difference vs Cell Type\",\n",
    "                \"Difference vs Global\",\n",
    "            ]\n",
    "            # Add identifiers for the assay, cell type, and category\n",
    "            df_comparison[\"Assay\"] = assay\n",
    "            df_comparison[\"Cell Type\"] = cell_type\n",
    "            df_comparison[\"Category\"] = category\n",
    "\n",
    "            subclass_dist = subclass_distributions[(assay, cell_type)][category]\n",
    "            df_comparison[\"(assay, ct) subclass %\"] = subclass_dist\n",
    "\n",
    "            # Collect the DataFrame\n",
    "            dfs_to_concat.append(df_comparison.reset_index())\n",
    "\n",
    "        # Concatenate all DataFrames along rows\n",
    "        final_df = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "        final_df.fillna(0, inplace=True)\n",
    "\n",
    "        new_columns = final_df.columns.tolist()\n",
    "        new_first = [\"index\", \"Category\", \"(assay, ct) subclass %\"]\n",
    "        for label in new_first:\n",
    "            new_columns.remove(label)\n",
    "        new_columns = new_first + new_columns\n",
    "        final_df = final_df[new_columns]\n",
    "\n",
    "        pair_dfs[(assay, cell_type)] = final_df\n",
    "\n",
    "        if metric == \"max\":\n",
    "            val_3rd_factor = final_df[\"Difference vs Global\"].max()\n",
    "        elif metric == \"min\":\n",
    "            val_3rd_factor = final_df[\"Difference vs Global\"].min()\n",
    "        elif metric == \"abs_max\":\n",
    "            val_3rd_factor = final_df[\"Difference vs Global\"].abs().max()\n",
    "        elif metric == \"abs_min\":\n",
    "            val_3rd_factor = final_df[\"Difference vs Global\"].abs().min()\n",
    "        elif metric == \"abs_sum\":\n",
    "            val_3rd_factor = final_df[\"Difference vs Global\"].abs().sum()\n",
    "\n",
    "        pairs_3rd_factor[(assay, cell_type)] = val_3rd_factor\n",
    "\n",
    "    # Subclass accuracy per assay\n",
    "    assay_labels = sorted(ct_full_df[ASSAY].unique())\n",
    "    ct_labels = sorted(ct_full_df[CELL_TYPE].unique())\n",
    "    assay_accuracies = {}\n",
    "    for assay_label in assay_labels:\n",
    "        assay_df = ct_full_df[ct_full_df[ASSAY] == assay_label]\n",
    "\n",
    "        # cell type subclass accuracy\n",
    "        subclass_size = assay_df.groupby([\"True class\"]).agg(\"size\")\n",
    "        pred_confusion_matrix = assay_df.groupby([\"True class\", \"Predicted class\"]).agg(\n",
    "            \"size\"\n",
    "        )\n",
    "        ct_accuracies = {\n",
    "            ct_label: pred_confusion_matrix[ct_label][ct_label]\n",
    "            / float(subclass_size[ct_label])\n",
    "            for ct_label in sorted(ct_labels)\n",
    "        }\n",
    "\n",
    "        assay_accuracies[assay_label] = ct_accuracies\n",
    "\n",
    "    # Concatenate all DataFrames along rows\n",
    "    if save_full_details:\n",
    "        all_pairs_df = pd.concat(pair_dfs, axis=0, ignore_index=True)\n",
    "        for assay in assay_labels:\n",
    "            for ct in ct_labels:\n",
    "                all_pairs_df.loc[\n",
    "                    (all_pairs_df[\"Assay\"] == assay) & (all_pairs_df[\"Cell Type\"] == ct),\n",
    "                    \"Accuracy\",\n",
    "                ] = assay_accuracies[assay][ct]\n",
    "\n",
    "        all_pairs_df.columns = [\n",
    "            \"Label\" if x == \"index\" else x for x in all_pairs_df.columns\n",
    "        ]\n",
    "        file_path = base_fig_dir / \"flagship\" / \"metadata_comparison_all.csv\"\n",
    "        all_pairs_df.to_csv(file_path, index=False)\n",
    "\n",
    "    pearson_3rd_factor = {}\n",
    "    for assay, acc_dict in assay_accuracies.items():\n",
    "        acc_vector = {ct: acc_dict[ct] for ct in ct_labels}\n",
    "        acc_vector = pd.Series(acc_vector)\n",
    "\n",
    "        diff_metric = {ct: pairs_3rd_factor[(assay, ct)] for ct in ct_labels}\n",
    "        diff_metric = pd.Series(diff_metric)\n",
    "        pearson = acc_vector.corr(diff_metric, method=\"pearson\")\n",
    "        pearson_3rd_factor[assay] = pearson\n",
    "\n",
    "    return pearson_3rd_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = compute_third_factor_correlation(ct_full_df, metadata_categories, save_full_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_third_factor_correlations(ct_full_df, metadata_categories):\n",
    "    \"\"\"Compute third factor correlations for all supported metrics.\"\"\"\n",
    "    for metric in [\"max\", \"min\", \"abs_max\", \"abs_min\", \"abs_sum\"]:\n",
    "        pearson_series = []\n",
    "        for metadata_category in metadata_categories:\n",
    "            pearson_dict = compute_third_factor_correlation(\n",
    "                ct_full_df, [metadata_category], save_full_details=False, metric=metric\n",
    "            )\n",
    "            pearson_df = pd.DataFrame(pearson_dict, index=[metadata_category])\n",
    "            pearson_series.append(pearson_df)\n",
    "\n",
    "        full_pearson_df = pd.concat(pearson_series, axis=0)\n",
    "\n",
    "        # Add max for each row and column\n",
    "        full_pearson_df[\"Max\"] = full_pearson_df.abs().max(axis=1)\n",
    "        max_row = full_pearson_df.abs().max(axis=0)\n",
    "        max_row.name = \"Max\"\n",
    "        full_pearson_df = pd.concat([full_pearson_df, max_row.to_frame().T], axis=0)\n",
    "\n",
    "        output_path = (\n",
    "            base_fig_dir / \"flagship\" / f\"3rd_factor_{metric}_pearson_correlation.csv\"\n",
    "        )\n",
    "        full_pearson_df.to_csv(output_path, float_format=\"%.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_all_third_factor_correlations(ct_full_df, metadata_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Various metadata proportions counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_df = ct_full_df[ct_full_df[ASSAY] == \"input\"]\n",
    "# ct_df = input_df[input_df[CELL_TYPE] == \"neutrophil\"]\n",
    "# for cat in metadata_categories:\n",
    "#     print(ct_df.value_counts(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata = metadata_handler.load_metadata(\"v2\")\n",
    "# metadata_2_uuid = UUIDMetadata.from_metadata(metadata)\n",
    "\n",
    "# for md5 in list(metadata_2_uuid.md5s):\n",
    "#     if md5 not in ct_full_df.index:\n",
    "#         del metadata_2_uuid[md5]\n",
    "\n",
    "# metadata_2_uuid.select_category_subsets(\"track_type\", [\"pval\", \"ctl_raw\", \"Unique_minusRaw\", \"gembs_neg\", \"Unique_raw\"])\n",
    "# uuid_df = pd.DataFrame.from_records(list(metadata_2_uuid.datasets))\n",
    "# uuid_df.set_index(\"md5sum\", inplace=True)\n",
    "# uuid_df[ASSAY].replace(ASSAY_MERGE_DICT, inplace=True)\n",
    "# uuid_df.groupby([CELL_TYPE, ASSAY]).aggregate(\"size\").unstack().fillna(0).to_csv(base_fig_dir/\"flagship\"/\"ct_assay_counts_unique_uuid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"harmonized_biomaterial_type == primary cell\")\n",
    "df = ct_full_df[ct_full_df[\"harmonized_biomaterial_type\"] == \"primary cell\"].value_counts(\n",
    "    CELL_TYPE\n",
    ")\n",
    "df = df / df.sum() * 100\n",
    "display(df.shape)\n",
    "display(df)\n",
    "\n",
    "\n",
    "# print(\"project == BLUEPRINT\")\n",
    "# df = ct_full_df[ct_full_df[\"project\"] == \"BLUEPRINT\"].value_counts(CELL_TYPE)\n",
    "print(\"project == NIH Roadmap Epigenomics\")\n",
    "df = ct_full_df[ct_full_df[\"project\"] == \"NIH Roadmap Epigenomics\"].value_counts(\n",
    "    CELL_TYPE\n",
    ")\n",
    "# print(\"project == CEEHRC\")\n",
    "# df = ct_full_df[ct_full_df[\"project\"] == \"CEEHRC\"].value_counts(CELL_TYPE)\n",
    "df = df / df.sum() * 100\n",
    "display(df.shape)\n",
    "display(df)\n",
    "\n",
    "print(\"paired_end == FALSE\")\n",
    "df = ct_full_df[ct_full_df[\"paired_end\"] == \"FALSE\"].value_counts(CELL_TYPE)\n",
    "df = df / df.sum() * 100\n",
    "display(df.shape)\n",
    "display(df)\n",
    "\n",
    "print(\"harmonized_sample_disease_high == Healthy/None\")\n",
    "df = ct_full_df[\n",
    "    ct_full_df[\"harmonized_sample_disease_high\"] == \"Healthy/None\"\n",
    "].value_counts(CELL_TYPE)\n",
    "df = df / df.sum() * 100\n",
    "display(df.shape)\n",
    "display(df)\n",
    "\n",
    "print(\"harmonized_biomaterial_type distribution\")\n",
    "df = ct_full_df[\"harmonized_biomaterial_type\"].value_counts()\n",
    "df = df / df.sum() * 100\n",
    "display(df.shape)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = ct_full_df[\n",
    "    (ct_full_df[ASSAY] == \"input\")\n",
    "    & (ct_full_df[\"harmonized_biomaterial_type\"] == \"primary cell\")\n",
    "    & (ct_full_df[\"project\"] == \"BLUEPRINT\")\n",
    "    & (ct_full_df[\"paired_end\"] == \"FALSE\")\n",
    "    & (ct_full_df[\"harmonized_sample_disease_high\"] == \"Healthy/None\")\n",
    "].value_counts(CELL_TYPE)\n",
    "intersection = intersection / intersection.sum() * 100\n",
    "display(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = ct_full_df[\n",
    "    (ct_full_df[\"harmonized_biomaterial_type\"] == \"primary cell\")\n",
    "    & (ct_full_df[\"project\"] == \"BLUEPRINT\")\n",
    "    & (ct_full_df[\"paired_end\"] == \"FALSE\")\n",
    "    & (ct_full_df[\"harmonized_sample_disease_high\"] == \"Healthy/None\")\n",
    "].value_counts(CELL_TYPE)\n",
    "intersection = intersection / intersection.sum() * 100\n",
    "display(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = ct_full_df[\n",
    "    (ct_full_df[ASSAY] == \"input\")\n",
    "    & (ct_full_df[\"harmonized_biomaterial_type\"] == \"primary cell\")\n",
    "    & (ct_full_df[\"harmonized_sample_disease_high\"] == \"Healthy/None\")\n",
    "    & (ct_full_df[\"project\"].isin([\"NIH Roadmap Epigenomics\", \"CEEHRC\"]))\n",
    "].value_counts(CELL_TYPE)\n",
    "intersection = intersection / intersection.sum() * 100\n",
    "display(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = ct_full_df[\n",
    "    (ct_full_df[\"harmonized_biomaterial_type\"] == \"primary cell\")\n",
    "    & (ct_full_df[\"harmonized_sample_disease_high\"] == \"Healthy/None\")\n",
    "    & (ct_full_df[\"project\"].isin([\"NIH Roadmap Epigenomics\", \"CEEHRC\"]))\n",
    "].value_counts(CELL_TYPE)\n",
    "intersection = intersection / intersection.sum() * 100\n",
    "display(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = ct_full_df[\n",
    "    (ct_full_df[\"harmonized_biomaterial_type\"] == \"primary cell\")\n",
    "    & (ct_full_df[\"harmonized_sample_disease_high\"] == \"Healthy/None\")\n",
    "].value_counts(CELL_TYPE)\n",
    "intersection = intersection / intersection.sum() * 100\n",
    "display(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = ct_full_df[\n",
    "    (ct_full_df[ASSAY] == \"input\")\n",
    "    & (ct_full_df[\"harmonized_biomaterial_type\"] == \"primary tissue\")\n",
    "    & (ct_full_df[\"project\"].isin([\"NIH Roadmap Epigenomics\", \"CEEHRC\"]))\n",
    "].value_counts(CELL_TYPE)\n",
    "intersection = intersection / intersection.sum() * 100\n",
    "display(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = ct_full_df[\n",
    "    (ct_full_df[ASSAY] == \"input\")\n",
    "    & (ct_full_df[\"harmonized_biomaterial_type\"] == \"primary cell\")\n",
    "    & (ct_full_df[\"harmonized_sample_disease_high\"] == \"Healthy/None\")\n",
    "].value_counts(CELL_TYPE)\n",
    "intersection = intersection / intersection.sum() * 100\n",
    "display(intersection)\n",
    "\n",
    "intersection = ct_full_df[\n",
    "    (ct_full_df[ASSAY] == \"input\")\n",
    "    & (ct_full_df[\"harmonized_biomaterial_type\"] == \"primary cell\")\n",
    "    & (ct_full_df[\"harmonized_sample_disease_high\"] == \"Healthy/None\")\n",
    "].value_counts(CELL_TYPE)\n",
    "intersection = intersection / intersection.sum() * 100\n",
    "display(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = ct_full_df[\n",
    "    (ct_full_df[\"harmonized_biomaterial_type\"] == \"primary tissue\")\n",
    "].value_counts(CELL_TYPE)\n",
    "intersection = intersection / intersection.sum() * 100\n",
    "display(intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subclass (assay, ct, life_stage) accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_labels = sorted(ct_full_df[ASSAY].unique())\n",
    "ct_labels = sorted(ct_full_df[CELL_TYPE].unique())\n",
    "life_stages = ct_full_df[\"harmonized_donor_life_stage\"].unique().tolist()\n",
    "life_stages.remove(\"unknown\")\n",
    "\n",
    "acc_list = []\n",
    "for assay_label in assay_labels:\n",
    "    assay_df = ct_full_df[ct_full_df[ASSAY] == assay_label]\n",
    "    for ct in ct_labels:\n",
    "        ct_df = assay_df[assay_df[CELL_TYPE] == ct]\n",
    "        for life_stage in life_stages:\n",
    "            life_stage_df = ct_df[ct_df[\"harmonized_donor_life_stage\"] == life_stage]\n",
    "            acc = life_stage_df[\"Predicted class\"].eq(life_stage_df[\"True class\"]).mean()\n",
    "            size = len(life_stage_df)\n",
    "            acc_list.append((assay_label, ct, life_stage, size, acc))\n",
    "\n",
    "acc_df = pd.DataFrame(\n",
    "    acc_list, columns=[\"Assay\", \"Cell Type\", \"Life Stage\", \"Size\", \"Accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_df.to_csv(base_fig_dir / \"flagship\" / \"assay_ct_life_stage_accuracy.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
