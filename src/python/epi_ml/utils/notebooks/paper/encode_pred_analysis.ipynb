{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to analyse encode predictions.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches, pointless-statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import functools\n",
    "import re\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.core.metadata import Metadata\n",
    "from epi_ml.utils.classification_merging_utils import merge_dataframes\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    add_second_highest_prediction,\n",
    "    display_perc,\n",
    "    merge_life_stages,\n",
    ")\n",
    "\n",
    "# from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANCER = \"harmonized_sample_cancer_high\"\n",
    "CORE_ASSAYS = ASSAY_ORDER[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_dir = base_data_dir / \"metadata\" / \"encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_predictions_dir = base_data_dir / \"training_results\" / \"predictions\" / \"encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [encode_metadata_dir, encode_predictions_dir]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Directory {path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_ct = [\n",
    "    \"T cell\",\n",
    "    \"neutrophil\",\n",
    "    \"brain\",\n",
    "    \"monocyte\",\n",
    "    \"lymphocyte of B lineage\",\n",
    "    \"myeloid cell\",\n",
    "    \"venous blood\",\n",
    "    \"macrophage\",\n",
    "    \"mesoderm-derived structure\",\n",
    "    \"endoderm-derived structure\",\n",
    "    \"colon\",\n",
    "    \"connective tissue cell\",\n",
    "    \"hepatocyte\",\n",
    "    \"mammary gland epithelial cell\",\n",
    "    \"muscle organ\",\n",
    "    \"extraembryonic cell\",\n",
    "]\n",
    "accepted_ct = [ct.lower() for ct in accepted_ct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge chip + rna metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata_path = encode_metadata_dir / \"encode_metadata_2023-10-25.csv\"\n",
    "chip_metadata_df = pd.read_csv(full_metadata_path)\n",
    "chip_metadata_df.set_index(\"md5sum\", inplace=True)\n",
    "chip_metadata_df[\"md5sum\"] = chip_metadata_df.index\n",
    "\n",
    "# Necessary for the augmentation script later\n",
    "if not full_metadata_path.with_suffix(\".json\").exists():\n",
    "    dict_meta = chip_metadata_df.to_dict(orient=\"index\")\n",
    "    metadata_obj = Metadata.from_dict(dict_meta, allow_non_md5sum_index=True)  # type: ignore\n",
    "    metadata_obj.save(full_metadata_path.with_suffix(\".json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in [ASSAY, CELL_TYPE, \"cell_type\", \"life_stage\", \"donor_sex\", \"cancer_status\"]:\n",
    "    try:\n",
    "        print(chip_metadata_df[cat].value_counts(dropna=False), \"\\n\")\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNA experience report metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_exp_metadata_path = (\n",
    "    encode_metadata_dir / \"ENCODE_RNA_2023mar_hg38_BW_default_exp_report.tsv\"\n",
    ")\n",
    "rna_exp_metadata_df = pd.read_csv(rna_exp_metadata_path, sep=\"\\t\", skiprows=1)\n",
    "print(rna_exp_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_exp_metadata_df[ASSAY] = rna_exp_metadata_df[\"Assay title\"]\n",
    "rna_exp_metadata_df[\"life_stage\"] = rna_exp_metadata_df[\"Life stage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "donor_sex = []\n",
    "for row in rna_exp_metadata_df.iterrows():\n",
    "    file, others = row\n",
    "    biosample_summary = others[\"Biosample summary\"]\n",
    "    match = re.search(r\"female|male|mixed\", biosample_summary)\n",
    "    if match:\n",
    "        value = match.group(0)\n",
    "        donor_sex.append(value)\n",
    "        if value == \"mixed\":\n",
    "            print(file, biosample_summary)\n",
    "    else:\n",
    "        donor_sex.append(\"unknown\")\n",
    "\n",
    "rna_exp_metadata_df.loc[:, \"donor_sex\"] = donor_sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNA BW metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_file_metadata_path = (\n",
    "    encode_metadata_dir / \"metadata--ENCODE_RNA_2023mar_hg38_BW_default.tsv\"\n",
    ")\n",
    "rna_file_metadata_df = pd.read_csv(rna_file_metadata_path, sep=\"\\t\", skiprows=0)\n",
    "print(rna_file_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_accessions_file = set(rna_file_metadata_df[\"Experiment accession\"])\n",
    "exp_accessions_report = set(rna_exp_metadata_df[\"Accession\"])\n",
    "print(\n",
    "    len(exp_accessions_file),\n",
    "    len(exp_accessions_report),\n",
    "    len(exp_accessions_file & exp_accessions_report),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge both RNA metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_file_metadata_df = rna_file_metadata_df.merge(\n",
    "    rna_exp_metadata_df,\n",
    "    left_on=\"Experiment accession\",\n",
    "    right_on=\"Accession\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_file\", \"_report\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_file_metadata_df[\"filename\"] = rna_file_metadata_df[\"File accession\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rna_file_metadata_df[\"Biosample term id\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_types = set()\n",
    "# for val in list(rna_file_metadata_df[\"Related series\"].value_counts(dropna=False).index):\n",
    "#     if isinstance(val, str):\n",
    "#         ref_type = val.split(\"/\")[1]\n",
    "#         ref_types.add(ref_type)\n",
    "# print(ref_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_file_metadata_df.loc[:, \"in_EpiAtlas\"] = rna_file_metadata_df[\n",
    "    \"Related series\"\n",
    "].str.contains(\"reference-epigenomes\")\n",
    "rna_file_metadata_df[\"in_EpiAtlas\"].fillna(False, inplace=True)\n",
    "print(rna_file_metadata_df[\"in_EpiAtlas\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNA + ChIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_metadata_df[\"filename\"] = chip_metadata_df.index\n",
    "new_rna_chip_metadata_df = merge_dataframes(\n",
    "    chip_metadata_df, rna_file_metadata_df, on=\"filename\", verbose=True\n",
    ")\n",
    "new_rna_chip_metadata_df.set_index(\"filename\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [ASSAY, \"life_stage\", \"donor_sex\", \"cancer_status\"]:\n",
    "    try:\n",
    "        print(new_rna_chip_metadata_df[col].value_counts(dropna=False), \"\\n\")\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "new_rna_chip_metadata_df[\"cancer_status\"].fillna(\"unknown\", inplace=True)\n",
    "new_rna_chip_metadata_df[\"life_stage\"].replace(\n",
    "    to_replace=\".*,.*\", regex=True, value=\"unknown\", inplace=True\n",
    ")\n",
    "\n",
    "for col in [ASSAY, \"life_stage\", \"donor_sex\", \"cancer_status\"]:\n",
    "    try:\n",
    "        print(new_rna_chip_metadata_df[col].value_counts(dropna=False), \"\\n\")\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rna_chip_metadata_df[\"Biosample term id\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all pred if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_pred_dfs = {}\n",
    "nb_chip_files = 9619\n",
    "for folder in encode_predictions_dir.glob(\"*1l_3000n\"):\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "    cat = folder.name.split(\"_1l_3000n\")[0]  # [category]_1l_3000n\n",
    "    pred_file = list(folder.rglob(\"complete_no_valid_oversample*all_augmented.csv\"))[0]\n",
    "    encode_df = pd.read_csv(pred_file)\n",
    "    chip_pred_dfs[cat] = encode_df\n",
    "    print(cat, encode_df.shape)\n",
    "    assert encode_df.shape[0] == nb_chip_files\n",
    "\n",
    "assert len(chip_pred_dfs) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs_rna = {}\n",
    "nb_rna_files = 1790\n",
    "for folder in encode_predictions_dir.glob(\"*1l_3000n\"):\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "    cat = folder.name.split(\"_1l_3000n\")[0]  # [category]_1l_3000n\n",
    "    try:\n",
    "        pred_file = list(folder.rglob(\"complete_no_valid_oversample*rna*augmented*csv\"))[\n",
    "            0\n",
    "        ]\n",
    "        print(\"Using augmented file\", pred_file)\n",
    "    except IndexError as err:\n",
    "        pred_file = list(folder.rglob(\"complete_no_valid_oversample*rna*csv\"))[0]\n",
    "        print(\"Augmenting file\", pred_file)\n",
    "\n",
    "        # Augment the prediction file with some additional columns\n",
    "        # script.py <prediction_file> <metadata_file>\n",
    "        # output_template = \"Augmented prediction file saved to {new_path}\"\n",
    "        script_path = (\n",
    "            Path.home()\n",
    "            / \"Projects/sources/epi_ml/src/python/epi_ml/utils/augment_predict_file.py\"\n",
    "        )\n",
    "        if not script_path.exists():\n",
    "            raise FileNotFoundError(f\"Script {script_path} does not exist.\") from err\n",
    "\n",
    "        output = subprocess.run(\n",
    "            [\n",
    "                \"python\",\n",
    "                str(script_path),\n",
    "                \"-v\",\n",
    "                str(pred_file),\n",
    "                str(full_metadata_path.with_suffix(\".json\")),\n",
    "            ],\n",
    "            check=False,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        if output.returncode != 0:\n",
    "            raise RuntimeError(\n",
    "                f\"Error running script: {output.stderr.decode('utf-8')}\"\n",
    "            ) from err\n",
    "\n",
    "        stdout = output.stdout.decode(\"utf-8\")\n",
    "        pred_file = stdout.strip().split(\"Augmented prediction file saved to \", 1)[-1]\n",
    "        pred_file = Path(pred_file).resolve()\n",
    "\n",
    "    encode_df = pd.read_csv(pred_file)\n",
    "    encode_df[\"md5sum\"] = encode_df[\"md5sum\"].str.split(pat=\"_\", n=1, expand=True)[0]\n",
    "    pred_dfs_rna[cat] = encode_df\n",
    "    print(cat, encode_df.shape)\n",
    "    assert encode_df.shape[0] == nb_rna_files\n",
    "\n",
    "assert len(pred_dfs_rna) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same_col_names = 8\n",
    "# # Make all different columns have unique relevant names except for the pred vector\n",
    "# for cat, df in pred_dfs_rna.items():\n",
    "#     df = df.drop(columns=[\"Same?\"])\n",
    "#     old_names = df.columns[1 : same_col_names - 1]\n",
    "#     new_names = [f\"{old_name} ({cat})\" for old_name in old_names]\n",
    "#     df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "#     df.set_index(\"md5sum\", inplace=True)\n",
    "#     pred_dfs_rna[cat] = df\n",
    "\n",
    "# df_order = [ASSAY, CELL_TYPE, SEX, LIFE_STAGE, CANCER]\n",
    "# df_list = [pred_dfs_rna[cat] for cat in df_order]\n",
    "# full_rna_merged_df = functools.reduce(merge_dataframes, df_list)\n",
    "# full_rna_merged_df.to_csv(encode_predictions_dir / \"full_rna_merged_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_diff_columns = [\"mrna_seq\", \"rna_seq\", \"wgbs-pbat\", \"wgbs-standard\"]\n",
    "\n",
    "concat_pred_dfs = {}\n",
    "for cat, chip_results in chip_pred_dfs.items():\n",
    "    rna_results = pred_dfs_rna[cat]\n",
    "\n",
    "    if cat == ASSAY:\n",
    "        chip_results.loc[:, assay_diff_columns] = \"NA\"\n",
    "\n",
    "    if not chip_results.columns.equals(rna_results.columns):\n",
    "        raise ValueError(\n",
    "            f\"Columns are not the same for {cat}. Chip: {chip_results.columns}, RNA: {rna_results.columns}\"\n",
    "        )\n",
    "\n",
    "    all_results = pd.concat([chip_results, rna_results])\n",
    "\n",
    "    assert all_results.shape == (\n",
    "        chip_results.shape[0] + rna_results.shape[0],\n",
    "        chip_results.shape[1],\n",
    "    )\n",
    "\n",
    "    concat_pred_dfs[cat] = all_results\n",
    "    assert len(all_results) == chip_results.shape[0] + rna_results.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_col_names = 8\n",
    "# Make all different columns have unique relevant names except for the pred vector\n",
    "for cat, df in concat_pred_dfs.items():\n",
    "    df.drop(columns=[\"Same?\"], inplace=True)\n",
    "    old_names = df.columns[1 : same_col_names - 1]\n",
    "    new_names = [f\"{old_name} ({cat})\" for old_name in old_names]\n",
    "    df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "    df.set_index(\"md5sum\", inplace=True)\n",
    "    concat_pred_dfs[cat] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order = [ASSAY, CELL_TYPE, SEX, LIFE_STAGE, CANCER]\n",
    "df_list = [concat_pred_dfs[cat] for cat in df_order]\n",
    "full_merged_df = functools.reduce(merge_dataframes, df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_columns = new_rna_chip_metadata_df.columns\n",
    "result_columns = full_merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rna_chip_metadata_df[\"Biosample term id\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_plus_metadata_df = full_merged_df.merge(\n",
    "    new_rna_chip_metadata_df,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_delete\"),\n",
    ")\n",
    "for col in preds_plus_metadata_df.columns:\n",
    "    if col.endswith(\"_delete\"):\n",
    "        print(col)\n",
    "# preds_plus_metadata_df = preds_plus_metadata_df.filter(regex=r\"^(?:(?!_delete).)+$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_plus_metadata_df[\"Biosample term id\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_col_order = [col for col in meta_columns if col in preds_plus_metadata_df.columns]\n",
    "results_col_order = [\n",
    "    col for col in result_columns if col in preds_plus_metadata_df.columns\n",
    "]\n",
    "\n",
    "new_order = results_col_order + meta_col_order\n",
    "preds_plus_metadata_df = preds_plus_metadata_df.loc[:, new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pairs in [\n",
    "    (ASSAY, \"assay_epiclass\"),\n",
    "    (SEX, \"donor_sex\"),\n",
    "    (LIFE_STAGE, \"life_stage\"),\n",
    "    (CANCER, \"cancer_status\"),\n",
    "]:\n",
    "    name1 = f\"True class ({pairs[0]})\"\n",
    "    name2 = pairs[1]\n",
    "    print(name1, name2)\n",
    "    preds_plus_metadata_df[name1] = preds_plus_metadata_df[name2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_plus_metadata_df[CANCER] = preds_plus_metadata_df[\"cancer_status\"]\n",
    "preds_plus_metadata_df[LIFE_STAGE] = preds_plus_metadata_df[\"life_stage\"]\n",
    "preds_plus_metadata_df[SEX] = preds_plus_metadata_df[\"donor_sex\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EpiAtlas overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_epiatlas_mapping_path = encode_metadata_dir / \"ENCODE_IHEC_keys.tsv\"\n",
    "encode_epiatlas_mapping_df = pd.read_csv(encode_epiatlas_mapping_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_epiatlas = encode_epiatlas_mapping_df[\n",
    "    encode_epiatlas_mapping_df[\"is_EpiAtlas_EpiRR\"].notnull()\n",
    "][\"ENC_ID\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_merged_df[\"in_EpiAtlas\"] = full_merged_df.index.isin(in_epiatlas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting ontology info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_dir = base_data_dir / \"metadata\" / \"encode\"\n",
    "curie_def_df = pd.read_csv(\n",
    "    encode_metadata_dir / \"EpiAtlas_list-curie_term_HSOI.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"code\", \"term\", CELL_TYPE],\n",
    ")\n",
    "encode_ontology_df = pd.read_csv(encode_metadata_dir / \"encode_ontol+assay.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(encode_ontology_df.shape)\n",
    "# print(encode_ontology_df.head())\n",
    "# print(curie_def_df.shape)\n",
    "# print(curie_def_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = encode_ontology_df.merge(\n",
    "    curie_def_df, left_on=\"Biosample term id\", right_on=\"code\", how=\"left\"\n",
    ")\n",
    "metadata_df = metadata_df.drop(columns=[\"code\", \"term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_id_to_cell_type = dict(zip(metadata_df[\"Biosample term id\"], metadata_df[CELL_TYPE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df[CELL_TYPE] = metadata_df[CELL_TYPE].str.lower().copy()\n",
    "metadata_df[\"filename\"] = metadata_df[\"ENC_ID\"].copy()\n",
    "metadata_df = metadata_df.set_index(\"filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_plus_metadata_df[CELL_TYPE] = preds_plus_metadata_df[\"Biosample term id\"]\n",
    "preds_plus_metadata_df[CELL_TYPE].replace(term_id_to_cell_type, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_plus_metadata_df = preds_plus_metadata_df.merge(\n",
    "    metadata_df, left_index=True, right_index=True, how=\"left\", suffixes=(\"\", \"_delete\")\n",
    ")\n",
    "preds_plus_metadata_df.drop(\n",
    "    columns=[col for col in preds_plus_metadata_df.columns if col.endswith(\"_delete\")],\n",
    "    inplace=True,\n",
    ")\n",
    "preds_plus_metadata_df.drop(columns=[\"ENC_ID\"], inplace=True)\n",
    "preds_plus_metadata_df.dropna(axis=0, how=\"all\", inplace=True)\n",
    "preds_plus_metadata_df.dropna(axis=1, how=\"all\", inplace=True)\n",
    "preds_plus_metadata_df.fillna(\"unknown\", inplace=True)\n",
    "\n",
    "preds_plus_metadata_df[f\"True class ({CELL_TYPE})\"] = preds_plus_metadata_df[CELL_TYPE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in [ASSAY, CELL_TYPE, CANCER, LIFE_STAGE, SEX]:\n",
    "    counts = preds_plus_metadata_df[cat].value_counts(dropna=False)\n",
    "    display_perc(counts / counts.sum() * 100)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_plus_metadata_df.to_csv(\n",
    "    encode_predictions_dir / \"encode_predictions_chip_rna_augmented_merged.csv\",\n",
    "    index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code not adapted to RNA-seq starting here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_good_ct_df = metadata_df[metadata_df[CELL_TYPE].isin(accepted_ct)]\n",
    "only_good_ct_count = only_good_ct_df[CELL_TYPE].value_counts()\n",
    "display_perc(only_good_ct_count / only_good_ct_count.sum() * 100)\n",
    "\n",
    "# counts = metadata_df[CELL_TYPE].value_counts(dropna=False)\n",
    "# display_perc(counts / counts.sum() * 100)\n",
    "\n",
    "# counts_good = counts[counts.index.isin(accepted_ct)]\n",
    "# display_perc(counts_good / counts.sum() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing harmonized_sample_ontology_intermediate details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check term on missing CELL_TYPE\n",
    "missing_cell_type = metadata_df[metadata_df[CELL_TYPE].isna()]\n",
    "print(missing_cell_type.shape)\n",
    "\n",
    "biosample_cols = [\"Biosample term id\", \"Biosample term name\"]\n",
    "\n",
    "missing_count = missing_cell_type[biosample_cols].value_counts()\n",
    "display(missing_count.shape)\n",
    "with pd.option_context(\n",
    "    \"display.float_format\",\n",
    "    \"{:.2f}\".format,  # pylint: disable=consider-using-f-string\n",
    "    \"display.max_rows\",\n",
    "    None,\n",
    "):\n",
    "    display(missing_count / missing_count.sum() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cell_types = [\n",
    "    name for name in missing_cell_type[\"Biosample term name\"].unique() if \"T cell\" in name\n",
    "]\n",
    "b_cell_types = [\n",
    "    name for name in missing_cell_type[\"Biosample term name\"].unique() if \"B cell\" in name\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cell_count = missing_cell_type[\n",
    "    missing_cell_type[\"Biosample term name\"].isin(t_cell_types)\n",
    "][biosample_cols].value_counts()\n",
    "display(t_cell_count, t_cell_count.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_cell_count = missing_cell_type[\n",
    "    missing_cell_type[\"Biosample term name\"].isin(b_cell_types)\n",
    "][biosample_cols].value_counts()\n",
    "display(b_cell_count, b_cell_count.sum())\n",
    "\n",
    "perc_missing = (\n",
    "    (t_cell_count.sum() + b_cell_count.sum()) / missing_cell_type.shape[0] * 100\n",
    ")\n",
    "print(f\"t+b cells, percentage of missing cell types: {perc_missing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_folder = (\n",
    "    base_data_dir\n",
    "    / \"training_results/dfreeze_v2/hg38_100kb_all_none/harmonized_sample_ontology_intermediate_1l_3000n/complete-no_valid-oversampling\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df[\"Assay\"] = metadata_df[\"Assay\"].str.lower()\n",
    "metadata_df[CELL_TYPE] = metadata_df[CELL_TYPE].str.lower()\n",
    "df = metadata_df.dropna(subset=[CELL_TYPE])  # drop rows with missing cell type\n",
    "df = metadata_df.dropna(subset=[\"Assay\"])  # drop rows with missing assay\n",
    "non_core_metadata_df = df[~df[\"Assay\"].isin(ASSAY_ORDER)]\n",
    "core_metadata_df = df[df[\"Assay\"].isin(ASSAY_ORDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_core_metadata_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = metadata_df[\"Assay\"].value_counts(dropna=False)\n",
    "# print(len(counts))\n",
    "# counts.to_csv(\n",
    "#     path_or_buf=Path().home() / \"downloads\" / \"encode_assay_counts.csv\",\n",
    "#     sep=\",\",\n",
    "#     header=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(non_core_metadata_df[CELL_TYPE].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_core_metadata_df[CELL_TYPE] = non_core_metadata_df[CELL_TYPE].str.lower().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the predictions for the 16 cell types\n",
    "accepted_ct = [\n",
    "    \"T cell\",\n",
    "    \"neutrophil\",\n",
    "    \"brain\",\n",
    "    \"monocyte\",\n",
    "    \"lymphocyte of B lineage\",\n",
    "    \"myeloid cell\",\n",
    "    \"venous blood\",\n",
    "    \"macrophage\",\n",
    "    \"mesoderm-derived structure\",\n",
    "    \"endoderm-derived structure\",\n",
    "    \"colon\",\n",
    "    \"connective tissue cell\",\n",
    "    \"hepatocyte\",\n",
    "    \"mammary gland epithelial cell\",\n",
    "    \"muscle organ\",\n",
    "    \"extraembryonic cell\",\n",
    "]\n",
    "accepted_ct = [ct.lower() for ct in accepted_ct]\n",
    "print(non_core_metadata_df.shape)\n",
    "\n",
    "metadata_16ct = non_core_metadata_df[non_core_metadata_df[CELL_TYPE].isin(accepted_ct)]\n",
    "print(metadata_16ct.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_counts = metadata_16ct[\"Assay\"].value_counts(dropna=False)\n",
    "display_perc(assay_counts / assay_counts.sum() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs_dict = {}\n",
    "for folder in pred_folder.glob(\"*\"):\n",
    "    if not folder.is_dir():\n",
    "        print(f\"Skipping {folder}\")\n",
    "        continue\n",
    "    pred_file = list(folder.glob(\"predictions/*.csv\"))\n",
    "\n",
    "    if len(pred_file) > 1:\n",
    "        print(f\"More than one prediction file found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    if len(pred_file) == 0:\n",
    "        print(f\"No prediction file found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    pred_file = pred_file[0]\n",
    "\n",
    "    pred_df = pd.read_csv(pred_file)\n",
    "    name = folder.name.replace(\"complete_no_valid_oversample_\", \"\")\n",
    "\n",
    "    for col in [\"True class\", \"Predicted class\"]:\n",
    "        pred_df[col] = pred_df[col].str.lower()\n",
    "\n",
    "    # Remove epiatlas overlap\n",
    "    pred_df = pred_df[~pred_df[\"md5sum\"].isin(in_epiatlas)]\n",
    "\n",
    "    pred_dfs_dict[name] = pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cell_type_acc(\n",
    "    metadata_df: pd.DataFrame,\n",
    "    pred_dfs_dict: Dict[str, pd.DataFrame],\n",
    "    min_pred: float = 0.6,\n",
    ") -> None:\n",
    "    \"\"\"Compute the accuracy of the predictions for the 16 cell types.\"\"\"\n",
    "    meta_df = metadata_df[metadata_df[CELL_TYPE].isin(accepted_ct)].copy()\n",
    "\n",
    "    # print(\"Assay counts for 16 cell types\")\n",
    "    # values_count = meta_df[\"Assay\"].value_counts(dropna=False)\n",
    "    # display(values_count)\n",
    "    # display_perc(values_count / values_count.sum() * 100)\n",
    "\n",
    "    # print(\"Cell types distribution\")\n",
    "    # values_count = meta_df[CELL_TYPE].value_counts(dropna=False)\n",
    "    # display(values_count)\n",
    "    # display_perc(values_count / values_count.sum() * 100)\n",
    "\n",
    "    for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "        print(name)\n",
    "        pred_w_ct = pred_df.merge(\n",
    "            meta_df, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"inner\"\n",
    "        )\n",
    "        N = pred_w_ct.shape[0]\n",
    "\n",
    "        # Calculate results for all predictions\n",
    "        true, pred = pred_w_ct[CELL_TYPE], pred_w_ct[\"Predicted class\"]\n",
    "\n",
    "        total_correct = (true == pred).sum()\n",
    "        acc = total_correct / N\n",
    "        f1 = f1_score(true, pred, labels=pred.unique(), average=\"macro\")\n",
    "\n",
    "        print(f\"Acc (pred>0.0): {total_correct}/{N} ({acc:.2%})\")\n",
    "        print(f\"F1 (pred>0.0): {f1:.2f}\")\n",
    "\n",
    "        # Calculate results for predictions with max_pred\n",
    "        pred_w_ct_filtered = pred_w_ct[pred_w_ct[\"Max pred\"] > min_pred]\n",
    "        true, pred = pred_w_ct_filtered[CELL_TYPE], pred_w_ct_filtered[\"Predicted class\"]\n",
    "\n",
    "        total_correct_filtered = (true == pred).sum()\n",
    "        perc_filtered = total_correct_filtered / pred_w_ct_filtered.shape[0]\n",
    "\n",
    "        f1 = f1_score(true, pred, labels=pred.unique(), average=\"macro\")\n",
    "\n",
    "        print(\n",
    "            f\"Acc (pred>{min_pred:.1f}): {total_correct_filtered}/{pred_w_ct_filtered.shape[0]} ({perc_filtered:.2%})\"\n",
    "        )\n",
    "        diff = N - pred_w_ct_filtered.shape[0]\n",
    "        print(f\"F1 (pred>{min_pred}): {f1:.2f}\")\n",
    "        print(f\"Samples ignored at {min_pred:.1f}: {diff} ({diff/N:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cell_type_acc(non_core_metadata_df, pred_dfs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cell_type_acc(core_metadata_df, pred_dfs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_pred_dfs_dict = {k: v for k, v in pred_dfs_dict.items() if \"-ct16\" in k}\n",
    "for label in core_metadata_df[\"Assay\"].unique():\n",
    "    print(label)\n",
    "    compute_cell_type_acc(\n",
    "        core_metadata_df[core_metadata_df[\"Assay\"] == label], limited_pred_dfs_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_logdir = (\n",
    "    base_fig_dir\n",
    "    / \"encode_predictions\"\n",
    "    / \"confusion_matrices\"\n",
    "    / \"sample_ontology\"\n",
    "    / \"core\"\n",
    ")\n",
    "if not this_logdir.exists():\n",
    "    this_logdir.mkdir(parents=True)\n",
    "\n",
    "meta_df = core_metadata_df[core_metadata_df[CELL_TYPE].isin(accepted_ct)].copy()\n",
    "\n",
    "limited_pred_dfs_dict = {k: v for k, v in pred_dfs_dict.items() if \"-ct16\" in k}\n",
    "for name, df in limited_pred_dfs_dict.items():\n",
    "    pred_w_ct = df.merge(meta_df, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"inner\")\n",
    "    for threshold in [0, 0.6, 0.9]:\n",
    "        sub_df = pred_w_ct[pred_w_ct[\"Max pred\"] >= threshold]\n",
    "\n",
    "        true, pred = sub_df[CELL_TYPE], sub_df[\"Predicted class\"]\n",
    "        f1 = f1_score(true, pred, labels=pred.unique(), average=\"macro\")\n",
    "        # cm = confusion_matrix(true, pred, labels=accepted_ct)\n",
    "\n",
    "        # writer = ConfusionMatrixWriter(labels=accepted_ct, confusion_matrix=cm)\n",
    "        # writer.to_all_formats(\n",
    "        #     logdir=this_logdir,\n",
    "        #     name=f\"{name}-core-confusion_matrix-{threshold*100}\",\n",
    "        # )\n",
    "        # plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download note\n",
    "~~~bash\n",
    "paper_dir=\"/home/local/USHERBROOKE/rabj2301/Projects/epiclass/output/paper/data/training_results/dfreeze_v2/hg38_100kb_all_none/assay_epiclass_1l_3000n\"\n",
    "cd $paper_dir\n",
    "base_path=\"/lustre06/project/6007515/rabyj/epiclass-project/output/epiclass-logs/epiatlas-dfreeze-v2.1/hg38_100kb_all_none/assay_epiclass_1l_3000n\"\n",
    "rsync -avR --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"*confusion*\" --exclude \"*.md5\" narval:${base_path}/./*c/complete_no_valid_oversample .\n",
    "\n",
    "paper_dir=\"/home/local/USHERBROOKE/rabj2301/Projects/epiclass/output/paper/data/training_results/dfreeze_v2\"\n",
    "cd $paper_dir\n",
    "base_path=\"/lustre06/project/6007515/rabyj/epiclass-project/output/epiclass-logs/epiatlas-dfreeze-v2.1\"\n",
    "rsync -avR --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"*confusion*\" --exclude \"*.md5\" narval:${base_path}/./hg38_100kb_all_none_w_encode_noncore/assay_epiclass_1l_3000n/complete_no_valid_oversample-0 .\n",
    "\n",
    "find -type f -name \"*.list*.csv\" -print0 | xargs -0 rename 's/\\.list//g'\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "assay7_folder = (\n",
    "    data_dir / f\"hg38_100kb_all_none/{ASSAY}_1l_3000n/7c/complete_no_valid_oversample\"\n",
    ")\n",
    "assay11_folder = (\n",
    "    data_dir / f\"hg38_100kb_all_none/{ASSAY}_1l_3000n/11c/complete_no_valid_oversample\"\n",
    ")\n",
    "assay13_folder = (\n",
    "    data_dir\n",
    "    / f\"hg38_100kb_all_none_w_encode_noncore/{ASSAY}_1l_3000n/complete_no_valid_oversample-0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_path = encode_metadata_dir / \"ENCODE_IHEC_keys.tsv\"\n",
    "assay_core_metadata_df = pd.read_csv(encode_metadata_path, sep=\"\\t\")\n",
    "print(assay_core_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_core_metadata_df[\"assay_epiclass\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs_dict = {}\n",
    "for name, folder in zip(\n",
    "    [\"7c\", \"11c\", \"13c\"], [assay7_folder, assay11_folder, assay13_folder]\n",
    "):\n",
    "    if not folder.exists():\n",
    "        print(f\"Folder {folder} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    pred_folder = folder / \"predictions\" / \"encode\"\n",
    "    if not pred_folder.exists():\n",
    "        print(f\"Folder {pred_folder} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    pred_file = list(pred_folder.glob(\"*.csv\"))\n",
    "    if len(pred_file) != 1:\n",
    "        print(f\"Found {len(pred_file)} files in {pred_folder}.\")\n",
    "        continue\n",
    "    pred_file = pred_file[0]\n",
    "\n",
    "    pred_df = pd.read_csv(pred_file, sep=\",\")\n",
    "    try:\n",
    "        pred_df.drop(columns=[\"Same?\"], inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Add assay metadata\n",
    "    pred_df = pred_df.merge(\n",
    "        assay_core_metadata_df, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    pred_df[\"True class\"] = pred_df[\"assay_epiclass\"]\n",
    "    pred_dfs_dict[name] = pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core7 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = data_dir = base_data_dir / \"training_results\" / \"encode_predictions\"\n",
    "for name, df in pred_dfs_dict.items():\n",
    "    print(name)\n",
    "    # print(df.shape)\n",
    "\n",
    "    # Only consider files already labeled with core7 assays\n",
    "    df = df[df[ASSAY].isin(CORE_ASSAYS)]\n",
    "\n",
    "    # Only consider non-EpiAtlas samples\n",
    "    df = df[df[\"is_EpiAtlas_EpiRR\"].isna()]\n",
    "\n",
    "    # df.to_csv(output_dir / f\"encode_only-core-{name}_predictions.csv\", index=False)\n",
    "    # break\n",
    "\n",
    "    # Calculate results for all predictions\n",
    "    correct_pred = df[\"Predicted class\"] == df[\"True class\"]\n",
    "    total_correct = correct_pred.sum()\n",
    "    total = df.shape[0]\n",
    "    perc = total_correct / total\n",
    "    print(f\"Acc (pred>=0.0) {total_correct}/{total} ({perc:.2%})\")\n",
    "\n",
    "    for assay in CORE_ASSAYS:\n",
    "        min_pred = 0.6\n",
    "        df_assay = df[df[ASSAY] == assay]\n",
    "        df_assay = df_assay[df_assay[\"Max pred\"] >= min_pred]\n",
    "        correct_pred = df_assay[\"Predicted class\"] == df_assay[\"True class\"]\n",
    "        total_correct = correct_pred.sum()\n",
    "        total = df_assay.shape[0]\n",
    "        perc = total_correct / total\n",
    "        print(\n",
    "            f\"Acc (pred>={min_pred:.1f}) {assay} = {total_correct}/{total} ({perc:.2%})\"\n",
    "        )\n",
    "\n",
    "    # Calculate results for predictions with max_pred > 0.6\n",
    "    df_filtered = df[df[\"Max pred\"] >= 0.6]\n",
    "    correct_pred_filtered = df_filtered[\"Predicted class\"] == df_filtered[\"True class\"]\n",
    "    total_correct_filtered = correct_pred_filtered.sum()\n",
    "    total_filtered = df_filtered.shape[0]\n",
    "    perc_filtered = total_correct_filtered / total_filtered\n",
    "    print(\n",
    "        f\"Acc (pred>=0.6): {total_correct_filtered}/{total_filtered} ({perc_filtered:.2%})\"\n",
    "    )\n",
    "\n",
    "    # df_filtered_wrong = df_filtered[~correct_pred_filtered]\n",
    "    # groupby = (\n",
    "    #     df_filtered_wrong.groupby([\"True class\", \"Predicted class\"])\n",
    "    #     .size()\n",
    "    #     .sort_values(ascending=False)\n",
    "    # )\n",
    "    # display(\"Mislabels:\", groupby)\n",
    "\n",
    "    # df_filtered_wrong.to_csv(\n",
    "    #     output_dir / f\"encode_only_mislabels_minPred0.6_{name}.csv\", index=False\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-core 7c preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7c preds on non-core assays\n",
    "name = \"7c\"\n",
    "df = pred_dfs_dict[name]\n",
    "\n",
    "df = df.merge(non_core_metadata_df, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"left\")\n",
    "df = df[~df[\"True class\"].isin(ASSAY_ORDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "display(df[\"Assay\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = data_dir = (\n",
    "    base_data_dir / \"training_results\" / \"predictions\" / \"encode\" / \"assay_epiclass\"\n",
    ")\n",
    "for min_pred in [0, 0.6, 0.8]:\n",
    "    df_filtered = df[df[\"Max pred\"] >= min_pred]\n",
    "    groupby = (\n",
    "        df_filtered.groupby([\"Predicted class\", \"Assay\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"Count\")\n",
    "        .sort_values(by=[\"Predicted class\", \"Count\"], ascending=[True, False])\n",
    "        .set_index([\"Predicted class\", \"Assay\"])[\"Count\"]\n",
    "    )\n",
    "    # groupby.to_csv(\n",
    "    #     output_dir / f\"encode_non-core_{name}_predictions_minPred{min_pred}.csv\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_dir = base_data_dir / \"metadata/encode\"\n",
    "non_core_categories_path = (\n",
    "    encode_metadata_dir / \"non-core_encode_assay_category_2024-08-29.csv\"\n",
    ")\n",
    "if not non_core_categories_path.exists():\n",
    "    raise FileNotFoundError(f\"File {non_core_categories_path} does not exist.\")\n",
    "\n",
    "non_core_categories_df = pd.read_csv(non_core_categories_path, sep=\",\")\n",
    "print(non_core_categories_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_core_categories_df.columns = [\"assay\", \"assay_category\", \"note\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_cats = df.merge(\n",
    "    non_core_categories_df[[\"assay\", \"assay_category\"]],\n",
    "    left_on=\"Assay\",\n",
    "    right_on=\"assay\",\n",
    "    how=\"left\",\n",
    ")\n",
    "print(df_w_cats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Assay\" in df_w_cats.columns:\n",
    "    df_w_cats.drop(columns=[\"Assay\"], inplace=True)\n",
    "df_w_cats[\"assay_category\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print non-core assay categories for each predicted class\n",
    "min_pred = 0.6\n",
    "for predicted_class, group in df_w_cats.groupby(\"Predicted class\"):\n",
    "    print(predicted_class, group.shape[0])\n",
    "    group = group[group[\"Max pred\"] >= min_pred]\n",
    "    print(f\"min_pred={min_pred}: {group.shape[0]} samples left\")\n",
    "    groupby = (\n",
    "        group.groupby([\"assay_category\", \"assay\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"Count\")\n",
    "        .sort_values(by=[\"assay_category\", \"Count\"], ascending=[True, False])\n",
    "        .set_index([\"assay_category\", \"assay\"])[\"Count\"]\n",
    "    )\n",
    "    with pd.option_context(\n",
    "        \"display.max_rows\",\n",
    "        None,\n",
    "    ):\n",
    "        # display(groupby)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_non_core_preds_df(df: pd.DataFrame, min_pred: float = 0.6):\n",
    "    \"\"\"Create a DataFrame of non-core assay predictions.\"\"\"\n",
    "    results = {}\n",
    "    assay_categories = dict(zip(df[\"assay\"], df[\"assay_category\"]))\n",
    "\n",
    "    for assay, group in df.groupby(\"assay\"):\n",
    "        # N = group.shape[0]\n",
    "        # if N < 3:\n",
    "        #     continue\n",
    "\n",
    "        group = group[group[\"Max pred\"] >= min_pred]\n",
    "        # N_post_filter = group.shape[0]\n",
    "        # if N_post_filter == 0 or N_post_filter < min_n:\n",
    "        #     continue\n",
    "\n",
    "        groupby = (\n",
    "            group.groupby([\"Predicted class\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"Count\")  # type: ignore\n",
    "            .sort_values(by=[\"Count\"], ascending=False)\n",
    "        )\n",
    "\n",
    "        results[assay] = dict(zip(groupby[\"Predicted class\"], groupby[\"Count\"]))\n",
    "\n",
    "    result_df = pd.DataFrame(results).fillna(0)\n",
    "    result_df = result_df.astype(int)\n",
    "    result_df = result_df.T  # assay as row/index\n",
    "    result_df[\"assay_category\"] = result_df.index.map(assay_categories)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pred = 0.6\n",
    "predicted_classes_df = create_non_core_preds_df(df_w_cats, min_pred=min_pred)\n",
    "# predicted_classes_df.to_csv(\n",
    "#     output_dir / f\"encode_non-core_7c_predictions_per_assay_minPred{min_pred:.2f}.csv\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes_df[\"assay_category\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_dataframe(df_w_cats):\n",
    "    \"\"\"Create a structured dataframe with the percentage of predictions for each assay category.\"\"\"\n",
    "    # Create an empty list to store our data\n",
    "    data = []\n",
    "\n",
    "    # Iterate through the grouped data\n",
    "    for predicted_class, group in df_w_cats.groupby(\"Predicted class\"):\n",
    "        for min_pred in list(np.arange(0, 1, 0.05)) + [0.99]:\n",
    "            df_filtered = group[group[\"Max pred\"] >= min_pred]\n",
    "            counts = df_filtered[\"assay_category\"].value_counts(dropna=False)\n",
    "            total = counts.sum()\n",
    "\n",
    "            # Calculate percentages\n",
    "            percentages = (counts / total * 100).round(2)\n",
    "\n",
    "            # Add data for each assay category\n",
    "            for assay_category, percentage in percentages.items():\n",
    "                data.append(\n",
    "                    {\n",
    "                        \"Predicted class\": predicted_class,\n",
    "                        \"Min pred\": min_pred,\n",
    "                        \"assay_category\": assay_category,\n",
    "                        \"Percentage\": percentage,\n",
    "                        \"Count\": counts[assay_category],\n",
    "                        \"Total samples\": total,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Create the dataframe\n",
    "    df_structured = pd.DataFrame(data)\n",
    "\n",
    "    # Set the multi-index\n",
    "    df_structured = df_structured.set_index(\n",
    "        [\"Predicted class\", \"Min pred\", \"assay_category\"]\n",
    "    )\n",
    "\n",
    "    return df_structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_category_df = create_structured_dataframe(df_w_cats)\n",
    "display(assay_category_df)\n",
    "# output_path = output_dir / \"encode_non-core_7c_predictions_assay_category.csv\"\n",
    "# assay_category_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_fig_dir = base_fig_dir / \"encode_predictions\" / \"assay_epiclass\" / \"non-core\"\n",
    "if not section_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {section_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X = assay_epiclass, stack = assay_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = section_fig_dir / \"stacked_bar_X_assay_epiclass\"\n",
    "fig_dir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "bar_df = assay_category_df.reset_index()\n",
    "\n",
    "predicted_class_order = [\n",
    "    \"h3k27ac\",\n",
    "    \"h3k4me3\",\n",
    "    \"h3k4me1\",\n",
    "    \"h3k9me3\",\n",
    "    \"h3k27me3\",\n",
    "    \"h3k36me3\",\n",
    "    \"input\",\n",
    "]\n",
    "assay_category_color_map = {\n",
    "    cat: px.colors.qualitative.Safe[i]\n",
    "    for i, cat in enumerate(sorted(bar_df[\"assay_category\"].unique()))\n",
    "}\n",
    "\n",
    "for min_pred in [0, 0.6, 0.9]:\n",
    "    sub_df = bar_df[\n",
    "        (bar_df[\"Min pred\"] > min_pred - 0.01) & (bar_df[\"Min pred\"] < min_pred + 0.01)\n",
    "    ]\n",
    "    fig = px.bar(\n",
    "        sub_df,\n",
    "        x=\"Predicted class\",\n",
    "        y=\"Percentage\",\n",
    "        color=\"assay_category\",\n",
    "        title=f\"Assay Category Composition for Each Predicted Class at predScore >= {min_pred:.2f}\",\n",
    "        labels={\"Percentage\": \"Percentage (%)\", \"Predicted class\": \"Predicted Class\"},\n",
    "        barmode=\"stack\",\n",
    "        category_orders={\"Predicted class\": predicted_class_order},\n",
    "        color_discrete_map=assay_category_color_map,\n",
    "    )\n",
    "\n",
    "    figname = f\"histogram_encode_non-core_assay_epiclass_minPred{min_pred:.2f}\"\n",
    "    # fig.write_html(fig_dir / f\"{figname}.html\")\n",
    "    # fig.write_image(fig_dir / f\"{figname}.png\")\n",
    "    # fig.write_image(fig_dir / f\"{figname}.svg\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X = assay_category, stack = assay_epiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_w_cats[df_w_cats[\"assay_category\"] != \"not_looked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_epiclass_order = [\n",
    "    \"h3k27ac\",\n",
    "    \"h3k4me3\",\n",
    "    \"h3k4me1\",\n",
    "    \"h3k9me3\",\n",
    "    \"h3k27me3\",\n",
    "    \"h3k36me3\",\n",
    "    \"input\",\n",
    "]\n",
    "assay_epiclass_order = {assay: i for i, assay in enumerate(assay_epiclass_order)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = section_fig_dir / \"stacked_bar_X_assay_category\"\n",
    "fig_dir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "assay_categories_order = [\n",
    "    \"trx_reg\",\n",
    "    \"heterochrom\",\n",
    "    \"polycomb\",\n",
    "    \"splicing\",\n",
    "    \"insulator\",\n",
    "    \"other/mixed\",\n",
    "]\n",
    "\n",
    "for min_pred in [0, 0.6]:\n",
    "    sub_df = df[df[\"Max pred\"] >= min_pred]\n",
    "    groupby = (\n",
    "        sub_df.groupby([\"assay_category\", \"Predicted class\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"Count\")\n",
    "        .sort_values(by=[\"assay_category\", \"Count\"], ascending=[True, False])\n",
    "    )\n",
    "    groupby[\"Percentage\"] = groupby.groupby(\"assay_category\")[\"Count\"].transform(\n",
    "        lambda x: (x / x.sum()) * 100\n",
    "    )\n",
    "\n",
    "    # Add order for plotting\n",
    "    groupby[\"assay_order\"] = groupby[\"Predicted class\"].map(assay_epiclass_order)\n",
    "    groupby = groupby.sort_values(\n",
    "        by=[\"assay_category\", \"assay_order\"], ascending=[False, True]\n",
    "    )\n",
    "\n",
    "    # Main plot\n",
    "    fig = px.bar(\n",
    "        groupby,\n",
    "        x=\"assay_category\",\n",
    "        y=\"Percentage\",\n",
    "        color=\"Predicted class\",\n",
    "        barmode=\"stack\",\n",
    "        category_orders={\"assay_category\": assay_categories_order},\n",
    "        color_discrete_map=assay_colors,\n",
    "        title=f\"core7 predictions for non-core assays, predScore >= {min_pred:.2f}\",\n",
    "        labels={\"Percentage\": \"Percentage (%)\", \"assay_category\": \"Assay Category\"},\n",
    "    )\n",
    "\n",
    "    # Modify x-axis labels\n",
    "    total_counts = groupby.groupby(\"assay_category\")[\"Count\"].sum()\n",
    "\n",
    "    ticktext = [\n",
    "        f\"{assay_category} (N={total_counts[assay_category]})\"\n",
    "        for assay_category in assay_categories_order\n",
    "    ]\n",
    "    fig.update_xaxes(tickvals=assay_categories_order, ticktext=ticktext)\n",
    "\n",
    "    # Save and display\n",
    "    figname = f\"histogram_encode_non-core_assay_epiclass_minPred{min_pred:.2f}\"\n",
    "    # fig.write_html(fig_dir / f\"{figname}.html\")\n",
    "    # fig.write_image(fig_dir / f\"{figname}.png\")\n",
    "    # fig.write_image(fig_dir / f\"{figname}.svg\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assay category evolution with min_predScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_assay_category_graphs(df, output_dir: Path):\n",
    "    \"\"\"Graph assay category distribution for each predicted class.\"\"\"\n",
    "    # Get unique predicted classes\n",
    "    predicted_classes = df.index.get_level_values(\"Predicted class\").unique()\n",
    "    assay_categories = df.index.get_level_values(\"assay_category\").unique()\n",
    "\n",
    "    graph_colors = {\n",
    "        cat: px.colors.qualitative.Safe[i]\n",
    "        for i, cat in enumerate(sorted(assay_categories))\n",
    "    }\n",
    "\n",
    "    # Create a figure for each predicted class\n",
    "    for predicted_class in predicted_classes:\n",
    "        df_class = df.loc[predicted_class]\n",
    "\n",
    "        # Get unique assay categories for this predicted class\n",
    "        assay_categories = df_class.index.get_level_values(\"assay_category\").unique()\n",
    "\n",
    "        total_samples_at_zero = df_class.xs(0, level=\"Min pred\")[\"Total samples\"].iloc[0]\n",
    "\n",
    "        # Create the figure\n",
    "        fig = go.Figure()\n",
    "\n",
    "        for assay_category in assay_categories:\n",
    "            df_assay = df_class.xs(assay_category, level=\"assay_category\")\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_assay.index,\n",
    "                    y=df_assay[\"Percentage\"],\n",
    "                    mode=\"lines+markers\",\n",
    "                    name=assay_category,\n",
    "                    marker=dict(color=graph_colors[assay_category]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        conserved_percentages = (\n",
    "            df_class.groupby(\"Min pred\")[\"Total samples\"].first()\n",
    "            / total_samples_at_zero\n",
    "            * 100\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=conserved_percentages.index,\n",
    "                y=conserved_percentages.values,\n",
    "                mode=\"lines+markers\",\n",
    "                name=\"Samples Conserved\",\n",
    "                line=dict(dash=\"dash\", color=\"black\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f\"Composition for Predicted Class: {predicted_class}\",\n",
    "            xaxis_title=\"Min pred\",\n",
    "            yaxis_title=\"Percentage Composition\",\n",
    "            legend_title=\"Assay Category\",\n",
    "            hovermode=\"x unified\",\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(range=[-0.01, 1.01])\n",
    "        fig.update_yaxes(range=[0, 100])\n",
    "\n",
    "        # Save\n",
    "        # filename = f\"encode_non-core_7c_predictions_assay_category_{predicted_class}\"\n",
    "        # fig.write_image(output_dir / f\"{filename}.png\")\n",
    "        # fig.write_image(output_dir / f\"{filename}.svg\")\n",
    "        # fig.write_html(output_dir / f\"{filename}.html\")\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_structured is your dataframe from the previous step\n",
    "fig_dir = (\n",
    "    base_fig_dir\n",
    "    / \"encode_predictions\"\n",
    "    / \"assay_epiclass\"\n",
    "    / \"non-core\"\n",
    "    / \"line_graphs_over_min_pred\"\n",
    ")\n",
    "fig_dir.mkdir(parents=False, exist_ok=True)\n",
    "# create_assay_category_graphs(df=assay_category_df, output_dir=fig_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTHER - Sex, life stage, cancer\n",
    "Throwing all the predictions together to get acc/F1 for each of 5 classifiers, on core/non-core data respectively. (for assay and cell type it gets more messy, cannot do non-core directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata\n",
    "full_metadata_path = encode_metadata_dir / \"encode_metadata_2023-10-25.csv\"\n",
    "full_metadata_df = pd.read_csv(full_metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata_df[ASSAY].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs_dict = {}\n",
    "for folder in encode_predictions_dir.glob(\"*\"):\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "    if any(label in folder.name for label in [\"assay\", \"ontology\", \"track\"]):\n",
    "        continue\n",
    "\n",
    "    pred_file = list(folder.rglob(\"*.csv\"))\n",
    "    if len(pred_file) != 1:\n",
    "        print(f\"Found {len(pred_file)} files in {folder}.\")\n",
    "        continue\n",
    "\n",
    "    pred_file = pred_file[0]\n",
    "\n",
    "    pred_df = pd.read_csv(pred_file, sep=\",\")\n",
    "    try:\n",
    "        pred_df.drop(columns=[\"Same?\"], inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    pred_df = pred_df.merge(full_metadata_df, on=\"md5sum\", how=\"left\")\n",
    "\n",
    "    pred_dfs_dict[folder.name.replace(\"_1l_3000n\", \"\")] = pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANCER = \"harmonized_sample_cancer_high\"\n",
    "true_class_mapping = {\n",
    "    SEX: \"donor_sex\",\n",
    "    LIFE_STAGE: \"life_stage\",\n",
    "    CANCER: \"cancer_status\",\n",
    "}\n",
    "\n",
    "for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "    pred_df[\"True class\"] = pred_df[true_class_mapping[name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "    print(name)\n",
    "    display(pred_df[\"True class\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "    is_unknown = pred_df[\"True class\"].copy().str.contains(r\"unknown|,\", case=False)\n",
    "    new_pred_df = pred_df[~is_unknown]\n",
    "    pred_dfs_dict[name] = new_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "    print(name)\n",
    "    display(pred_df[\"True class\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing EpiAtlas EpiRR overlap with ENCODE dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "    print(name)\n",
    "    new_pred_df = pred_df[~pred_df[\"md5sum\"].isin(in_epiatlas)]\n",
    "    print(pred_df.shape, new_pred_df.shape)\n",
    "    print(f\"Removed {pred_df.shape[0] - new_pred_df.shape[0]} EpiAtlas samples.\\n\")\n",
    "    pred_dfs_dict[name] = new_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies per assay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(df: pd.DataFrame):\n",
    "    \"\"\"Compute the accuracy and f1 of the predictions.\"\"\"\n",
    "    acc = (df[\"True class\"] == df[\"Predicted class\"]).mean()\n",
    "    f1 = f1_score(\n",
    "        df[\"True class\"],\n",
    "        df[\"Predicted class\"],\n",
    "        labels=df[\"Predicted class\"].unique(),\n",
    "        average=\"macro\",\n",
    "    )\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "assays = ASSAY_ORDER + [\"CTCF\", \"non-core\"]\n",
    "\n",
    "all_acc_per_assay = {}\n",
    "for name, df in pred_dfs_dict.items():\n",
    "    if \"Max pred\" not in df.columns:\n",
    "        raise ValueError(f\"Column 'Max pred' not found in {name}.\")\n",
    "\n",
    "    # {assay: [(min_pred, acc, f1, nb_samples), ...], ...}\n",
    "    acc_per_assay: Dict[str, List[Tuple[str, float, float, int]]] = {}\n",
    "    for label in assays:\n",
    "        acc_per_assay[label] = []\n",
    "        if label not in df[ASSAY].unique():\n",
    "            continue\n",
    "        assay_df = df[df[ASSAY] == label]\n",
    "        for min_pred in [\"0.0\", \"0.6\", \"0.9\"]:\n",
    "            sub_df = assay_df[assay_df[\"Max pred\"] > float(min_pred)]\n",
    "            acc, f1 = compute_metrics(sub_df)\n",
    "            acc_per_assay[label].append((min_pred, acc, f1, len(sub_df)))\n",
    "\n",
    "    # Avg accuracy\n",
    "    for label in [\"avg-all\", \"avg-core\", \"avg-non-core\"]:\n",
    "        acc_per_assay[label] = []\n",
    "\n",
    "    for min_pred in [\"0.0\", \"0.6\", \"0.9\"]:\n",
    "        sub_df = df[df[\"Max pred\"] > float(min_pred)]\n",
    "        acc, f1 = compute_metrics(sub_df)\n",
    "        acc_per_assay[\"avg-all\"].append((min_pred, acc, f1, len(sub_df)))\n",
    "\n",
    "        core_df = sub_df[sub_df[ASSAY].isin(ASSAY_ORDER)]\n",
    "        acc, f1 = compute_metrics(core_df)\n",
    "        acc_per_assay[\"avg-core\"].append((min_pred, acc, f1, len(sub_df)))\n",
    "\n",
    "        non_core_df = sub_df[~sub_df[ASSAY].isin(ASSAY_ORDER)]\n",
    "        acc, f1 = compute_metrics(non_core_df)\n",
    "        acc_per_assay[\"avg-non-core\"].append((min_pred, acc, f1, len(sub_df)))\n",
    "\n",
    "    all_acc_per_assay[name] = acc_per_assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc per assay to table\n",
    "# cols = [classifier+task, assay, min_pred, acc, nb_samples]\n",
    "rows = []\n",
    "for name, acc_per_assay in all_acc_per_assay.items():\n",
    "    for assay, values in acc_per_assay.items():\n",
    "        for min_pred, acc, f1, nb_samples in values:\n",
    "            rows.append([name, assay, min_pred, acc, f1, nb_samples])\n",
    "df_acc_per_assay = pd.DataFrame(\n",
    "    rows, columns=[\"task_name\", ASSAY, \"min_predScore\", \"acc\", \"f1-score\", \"nb_samples\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_acc_per_assay.to_csv(\n",
    "#     base_fig_dir\n",
    "#     / \"encode_predictions\"\n",
    "#     / \"sex_cancer_life-stage_acc_per_assay_NO_EpiAtlas.tsv\",\n",
    "#     sep=\"\\t\",\n",
    "#     index=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_predScore_color_map = {\"0.0\": \"blue\", \"0.6\": \"orange\", \"0.9\": \"red\"}\n",
    "\n",
    "df_acc_per_assay[\"scatter_name\"] = df_acc_per_assay[\"task_name\"].replace(\n",
    "    \"harmonized_\", \"\", regex=True\n",
    ")\n",
    "\n",
    "df_acc_per_assay = df_acc_per_assay.sort_values(\n",
    "    by=[ASSAY, \"min_predScore\", \"scatter_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple min_predScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_fig_dir = base_fig_dir / \"encode_predictions\" / \"acc_per_assay\"\n",
    "if not this_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Folder {this_fig_dir} does not exist\")\n",
    "\n",
    "for graph_type in [\"core\", \"non-core\"]:\n",
    "    graph_df = df_acc_per_assay.copy()\n",
    "    if graph_type == \"core\":\n",
    "        graph_df = graph_df[graph_df[ASSAY].isin(CORE_ASSAYS)]\n",
    "        minY = 0.55\n",
    "        maxY = 1.001\n",
    "    elif graph_type == \"non-core\":\n",
    "        graph_df = graph_df[~graph_df[ASSAY].isin(CORE_ASSAYS)]\n",
    "        minY = 0\n",
    "        maxY = 1\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid graph type: {graph_type}\")\n",
    "\n",
    "    unique_assays = list(graph_df[ASSAY].unique())\n",
    "\n",
    "    # Calculate average over assays\n",
    "    avg_df = (\n",
    "        graph_df.groupby([\"min_predScore\", \"scatter_name\"])[\"acc\"].mean().reset_index()\n",
    "    )\n",
    "    avg_df[ASSAY] = \"Average\"\n",
    "\n",
    "    traces_per_assay = graph_df[\"scatter_name\"].nunique()\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for min_pred in [\"0.0\", \"0.6\", \"0.9\"]:\n",
    "        df_subset = graph_df[graph_df[\"min_predScore\"] == min_pred]\n",
    "        avg_subset = avg_df[avg_df[\"min_predScore\"] == min_pred]\n",
    "\n",
    "        # Add average over assay trace\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[\"Average - \" + name for name in avg_subset[\"scatter_name\"]],\n",
    "                y=avg_subset[\"acc\"],\n",
    "                mode=\"markers\",\n",
    "                name=f\"Avg Min Pred Score: {min_pred}\",\n",
    "                marker=dict(\n",
    "                    color=min_predScore_color_map[min_pred],\n",
    "                    size=9,\n",
    "                    symbol=\"star\",\n",
    "                ),\n",
    "                hoverinfo=\"y+x\",\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Add individual assay traces\n",
    "        hovertext = list(\n",
    "            zip(\n",
    "                df_subset[ASSAY], df_subset[\"nb_samples\"].apply(lambda x: f\"Samples: {x}\")\n",
    "            )\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_subset[ASSAY] + \" - \" + df_subset[\"scatter_name\"],\n",
    "                y=df_subset[\"acc\"],\n",
    "                mode=\"markers\",\n",
    "                name=f\"Min Pred Score: {min_pred}\",\n",
    "                marker=dict(\n",
    "                    color=min_predScore_color_map[min_pred],\n",
    "                    size=9,\n",
    "                ),\n",
    "                text=hovertext,\n",
    "                hoverinfo=\"text+y+x\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Modify x-axis tick labels\n",
    "\n",
    "    ticktext = []\n",
    "    tick_group = list(df_subset[\"scatter_name\"].unique())\n",
    "    for i, tick in enumerate(tick_group):\n",
    "        tick_group[i] = f\"<b>{tick}</b>\"\n",
    "\n",
    "    for i in range(len(unique_assays) + 1):\n",
    "        ticktext.extend(tick_group)\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        tickmode=\"array\", ticktext=ticktext, tickvals=list(range(len(ticktext)))\n",
    "    )\n",
    "\n",
    "    # Add assay labels on top + vertical lines between assay groups\n",
    "    fig.add_annotation(\n",
    "        x=len(tick_group) / 2 - 0.5,\n",
    "        y=1.05,\n",
    "        yref=\"paper\",\n",
    "        text=\"Average\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=14),\n",
    "    )\n",
    "\n",
    "    fig.add_vline(\n",
    "        x=len(tick_group) - 0.5, line_width=2, line_dash=\"solid\", line_color=\"black\"\n",
    "    )\n",
    "    fig.add_hline(y=1, line_width=1, line_color=\"black\")\n",
    "\n",
    "    for i, label in enumerate(unique_assays):\n",
    "        fig.add_annotation(\n",
    "            x=(i + 1) * len(tick_group) + len(tick_group) / 2 - 0.5,\n",
    "            y=1.05,\n",
    "            yref=\"paper\",\n",
    "            text=label,\n",
    "            showarrow=False,\n",
    "            font=dict(size=14),\n",
    "        )\n",
    "        fig.add_vline(\n",
    "            x=(i + 1) * len(tick_group) - 0.5,\n",
    "            line_width=1,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"black\",\n",
    "        )\n",
    "\n",
    "    # titles + yaxis range\n",
    "    fig.update_layout(\n",
    "        title=\"ENCODE data - Label match per Assay and Task\",\n",
    "        xaxis_title=\"Assay - Task\",\n",
    "        yaxis_title=\"Match %\",\n",
    "        xaxis_tickangle=-45,\n",
    "        showlegend=True,\n",
    "        height=600,\n",
    "        width=1200,\n",
    "        yaxis=dict(tickformat=\".2%\", range=[minY, maxY]),\n",
    "    )\n",
    "\n",
    "    # Show/Write the plot\n",
    "    # print(f\"Graphing {graph_type}\")\n",
    "    # figname = f\"encode_{graph_type}_acc_per_assay_minY{minY:.2f}\"\n",
    "    # fig.write_html(this_fig_dir / f\"{figname}.html\")\n",
    "    # fig.write_image(this_fig_dir / f\"{figname}.png\")\n",
    "    # fig.write_image(this_fig_dir / f\"{figname}.svg\")\n",
    "    # fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_predScore = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_fig_dir = base_fig_dir / \"encode_predictions\" / \"acc_per_assay\"\n",
    "if not this_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Folder {this_fig_dir} does not exist\")\n",
    "\n",
    "graph_df = df_acc_per_assay.copy()\n",
    "\n",
    "graph_df = graph_df[graph_df[\"min_predScore\"] == \"0.6\"]\n",
    "graph_df = graph_df[graph_df[ASSAY].isin(CORE_ASSAYS)]\n",
    "\n",
    "minY = 0.55\n",
    "maxY = 1.001\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for task in graph_df[\"task_name\"].unique():\n",
    "    task_df = graph_df[graph_df[\"task_name\"] == task]\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=task_df[\"acc\"],\n",
    "            name=task,\n",
    "            boxpoints=\"all\",\n",
    "            boxmean=True,\n",
    "            hovertext=task_df[ASSAY],\n",
    "            jitter=0.1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    # title=\"ENCODE data - Label match per Assay and Task\",\n",
    "    # xaxis_title=\"Assay - Task\",\n",
    "    # yaxis_title=\"Match %\",\n",
    "    # xaxis_tickangle=-45,\n",
    "    # showlegend=True,\n",
    "    # height=600,\n",
    "    # width=1200,\n",
    "    yaxis=dict(tickformat=\".2%\", range=[0.65, maxY]),\n",
    ")\n",
    "\n",
    "# Show/Write the plot\n",
    "# print(f\"Graphing {graph_type}\")\n",
    "# figname = f\"encode_{graph_type}_acc_per_assay_minY{minY:.2f}\"\n",
    "# fig.write_html(this_fig_dir / f\"{figname}.html\")\n",
    "# fig.write_image(this_fig_dir / f\"{figname}.png\")\n",
    "# fig.write_image(this_fig_dir / f\"{figname}.svg\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm_logdir = base_fig_dir / \"encode_predictions\" / \"confusion_matrices\"\n",
    "# for graph_type in [\"core\", \"non-core\"]:\n",
    "#     for name, df in pred_dfs_dict.items():\n",
    "#         logdir = cm_logdir / name\n",
    "#         if not logdir.exists():\n",
    "#             logdir.mkdir(parents=True)\n",
    "\n",
    "#         if \"Max pred\" not in df.columns:\n",
    "#             raise ValueError(f\"Column 'Max pred' not found in {name}.\")\n",
    "\n",
    "#         if graph_type == \"core\":\n",
    "#             df = df[df[ASSAY].isin(CORE_ASSAYS)].copy()\n",
    "#         elif graph_type == \"non-core\":\n",
    "#             df = df[~df[ASSAY].isin(CORE_ASSAYS)].copy()\n",
    "\n",
    "#         for threshold in [0, 0.6, 0.9]:\n",
    "#             sub_df = df[df[\"Max pred\"] >= threshold]\n",
    "\n",
    "#             true, pred = sub_df[\"True class\"], sub_df[\"Predicted class\"]\n",
    "#             labels = sub_df[\"True class\"].unique()\n",
    "#             cm = confusion_matrix(true, pred, labels=labels)\n",
    "\n",
    "#             writer = ConfusionMatrixWriter(labels=labels, confusion_matrix=cm)\n",
    "#             writer.to_all_formats(\n",
    "#                 logdir=logdir,\n",
    "#                 name=f\"{name}-{graph_type}-confusion_matrix-{threshold*100}\",\n",
    "#             )\n",
    "#             plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### track type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_type_pred_path = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"predictions\"\n",
    "    / \"encode\"\n",
    "    / \"track_type\"\n",
    "    / \"split0_test_prediction_100kb_all_none_all.list.csv\"\n",
    ")\n",
    "track_type_pred_df = pd.read_csv(track_type_pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_type_pred_df[\"Max_pred_track_type\"] = track_type_pred_df.loc[\n",
    "    :, track_type_pred_df.columns[3:]\n",
    "].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_type_df = track_type_pred_df.merge(\n",
    "    full_metadata_df, left_on=\"Unnamed: 0\", right_on=\"md5sum\", how=\"inner\"\n",
    ")\n",
    "\n",
    "print(track_type_df.shape, encode_df.shape, track_type_pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write each table in a separate excel sheet\n",
    "output = track_type_pred_path.parent / \"track_type_predictions_pivot.csv\"\n",
    "output.unlink(missing_ok=True)\n",
    "\n",
    "with open(output, \"a\", encoding=\"utf8\") as csv_stream:\n",
    "    for min_pred in [0, 0.6, 0.8]:\n",
    "        df = track_type_df[track_type_df[\"Max_pred_track_type\"] >= min_pred]\n",
    "        pivot = df.pivot_table(\n",
    "            index=ASSAY,\n",
    "            columns=\"Predicted class\",\n",
    "            values=\"Max_pred_track_type\",\n",
    "            aggfunc=\"count\",\n",
    "            fill_value=0,\n",
    "            margins=True,\n",
    "        ).astype(int)\n",
    "        relative_pivot = pivot.div(pivot[\"All\"], axis=0) * 100\n",
    "\n",
    "        # csv_stream.write(f\"Count Pivot - Min pred: {min_pred}\\n\")\n",
    "        # pivot.to_csv(csv_stream)\n",
    "        # csv_stream.write(\"\\n\")\n",
    "\n",
    "        # csv_stream.write(f\"Relative Pivot - Min pred: {min_pred}\\n\")\n",
    "        # relative_pivot.to_csv(csv_stream)\n",
    "        # csv_stream.write(\"\\n\")\n",
    "\n",
    "        # display(pivot)\n",
    "        # with pd.option_context(\"display.float_format\", \"{:.2f}\".format):\n",
    "        #     display(relative_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNA-Seq 5 classifiers accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_df = preds_plus_metadata_df[preds_plus_metadata_df[ASSAY].str.contains(\"RNA\")].copy()\n",
    "rna_df[\"in_EpiAtlas\"] = rna_df[\"in_EpiAtlas\"].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No EpiATLAS overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_df = rna_df[~rna_df[\"in_EpiAtlas\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [ASSAY, f\"True class ({ASSAY})\"]:\n",
    "    rna_df.loc[:, col].replace(\n",
    "        {\n",
    "            \"total RNA-seq\": \"rna_seq\",\n",
    "            \"polyA plus RNA-seq\": \"mrna_seq\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RNA-Seq assay accuracy, if mrna_seq != rna_seq\\n\")\n",
    "for min_pred in [0, 0.6, 0.8]:\n",
    "    df = rna_df[rna_df[f\"Max pred ({ASSAY})\"] >= min_pred]\n",
    "    acc = len(df[df[f\"True class ({ASSAY})\"] == df[f\"Predicted class ({ASSAY})\"]]) / len(\n",
    "        df\n",
    "    )\n",
    "    print(\n",
    "        f\"Min pred: {min_pred}, Accuracy: {acc:.4f}. Samples: {len(df)}/{rna_df.shape[0]}\\n\"\n",
    "    )\n",
    "    groupby = (\n",
    "        df.groupby([ASSAY, f\"Predicted class ({ASSAY})\"])\n",
    "        .size()\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"Count\"})\n",
    "        .sort_values([ASSAY, \"Count\"], ascending=False)\n",
    "    )\n",
    "    print(groupby, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RNA-Seq assay accuracy, if mrna_seq == rna_seq\\n\")\n",
    "df = rna_df.copy()\n",
    "for cat in [ASSAY, f\"Predicted class ({ASSAY})\", f\"True class ({ASSAY})\"]:\n",
    "    df.loc[df[cat] == \"mrna_seq\", cat] = \"rna_seq\"\n",
    "\n",
    "for min_pred in [0, 0.6, 0.8]:\n",
    "    sub_df = df[df[f\"Max pred ({ASSAY})\"] >= min_pred]\n",
    "    acc = len(\n",
    "        sub_df[sub_df[f\"True class ({ASSAY})\"] == sub_df[f\"Predicted class ({ASSAY})\"]]\n",
    "    ) / len(sub_df)\n",
    "    print(\n",
    "        f\"Min pred: {min_pred}, Accuracy: {acc:.4f}. Samples: {len(sub_df)}/{rna_df.shape[0]}\\n\"\n",
    "    )\n",
    "\n",
    "    groupby = (\n",
    "        sub_df.groupby([ASSAY, f\"Predicted class ({ASSAY})\"])\n",
    "        .size()\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"Count\"})\n",
    "        .sort_values(by=[ASSAY, \"Count\"], ascending=[True, False])\n",
    "    )\n",
    "    print(groupby, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell type, Life Stage, Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_df = merge_life_stages(\n",
    "    df=rna_df,\n",
    "    column_name_templates=[\n",
    "        \"{}\",\n",
    "        \"True class ({})\",\n",
    "        \"Predicted class ({})\",\n",
    "        \"Max pred ({})\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in [CELL_TYPE, LIFE_STAGE, f\"{LIFE_STAGE}_merged\", SEX]:\n",
    "    print(cat.upper(), \"\\n\")\n",
    "    cat_df = rna_df[rna_df[cat] != \"unknown\"].copy()\n",
    "\n",
    "    if cat == CELL_TYPE:\n",
    "        cat_df = cat_df[cat_df[cat].isin(accepted_ct)]\n",
    "\n",
    "    print(cat_df[cat].value_counts(dropna=False), \"\\n\")\n",
    "\n",
    "    if cat_df[cat].isna().sum() > 0:\n",
    "        raise ValueError(f\"Missing values in {cat}\")\n",
    "\n",
    "    for min_pred in [0, 0.6, 0.8]:\n",
    "        sub_df = cat_df[cat_df[f\"Max pred ({cat})\"] >= min_pred]\n",
    "\n",
    "        y_true = sub_df[f\"True class ({cat})\"]\n",
    "        y_pred = sub_df[f\"Predicted class ({cat})\"]\n",
    "\n",
    "        acc = (y_true == y_pred).sum() / len(sub_df)\n",
    "        print(\n",
    "            f\"Min pred: {min_pred}, Accuracy: {acc:.4f}. Samples: {len(sub_df)}/{cat_df.shape[0]} ({len(sub_df)/cat_df.shape[0]:.2%})\\n\"\n",
    "        )\n",
    "\n",
    "        groupby = (\n",
    "            sub_df.groupby([cat, f\"Predicted class ({cat})\"])\n",
    "            .size()\n",
    "            .reset_index()\n",
    "            .rename(columns={0: \"Count\"})\n",
    "            .sort_values(by=[cat, \"Count\"], ascending=[True, False])\n",
    "        )\n",
    "        print(f\"{cat}\\tPredicted class ({cat})\\tCount\")\n",
    "        for vals in groupby.astype(str).values:\n",
    "            print(\"\\t\".join(vals.tolist()))\n",
    "        print()\n",
    "        print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
