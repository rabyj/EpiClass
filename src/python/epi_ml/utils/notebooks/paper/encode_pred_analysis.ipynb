{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to analyse encode predictions.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches, pointless-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    add_second_highest_prediction,\n",
    ")\n",
    "\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting GO info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_dir = base_data_dir / \"metadata\" / \"encode\"\n",
    "curie_def_df = pd.read_csv(\n",
    "    encode_metadata_dir / \"EpiAtlas_list-curie_term_HSOI.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"code\", \"term\", CELL_TYPE],\n",
    ")\n",
    "encode_ontology_df = pd.read_csv(encode_metadata_dir / \"encode_ontol+assay.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_ontology_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = encode_ontology_df.merge(\n",
    "    curie_def_df, left_on=\"Biosample term id\", right_on=\"code\", how=\"left\"\n",
    ")\n",
    "metadata_df = metadata_df.drop(columns=[\"code\", \"term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = metadata_df[CELL_TYPE].value_counts(dropna=False)\n",
    "display(counts / counts.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing harmonized_sample_ontology_intermediate details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check term on missing CELL_TYPE\n",
    "missing_cell_type = metadata_df[metadata_df[CELL_TYPE].isna()]\n",
    "print(missing_cell_type.shape)\n",
    "\n",
    "biosample_cols = [\"Biosample term id\", \"Biosample term name\"]\n",
    "\n",
    "missing_count = missing_cell_type[biosample_cols].value_counts()\n",
    "display(missing_count.shape)\n",
    "with pd.option_context(\n",
    "    \"display.float_format\",\n",
    "    \"{:.2f}\".format,  # pylint: disable=consider-using-f-string\n",
    "    \"display.max_rows\",\n",
    "    None,\n",
    "):\n",
    "    display(missing_count / missing_count.sum() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cell_types = [\n",
    "    name for name in missing_cell_type[\"Biosample term name\"].unique() if \"T cell\" in name\n",
    "]\n",
    "b_cell_types = [\n",
    "    name for name in missing_cell_type[\"Biosample term name\"].unique() if \"B cell\" in name\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cell_count = missing_cell_type[\n",
    "    missing_cell_type[\"Biosample term name\"].isin(t_cell_types)\n",
    "][biosample_cols].value_counts()\n",
    "display(t_cell_count, t_cell_count.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_cell_count = missing_cell_type[\n",
    "    missing_cell_type[\"Biosample term name\"].isin(b_cell_types)\n",
    "][biosample_cols].value_counts()\n",
    "display(b_cell_count, b_cell_count.sum())\n",
    "\n",
    "perc_missing = (\n",
    "    (t_cell_count.sum() + b_cell_count.sum()) / missing_cell_type.shape[0] * 100\n",
    ")\n",
    "print(f\"t+b cells, percentage of missing cell types: {perc_missing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match predictions from various trainings with GO info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_folder = (\n",
    "    base_data_dir\n",
    "    / \"training_results/dfreeze_v2/hg38_100kb_all_none/harmonized_sample_ontology_intermediate_1l_3000n/complete-no_valid-oversampling\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df[\"Assay\"] = metadata_df[\"Assay\"].str.lower()\n",
    "df = metadata_df.dropna(subset=[CELL_TYPE])  # drop rows with missing cell type\n",
    "df = df.dropna(subset=[\"Assay\"])  # drop rows with missing assay\n",
    "non_core_metadata_df = df[~df[\"Assay\"].isin(ASSAY_ORDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_core_metadata_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = metadata_df[\"Assay\"].value_counts(dropna=False)\n",
    "# print(len(counts))\n",
    "# counts.to_csv(\n",
    "#     path_or_buf=Path().home() / \"downloads\" / \"encode_assay_counts.csv\",\n",
    "#     sep=\",\",\n",
    "#     header=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(non_core_metadata_df[CELL_TYPE].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the predictions for the 16 cell types\n",
    "accepted_ct = [\n",
    "    \"T cell\",\n",
    "    \"neutrophil\",\n",
    "    \"brain\",\n",
    "    \"monocyte\",\n",
    "    \"lymphocyte of B lineage\",\n",
    "    \"myeloid cell\",\n",
    "    \"venous blood\",\n",
    "    \"macrophage\",\n",
    "    \"mesoderm-derived structure\",\n",
    "    \"endoderm-derived structure\",\n",
    "    \"colon\",\n",
    "    \"connective tissue cell\",\n",
    "    \"hepatocyte\",\n",
    "    \"mammary gland epithelial cell\",\n",
    "    \"muscle organ\",\n",
    "    \"extraembryonic cell\",\n",
    "]\n",
    "print(non_core_metadata_df.shape)\n",
    "metadata_16ct = non_core_metadata_df[non_core_metadata_df[CELL_TYPE].isin(accepted_ct)]\n",
    "print(metadata_16ct.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metadata_16ct[\"Assay\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs_dict = {}\n",
    "for folder in pred_folder.glob(\"*\"):\n",
    "    if not folder.is_dir():\n",
    "        print(f\"Skipping {folder}\")\n",
    "        continue\n",
    "    pred_file = list(folder.glob(\"predictions/*.csv\"))\n",
    "\n",
    "    if len(pred_file) > 1:\n",
    "        print(f\"More than one prediction file found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    if len(pred_file) == 0:\n",
    "        print(f\"No prediction file found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    pred_file = pred_file[0]\n",
    "\n",
    "    pred_df = pd.read_csv(pred_file)\n",
    "    name = folder.name.replace(\"complete_no_valid_oversample_\", \"\")\n",
    "    pred_dfs_dict[name] = pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_cols = [\"Assay\", CELL_TYPE, \"Predicted class\", \"correct_pred\"]\n",
    "\n",
    "for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "    print(name)\n",
    "    pred_w_ct = pred_df.merge(\n",
    "        metadata_16ct, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"inner\"\n",
    "    )\n",
    "    N = pred_w_ct.shape[0]\n",
    "\n",
    "    # Calculate results for all predictions\n",
    "    pred_w_ct[\"correct_pred\"] = pred_w_ct[\"Predicted class\"] == pred_w_ct[CELL_TYPE]\n",
    "    counts = pred_w_ct.groupby(groupby_cols).size().sort_values(ascending=False)\n",
    "    total_correct = counts.loc[:, :, :, True].sum()\n",
    "    perc = total_correct / N\n",
    "    print(f\"Acc (pred>0.0) {total_correct}/{N} ({perc:.2%})\")\n",
    "\n",
    "    # Calculate results for predictions with max_pred > 0.8\n",
    "    pred_w_ct_filtered = pred_w_ct[pred_w_ct[\"Max pred\"] > 0.8]\n",
    "    counts_filtered = (\n",
    "        pred_w_ct_filtered.groupby(groupby_cols).size().sort_values(ascending=False)\n",
    "    )\n",
    "    total_correct_filtered = counts_filtered.loc[:, :, :, True].sum()\n",
    "    perc_filtered = total_correct_filtered / pred_w_ct_filtered.shape[0]\n",
    "    print(\n",
    "        f\"Acc (pred>0.8): {total_correct_filtered}/{pred_w_ct_filtered.shape[0]} ({perc_filtered:.2%})\"\n",
    "    )\n",
    "    diff = N - pred_w_ct_filtered.shape[0]\n",
    "    print(f\"Samples ignored at 0.8: {diff} ({diff/N:.2%})\\n\")\n",
    "\n",
    "    # Uncomment the following lines if you want to display additional information\n",
    "    # if \"assay\" in name.lower():\n",
    "    #     with pd.option_context(\n",
    "    #         \"display.float_format\",\n",
    "    #         \"{:.3f}\".format,\n",
    "    #         \"display.max_rows\",\n",
    "    #         None,\n",
    "    #     ):\n",
    "    #         values_count = pred_w_ct[\"Assay\"].value_counts()\n",
    "    #         # display(values_count)\n",
    "    #         display(values_count / values_count.sum())\n",
    "    #         display(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sns_confusion_matrix(pred_w_ct: pd.DataFrame):\n",
    "    \"\"\"Create a confusion matrix plot using seaborn.\"\"\"\n",
    "    pred_w_ct[\"Assay\"] = pred_w_ct[\"Assay\"].str.lower()\n",
    "    pred_w_ct = pred_w_ct.dropna(subset=[CELL_TYPE])  # drop rows with missing cell type\n",
    "    pred_w_ct = pred_w_ct[~pred_w_ct[\"Assay\"].isin(ASSAY_ORDER)]\n",
    "    pred_w_ct = pred_w_ct[pred_w_ct[\"Max pred\"] > 0.8]\n",
    "\n",
    "    # Count real samples for each cell type\n",
    "    real_samples_count = pred_w_ct[CELL_TYPE].value_counts()\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        pred_w_ct[CELL_TYPE], pred_w_ct[\"Predicted class\"], labels=accepted_ct\n",
    "    )\n",
    "\n",
    "    # Convert to percentages (each row sums to 1)\n",
    "    cm_percentage = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Create x-axis labels with sample counts\n",
    "    ticklabels_w_count = [\n",
    "        f\"{ct}\\n(n={real_samples_count.get(ct, 0)})\" for ct in accepted_ct\n",
    "    ]\n",
    "\n",
    "    # Create a heatmap of the percentage-based confusion matrix\n",
    "    plt.figure(figsize=(24, 20))  # Increased figure size\n",
    "    sns.heatmap(\n",
    "        cm_percentage,\n",
    "        annot=True,\n",
    "        fmt=\".2%\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=accepted_ct,\n",
    "        yticklabels=ticklabels_w_count,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        annot_kws={\"size\": 10},  # Increased annotation font size\n",
    "        cbar_kws={\"shrink\": 0.8},\n",
    "    )  # Adjust colorbar size\n",
    "\n",
    "    plt.title(f\"Confusion Matrix (%) for {name}\", fontsize=20)\n",
    "    plt.xlabel(\"Predicted\", fontsize=16)\n",
    "    plt.ylabel(\"Actual\", fontsize=16)\n",
    "    plt.xticks(fontsize=10, rotation=90, ha=\"center\")\n",
    "    plt.yticks(fontsize=12, rotation=0)\n",
    "\n",
    "    # Adjust bottom margin to accommodate longer x-axis labels\n",
    "    plt.gcf().subplots_adjust(bottom=0.2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    print(f\"Accuracy: {accuracy:.2%} ({np.trace(cm)} / {np.sum(cm)})\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "    print(name)\n",
    "    pred_df_w_ct = pred_df.merge(\n",
    "        metadata_df, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"left\"\n",
    "    )\n",
    "    # sns_confusion_matrix(pred_df_w_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download note\n",
    "~~~bash\n",
    "paper_dir=\"/home/local/USHERBROOKE/rabj2301/Projects/epiclass/output/paper/data/training_results/dfreeze_v2/hg38_100kb_all_none/assay_epiclass_1l_3000n\"\n",
    "cd $paper_dir\n",
    "base_path=\"/lustre06/project/6007515/rabyj/epiclass-project/output/epiclass-logs/epiatlas-dfreeze-v2.1/hg38_100kb_all_none/assay_epiclass_1l_3000n\"\n",
    "rsync -avR --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"*confusion*\" --exclude \"*.md5\" narval:${base_path}/./*c/complete_no_valid_oversample .\n",
    "\n",
    "paper_dir=\"/home/local/USHERBROOKE/rabj2301/Projects/epiclass/output/paper/data/training_results/dfreeze_v2\"\n",
    "cd $paper_dir\n",
    "base_path=\"/lustre06/project/6007515/rabyj/epiclass-project/output/epiclass-logs/epiatlas-dfreeze-v2.1\"\n",
    "rsync -avR --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"*confusion*\" --exclude \"*.md5\" narval:${base_path}/./hg38_100kb_all_none_w_encode_noncore/assay_epiclass_1l_3000n/complete_no_valid_oversample-0 .\n",
    "\n",
    "find -type f -name \"*.list*.csv\" -print0 | xargs -0 rename 's/\\.list//g'\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "assay7_folder = (\n",
    "    data_dir / f\"hg38_100kb_all_none/{ASSAY}_1l_3000n/7c/complete_no_valid_oversample\"\n",
    ")\n",
    "assay11_folder = (\n",
    "    data_dir / f\"hg38_100kb_all_none/{ASSAY}_1l_3000n/11c/complete_no_valid_oversample\"\n",
    ")\n",
    "assay13_folder = (\n",
    "    data_dir\n",
    "    / f\"hg38_100kb_all_none_w_encode_noncore/{ASSAY}_1l_3000n/complete_no_valid_oversample-0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_path = encode_metadata_dir / \"ENCODE_IHEC_keys.tsv\"\n",
    "core_metadata_df = pd.read_csv(encode_metadata_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(core_metadata_df.head())\n",
    "print(core_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_metadata_df[\"assay_epiclass\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs_dict = {}\n",
    "for name, folder in zip(\n",
    "    [\"7c\", \"11c\", \"13c\"], [assay7_folder, assay11_folder, assay13_folder]\n",
    "):\n",
    "    if not folder.exists():\n",
    "        print(f\"Folder {folder} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    pred_folder = folder / \"predictions\" / \"encode\"\n",
    "    if not pred_folder.exists():\n",
    "        print(f\"Folder {pred_folder} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    pred_file = list(pred_folder.glob(\"*.csv\"))\n",
    "    if len(pred_file) != 1:\n",
    "        print(f\"Found {len(pred_file)} files in {pred_folder}.\")\n",
    "        continue\n",
    "    pred_file = pred_file[0]\n",
    "\n",
    "    pred_df = pd.read_csv(pred_file, sep=\",\")\n",
    "    try:\n",
    "        pred_df.drop(columns=[\"Same?\"], inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Add assay metadata\n",
    "    pred_df = pred_df.merge(\n",
    "        core_metadata_df, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    pred_df[\"True class\"] = pred_df[\"assay_epiclass\"]\n",
    "    pred_dfs_dict[name] = pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core7 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = data_dir = base_data_dir / \"training_results\" / \"encode_predictions\"\n",
    "for name, df in pred_dfs_dict.items():\n",
    "    print(name)\n",
    "    # print(df.shape)\n",
    "\n",
    "    # Only consider files already labeled with core7 assays\n",
    "    df = df[df[ASSAY].isin(ASSAY_ORDER)]\n",
    "\n",
    "    # Only consider non-EpiAtlas samples\n",
    "    df = df[df[\"is_EpiAtlas_EpiRR\"].isna()]\n",
    "\n",
    "    # df.to_csv(output_dir / f\"encode_only-core-{name}_predictions.csv\", index=False)\n",
    "    # break\n",
    "\n",
    "    # Calculate results for all predictions\n",
    "    correct_pred = df[\"Predicted class\"] == df[\"True class\"]\n",
    "    total_correct = correct_pred.sum()\n",
    "    total = df.shape[0]\n",
    "    perc = total_correct / total\n",
    "    print(f\"Acc (pred>=0.0) {total_correct}/{total} ({perc:.2%})\")\n",
    "\n",
    "    # Calculate results for predictions with max_pred > 0.6\n",
    "    df_filtered = df[df[\"Max pred\"] >= 0.6]\n",
    "    correct_pred_filtered = df_filtered[\"Predicted class\"] == df_filtered[\"True class\"]\n",
    "    total_correct_filtered = correct_pred_filtered.sum()\n",
    "    total_filtered = df_filtered.shape[0]\n",
    "    perc_filtered = total_correct_filtered / total_filtered\n",
    "    print(\n",
    "        f\"Acc (pred>=0.6): {total_correct_filtered}/{total_filtered} ({perc_filtered:.2%})\"\n",
    "    )\n",
    "\n",
    "    df_filtered_wrong = df_filtered[~correct_pred_filtered]\n",
    "    groupby = (\n",
    "        df_filtered_wrong.groupby([\"True class\", \"Predicted class\"])\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    display(\"Mislabels:\", groupby)\n",
    "\n",
    "    # df_filtered_wrong.to_csv(\n",
    "    #     output_dir / f\"encode_only_mislabels_minPred0.6_{name}.csv\", index=False\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-core 7c preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7c preds on non-core assays\n",
    "name = \"7c\"\n",
    "df = pred_dfs_dict[name]\n",
    "df = df.merge(metadata_df, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"left\")\n",
    "df = df[~df[\"Assay\"].isin(ASSAY_ORDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.columns)\n",
    "# display(df[\"Assay\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = data_dir = (\n",
    "    base_data_dir / \"training_results\" / \"predictions\" / \"encode\" / \"assay_epiclass\"\n",
    ")\n",
    "for min_pred in [0, 0.6, 0.8]:\n",
    "    df_filtered = df[df[\"Max pred\"] >= min_pred]\n",
    "    groupby = (\n",
    "        df_filtered.groupby([\"Predicted class\", \"Assay\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"Count\")\n",
    "        .sort_values([\"Predicted class\", \"Count\"], ascending=[True, False])\n",
    "        .set_index([\"Predicted class\", \"Assay\"])[\"Count\"]\n",
    "    )\n",
    "    groupby.to_csv(\n",
    "        output_dir / f\"encode_non-core_{name}_predictions_minPred{min_pred}.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for predicted_class, group in df.groupby(\"Predicted class\"):\n",
    "#     print(f\"\\nPredicted class: {predicted_class}\")\n",
    "#     with pd.option_context(\"display.max_rows\",None,):\n",
    "#         display(group[\"Assay\"].value_counts(dropna=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
