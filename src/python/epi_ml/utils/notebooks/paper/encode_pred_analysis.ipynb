{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to analyse encode predictions.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches, pointless-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    add_second_highest_prediction,\n",
    "    display_perc,\n",
    ")\n",
    "\n",
    "# from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting GO info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_dir = base_data_dir / \"metadata\" / \"encode\"\n",
    "curie_def_df = pd.read_csv(\n",
    "    encode_metadata_dir / \"EpiAtlas_list-curie_term_HSOI.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"code\", \"term\", CELL_TYPE],\n",
    ")\n",
    "encode_ontology_df = pd.read_csv(encode_metadata_dir / \"encode_ontol+assay.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_ontology_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = encode_ontology_df.merge(\n",
    "    curie_def_df, left_on=\"Biosample term id\", right_on=\"code\", how=\"left\"\n",
    ")\n",
    "metadata_df = metadata_df.drop(columns=[\"code\", \"term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = metadata_df[CELL_TYPE].value_counts(dropna=False)\n",
    "display(counts / counts.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing harmonized_sample_ontology_intermediate details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check term on missing CELL_TYPE\n",
    "missing_cell_type = metadata_df[metadata_df[CELL_TYPE].isna()]\n",
    "print(missing_cell_type.shape)\n",
    "\n",
    "biosample_cols = [\"Biosample term id\", \"Biosample term name\"]\n",
    "\n",
    "missing_count = missing_cell_type[biosample_cols].value_counts()\n",
    "display(missing_count.shape)\n",
    "with pd.option_context(\n",
    "    \"display.float_format\",\n",
    "    \"{:.2f}\".format,  # pylint: disable=consider-using-f-string\n",
    "    \"display.max_rows\",\n",
    "    None,\n",
    "):\n",
    "    display(missing_count / missing_count.sum() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cell_types = [\n",
    "    name for name in missing_cell_type[\"Biosample term name\"].unique() if \"T cell\" in name\n",
    "]\n",
    "b_cell_types = [\n",
    "    name for name in missing_cell_type[\"Biosample term name\"].unique() if \"B cell\" in name\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cell_count = missing_cell_type[\n",
    "    missing_cell_type[\"Biosample term name\"].isin(t_cell_types)\n",
    "][biosample_cols].value_counts()\n",
    "display(t_cell_count, t_cell_count.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_cell_count = missing_cell_type[\n",
    "    missing_cell_type[\"Biosample term name\"].isin(b_cell_types)\n",
    "][biosample_cols].value_counts()\n",
    "display(b_cell_count, b_cell_count.sum())\n",
    "\n",
    "perc_missing = (\n",
    "    (t_cell_count.sum() + b_cell_count.sum()) / missing_cell_type.shape[0] * 100\n",
    ")\n",
    "print(f\"t+b cells, percentage of missing cell types: {perc_missing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match predictions from various trainings with GO info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_folder = (\n",
    "    base_data_dir\n",
    "    / \"training_results/dfreeze_v2/hg38_100kb_all_none/harmonized_sample_ontology_intermediate_1l_3000n/complete-no_valid-oversampling\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df[\"Assay\"] = metadata_df[\"Assay\"].str.lower()\n",
    "df = metadata_df.dropna(subset=[CELL_TYPE])  # drop rows with missing cell type\n",
    "df = df.dropna(subset=[\"Assay\"])  # drop rows with missing assay\n",
    "non_core_metadata_df = df[~df[\"Assay\"].isin(ASSAY_ORDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_core_metadata_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = metadata_df[\"Assay\"].value_counts(dropna=False)\n",
    "# print(len(counts))\n",
    "# counts.to_csv(\n",
    "#     path_or_buf=Path().home() / \"downloads\" / \"encode_assay_counts.csv\",\n",
    "#     sep=\",\",\n",
    "#     header=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(non_core_metadata_df[CELL_TYPE].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the predictions for the 16 cell types\n",
    "accepted_ct = [\n",
    "    \"T cell\",\n",
    "    \"neutrophil\",\n",
    "    \"brain\",\n",
    "    \"monocyte\",\n",
    "    \"lymphocyte of B lineage\",\n",
    "    \"myeloid cell\",\n",
    "    \"venous blood\",\n",
    "    \"macrophage\",\n",
    "    \"mesoderm-derived structure\",\n",
    "    \"endoderm-derived structure\",\n",
    "    \"colon\",\n",
    "    \"connective tissue cell\",\n",
    "    \"hepatocyte\",\n",
    "    \"mammary gland epithelial cell\",\n",
    "    \"muscle organ\",\n",
    "    \"extraembryonic cell\",\n",
    "]\n",
    "print(non_core_metadata_df.shape)\n",
    "metadata_16ct = non_core_metadata_df[non_core_metadata_df[CELL_TYPE].isin(accepted_ct)]\n",
    "print(metadata_16ct.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metadata_16ct[\"Assay\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs_dict = {}\n",
    "for folder in pred_folder.glob(\"*\"):\n",
    "    if not folder.is_dir():\n",
    "        print(f\"Skipping {folder}\")\n",
    "        continue\n",
    "    pred_file = list(folder.glob(\"predictions/*.csv\"))\n",
    "\n",
    "    if len(pred_file) > 1:\n",
    "        print(f\"More than one prediction file found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    if len(pred_file) == 0:\n",
    "        print(f\"No prediction file found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    pred_file = pred_file[0]\n",
    "\n",
    "    pred_df = pd.read_csv(pred_file)\n",
    "    name = folder.name.replace(\"complete_no_valid_oversample_\", \"\")\n",
    "    pred_dfs_dict[name] = pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_cols = [\"Assay\", CELL_TYPE, \"Predicted class\", \"correct_pred\"]\n",
    "\n",
    "for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "    print(name)\n",
    "    pred_w_ct = pred_df.merge(\n",
    "        metadata_16ct, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"inner\"\n",
    "    )\n",
    "    N = pred_w_ct.shape[0]\n",
    "\n",
    "    # Calculate results for all predictions\n",
    "    pred_w_ct[\"correct_pred\"] = pred_w_ct[\"Predicted class\"] == pred_w_ct[CELL_TYPE]\n",
    "    counts = pred_w_ct.groupby(groupby_cols).size().sort_values(ascending=False)\n",
    "    total_correct = counts.loc[:, :, :, True].sum()\n",
    "    perc = total_correct / N\n",
    "    print(f\"Acc (pred>0.0) {total_correct}/{N} ({perc:.2%})\")\n",
    "\n",
    "    # Calculate results for predictions with max_pred > 0.8\n",
    "    pred_w_ct_filtered = pred_w_ct[pred_w_ct[\"Max pred\"] > 0.8]\n",
    "    counts_filtered = (\n",
    "        pred_w_ct_filtered.groupby(groupby_cols).size().sort_values(ascending=False)\n",
    "    )\n",
    "    total_correct_filtered = counts_filtered.loc[:, :, :, True].sum()\n",
    "    perc_filtered = total_correct_filtered / pred_w_ct_filtered.shape[0]\n",
    "    print(\n",
    "        f\"Acc (pred>0.8): {total_correct_filtered}/{pred_w_ct_filtered.shape[0]} ({perc_filtered:.2%})\"\n",
    "    )\n",
    "    diff = N - pred_w_ct_filtered.shape[0]\n",
    "    print(f\"Samples ignored at 0.8: {diff} ({diff/N:.2%})\\n\")\n",
    "\n",
    "    # Uncomment the following lines if you want to display additional information\n",
    "    # if \"assay\" in name.lower():\n",
    "    #     with pd.option_context(\n",
    "    #         \"display.float_format\",\n",
    "    #         \"{:.3f}\".format,\n",
    "    #         \"display.max_rows\",\n",
    "    #         None,\n",
    "    #     ):\n",
    "    #         values_count = pred_w_ct[\"Assay\"].value_counts()\n",
    "    #         # display(values_count)\n",
    "    #         display(values_count / values_count.sum())\n",
    "    #         display(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sns_confusion_matrix(pred_w_ct: pd.DataFrame):\n",
    "    \"\"\"Create a confusion matrix plot using seaborn.\"\"\"\n",
    "    pred_w_ct[\"Assay\"] = pred_w_ct[\"Assay\"].str.lower()\n",
    "    pred_w_ct = pred_w_ct.dropna(subset=[CELL_TYPE])  # drop rows with missing cell type\n",
    "    pred_w_ct = pred_w_ct[~pred_w_ct[\"Assay\"].isin(ASSAY_ORDER)]\n",
    "    pred_w_ct = pred_w_ct[pred_w_ct[\"Max pred\"] > 0.8]\n",
    "\n",
    "    # Count real samples for each cell type\n",
    "    real_samples_count = pred_w_ct[CELL_TYPE].value_counts()\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        pred_w_ct[CELL_TYPE], pred_w_ct[\"Predicted class\"], labels=accepted_ct\n",
    "    )\n",
    "\n",
    "    # Convert to percentages (each row sums to 1)\n",
    "    cm_percentage = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Create x-axis labels with sample counts\n",
    "    ticklabels_w_count = [\n",
    "        f\"{ct}\\n(n={real_samples_count.get(ct, 0)})\" for ct in accepted_ct\n",
    "    ]\n",
    "\n",
    "    # Create a heatmap of the percentage-based confusion matrix\n",
    "    plt.figure(figsize=(24, 20))  # Increased figure size\n",
    "    sns.heatmap(\n",
    "        cm_percentage,\n",
    "        annot=True,\n",
    "        fmt=\".2%\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=accepted_ct,\n",
    "        yticklabels=ticklabels_w_count,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        annot_kws={\"size\": 10},  # Increased annotation font size\n",
    "        cbar_kws={\"shrink\": 0.8},\n",
    "    )  # Adjust colorbar size\n",
    "\n",
    "    plt.title(f\"Confusion Matrix (%) for {name}\", fontsize=20)\n",
    "    plt.xlabel(\"Predicted\", fontsize=16)\n",
    "    plt.ylabel(\"Actual\", fontsize=16)\n",
    "    plt.xticks(fontsize=10, rotation=90, ha=\"center\")\n",
    "    plt.yticks(fontsize=12, rotation=0)\n",
    "\n",
    "    # Adjust bottom margin to accommodate longer x-axis labels\n",
    "    plt.gcf().subplots_adjust(bottom=0.2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    print(f\"Accuracy: {accuracy:.2%} ({np.trace(cm)} / {np.sum(cm)})\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "    print(name)\n",
    "    pred_df_w_ct = pred_df.merge(\n",
    "        metadata_df, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"left\"\n",
    "    )\n",
    "    # sns_confusion_matrix(pred_df_w_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download note\n",
    "~~~bash\n",
    "paper_dir=\"/home/local/USHERBROOKE/rabj2301/Projects/epiclass/output/paper/data/training_results/dfreeze_v2/hg38_100kb_all_none/assay_epiclass_1l_3000n\"\n",
    "cd $paper_dir\n",
    "base_path=\"/lustre06/project/6007515/rabyj/epiclass-project/output/epiclass-logs/epiatlas-dfreeze-v2.1/hg38_100kb_all_none/assay_epiclass_1l_3000n\"\n",
    "rsync -avR --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"*confusion*\" --exclude \"*.md5\" narval:${base_path}/./*c/complete_no_valid_oversample .\n",
    "\n",
    "paper_dir=\"/home/local/USHERBROOKE/rabj2301/Projects/epiclass/output/paper/data/training_results/dfreeze_v2\"\n",
    "cd $paper_dir\n",
    "base_path=\"/lustre06/project/6007515/rabyj/epiclass-project/output/epiclass-logs/epiatlas-dfreeze-v2.1\"\n",
    "rsync -avR --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"*confusion*\" --exclude \"*.md5\" narval:${base_path}/./hg38_100kb_all_none_w_encode_noncore/assay_epiclass_1l_3000n/complete_no_valid_oversample-0 .\n",
    "\n",
    "find -type f -name \"*.list*.csv\" -print0 | xargs -0 rename 's/\\.list//g'\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "assay7_folder = (\n",
    "    data_dir / f\"hg38_100kb_all_none/{ASSAY}_1l_3000n/7c/complete_no_valid_oversample\"\n",
    ")\n",
    "assay11_folder = (\n",
    "    data_dir / f\"hg38_100kb_all_none/{ASSAY}_1l_3000n/11c/complete_no_valid_oversample\"\n",
    ")\n",
    "assay13_folder = (\n",
    "    data_dir\n",
    "    / f\"hg38_100kb_all_none_w_encode_noncore/{ASSAY}_1l_3000n/complete_no_valid_oversample-0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_path = encode_metadata_dir / \"ENCODE_IHEC_keys.tsv\"\n",
    "core_metadata_df = pd.read_csv(encode_metadata_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(core_metadata_df.head())\n",
    "print(core_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_metadata_df[\"assay_epiclass\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs_dict = {}\n",
    "for name, folder in zip(\n",
    "    [\"7c\", \"11c\", \"13c\"], [assay7_folder, assay11_folder, assay13_folder]\n",
    "):\n",
    "    if not folder.exists():\n",
    "        print(f\"Folder {folder} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    pred_folder = folder / \"predictions\" / \"encode\"\n",
    "    if not pred_folder.exists():\n",
    "        print(f\"Folder {pred_folder} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    pred_file = list(pred_folder.glob(\"*.csv\"))\n",
    "    if len(pred_file) != 1:\n",
    "        print(f\"Found {len(pred_file)} files in {pred_folder}.\")\n",
    "        continue\n",
    "    pred_file = pred_file[0]\n",
    "\n",
    "    pred_df = pd.read_csv(pred_file, sep=\",\")\n",
    "    try:\n",
    "        pred_df.drop(columns=[\"Same?\"], inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Add assay metadata\n",
    "    pred_df = pred_df.merge(\n",
    "        core_metadata_df, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    pred_df[\"True class\"] = pred_df[\"assay_epiclass\"]\n",
    "    pred_dfs_dict[name] = pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core7 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = data_dir = base_data_dir / \"training_results\" / \"encode_predictions\"\n",
    "for name, df in pred_dfs_dict.items():\n",
    "    print(name)\n",
    "    # print(df.shape)\n",
    "\n",
    "    # Only consider files already labeled with core7 assays\n",
    "    df = df[df[ASSAY].isin(ASSAY_ORDER)]\n",
    "\n",
    "    # Only consider non-EpiAtlas samples\n",
    "    df = df[df[\"is_EpiAtlas_EpiRR\"].isna()]\n",
    "\n",
    "    # df.to_csv(output_dir / f\"encode_only-core-{name}_predictions.csv\", index=False)\n",
    "    # break\n",
    "\n",
    "    # Calculate results for all predictions\n",
    "    correct_pred = df[\"Predicted class\"] == df[\"True class\"]\n",
    "    total_correct = correct_pred.sum()\n",
    "    total = df.shape[0]\n",
    "    perc = total_correct / total\n",
    "    print(f\"Acc (pred>=0.0) {total_correct}/{total} ({perc:.2%})\")\n",
    "\n",
    "    # Calculate results for predictions with max_pred > 0.6\n",
    "    df_filtered = df[df[\"Max pred\"] >= 0.6]\n",
    "    correct_pred_filtered = df_filtered[\"Predicted class\"] == df_filtered[\"True class\"]\n",
    "    total_correct_filtered = correct_pred_filtered.sum()\n",
    "    total_filtered = df_filtered.shape[0]\n",
    "    perc_filtered = total_correct_filtered / total_filtered\n",
    "    print(\n",
    "        f\"Acc (pred>=0.6): {total_correct_filtered}/{total_filtered} ({perc_filtered:.2%})\"\n",
    "    )\n",
    "\n",
    "    df_filtered_wrong = df_filtered[~correct_pred_filtered]\n",
    "    groupby = (\n",
    "        df_filtered_wrong.groupby([\"True class\", \"Predicted class\"])\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    display(\"Mislabels:\", groupby)\n",
    "\n",
    "    # df_filtered_wrong.to_csv(\n",
    "    #     output_dir / f\"encode_only_mislabels_minPred0.6_{name}.csv\", index=False\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-core 7c preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7c preds on non-core assays\n",
    "name = \"7c\"\n",
    "df = pred_dfs_dict[name]\n",
    "df = df.merge(metadata_df, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"left\")\n",
    "df = df[~df[\"Assay\"].isin(ASSAY_ORDER)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "display(df[\"Assay\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = data_dir = (\n",
    "    base_data_dir / \"training_results\" / \"predictions\" / \"encode\" / \"assay_epiclass\"\n",
    ")\n",
    "for min_pred in [0, 0.6, 0.8]:\n",
    "    df_filtered = df[df[\"Max pred\"] >= min_pred]\n",
    "    groupby = (\n",
    "        df_filtered.groupby([\"Predicted class\", \"Assay\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"Count\")\n",
    "        .sort_values([\"Predicted class\", \"Count\"], ascending=[True, False])\n",
    "        .set_index([\"Predicted class\", \"Assay\"])[\"Count\"]\n",
    "    )\n",
    "    # groupby.to_csv(\n",
    "    #     output_dir / f\"encode_non-core_{name}_predictions_minPred{min_pred}.csv\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_dir = base_data_dir / \"metadata/encode\"\n",
    "non_core_categories_path = encode_metadata_dir / \"non-core_encode_assay_counts_v2.tsv\"\n",
    "if not non_core_categories_path.exists():\n",
    "    raise FileNotFoundError(f\"File {non_core_categories_path} does not exist.\")\n",
    "\n",
    "non_core_categories_df = pd.read_csv(non_core_categories_path, sep=\"\\t\")\n",
    "print(non_core_categories_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_cats = df.merge(\n",
    "    non_core_categories_df[[\"assay\", \"assay_category\"]],\n",
    "    left_on=\"Assay\",\n",
    "    right_on=\"assay\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_cats.drop(columns=[\"assay\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print non-core assay categories for each predicted class\n",
    "min_pred = 0.6\n",
    "for predicted_class, group in df_w_cats.groupby(\"Predicted class\"):\n",
    "    print(predicted_class, group.shape[0])\n",
    "    group = group[group[\"Max pred\"] >= min_pred]\n",
    "    print(f\"min_pred={min_pred}: {group.shape[0]} samples left\")\n",
    "    groupby = (\n",
    "        group.groupby([\"assay_category\", \"Assay\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"Count\")\n",
    "        .sort_values([\"assay_category\", \"Count\"], ascending=[True, False])\n",
    "        .set_index([\"assay_category\", \"Assay\"])[\"Count\"]\n",
    "    )\n",
    "    with pd.option_context(\n",
    "        \"display.max_rows\",\n",
    "        None,\n",
    "    ):\n",
    "        # display(groupby)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_non_core_preds_df(df: pd.DataFrame, min_pred: float = 0.6):\n",
    "    \"\"\"Create a DataFrame of non-core assay predictions.\"\"\"\n",
    "    results = {}\n",
    "    assay_categories = dict(zip(df[\"Assay\"], df[\"assay_category\"]))\n",
    "\n",
    "    for assay, group in df.groupby(\"Assay\"):\n",
    "        # N = group.shape[0]\n",
    "        # if N < 3:\n",
    "        #     continue\n",
    "\n",
    "        group = group[group[\"Max pred\"] >= min_pred]\n",
    "        # N_post_filter = group.shape[0]\n",
    "        # if N_post_filter == 0 or N_post_filter < min_n:\n",
    "        #     continue\n",
    "\n",
    "        groupby = (\n",
    "            group.groupby([\"Predicted class\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"Count\")  # type: ignore\n",
    "            .sort_values([\"Count\"], ascending=False)\n",
    "        )\n",
    "\n",
    "        results[assay] = dict(zip(groupby[\"Predicted class\"], groupby[\"Count\"]))\n",
    "\n",
    "    result_df = pd.DataFrame(results).fillna(0)\n",
    "    result_df = result_df.astype(int)\n",
    "    result_df = result_df.T  # assay as row/index\n",
    "    result_df[\"Assay category\"] = result_df.index.map(assay_categories)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes_df = create_non_core_preds_df(df_w_cats, min_pred=0.6)\n",
    "predicted_classes_df.to_csv(\n",
    "    output_dir / f\"encode_non-core_7c_predictions_per_assay_minPred{min_pred:.2f}.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_dataframe(df_w_cats):\n",
    "    \"\"\"Create a structured dataframe with the percentage of predictions for each assay category.\"\"\"\n",
    "    # Create an empty list to store our data\n",
    "    data = []\n",
    "\n",
    "    # Iterate through the grouped data\n",
    "    for predicted_class, group in df_w_cats.groupby(\"Predicted class\"):\n",
    "        for min_pred in list(np.arange(0, 1, 0.05)) + [0.99]:\n",
    "            df_filtered = group[group[\"Max pred\"] >= min_pred]\n",
    "            counts = df_filtered[\"assay_category\"].value_counts(dropna=False)\n",
    "            total = counts.sum()\n",
    "\n",
    "            # Calculate percentages\n",
    "            percentages = (counts / total * 100).round(2)\n",
    "\n",
    "            # Add data for each assay category\n",
    "            for assay_category, percentage in percentages.items():\n",
    "                data.append(\n",
    "                    {\n",
    "                        \"Predicted class\": predicted_class,\n",
    "                        \"Min pred\": min_pred,\n",
    "                        \"assay_category\": assay_category,\n",
    "                        \"Percentage\": percentage,\n",
    "                        \"Count\": counts[assay_category],\n",
    "                        \"Total samples\": total,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Create the dataframe\n",
    "    df_structured = pd.DataFrame(data)\n",
    "\n",
    "    # Set the multi-index\n",
    "    df_structured = df_structured.set_index(\n",
    "        [\"Predicted class\", \"Min pred\", \"assay_category\"]\n",
    "    )\n",
    "\n",
    "    return df_structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_category_df = create_structured_dataframe(df_w_cats)\n",
    "\n",
    "# output_path = output_dir / \"encode_non-core_7c_predictions_assay_category.csv\"\n",
    "# assay_category_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_assay_category_graphs(df, output_dir: Path):\n",
    "    \"\"\"Graph assay category distribution for each predicted class.\"\"\"\n",
    "    # Get unique predicted classes\n",
    "    predicted_classes = df.index.get_level_values(\"Predicted class\").unique()\n",
    "\n",
    "    graph_colors = {\n",
    "        cat: px.colors.qualitative.Safe[i]\n",
    "        for i, cat in enumerate(df[\"assay_category\"].unique())\n",
    "    }\n",
    "\n",
    "    # Create a figure for each predicted class\n",
    "    for predicted_class in predicted_classes:\n",
    "        df_class = df.loc[predicted_class]\n",
    "\n",
    "        # Get unique assay categories for this predicted class\n",
    "        assay_categories = df_class.index.get_level_values(\"assay_category\").unique()\n",
    "\n",
    "        total_samples_at_zero = df_class.xs(0, level=\"Min pred\")[\"Total samples\"].iloc[0]\n",
    "\n",
    "        # Create the figure\n",
    "        fig = go.Figure()\n",
    "\n",
    "        for assay_category in assay_categories:\n",
    "            df_assay = df_class.xs(assay_category, level=\"assay_category\")\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_assay.index,\n",
    "                    y=df_assay[\"Percentage\"],\n",
    "                    mode=\"lines+markers\",\n",
    "                    name=assay_category,\n",
    "                    marker=dict(color=graph_colors[assay_category]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        conserved_percentages = (\n",
    "            df_class.groupby(\"Min pred\")[\"Total samples\"].first()\n",
    "            / total_samples_at_zero\n",
    "            * 100\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=conserved_percentages.index,\n",
    "                y=conserved_percentages.values,\n",
    "                mode=\"lines+markers\",\n",
    "                name=\"Samples Conserved\",\n",
    "                line=dict(dash=\"dash\", color=\"black\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f\"Composition for Predicted Class: {predicted_class}\",\n",
    "            xaxis_title=\"Min pred\",\n",
    "            yaxis_title=\"Percentage Composition\",\n",
    "            legend_title=\"Assay Category\",\n",
    "            hovermode=\"x unified\",\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(range=[-0.01, 1.01])\n",
    "        fig.update_yaxes(range=[0, 100])\n",
    "\n",
    "        # Show the figure\n",
    "        fig.show()\n",
    "\n",
    "        fig.write_image(\n",
    "            output_dir\n",
    "            / f\"encode_non-core_7c_predictions_assay_category_{predicted_class}.png\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_structured is your dataframe from the previous step\n",
    "create_assay_category_graphs(df=assay_category_df, output_dir=output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
