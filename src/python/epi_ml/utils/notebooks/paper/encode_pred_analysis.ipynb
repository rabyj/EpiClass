{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to analyse encode non-core predictions.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches, pointless-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_MERGE_DICT,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    ")\n",
    "\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting GO info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_dir = base_data_dir / \"metadata\" / \"encode\"\n",
    "curie_def_df = pd.read_csv(\n",
    "    encode_metadata_dir / \"EpiAtlas_list-curie_term_HSOI.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=[\"code\", \"term\", CELL_TYPE],\n",
    ")\n",
    "encode_ontology_df = pd.read_csv(encode_metadata_dir / \"encode_ontol+assay.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = encode_ontology_df.merge(\n",
    "    curie_def_df, left_on=\"Biosample term id\", right_on=\"code\", how=\"left\"\n",
    ")\n",
    "merged_df = merged_df.drop(columns=[\"code\", \"term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = merged_df[CELL_TYPE].value_counts(dropna=False)\n",
    "display(counts / counts.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing harmonized_sample_ontology_intermediate details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check term on missing CELL_TYPE\n",
    "missing_cell_type = merged_df[merged_df[CELL_TYPE].isna()]\n",
    "print(missing_cell_type.shape)\n",
    "\n",
    "biosample_cols = [\"Biosample term id\", \"Biosample term name\"]\n",
    "\n",
    "missing_count = missing_cell_type[biosample_cols].value_counts()\n",
    "display(missing_count.shape)\n",
    "with pd.option_context(\n",
    "    \"display.float_format\",\n",
    "    \"{:.2f}\".format,  # pylint: disable=consider-using-f-string\n",
    "    \"display.max_rows\",\n",
    "    None,\n",
    "):\n",
    "    display(missing_count / missing_count.sum() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cell_types = [\n",
    "    name for name in missing_cell_type[\"Biosample term name\"].unique() if \"T cell\" in name\n",
    "]\n",
    "b_cell_types = [\n",
    "    name for name in missing_cell_type[\"Biosample term name\"].unique() if \"B cell\" in name\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cell_count = missing_cell_type[\n",
    "    missing_cell_type[\"Biosample term name\"].isin(t_cell_types)\n",
    "][biosample_cols].value_counts()\n",
    "display(t_cell_count, t_cell_count.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_cell_count = missing_cell_type[\n",
    "    missing_cell_type[\"Biosample term name\"].isin(b_cell_types)\n",
    "][biosample_cols].value_counts()\n",
    "display(b_cell_count, b_cell_count.sum())\n",
    "\n",
    "perc_missing = (\n",
    "    (t_cell_count.sum() + b_cell_count.sum()) / missing_cell_type.shape[0] * 100\n",
    ")\n",
    "print(f\"t+b cells, percentage of missing cell types: {perc_missing:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match predictions from various trainings with GO info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_folder = (\n",
    "    base_data_dir\n",
    "    / \"training_results/dfreeze_v2/hg38_100kb_all_none/harmonized_sample_ontology_intermediate_1l_3000n/complete-no_valid-oversampling\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the predictions for the 16 cell types\n",
    "accepted_ct = [\n",
    "    \"T cell\",\n",
    "    \"neutrophil\",\n",
    "    \"brain\",\n",
    "    \"monocyte\",\n",
    "    \"lymphocyte of B lineage\",\n",
    "    \"myeloid cell\",\n",
    "    \"venous blood\",\n",
    "    \"macrophage\",\n",
    "    \"mesoderm-derived structure\",\n",
    "    \"endoderm-derived structure\",\n",
    "    \"colon\",\n",
    "    \"connective tissue cell\",\n",
    "    \"hepatocyte\",\n",
    "    \"mammary gland epithelial cell\",\n",
    "    \"muscle organ\",\n",
    "    \"extraembryonic cell\",\n",
    "]\n",
    "print(merged_df.shape)\n",
    "merged_df = merged_df[merged_df[CELL_TYPE].isin(accepted_ct)]\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs_dict = {}\n",
    "for folder in pred_folder.glob(\"*\"):\n",
    "    if not folder.is_dir():\n",
    "        print(f\"Skipping {folder}\")\n",
    "        continue\n",
    "    pred_file = list(folder.glob(\"predictions/*.csv\"))\n",
    "\n",
    "    if len(pred_file) > 1:\n",
    "        print(f\"More than one prediction file found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    if len(pred_file) == 0:\n",
    "        print(f\"No prediction file found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    pred_file = pred_file[0]\n",
    "\n",
    "    pred_df = pd.read_csv(pred_file)\n",
    "    name = folder.name.replace(\"complete_no_valid_oversample_\", \"\")\n",
    "    pred_dfs_dict[name] = pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "    print(name)\n",
    "    pred_w_ct = pred_df.merge(merged_df, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"left\")\n",
    "    pred_w_ct[\"Assay\"] = pred_w_ct[\"Assay\"].str.lower()\n",
    "    pred_w_ct = pred_w_ct.dropna(subset=[CELL_TYPE])  # drop rows with missing cell type\n",
    "    pred_w_ct = pred_w_ct[~pred_w_ct[\"Assay\"].isin(ASSAY_ORDER)]\n",
    "\n",
    "    pred_w_ct[\"correct_pred\"] = pred_w_ct[\"Predicted class\"] == pred_w_ct[CELL_TYPE]\n",
    "    counts = (\n",
    "        pred_w_ct.groupby([\"Assay\", CELL_TYPE, \"Predicted class\", \"correct_pred\"])\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    total_correct = counts.loc[:, :, :, True].sum()\n",
    "\n",
    "    perc = total_correct / pred_w_ct.shape[0]\n",
    "    print(f\"Total correct: {total_correct}/{pred_w_ct.shape[0]} ({perc:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_cols = [\"Assay\", CELL_TYPE, \"Predicted class\", \"correct_pred\"]\n",
    "\n",
    "for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "    print(name)\n",
    "    pred_w_ct = pred_df.merge(merged_df, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"left\")\n",
    "    pred_w_ct[\"Assay\"] = pred_w_ct[\"Assay\"].str.lower()\n",
    "    pred_w_ct = pred_w_ct.dropna(subset=[CELL_TYPE])  # drop rows with missing cell type\n",
    "    pred_w_ct = pred_w_ct[~pred_w_ct[\"Assay\"].isin(ASSAY_ORDER)]\n",
    "    N = pred_w_ct.shape[0]\n",
    "\n",
    "    # Calculate results for all predictions\n",
    "    pred_w_ct[\"correct_pred\"] = pred_w_ct[\"Predicted class\"] == pred_w_ct[CELL_TYPE]\n",
    "    counts = pred_w_ct.groupby(groupby_cols).size().sort_values(ascending=False)\n",
    "    total_correct = counts.loc[:, :, :, True].sum()\n",
    "    perc = total_correct / N\n",
    "    print(f\"Acc (pred>0.0) {total_correct}/{N} ({perc:.2%})\")\n",
    "\n",
    "    # Calculate results for predictions with max_pred > 0.8\n",
    "    pred_w_ct_filtered = pred_w_ct[pred_w_ct[\"Max pred\"] > 0.8]\n",
    "    counts_filtered = (\n",
    "        pred_w_ct_filtered.groupby(groupby_cols).size().sort_values(ascending=False)\n",
    "    )\n",
    "    total_correct_filtered = counts_filtered.loc[:, :, :, True].sum()\n",
    "    perc_filtered = total_correct_filtered / pred_w_ct_filtered.shape[0]\n",
    "    print(\n",
    "        f\"Acc (pred>0.8): {total_correct_filtered}/{pred_w_ct_filtered.shape[0]} ({perc_filtered:.2%})\"\n",
    "    )\n",
    "    diff = N - pred_w_ct_filtered.shape[0]\n",
    "    print(f\"Samples ignored at 0.8: {diff} ({diff/N:.2%})\\n\")\n",
    "\n",
    "    # Uncomment the following lines if you want to display additional information\n",
    "    # if \"assay\" in name.lower():\n",
    "    #     with pd.option_context(\n",
    "    #         \"display.float_format\",\n",
    "    #         \"{:.3f}\".format,\n",
    "    #         \"display.max_rows\",\n",
    "    #         None,\n",
    "    #     ):\n",
    "    #         values_count = pred_w_ct[\"Assay\"].value_counts()\n",
    "    #         # display(values_count)\n",
    "    #         display(values_count / values_count.sum())\n",
    "    #         display(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "    print(name)\n",
    "    pred_w_ct = pred_df.merge(merged_df, left_on=\"md5sum\", right_on=\"ENC_ID\", how=\"left\")\n",
    "    pred_w_ct[\"Assay\"] = pred_w_ct[\"Assay\"].str.lower()\n",
    "    pred_w_ct = pred_w_ct.dropna(subset=[CELL_TYPE])  # drop rows with missing cell type\n",
    "    pred_w_ct = pred_w_ct[~pred_w_ct[\"Assay\"].isin(ASSAY_ORDER)]\n",
    "    pred_w_ct = pred_w_ct[pred_w_ct[\"Max pred\"] > 0.8]\n",
    "\n",
    "    # Count real samples for each cell type\n",
    "    real_samples_count = pred_w_ct[CELL_TYPE].value_counts()\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        pred_w_ct[CELL_TYPE], pred_w_ct[\"Predicted class\"], labels=accepted_ct\n",
    "    )\n",
    "\n",
    "    # Convert to percentages (each row sums to 1)\n",
    "    cm_percentage = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Create x-axis labels with sample counts\n",
    "    ticklabels_w_count = [\n",
    "        f\"{ct}\\n(n={real_samples_count.get(ct, 0)})\" for ct in accepted_ct\n",
    "    ]\n",
    "\n",
    "    # Create a heatmap of the percentage-based confusion matrix\n",
    "    plt.figure(figsize=(24, 20))  # Increased figure size\n",
    "    sns.heatmap(\n",
    "        cm_percentage,\n",
    "        annot=True,\n",
    "        fmt=\".2%\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=accepted_ct,\n",
    "        yticklabels=ticklabels_w_count,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        annot_kws={\"size\": 10},  # Increased annotation font size\n",
    "        cbar_kws={\"shrink\": 0.8},\n",
    "    )  # Adjust colorbar size\n",
    "\n",
    "    plt.title(f\"Confusion Matrix (%) for {name}\", fontsize=20)\n",
    "    plt.xlabel(\"Predicted\", fontsize=16)\n",
    "    plt.ylabel(\"Actual\", fontsize=16)\n",
    "    plt.xticks(fontsize=10, rotation=90, ha=\"center\")\n",
    "    plt.yticks(fontsize=12, rotation=0)\n",
    "\n",
    "    # Adjust bottom margin to accommodate longer x-axis labels\n",
    "    plt.gcf().subplots_adjust(bottom=0.2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    print(f\"Accuracy: {accuracy:.2%} ({np.trace(cm)} / {np.sum(cm)})\")\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
