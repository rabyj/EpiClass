{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to analyse encode predictions.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-argument, too-many-branches, pointless-statement, unreachable, unused-import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import functools\n",
    "import gc\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.classification_merging_utils import merge_dataframes\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_ORDER,\n",
    "    CELL_TYPE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    "    IHECColorMap,\n",
    "    MetadataHandler,\n",
    "    SplitResultsHandler,\n",
    "    display_perc,\n",
    "    merge_life_stages,\n",
    ")\n",
    "\n",
    "# from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANCER = \"harmonized_sample_cancer_high\"\n",
    "CORE_ASSAYS = ASSAY_ORDER[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_handler = MetadataHandler(paper_dir)\n",
    "split_results_handler = SplitResultsHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_dir = base_data_dir / \"metadata\" / \"encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_predictions_dir = base_data_dir / \"training_results\" / \"predictions\" / \"encode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [encode_metadata_dir, encode_predictions_dir]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Directory {path} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_cts = [\n",
    "    \"T cell\",\n",
    "    \"neutrophil\",\n",
    "    \"brain\",\n",
    "    \"monocyte\",\n",
    "    \"lymphocyte of B lineage\",\n",
    "    \"myeloid cell\",\n",
    "    \"venous blood\",\n",
    "    \"macrophage\",\n",
    "    \"mesoderm-derived structure\",\n",
    "    \"endoderm-derived structure\",\n",
    "    \"colon\",\n",
    "    \"connective tissue cell\",\n",
    "    \"hepatocyte\",\n",
    "    \"mammary gland epithelial cell\",\n",
    "    \"muscle organ\",\n",
    "    \"extraembryonic cell\",\n",
    "]\n",
    "accepted_cts = [ct.lower() for ct in accepted_cts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get complete metadata\n",
    "\n",
    "Was created in `encode_metadata_creation.ipynb` using new web API downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata_path = (\n",
    "    encode_metadata_dir / \"new_meta\" / \"encode_full_metadata_2025-02_no_revoked.csv\"\n",
    ")\n",
    "complete_metadata_df = pd.read_csv(full_metadata_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 categories value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in [\n",
    "    ASSAY,\n",
    "    CELL_TYPE,\n",
    "    \"cell_type\",\n",
    "    \"epiclass_sample_ontology\",\n",
    "    \"donor_life_stage\",\n",
    "    \"donor_sex\",\n",
    "    \"cancer_status\",\n",
    "]:\n",
    "    try:\n",
    "        print(complete_metadata_df[cat].value_counts(dropna=False), \"\\n\")\n",
    "    except KeyError:\n",
    "        print(f\"Column {cat} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"cancer_status\" not in complete_metadata_df.columns:\n",
    "    chip_path = (\n",
    "        encode_metadata_dir / \"old_meta\" / \"encode_metadata_2023-10-25_clean-v2.csv\"\n",
    "    )\n",
    "    chip_metadata_df = pd.read_csv(chip_path, low_memory=False)\n",
    "\n",
    "    cancer_status = chip_metadata_df[[\"md5sum\", \"cancer_status\"]]\n",
    "    complete_metadata_df = complete_metadata_df.merge(\n",
    "        cancer_status, how=\"left\", left_on=\"FILE_accession\", right_on=\"md5sum\"\n",
    "    )\n",
    "    complete_metadata_df.drop(columns=\"md5sum\", inplace=True)\n",
    "    del chip_metadata_df\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all available predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_pred_dfs = {}\n",
    "nb_chip_files = 9619\n",
    "for folder in encode_predictions_dir.glob(\"*1l_3000n\"):\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "    cat = folder.name.split(\"_1l_3000n\")[0]  # [category]_1l_3000n\n",
    "    pred_file = list(folder.rglob(\"complete_no_valid_oversample*all_augmented.csv\"))[0]\n",
    "    encode_df = pd.read_csv(pred_file)\n",
    "    chip_pred_dfs[cat] = encode_df\n",
    "    print(cat, encode_df.shape)\n",
    "    assert encode_df.shape[0] == nb_chip_files\n",
    "\n",
    "assert len(chip_pred_dfs) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs_rna = {}\n",
    "nb_rna_files = 1790\n",
    "for folder in encode_predictions_dir.glob(\"*1l_3000n\"):\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "    cat = folder.name.split(\"_1l_3000n\")[0]  # [category]_1l_3000n\n",
    "    try:\n",
    "        pred_file = list(folder.rglob(\"complete_no_valid_oversample*rna*augmented*csv\"))[\n",
    "            0\n",
    "        ]\n",
    "        print(\"Using augmented file\", pred_file)\n",
    "    except IndexError as err:\n",
    "        pred_file = list(folder.rglob(\"complete_no_valid_oversample*rna*csv\"))[0]\n",
    "        print(\"Augmenting file\", pred_file)\n",
    "\n",
    "        # Augment the prediction file with some additional columns\n",
    "        # script.py <prediction_file> <metadata_file>\n",
    "        # output_template = \"Augmented prediction file saved to {new_path}\"\n",
    "        script_path = (\n",
    "            Path.home()\n",
    "            / \"Projects/sources/epi_ml/src/python/epi_ml/utils/augment_predict_file.py\"\n",
    "        )\n",
    "        if not script_path.exists():\n",
    "            raise FileNotFoundError(f\"Script {script_path} does not exist.\") from err\n",
    "\n",
    "        if not full_metadata_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Metadata file {full_metadata_path} does not exist.\"\n",
    "            ) from err\n",
    "\n",
    "        output = subprocess.run(\n",
    "            [\n",
    "                \"python\",\n",
    "                str(script_path),\n",
    "                \"-v\",\n",
    "                str(pred_file),\n",
    "                str(full_metadata_path.with_suffix(\".json\")),\n",
    "            ],\n",
    "            check=False,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        if output.returncode != 0:\n",
    "            raise RuntimeError(\n",
    "                f\"Error running script: {output.stderr.decode('utf-8')}\"\n",
    "            ) from err\n",
    "\n",
    "        stdout = output.stdout.decode(\"utf-8\")\n",
    "        pred_file = stdout.strip().split(\"Augmented prediction file saved to \", 1)[-1]\n",
    "        pred_file = Path(pred_file).resolve()\n",
    "\n",
    "    encode_df = pd.read_csv(pred_file)\n",
    "    encode_df[\"md5sum\"] = encode_df[\"md5sum\"].str.split(pat=\"_\", n=1, expand=True)[0]\n",
    "    pred_dfs_rna[cat] = encode_df\n",
    "    print(cat, encode_df.shape)\n",
    "    assert encode_df.shape[0] == nb_rna_files\n",
    "\n",
    "assert len(pred_dfs_rna) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_results_file = encode_predictions_dir / \"full_rna_merged_df.csv\"\n",
    "if not rna_results_file.exists():\n",
    "    rna_dict = copy.deepcopy(pred_dfs_rna)\n",
    "\n",
    "    same_col_names = 8\n",
    "    # Make all different columns have unique relevant names except for the pred vector\n",
    "    for cat, df in rna_dict.items():\n",
    "        df = df.drop(columns=[\"Same?\"])\n",
    "        old_names = df.columns[1 : same_col_names - 1]\n",
    "        new_names = [f\"{old_name} ({cat})\" for old_name in old_names]\n",
    "        df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "        df.set_index(\"md5sum\", inplace=True)\n",
    "\n",
    "        y_true_col = [col for col in df.columns if \"True class\" in col][0]\n",
    "        if df[y_true_col].nunique() == 1:\n",
    "            df = df.drop(y_true_col, axis=1)\n",
    "        rna_dict[cat] = df\n",
    "\n",
    "    df_order = [ASSAY, CELL_TYPE, SEX, LIFE_STAGE, CANCER]\n",
    "    df_list = [rna_dict[cat] for cat in df_order]\n",
    "    full_rna_merged_df = functools.reduce(merge_dataframes, df_list)\n",
    "    full_rna_merged_df.to_csv(encode_predictions_dir / \"full_rna_merged_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_diff_columns = [\"mrna_seq\", \"rna_seq\", \"wgbs-pbat\", \"wgbs-standard\"]\n",
    "\n",
    "concat_pred_dfs = {}\n",
    "for cat, chip_results in chip_pred_dfs.items():\n",
    "    rna_results = pred_dfs_rna[cat]\n",
    "\n",
    "    if cat == ASSAY:\n",
    "        chip_results.loc[:, assay_diff_columns] = \"NA\"\n",
    "\n",
    "    if not chip_results.columns.equals(rna_results.columns):\n",
    "        raise ValueError(\n",
    "            f\"Columns are not the same for {cat}. Chip: {chip_results.columns}, RNA: {rna_results.columns}\"\n",
    "        )\n",
    "\n",
    "    all_results = pd.concat([chip_results, rna_results])\n",
    "\n",
    "    assert all_results.shape == (\n",
    "        chip_results.shape[0] + rna_results.shape[0],\n",
    "        chip_results.shape[1],\n",
    "    )\n",
    "\n",
    "    concat_pred_dfs[cat] = all_results\n",
    "    assert len(all_results) == chip_results.shape[0] + rna_results.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_col_names = 8\n",
    "# Make all different columns have unique relevant names except for the pred vector\n",
    "for cat, df in concat_pred_dfs.items():\n",
    "    df.drop(columns=[\"Same?\"], inplace=True)\n",
    "    old_names = df.columns[1 : same_col_names - 1]\n",
    "    new_names = [f\"{old_name} ({cat})\" for old_name in old_names]\n",
    "    df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
    "    df.set_index(\"md5sum\", inplace=True)\n",
    "    concat_pred_dfs[cat] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order = [ASSAY, CELL_TYPE, SEX, LIFE_STAGE, CANCER]\n",
    "df_list = [concat_pred_dfs[cat] for cat in df_order]\n",
    "full_merged_df = functools.reduce(merge_dataframes, df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_plus_metadata_df: pd.DataFrame = full_merged_df.merge(\n",
    "    complete_metadata_df,\n",
    "    left_index=True,\n",
    "    right_on=\"FILE_accession\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_delete\"),\n",
    ")\n",
    "\n",
    "for col in preds_plus_metadata_df.columns:\n",
    "    if col.endswith(\"_delete\"):\n",
    "        print(col)\n",
    "        preds_plus_metadata_df.drop(columns=col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(preds_plus_metadata_df, pd.DataFrame)  # pylance being weird\n",
    "\n",
    "meta_col_order = [\n",
    "    col for col in complete_metadata_df.columns if col in preds_plus_metadata_df.columns\n",
    "]\n",
    "results_col_order = [\n",
    "    col for col in full_merged_df.columns if col in preds_plus_metadata_df.columns\n",
    "]\n",
    "\n",
    "new_order = results_col_order + meta_col_order\n",
    "preds_plus_metadata_df = preds_plus_metadata_df.loc[:, new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pairs in [\n",
    "    (ASSAY, ASSAY),\n",
    "    (SEX, SEX),\n",
    "    (LIFE_STAGE, LIFE_STAGE),\n",
    "    (CANCER, \"cancer_status\"),\n",
    "    (CELL_TYPE, \"epiclass_sample_ontology\"),\n",
    "]:\n",
    "    name1 = f\"True class ({pairs[0]})\"\n",
    "    name2 = pairs[1]\n",
    "    print(name1, name2)\n",
    "    preds_plus_metadata_df[name1] = preds_plus_metadata_df[name2]\n",
    "    preds_plus_metadata_df[pairs[0]] = preds_plus_metadata_df[pairs[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_data_dir / \"training_results\" / \"predictions\" / \"encode\"\n",
    "preds_plus_metadata_df.to_csv(\n",
    "    logdir / \"complete_encode_predictions_augmented_2025-02_metadata.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove datasets overlapping with EpiATLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_plus_metadata_df[\"in_epiatlas\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_plus_metadata_df[\"in_epiatlas\"].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "all_preds_no_epiatlas = preds_plus_metadata_df[\n",
    "    preds_plus_metadata_df[\"in_epiatlas\"].astype(str) == \"False\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_preds_no_epiatlas.shape)\n",
    "display(all_preds_no_epiatlas[ASSAY].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell type metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the predictions for the 16 cell types\n",
    "accepted_cts = [\n",
    "    \"T cell\",\n",
    "    \"neutrophil\",\n",
    "    \"brain\",\n",
    "    \"monocyte\",\n",
    "    \"lymphocyte of B lineage\",\n",
    "    \"myeloid cell\",\n",
    "    \"venous blood\",\n",
    "    \"macrophage\",\n",
    "    \"mesoderm-derived structure\",\n",
    "    \"endoderm-derived structure\",\n",
    "    \"colon\",\n",
    "    \"connective tissue cell\",\n",
    "    \"hepatocyte\",\n",
    "    \"mammary gland epithelial cell\",\n",
    "    \"muscle organ\",\n",
    "    \"extraembryonic cell\",\n",
    "]\n",
    "accepted_cts = [ct.lower() for ct in accepted_cts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    df: pd.DataFrame, cat_label: str | None = None, min_pred: float | None = None\n",
    ") -> Tuple[float, float, int]:\n",
    "    \"\"\"Compute the accuracy and f1 of the predictions.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing the predictions and true classes.\n",
    "        cat_label: Label for the category being evaluated, for\n",
    "        labels of the form \"True class (category)\".\n",
    "        min_pred: Minimum prediction score to consider.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of accuracy, f1 and number of samples.\n",
    "    \"\"\"\n",
    "\n",
    "    true_label = \"True class\"\n",
    "    pred_label = \"Predicted class\"\n",
    "    max_pred_label = \"Max pred\"\n",
    "    if cat_label:\n",
    "        true_label = f\"{true_label} ({cat_label})\"\n",
    "        pred_label = f\"{pred_label} ({cat_label})\"\n",
    "        max_pred_label = f\"{max_pred_label} ({cat_label})\"\n",
    "\n",
    "    sub_df = df.copy()\n",
    "    if min_pred:\n",
    "        try:\n",
    "            sub_df = sub_df[sub_df[max_pred_label] >= min_pred]\n",
    "        except KeyError as err:\n",
    "            raise KeyError(\n",
    "                f\"Column '{max_pred_label}' not found in DataFrame and min_pred is not None.\"\n",
    "            ) from err\n",
    "\n",
    "    y_true = sub_df[true_label]\n",
    "    y_pred = sub_df[pred_label]\n",
    "\n",
    "    acc = (y_true == y_pred).mean()\n",
    "\n",
    "    f1: float = f1_score(  # type: ignore\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=y_pred.unique(),\n",
    "        average=\"macro\",\n",
    "    )\n",
    "    return acc, f1, sub_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_df = preds_plus_metadata_df.copy(deep=True)\n",
    "cell_type_df = cell_type_df[cell_type_df[CELL_TYPE].isin(accepted_cts)]\n",
    "print(cell_type_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_df[CELL_TYPE].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_assays = ASSAY_ORDER + [\"mrna_seq\"]\n",
    "cell_type_core_df = cell_type_df[cell_type_df[ASSAY].isin(core_assays)]\n",
    "cell_type_noncore_df = cell_type_df[~cell_type_df[ASSAY].isin(core_assays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [cell_type_core_df, cell_type_noncore_df]:\n",
    "    print(df.shape)\n",
    "    N = df.shape[0]\n",
    "    display(df[CELL_TYPE].value_counts(dropna=False))\n",
    "    display(df[\"assay\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, name in zip([cell_type_core_df, cell_type_noncore_df], [\"core\", \"noncore\"]):\n",
    "    print(name)\n",
    "    full_N = df.shape[0]\n",
    "    for min_pred in [0, 0.6, 0.8, 0.9]:\n",
    "        acc, f1, N = compute_metrics(df, CELL_TYPE, min_pred)\n",
    "        print(\n",
    "            f\"Min pred: {min_pred}, N: {N} ({N/full_N:.2%}), Acc: {acc:.3f}, F1: {f1:.3f}\"\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other cell type trainings metrics\n",
    "\n",
    "Includes cell type classifiers trained with single assays. (e.g. only h3k4me1 files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_folder = (\n",
    "    base_data_dir\n",
    "    / f\"training_results/dfreeze_v2/hg38_100kb_all_none/{CELL_TYPE}_1l_3000n/complete-no_valid-oversampling\"\n",
    ")\n",
    "if not pred_folder.exists():\n",
    "    raise FileNotFoundError(f\"Directory {pred_folder} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_ct_dfs = {}\n",
    "for folder in pred_folder.glob(\"*\"):\n",
    "    if not folder.is_dir():\n",
    "        print(f\"Skipping {folder}\")\n",
    "        continue\n",
    "    pred_file = list(folder.glob(\"predictions/*.csv\"))\n",
    "\n",
    "    if len(pred_file) > 1:\n",
    "        print(f\"More than one prediction file found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    if len(pred_file) == 0:\n",
    "        print(f\"No prediction file found in {folder}\")\n",
    "        continue\n",
    "\n",
    "    pred_file = pred_file[0]\n",
    "\n",
    "    pred_df = pd.read_csv(pred_file)\n",
    "    name = folder.name.replace(\"complete_no_valid_oversample_\", \"\")\n",
    "\n",
    "    for col in [\"True class\", \"Predicted class\"]:\n",
    "        pred_df[col] = pred_df[col].str.lower()\n",
    "\n",
    "    other_ct_dfs[name] = pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_ct_dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cell_type_acc(\n",
    "    metadata_df: pd.DataFrame,\n",
    "    pred_dfs_dict: Dict[str, pd.DataFrame],\n",
    "    min_pred: float = 0.6,\n",
    ") -> None:\n",
    "    \"\"\"Compute the accuracy of the predictions for the 16 cell types.\n",
    "    Inner meger of the metadata and predictions is performed.\n",
    "    \"\"\"\n",
    "    meta_df = metadata_df[metadata_df[CELL_TYPE].isin(accepted_cts)].copy()\n",
    "\n",
    "    # print(\"Assay counts for 16 cell types\")\n",
    "    # values_count = meta_df[\"Assay\"].value_counts(dropna=False)\n",
    "    # display(values_count)\n",
    "    # display_perc(values_count / values_count.sum() * 100)\n",
    "\n",
    "    # print(\"Cell types distribution\")\n",
    "    # values_count = meta_df[CELL_TYPE].value_counts(dropna=False)\n",
    "    # display(values_count)\n",
    "    # display_perc(values_count / values_count.sum() * 100)\n",
    "\n",
    "    for name, pred_df in sorted(pred_dfs_dict.items()):\n",
    "        print(name)\n",
    "        pred_w_ct = pred_df.merge(\n",
    "            meta_df, left_on=\"md5sum\", right_on=\"FILE_accession\", how=\"inner\"\n",
    "        )\n",
    "        N = pred_w_ct.shape[0]\n",
    "\n",
    "        # Calculate results for all predictions\n",
    "        true, pred = pred_w_ct[CELL_TYPE], pred_w_ct[\"Predicted class\"]\n",
    "\n",
    "        total_correct = (true == pred).sum()\n",
    "        acc = total_correct / N\n",
    "        f1 = f1_score(true, pred, labels=pred.unique(), average=\"macro\")\n",
    "\n",
    "        print(f\"Acc (pred>0.0): {total_correct}/{N} ({acc:.2%})\")\n",
    "        print(f\"F1 (pred>0.0): {f1:.2f}\")\n",
    "\n",
    "        # Calculate results for predictions with max_pred\n",
    "        pred_w_ct_filtered = pred_w_ct[pred_w_ct[\"Max pred\"] > min_pred]\n",
    "        true, pred = pred_w_ct_filtered[CELL_TYPE], pred_w_ct_filtered[\"Predicted class\"]\n",
    "\n",
    "        total_correct_filtered = (true == pred).sum()\n",
    "        perc_filtered = total_correct_filtered / pred_w_ct_filtered.shape[0]\n",
    "\n",
    "        f1 = f1_score(true, pred, labels=pred.unique(), average=\"macro\")\n",
    "\n",
    "        print(\n",
    "            f\"Acc (pred>{min_pred:.1f}): {total_correct_filtered}/{pred_w_ct_filtered.shape[0]} ({perc_filtered:.2%})\"\n",
    "        )\n",
    "        diff = N - pred_w_ct_filtered.shape[0]\n",
    "        print(f\"F1 (pred>{min_pred}): {f1:.2f}\")\n",
    "        print(f\"Samples ignored at {min_pred:.1f}: {diff} ({diff/N:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_core_assays = complete_metadata_df[ASSAY].isin(core_assays)\n",
    "non_core_metadata_df = complete_metadata_df[~mask_core_assays]\n",
    "core_metadata_df = complete_metadata_df[mask_core_assays]\n",
    "\n",
    "compute_cell_type_acc(non_core_metadata_df, other_ct_dfs)\n",
    "print(\"\\n\")\n",
    "compute_cell_type_acc(core_metadata_df, other_ct_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_logdir = (\n",
    "    base_fig_dir / \"encode_predictions\" / \"confusion_matrices\" / CELL_TYPE / \"core\"\n",
    ")\n",
    "if not conf_matrix_logdir.exists():\n",
    "    conf_matrix_logdir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = core_metadata_df[core_metadata_df[CELL_TYPE].isin(accepted_cts)].copy()\n",
    "\n",
    "limited_pred_dfs_dict = {k: v for k, v in other_ct_dfs.items() if \"-ct16\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for no_rna in [True, False]:\n",
    "    for task_name, df in limited_pred_dfs_dict.items():\n",
    "        pred_w_ct = df.merge(\n",
    "            meta_df, left_on=\"md5sum\", right_on=\"FILE_accession\", how=\"inner\"\n",
    "        )\n",
    "\n",
    "        if no_rna:\n",
    "            pred_w_ct = pred_w_ct[~pred_w_ct[ASSAY].str.contains(\"rna\")]\n",
    "\n",
    "        for threshold in [0, 0.6, 0.8]:\n",
    "            sub_df = pred_w_ct[pred_w_ct[\"Max pred\"] >= threshold]\n",
    "\n",
    "            true, pred = sub_df[CELL_TYPE], sub_df[\"Predicted class\"]\n",
    "            cm = confusion_matrix(true, pred, labels=accepted_cts)\n",
    "\n",
    "            filename = f\"{task_name}-core-confusion_matrix-{threshold*100}\"\n",
    "            if no_rna:\n",
    "                final_filename = f\"{filename}-no_rna\"\n",
    "                this_logdir = conf_matrix_logdir / \"no_rna\"\n",
    "                this_logdir.mkdir(parents=True, exist_ok=True)\n",
    "            else:\n",
    "                final_filename = filename\n",
    "                this_logdir = conf_matrix_logdir / \"with_rna\"\n",
    "                this_logdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            writer = ConfusionMatrixWriter(labels=accepted_cts, confusion_matrix=cm)\n",
    "            writer.to_all_formats(\n",
    "                logdir=this_logdir,\n",
    "                name=final_filename,\n",
    "            )\n",
    "            plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSAY metrics\n",
    "\n",
    "\n",
    "No RNA-seq here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download note\n",
    "~~~bash\n",
    "paper_dir=\"/home/local/USHERBROOKE/rabj2301/Projects/epiclass/output/paper/data/training_results/dfreeze_v2/hg38_100kb_all_none/assay_epiclass_1l_3000n\"\n",
    "cd $paper_dir\n",
    "base_path=\"/lustre06/project/6007515/rabyj/epiclass-project/output/epiclass-logs/epiatlas-dfreeze-v2.1/hg38_100kb_all_none/assay_epiclass_1l_3000n\"\n",
    "rsync -avR --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"*confusion*\" --exclude \"*.md5\" narval:${base_path}/./*c/complete_no_valid_oversample .\n",
    "\n",
    "paper_dir=\"/home/local/USHERBROOKE/rabj2301/Projects/epiclass/output/paper/data/training_results/dfreeze_v2\"\n",
    "cd $paper_dir\n",
    "base_path=\"/lustre06/project/6007515/rabyj/epiclass-project/output/epiclass-logs/epiatlas-dfreeze-v2.1\"\n",
    "rsync -avR --exclude \"*/EpiLaP/\" --exclude \"*.png\" --exclude \"*confusion*\" --exclude \"*.md5\" narval:${base_path}/./hg38_100kb_all_none_w_encode_noncore/assay_epiclass_1l_3000n/complete_no_valid_oversample-0 .\n",
    "\n",
    "find -type f -name \"*.list*.csv\" -print0 | xargs -0 rename 's/\\.list//g'\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\n",
    "assay7_folder = (\n",
    "    data_dir / f\"hg38_100kb_all_none/{ASSAY}_1l_3000n/7c/complete_no_valid_oversample\"\n",
    ")\n",
    "assay11_folder = (\n",
    "    data_dir / f\"hg38_100kb_all_none/{ASSAY}_1l_3000n/11c/complete_no_valid_oversample\"\n",
    ")\n",
    "assay13_folder = (\n",
    "    data_dir\n",
    "    / f\"hg38_100kb_all_none_w_encode_noncore/{ASSAY}_1l_3000n/13c/complete_no_valid_oversample\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs_dict = {}\n",
    "for name, folder in zip(\n",
    "    [\"7c\", \"11c\", \"13c\"], [assay7_folder, assay11_folder, assay13_folder]\n",
    "):\n",
    "    if not folder.exists():\n",
    "        print(f\"Folder {folder} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    pred_folder = folder / \"predictions\" / \"encode\"\n",
    "    if not pred_folder.exists():\n",
    "        print(f\"Folder {pred_folder} does not exist.\")\n",
    "        continue\n",
    "\n",
    "    pred_file = list(pred_folder.glob(\"*.csv\"))\n",
    "    if len(pred_file) != 1:\n",
    "        print(f\"Found {len(pred_file)} files in {pred_folder}.\")\n",
    "        continue\n",
    "    pred_file = pred_file[0]\n",
    "\n",
    "    pred_df = pd.read_csv(pred_file, sep=\",\")\n",
    "    try:\n",
    "        pred_df.drop(columns=[\"Same?\"], inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    # Add assay metadata\n",
    "    pred_df = pred_df.merge(\n",
    "        complete_metadata_df, left_on=\"md5sum\", right_on=\"FILE_accession\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    pred_df[\"True class\"] = pred_df[\"assay_epiclass\"]\n",
    "    pred_dfs_dict[name] = pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core7 preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = data_dir = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"predictions\"\n",
    "    / \"encode\"\n",
    "    / \"assay_epiclass_1l_3000n\"\n",
    ")\n",
    "for name, df in pred_dfs_dict.items():\n",
    "    print(name)\n",
    "    print(df.shape)\n",
    "\n",
    "    # Only consider files already labeled with core7 assays\n",
    "    df = df[df[ASSAY].isin(CORE_ASSAYS)]\n",
    "\n",
    "    # Only consider non-EpiAtlas samples\n",
    "    df = df[df[\"in_epiatlas\"].astype(str) == \"False\"]\n",
    "\n",
    "    # Calculate results for all predictions\n",
    "    correct_pred = df[\"Predicted class\"] == df[\"True class\"]\n",
    "    total_correct = correct_pred.sum()\n",
    "    total = df.shape[0]\n",
    "    perc = total_correct / total\n",
    "    print(f\"Acc (pred>=0.0) {total_correct}/{total} ({perc:.2%})\")\n",
    "\n",
    "    for assay in CORE_ASSAYS:\n",
    "        min_pred = 0.6\n",
    "        df_assay = df[df[ASSAY] == assay]\n",
    "        df_assay = df_assay[df_assay[\"Max pred\"] >= min_pred]\n",
    "        correct_pred = df_assay[\"Predicted class\"] == df_assay[\"True class\"]\n",
    "        total_correct = correct_pred.sum()\n",
    "        total = df_assay.shape[0]\n",
    "        perc = total_correct / total\n",
    "        print(\n",
    "            f\"Acc (pred>={min_pred:.1f}) {assay} = {total_correct}/{total} ({perc:.2%})\"\n",
    "        )\n",
    "\n",
    "    # Calculate results for predictions with max_pred > 0.6\n",
    "    df_filtered = df[df[\"Max pred\"] >= 0.6]\n",
    "    correct_pred_filtered = df_filtered[\"Predicted class\"] == df_filtered[\"True class\"]\n",
    "    total_correct_filtered = correct_pred_filtered.sum()\n",
    "    total_filtered = df_filtered.shape[0]\n",
    "    perc_filtered = total_correct_filtered / total_filtered\n",
    "    print(\n",
    "        f\"Acc (pred>=0.6): {total_correct_filtered}/{total_filtered} ({perc_filtered:.2%})\"\n",
    "    )\n",
    "\n",
    "    df_filtered_wrong = df_filtered[~correct_pred_filtered]\n",
    "    # groupby = (\n",
    "    #     df_filtered_wrong.groupby([\"True class\", \"Predicted class\"])\n",
    "    #     .size()\n",
    "    #     .sort_values(ascending=False)\n",
    "    # )\n",
    "    # display(\"Mislabels:\", groupby)\n",
    "\n",
    "    df_filtered_wrong.to_csv(\n",
    "        output_dir / f\"encode_only_mislabels_minPred0.6_{name}.csv\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non-core 7c preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_core_preds = all_preds_no_epiatlas[\n",
    "    all_preds_no_epiatlas[ASSAY].isin([\"ctcf\", \"non-core\"])\n",
    "]\n",
    "print(f\"Non-core datasets: {non_core_preds.shape[0]}\")\n",
    "\n",
    "groupby = non_core_preds.groupby([\"assay\"]).size()\n",
    "groupby = groupby[groupby > 3]\n",
    "selected_assays = groupby.index\n",
    "print(f\"Non-core targets/assays with > 3 files: {non_core_preds.shape[0]}\")\n",
    "\n",
    "non_core_preds = non_core_preds[non_core_preds[\"assay\"].isin(selected_assays)]\n",
    "print(f\"Non-core files respecting selected assays: {non_core_preds.shape[0]}\")\n",
    "\n",
    "N_high_conf = (non_core_preds[\"Max pred (assay_epiclass)\"] >= 0.6).sum()\n",
    "N_total = non_core_preds.shape[0]\n",
    "print(\n",
    "    f\"High confidence non-core predictions: {N_high_conf / N_total:.2%} ({N_high_conf}/{N_total})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7c preds on non-core assays\n",
    "name = \"7c\"\n",
    "df = pred_dfs_dict[name].copy(deep=True)\n",
    "df = df[~df[\"True class\"].isin(ASSAY_ORDER)]\n",
    "print(df.shape)\n",
    "print(df[\"assay\"].isna().sum())\n",
    "display(df[\"assay\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = data_dir = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"predictions\"\n",
    "    / \"encode\"\n",
    "    / \"assay_epiclass_1l_3000n\"\n",
    ")\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir(parents=True)\n",
    "\n",
    "# 7c preds on non-core assays\n",
    "name = \"7c\"\n",
    "df = pred_dfs_dict[name].copy(deep=True)\n",
    "df = df[~df[\"True class\"].isin(ASSAY_ORDER)]\n",
    "df = df[~df[\"assay\"].isna()]\n",
    "\n",
    "non_core_df = all_preds_no_epiatlas[\n",
    "    all_preds_no_epiatlas[ASSAY].isin([\"ctcf\", \"non-core\"])\n",
    "]\n",
    "for min_pred in [0, 0.6, 0.8]:\n",
    "    df_filtered = df[df[\"Max pred\"] >= min_pred]\n",
    "    groupby = (\n",
    "        df_filtered.groupby([\"Predicted class\", \"assay\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"Count\")\n",
    "        .sort_values(by=[\"Predicted class\", \"Count\"], ascending=[True, False])\n",
    "        .set_index([\"Predicted class\", \"assay\"])[\"Count\"]\n",
    "    )\n",
    "    groupby.to_csv(\n",
    "        output_dir / f\"encode_non-core_{name}_predictions_minPred{min_pred}.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_dir = base_data_dir / \"metadata/encode\"\n",
    "non_core_categories_path = (\n",
    "    encode_metadata_dir / \"non-core_encode_assay_category_2024-08-29.csv\"\n",
    ")\n",
    "if not non_core_categories_path.exists():\n",
    "    raise FileNotFoundError(f\"File {non_core_categories_path} does not exist.\")\n",
    "\n",
    "non_core_categories_df = pd.read_csv(\n",
    "    non_core_categories_path, sep=\",\", names=[\"assay\", \"assay_category\", \"note\"]\n",
    ")\n",
    "print(non_core_categories_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_cats = df.merge(\n",
    "    non_core_categories_df[[\"assay\", \"assay_category\"]],\n",
    "    left_on=\"assay\",\n",
    "    right_on=\"assay\",\n",
    "    how=\"left\",\n",
    ")\n",
    "print(df_w_cats.shape)\n",
    "\n",
    "df_w_cats.fillna(\"not_looked\", inplace=True)\n",
    "display(df_w_cats[\"assay_category\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_non_core_preds_df(df: pd.DataFrame, min_pred: float = 0.6):\n",
    "    \"\"\"Create a DataFrame of non-core assay predictions.\"\"\"\n",
    "    results = {}\n",
    "    assay_categories = dict(zip(df[\"assay\"], df[\"assay_category\"]))\n",
    "\n",
    "    for assay, group in df.groupby(\"assay\"):\n",
    "        # N = group.shape[0]\n",
    "        # if N < 3:\n",
    "        #     continue\n",
    "\n",
    "        group = group[group[\"Max pred\"] >= min_pred]\n",
    "        # N_post_filter = group.shape[0]\n",
    "        # if N_post_filter == 0 or N_post_filter < min_n:\n",
    "        #     continue\n",
    "\n",
    "        groupby = (\n",
    "            group.groupby([\"Predicted class\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"Count\")  # type: ignore\n",
    "            .sort_values(by=[\"Count\"], ascending=False)\n",
    "        )\n",
    "\n",
    "        results[assay] = dict(zip(groupby[\"Predicted class\"], groupby[\"Count\"]))\n",
    "\n",
    "    result_df = pd.DataFrame(results).fillna(0)\n",
    "    result_df = result_df.astype(int)\n",
    "    result_df = result_df.T  # assay as row/index\n",
    "    result_df[\"assay_category\"] = result_df.index.map(assay_categories)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_pred in [0, 0.6, 0.8]:\n",
    "    predicted_classes_df = create_non_core_preds_df(df_w_cats, min_pred=min_pred)\n",
    "    predicted_classes_df.to_csv(\n",
    "        output_dir\n",
    "        / f\"encode_non-core_{name}_predictions_per_assay_minPred{min_pred:.2f}.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_structured_dataframe(df_w_cats):\n",
    "    \"\"\"Create a structured dataframe with the percentage of predictions for each assay category.\"\"\"\n",
    "    # Create an empty list to store our data\n",
    "    data = []\n",
    "\n",
    "    # Iterate through the grouped data\n",
    "    for predicted_class, group in df_w_cats.groupby(\"Predicted class\"):\n",
    "        for min_pred in list(np.arange(0, 1, 0.05)) + [0.99]:\n",
    "            df_filtered = group[group[\"Max pred\"] >= min_pred]\n",
    "            counts = df_filtered[\"assay_category\"].value_counts(dropna=False)\n",
    "            total = counts.sum()\n",
    "\n",
    "            # Calculate percentages\n",
    "            percentages = (counts / total * 100).round(2)\n",
    "\n",
    "            # Add data for each assay category\n",
    "            for assay_category, percentage in percentages.items():\n",
    "                data.append(\n",
    "                    {\n",
    "                        \"Predicted class\": predicted_class,\n",
    "                        \"Min pred\": min_pred,\n",
    "                        \"assay_category\": assay_category,\n",
    "                        \"Percentage\": percentage,\n",
    "                        \"Count\": counts[assay_category],\n",
    "                        \"Total samples\": total,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Create the dataframe\n",
    "    df_structured = pd.DataFrame(data)\n",
    "\n",
    "    # Set the multi-index\n",
    "    df_structured = df_structured.set_index(\n",
    "        [\"Predicted class\", \"Min pred\", \"assay_category\"]\n",
    "    )\n",
    "\n",
    "    return df_structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_category_df = create_structured_dataframe(df_w_cats)\n",
    "assay_category_df.to_csv(output_dir / \"encode_non-core_7c_predictions_assay_category.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_fig_dir = base_fig_dir / \"encode_predictions\" / \"assay_epiclass\" / \"non-core\"\n",
    "if not section_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {section_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X = assay_epiclass, stack = assay_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = section_fig_dir / \"stacked_bar_X_assay_epiclass\"\n",
    "fig_dir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "bar_df = assay_category_df.reset_index()\n",
    "\n",
    "predicted_class_order = [\n",
    "    \"h3k27ac\",\n",
    "    \"h3k4me3\",\n",
    "    \"h3k4me1\",\n",
    "    \"h3k9me3\",\n",
    "    \"h3k27me3\",\n",
    "    \"h3k36me3\",\n",
    "    \"input\",\n",
    "]\n",
    "assay_category_color_map = {\n",
    "    cat: px.colors.qualitative.Safe[i]\n",
    "    for i, cat in enumerate(sorted(bar_df[\"assay_category\"].unique()))\n",
    "}\n",
    "\n",
    "for min_pred in [0, 0.6, 0.8, 0.9]:\n",
    "    sub_df = bar_df[\n",
    "        (bar_df[\"Min pred\"] > min_pred - 0.01) & (bar_df[\"Min pred\"] < min_pred + 0.01)\n",
    "    ]\n",
    "    fig = px.bar(\n",
    "        sub_df,\n",
    "        x=\"Predicted class\",\n",
    "        y=\"Percentage\",\n",
    "        color=\"assay_category\",\n",
    "        title=f\"Assay Category Composition for Each Predicted Class at predScore >= {min_pred:.2f}\",\n",
    "        labels={\"Percentage\": \"Percentage (%)\", \"Predicted class\": \"Predicted Class\"},\n",
    "        barmode=\"stack\",\n",
    "        category_orders={\"Predicted class\": predicted_class_order},\n",
    "        color_discrete_map=assay_category_color_map,\n",
    "    )\n",
    "\n",
    "    figname = f\"histogram_encode_non-core_assay_epiclass_minPred{min_pred:.2f}\"\n",
    "    # fig.write_html(fig_dir / f\"{figname}.html\")\n",
    "    # fig.write_image(fig_dir / f\"{figname}.png\")\n",
    "    # fig.write_image(fig_dir / f\"{figname}.svg\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X = assay_category, stack = assay_epiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_w_cats[df_w_cats[\"assay_category\"] != \"not_looked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_epiclass_order = [\n",
    "    \"h3k27ac\",\n",
    "    \"h3k4me3\",\n",
    "    \"h3k4me1\",\n",
    "    \"h3k9me3\",\n",
    "    \"h3k27me3\",\n",
    "    \"h3k36me3\",\n",
    "    \"input\",\n",
    "]\n",
    "assay_epiclass_order = {assay: i for i, assay in enumerate(assay_epiclass_order)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = section_fig_dir / \"stacked_bar_X_assay_category\"\n",
    "fig_dir.mkdir(parents=False, exist_ok=True)\n",
    "\n",
    "assay_categories_order = [\n",
    "    \"trx_reg\",\n",
    "    \"heterochrom\",\n",
    "    \"polycomb\",\n",
    "    \"splicing\",\n",
    "    \"insulator\",\n",
    "    \"other/mixed\",\n",
    "]\n",
    "\n",
    "for min_pred in [0, 0.6, 0.8]:\n",
    "    sub_df = df[df[\"Max pred\"] >= min_pred]\n",
    "    groupby = (\n",
    "        sub_df.groupby([\"assay_category\", \"Predicted class\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"Count\")\n",
    "        .sort_values(by=[\"assay_category\", \"Count\"], ascending=[True, False])\n",
    "    )\n",
    "    groupby[\"Percentage\"] = groupby.groupby(\"assay_category\")[\"Count\"].transform(\n",
    "        lambda x: (x / x.sum()) * 100\n",
    "    )\n",
    "\n",
    "    # Add order for plotting\n",
    "    groupby[\"assay_order\"] = groupby[\"Predicted class\"].map(assay_epiclass_order)\n",
    "    groupby = groupby.sort_values(\n",
    "        by=[\"assay_category\", \"assay_order\"], ascending=[False, True]\n",
    "    )\n",
    "\n",
    "    # Main plot\n",
    "    fig = px.bar(\n",
    "        groupby,\n",
    "        x=\"assay_category\",\n",
    "        y=\"Percentage\",\n",
    "        color=\"Predicted class\",\n",
    "        barmode=\"stack\",\n",
    "        category_orders={\"assay_category\": assay_categories_order},\n",
    "        color_discrete_map=assay_colors,\n",
    "        title=f\"core7 predictions for non-core assays, predScore >= {min_pred:.2f}\",\n",
    "        labels={\"Percentage\": \"Fraction (%)\", \"assay_category\": \"Assay Category\"},\n",
    "    )\n",
    "\n",
    "    # Modify x-axis labels\n",
    "    total_counts = groupby.groupby(\"assay_category\")[\"Count\"].sum()\n",
    "\n",
    "    ticktext = [\n",
    "        f\"{assay_category} (N={total_counts[assay_category]})\"\n",
    "        for assay_category in assay_categories_order\n",
    "    ]\n",
    "    fig.update_xaxes(tickvals=assay_categories_order, ticktext=ticktext)\n",
    "\n",
    "    # Save and display\n",
    "    figname = f\"histogram_encode_non-core_assay_epiclass_minPred{min_pred:.2f}\"\n",
    "    # fig.write_html(fig_dir / f\"{figname}.html\")\n",
    "    # fig.write_image(fig_dir / f\"{figname}.png\")\n",
    "    # fig.write_image(fig_dir / f\"{figname}.svg\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assay category evolution with min_predScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_assay_category_graphs(df, output_dir: Path | None = None):\n",
    "    \"\"\"Graph assay category distribution for each predicted class.\"\"\"\n",
    "    # Get unique predicted classes\n",
    "    predicted_classes = df.index.get_level_values(\"Predicted class\").unique()\n",
    "    assay_categories = df.index.get_level_values(\"assay_category\").unique()\n",
    "\n",
    "    graph_colors = {\n",
    "        cat: px.colors.qualitative.Safe[i]\n",
    "        for i, cat in enumerate(sorted(assay_categories))\n",
    "    }\n",
    "\n",
    "    # Create a figure for each predicted class\n",
    "    for predicted_class in predicted_classes:\n",
    "        df_class = df.loc[predicted_class]\n",
    "\n",
    "        # Get unique assay categories for this predicted class\n",
    "        assay_categories = df_class.index.get_level_values(\"assay_category\").unique()\n",
    "\n",
    "        total_samples_at_zero = df_class.xs(0, level=\"Min pred\")[\"Total samples\"].iloc[0]\n",
    "\n",
    "        # Create the figure\n",
    "        fig = go.Figure()\n",
    "\n",
    "        for assay_category in assay_categories:\n",
    "            df_assay = df_class.xs(assay_category, level=\"assay_category\")\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_assay.index,\n",
    "                    y=df_assay[\"Percentage\"],\n",
    "                    mode=\"lines+markers\",\n",
    "                    name=assay_category,\n",
    "                    marker=dict(color=graph_colors[assay_category]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        conserved_percentages = (\n",
    "            df_class.groupby(\"Min pred\")[\"Total samples\"].first()\n",
    "            / total_samples_at_zero\n",
    "            * 100\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=conserved_percentages.index,\n",
    "                y=conserved_percentages.values,\n",
    "                mode=\"lines+markers\",\n",
    "                name=\"Samples Conserved\",\n",
    "                line=dict(dash=\"dash\", color=\"black\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f\"Composition for Predicted Class: {predicted_class}\",\n",
    "            xaxis_title=\"Min pred\",\n",
    "            yaxis_title=\"Percentage Composition\",\n",
    "            legend_title=\"Assay Category\",\n",
    "            hovermode=\"x unified\",\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(range=[-0.01, 1.01])\n",
    "        fig.update_yaxes(range=[0, 100])\n",
    "\n",
    "        # Save\n",
    "        if output_dir:\n",
    "            filename = f\"encode_non-core_7c_predictions_assay_category_{predicted_class}\"\n",
    "            fig.write_image(output_dir / f\"{filename}.png\")\n",
    "            fig.write_image(output_dir / f\"{filename}.svg\")\n",
    "            fig.write_html(output_dir / f\"{filename}.html\")\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_structured is your dataframe from the previous step\n",
    "fig_dir = (\n",
    "    base_fig_dir\n",
    "    / \"encode_predictions\"\n",
    "    / \"assay_epiclass\"\n",
    "    / \"non-core\"\n",
    "    / \"line_graphs_over_min_pred\"\n",
    ")\n",
    "fig_dir.mkdir(parents=False, exist_ok=True)\n",
    "create_assay_category_graphs(df=assay_category_df, output_dir=fig_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTHER - Sex, life stage, cancer\n",
    "\n",
    "Throwing all the predictions together to get acc/F1 for each of 5 classifiers, on core/non-core data respectively. (for assay it gets more messy, cannot do non-core directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new life stage classification\n",
    "merge_life_stage = {\n",
    "    \"adult\": \"adult\",\n",
    "    \"embryo\": \"prenatal\",\n",
    "    \"fetal\": \"prenatal\",\n",
    "    \"newborn\": \"prenatal\",\n",
    "    \"child\": \"child\",\n",
    "}\n",
    "for label in [\n",
    "    LIFE_STAGE,\n",
    "    f\"True class ({LIFE_STAGE})\",\n",
    "    f\"Predicted class ({LIFE_STAGE})\",\n",
    "]:\n",
    "    new_label = label.replace(LIFE_STAGE, f\"{LIFE_STAGE}_merged\")\n",
    "    all_preds_no_epiatlas[new_label] = all_preds_no_epiatlas[label].map(merge_life_stage)\n",
    "\n",
    "all_preds_no_epiatlas[f\"Max pred ({LIFE_STAGE}_merged)\"] = all_preds_no_epiatlas[\n",
    "    f\"Max pred ({LIFE_STAGE})\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies per assay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat data for easy plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_acc_per_assay(all_preds_no_epiatlas: pd.DataFrame) -> Dict[str, Dict]:\n",
    "    \"\"\"Compute accuracy for each assay.\n",
    "    Checks core9 assays (core7, *rna_seq, wgbs-*) + CTCF + non-core\n",
    "\n",
    "    Args:\n",
    "    - all_preds_no_epiatlas: The dataframe containing the predictions.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with the accuracy for each assay.\n",
    "        Format: {task_name:{assay: [(min_pred, acc, f1, nb_samples), ...], ...}, ...}\n",
    "    \"\"\"\n",
    "    if not (all_preds_no_epiatlas[\"in_epiatlas\"].astype(str) == \"False\").all():\n",
    "        raise ValueError(\"all_preds_no_epiatlas should not contain EpiAtlas samples.\")\n",
    "\n",
    "    df = copy.deepcopy(all_preds_no_epiatlas)\n",
    "    core_assays = ASSAY_ORDER\n",
    "    all_assays = ASSAY_ORDER + [\"ctcf\", \"non-core\"]\n",
    "\n",
    "    # merging rna_seq and mrna_seq\n",
    "    for col in [ASSAY, f\"True class ({ASSAY})\", f\"Predicted class ({ASSAY})\"]:\n",
    "        try:\n",
    "            df[col] = df[col].str.replace(\"mrna_seq\", \"rna_seq\")\n",
    "        except KeyError as err:\n",
    "            raise ValueError(f\"Column '{col}' not found.\") from err\n",
    "\n",
    "    all_acc_per_assay = {}\n",
    "    for name in [ASSAY, CELL_TYPE, SEX, LIFE_STAGE, f\"{LIFE_STAGE}_merged\", CANCER]:\n",
    "        task_df = copy.deepcopy(df)\n",
    "        y_true_col = f\"True class ({name})\"\n",
    "        y_pred_col = f\"Predicted class ({name})\"\n",
    "        max_pred_label = f\"Max pred ({name})\"\n",
    "\n",
    "        if max_pred_label not in df.columns:\n",
    "            raise ValueError(f\"Column '{max_pred_label}' not found.\")\n",
    "\n",
    "        # remove unknown samples\n",
    "        for label in [y_true_col, y_pred_col]:\n",
    "            task_df[label].fillna(\"unknown\", inplace=True)\n",
    "            task_df = task_df[task_df[label] != \"unknown\"]\n",
    "\n",
    "        if name == CELL_TYPE:\n",
    "            task_df = task_df[task_df[CELL_TYPE].isin(accepted_cts)]\n",
    "\n",
    "        acc_per_assay: Dict[str, List[Tuple[str, float, float, int]]] = {}\n",
    "        for label in all_assays:\n",
    "            if label in [\"non-core\", \"ctcf\"] and name == ASSAY:\n",
    "                continue\n",
    "            acc_per_assay[label] = []\n",
    "            if label not in task_df[ASSAY].unique():\n",
    "                continue\n",
    "            assay_df = task_df[task_df[ASSAY] == label]\n",
    "            for min_pred in [\"0.0\", \"0.6\", \"0.8\", \"0.9\"]:\n",
    "                acc, f1, N = compute_metrics(\n",
    "                    assay_df, cat_label=name, min_pred=float(min_pred)\n",
    "                )\n",
    "                acc_per_assay[label].append((min_pred, acc, f1, N))\n",
    "\n",
    "        # Avg accuracy\n",
    "        for set_label in [\"avg-all\", \"avg-core\", \"avg-non-core\"]:\n",
    "            acc_per_assay[set_label] = []\n",
    "\n",
    "        for min_pred in [\"0.0\", \"0.6\", \"0.8\", \"0.9\"]:\n",
    "            core_df = task_df[task_df[ASSAY].isin(core_assays)]\n",
    "            acc, f1, N = compute_metrics(\n",
    "                core_df, cat_label=name, min_pred=float(min_pred)\n",
    "            )\n",
    "            acc_per_assay[\"avg-core\"].append((min_pred, acc, f1, N))\n",
    "\n",
    "            if name == ASSAY:\n",
    "                continue\n",
    "\n",
    "            non_core_df = task_df[~task_df[ASSAY].isin(core_assays)]\n",
    "            acc, f1, N = compute_metrics(\n",
    "                non_core_df, cat_label=name, min_pred=float(min_pred)\n",
    "            )\n",
    "            acc_per_assay[\"avg-non-core\"].append((min_pred, acc, f1, N))\n",
    "\n",
    "            compute_metrics(task_df, cat_label=name, min_pred=float(min_pred))\n",
    "            acc_per_assay[\"avg-all\"].append((min_pred, acc, f1, N))\n",
    "\n",
    "        all_acc_per_assay[name] = acc_per_assay\n",
    "\n",
    "    return all_acc_per_assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_save_acc_per_assay(\n",
    "    preds_no_epiatlas: pd.DataFrame, filename: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Take a dataframe containing predictions for multiple tasks, and compute accuracy for each assay.\n",
    "    Saves the results to a tsv file, to two hardcoded folders.\n",
    "\n",
    "    Returns:\n",
    "    - A dataframe with accuracy, f1 , N for each assay\n",
    "    \"\"\"\n",
    "    all_acc_per_assay = compute_all_acc_per_assay(preds_no_epiatlas)\n",
    "\n",
    "    # acc per assay to table\n",
    "    rows = []\n",
    "    for name, acc_per_assay in all_acc_per_assay.items():\n",
    "        for assay, values in acc_per_assay.items():\n",
    "            for min_pred, acc, f1, nb_samples in values:\n",
    "                rows.append([name, assay, min_pred, acc, f1, nb_samples])\n",
    "    df_acc_per_assay = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"task_name\", ASSAY, \"min_predScore\", \"acc\", \"f1-score\", \"nb_samples\"],\n",
    "    )\n",
    "\n",
    "    df_acc_per_assay = df_acc_per_assay.astype(\n",
    "        {\n",
    "            \"task_name\": \"str\",\n",
    "            \"assay_epiclass\": \"str\",\n",
    "            \"min_predScore\": \"float\",\n",
    "            \"acc\": \"float\",\n",
    "            \"f1-score\": \"float\",\n",
    "            \"nb_samples\": \"int\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # f1-score on ASSAY task, per assay, doesn't make sense\n",
    "    df_acc_per_assay.loc[df_acc_per_assay[\"task_name\"] == ASSAY, \"f1-score\"] = \"NA\"\n",
    "    print(f\"Saving {df_acc_per_assay.shape[0]} rows\")\n",
    "    path1 = base_fig_dir / \"encode_predictions\" / filename\n",
    "    df_acc_per_assay.to_csv(\n",
    "        path1,\n",
    "        sep=\"\\t\",\n",
    "        index=False,\n",
    "    )\n",
    "    print(f\"Saved to {path1}\")\n",
    "    path2 = base_data_dir / \"training_results\" / \"predictions\" / \"encode\" / filename\n",
    "    df_acc_per_assay.to_csv(\n",
    "        path2,\n",
    "        sep=\"\\t\",\n",
    "        index=False,\n",
    "    )\n",
    "    print(f\"Saved to {path2}\")\n",
    "\n",
    "    return df_acc_per_assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"5tasks_acc_per_assay_NO_EpiAtlas.tsv\"\n",
    "df_acc_per_assay = compute_and_save_acc_per_assay(all_preds_no_epiatlas, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the 16 cell types\n",
    "preds_no_epiatlas_16ct = all_preds_no_epiatlas[\n",
    "    all_preds_no_epiatlas[CELL_TYPE].isin(accepted_cts)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"5tasks_acc_per_assay_NO_EpiAtlas_16ct.tsv\"\n",
    "_ = compute_and_save_acc_per_assay(preds_no_epiatlas_16ct, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate min_pred graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encode_metrics_per_assay(\n",
    "    df_acc_per_assay: pd.DataFrame, min_pred: float = 0, logdir: Path | None = None\n",
    ") -> None:\n",
    "    \"\"\"Plot accuracy+F1 of each classification task, per assay\"\"\"\n",
    "    df = copy.deepcopy(df_acc_per_assay)\n",
    "\n",
    "    # Selecting min_pred\n",
    "    to_plot = df[\n",
    "        (df[\"min_predScore\"] > (min_pred - 0.01))\n",
    "        & (df[\"min_predScore\"] < (min_pred + 0.01))\n",
    "    ]\n",
    "\n",
    "    # sort tasks by avg_acc\n",
    "    averages = to_plot[to_plot[ASSAY] == \"avg-core\"]\n",
    "    avg_acc = list(zip(averages[\"acc\"], averages[\"task_name\"]))\n",
    "    task_order = [\n",
    "        task_name for _, task_name in sorted(avg_acc, key=lambda x: x[0], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Removing undesired assays\n",
    "    to_plot = to_plot[to_plot[ASSAY].isin(CORE_ASSAYS + [\"rna_seq\"])]\n",
    "\n",
    "    names = {\n",
    "        ASSAY: \"assay\",\n",
    "        LIFE_STAGE: \"life stage\",\n",
    "        f\"{LIFE_STAGE}_merged\": \"life stage (merged)\",\n",
    "        CELL_TYPE: \"cell type\",\n",
    "        SEX: \"sex\",\n",
    "        CANCER: \"cancer\",\n",
    "    }\n",
    "\n",
    "    # Plot each task\n",
    "    for metric in [\"acc\", \"f1-score\"]:\n",
    "        fig = go.Figure()\n",
    "        for task_name in task_order:\n",
    "            task_df = to_plot[to_plot[\"task_name\"] == task_name]\n",
    "\n",
    "            task_name = names[task_name]\n",
    "\n",
    "            if task_name == \"assay\" and metric == \"f1-score\":\n",
    "                continue\n",
    "            fig.add_trace(\n",
    "                go.Box(\n",
    "                    x=[task_name] * len(task_df),\n",
    "                    y=task_df[metric],\n",
    "                    name=metric,\n",
    "                    boxpoints=\"outliers\",\n",
    "                    boxmean=True,\n",
    "                    marker_color=\"gray\",\n",
    "                    showlegend=False,\n",
    "                    hoverinfo=\"skip\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[task_name] * len(task_df),\n",
    "                    y=task_df[metric],\n",
    "                    mode=\"markers\",\n",
    "                    name=task_name,\n",
    "                    marker_color=[assay_colors[assay] for assay in task_df[ASSAY]],\n",
    "                    hoverinfo=\"text\",\n",
    "                    hovertext=[\n",
    "                        f\"{assay}: {value:.3f}\"\n",
    "                        for assay, value in zip(task_df[ASSAY], task_df[metric])\n",
    "                    ],\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        y_axis_label = \"F1-score\" if metric == \"f1-score\" else \"Accuracy\"\n",
    "        fig.update_layout(\n",
    "            xaxis_title=\"Classification task\",\n",
    "            yaxis_title=y_axis_label,\n",
    "            font=dict(size=18),\n",
    "            width=800,\n",
    "            height=600,\n",
    "            title=f\"ENCODE: Task {y_axis_label} per assay (min_predScore={min_pred:.2f})\",\n",
    "        )\n",
    "\n",
    "        fig.update_yaxes(range=[0, 1.01])\n",
    "\n",
    "        # Show/Write the plot\n",
    "        if logdir:\n",
    "            filename = f\"encode_5tasks_metrics_per_assay-{metric}-{min_pred*100:.0f}\"\n",
    "            fig.write_image(logdir / f\"{filename}.png\")\n",
    "            fig.write_image(logdir / f\"{filename}.svg\")\n",
    "            fig.write_html(logdir / f\"{filename}.html\")\n",
    "\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = base_fig_dir / \"encode_predictions\" / \"metrics_per_assay\"\n",
    "if not logdir.exists():\n",
    "    logdir.mkdir(parents=True)\n",
    "\n",
    "for min_pred in [0, 0.6, 0.8, 0.9]:\n",
    "    plot_encode_metrics_per_assay(df_acc_per_assay, min_pred=min_pred, logdir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple min_predScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_acc_per_assay(graph_df, minY, maxY, logdir: Path | None = None):\n",
    "    \"\"\"Plot accuracy per assay, per min_predScore, per scatter_name/task_name,\n",
    "    for core vs non-core assays.\n",
    "\n",
    "    \"\"\"\n",
    "    min_predScore_color_map = {\"0.0\": \"blue\", \"0.6\": \"orange\", \"0.9\": \"red\"}\n",
    "\n",
    "    graph_df[\"scatter_name\"] = graph_df[\"task_name\"].replace(\n",
    "        \"harmonized_\", \"\", regex=True\n",
    "    )\n",
    "\n",
    "    graph_df = graph_df.sort_values(by=[ASSAY, \"min_predScore\", \"scatter_name\"])\n",
    "\n",
    "    for graph_type in [\"core\", \"non-core\"]:\n",
    "        graph_df = df_acc_per_assay.copy()\n",
    "        if graph_type == \"core\":\n",
    "            graph_df = graph_df[graph_df[ASSAY].isin(CORE_ASSAYS + [\"rna_seq\"])]\n",
    "            minY = 0.55\n",
    "            maxY = 1.001\n",
    "        elif graph_type == \"non-core\":\n",
    "            graph_df = graph_df[~graph_df[ASSAY].isin(CORE_ASSAYS)]\n",
    "            minY = 0\n",
    "            maxY = 1\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid graph type: {graph_type}\")\n",
    "\n",
    "        unique_assays = list(graph_df[ASSAY].unique())\n",
    "\n",
    "        # Calculate average over assays\n",
    "        avg_df = (\n",
    "            graph_df.groupby([\"min_predScore\", \"scatter_name\"])[\"acc\"]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "        avg_df[ASSAY] = \"Average\"\n",
    "\n",
    "        # traces_per_assay = graph_df[\"scatter_name\"].nunique()\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        for min_pred in [\"0.0\", \"0.6\", \"0.9\"]:\n",
    "            df_subset = graph_df[graph_df[\"min_predScore\"] == min_pred]\n",
    "            avg_subset = avg_df[avg_df[\"min_predScore\"] == min_pred]\n",
    "\n",
    "            # Add average over assay trace\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[\"Average - \" + name for name in avg_subset[\"scatter_name\"]],\n",
    "                    y=avg_subset[\"acc\"],\n",
    "                    mode=\"markers\",\n",
    "                    name=f\"Avg Min Pred Score: {min_pred}\",\n",
    "                    marker=dict(\n",
    "                        color=min_predScore_color_map[min_pred],\n",
    "                        size=9,\n",
    "                        symbol=\"star\",\n",
    "                    ),\n",
    "                    hoverinfo=\"y+x\",\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Add individual assay traces\n",
    "            hovertext = list(\n",
    "                zip(\n",
    "                    df_subset[ASSAY],\n",
    "                    df_subset[\"nb_samples\"].apply(lambda x: f\"Samples: {x}\"),\n",
    "                )\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_subset[ASSAY] + \" - \" + df_subset[\"scatter_name\"],\n",
    "                    y=df_subset[\"acc\"],\n",
    "                    mode=\"markers\",\n",
    "                    name=f\"Min Pred Score: {min_pred}\",\n",
    "                    marker=dict(\n",
    "                        color=min_predScore_color_map[min_pred],\n",
    "                        size=9,\n",
    "                    ),\n",
    "                    text=hovertext,\n",
    "                    hoverinfo=\"text+y+x\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Modify x-axis tick labels\n",
    "\n",
    "        ticktext = []\n",
    "        tick_group = list(df_subset[\"scatter_name\"].unique())\n",
    "        for i, tick in enumerate(tick_group):\n",
    "            tick_group[i] = f\"<b>{tick}</b>\"\n",
    "\n",
    "        for i in range(len(unique_assays) + 1):\n",
    "            ticktext.extend(tick_group)\n",
    "\n",
    "        fig.update_xaxes(\n",
    "            tickmode=\"array\", ticktext=ticktext, tickvals=list(range(len(ticktext)))\n",
    "        )\n",
    "\n",
    "        # Add assay labels on top + vertical lines between assay groups\n",
    "        fig.add_annotation(\n",
    "            x=len(tick_group) / 2 - 0.5,\n",
    "            y=1.05,\n",
    "            yref=\"paper\",\n",
    "            text=\"Average\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=14),\n",
    "        )\n",
    "\n",
    "        fig.add_vline(\n",
    "            x=len(tick_group) - 0.5, line_width=2, line_dash=\"solid\", line_color=\"black\"\n",
    "        )\n",
    "        fig.add_hline(y=1, line_width=1, line_color=\"black\")\n",
    "\n",
    "        for i, label in enumerate(unique_assays):\n",
    "            fig.add_annotation(\n",
    "                x=(i + 1) * len(tick_group) + len(tick_group) / 2 - 0.5,\n",
    "                y=1.05,\n",
    "                yref=\"paper\",\n",
    "                text=label,\n",
    "                showarrow=False,\n",
    "                font=dict(size=14),\n",
    "            )\n",
    "            fig.add_vline(\n",
    "                x=(i + 1) * len(tick_group) - 0.5,\n",
    "                line_width=1,\n",
    "                line_dash=\"dash\",\n",
    "                line_color=\"black\",\n",
    "            )\n",
    "\n",
    "        # titles + yaxis range\n",
    "        fig.update_layout(\n",
    "            title=\"ENCODE data - Label match per Assay and Task\",\n",
    "            xaxis_title=\"Assay - Task\",\n",
    "            yaxis_title=\"Match %\",\n",
    "            xaxis_tickangle=-45,\n",
    "            showlegend=True,\n",
    "            height=600,\n",
    "            width=1200,\n",
    "            yaxis=dict(tickformat=\".2%\", range=[minY, maxY]),\n",
    "        )\n",
    "\n",
    "        # Show/Write the plot\n",
    "        print(f\"Graphing {graph_type}\")\n",
    "        if logdir:\n",
    "            figname = f\"encode_{graph_type}_acc_per_assay_minY{minY:.2f}\"\n",
    "            fig.write_html(logdir / f\"{figname}.html\")\n",
    "            fig.write_image(logdir / f\"{figname}.png\")\n",
    "            fig.write_image(logdir / f\"{figname}.svg\")\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this_fig_dir = base_fig_dir / \"encode_predictions\" / \"acc_per_assay\"\n",
    "# if not this_fig_dir.exists():\n",
    "#     raise FileNotFoundError(f\"Folder {this_fig_dir} does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_confusion_matrix(all_preds: pd.DataFrame, output_dir: Path):\n",
    "    \"\"\"Graph confusion matrix for each classification task, for both core and non-core assays\"\"\"\n",
    "    df = copy.deepcopy(all_preds)\n",
    "    for graph_type in [\"core\", \"non-core\"]:\n",
    "        print(f\"Graphing {graph_type}\")\n",
    "        if graph_type == \"core\":\n",
    "            sub_df = df[df[ASSAY].isin(CORE_ASSAYS + [\"rna_seq\", \"mrna_seq\"])]\n",
    "        elif graph_type == \"non-core\":\n",
    "            sub_df = df[df[ASSAY].isin([\"ctcf\", \"non-core\"])]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid graph_type: {graph_type}\")\n",
    "\n",
    "        for name in [ASSAY, CELL_TYPE, SEX, LIFE_STAGE, f\"{LIFE_STAGE}_merged\", CANCER]:\n",
    "            logdir = output_dir / name\n",
    "            if not logdir.exists():\n",
    "                logdir.mkdir(parents=True)\n",
    "\n",
    "            if name == CELL_TYPE and graph_type == \"core\":\n",
    "                continue\n",
    "\n",
    "            task_df = copy.deepcopy(sub_df)\n",
    "            task_df = task_df.fillna(\"unknown\")\n",
    "            task_df = task_df[task_df[name] != \"unknown\"]\n",
    "\n",
    "            if name == CELL_TYPE:\n",
    "                task_df = task_df[task_df[CELL_TYPE].isin(accepted_cts)]\n",
    "\n",
    "            print(name, task_df.shape)\n",
    "\n",
    "            y_true_col = f\"True class ({name})\"\n",
    "            y_pred_col = f\"Predicted class ({name})\"\n",
    "            max_pred_label = f\"Max pred ({name})\"\n",
    "\n",
    "            for threshold in [0, 0.6, 0.8, 0.9]:\n",
    "                filtered_df = task_df[task_df[max_pred_label] >= threshold]\n",
    "\n",
    "                true, pred = filtered_df[y_true_col], filtered_df[y_pred_col]\n",
    "                if name == CELL_TYPE:\n",
    "                    labels = accepted_cts\n",
    "                else:\n",
    "                    labels = list(filtered_df[name].unique())\n",
    "\n",
    "                cm = confusion_matrix(true, pred, labels=labels)\n",
    "\n",
    "                writer = ConfusionMatrixWriter(labels=labels, confusion_matrix=cm)\n",
    "                writer.to_all_formats(\n",
    "                    logdir=logdir,\n",
    "                    name=f\"{name}-{graph_type}-confusion_matrix-{threshold*100:.0f}\",\n",
    "                )\n",
    "                plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_logdir = base_fig_dir / \"encode_predictions\" / \"confusion_matrices\"\n",
    "graph_confusion_matrix(all_preds_no_epiatlas, cm_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### track type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_type_pred_path = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"predictions\"\n",
    "    / \"encode\"\n",
    "    / \"track_type\"\n",
    "    / \"split0_test_prediction_100kb_all_none_all.list.csv\"\n",
    ")\n",
    "track_type_pred_df = pd.read_csv(track_type_pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_vector_cols = list(track_type_pred_df.columns[3:])\n",
    "track_type_pred_df[\"Max_pred_track_type\"] = track_type_pred_df.loc[\n",
    "    :, pred_vector_cols\n",
    "].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_type_df = track_type_pred_df.merge(\n",
    "    complete_metadata_df, left_on=\"Unnamed: 0\", right_on=\"FILE_accession\", how=\"inner\"\n",
    ")\n",
    "\n",
    "print(track_type_df.shape, encode_df.shape, track_type_pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write each table in a separate excel sheet\n",
    "output = track_type_pred_path.parent / \"track_type_predictions_pivot.csv\"\n",
    "output.unlink(missing_ok=True)\n",
    "\n",
    "with open(output, \"a\", encoding=\"utf8\") as csv_stream:\n",
    "    for min_pred in [0, 0.6, 0.8]:\n",
    "        df = track_type_df[track_type_df[\"Max_pred_track_type\"] >= min_pred]\n",
    "        pivot = df.pivot_table(\n",
    "            index=ASSAY,\n",
    "            columns=\"Predicted class\",\n",
    "            values=\"Max_pred_track_type\",\n",
    "            aggfunc=\"count\",\n",
    "            fill_value=0,\n",
    "            margins=True,\n",
    "        ).astype(int)\n",
    "        relative_pivot = pivot.div(pivot[\"All\"], axis=0) * 100\n",
    "\n",
    "        # csv_stream.write(f\"Count Pivot - Min pred: {min_pred}\\n\")\n",
    "        # pivot.to_csv(csv_stream)\n",
    "        # csv_stream.write(\"\\n\")\n",
    "\n",
    "        # csv_stream.write(f\"Relative Pivot - Min pred: {min_pred}\\n\")\n",
    "        # relative_pivot.to_csv(csv_stream)\n",
    "        # csv_stream.write(\"\\n\")\n",
    "\n",
    "        # display(pivot)\n",
    "        # with pd.option_context(\"display.float_format\", \"{:.2f}\".format):\n",
    "        #     display(relative_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNA-Seq Assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_df = all_preds_no_epiatlas[all_preds_no_epiatlas[ASSAY].str.contains(\"rna\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [ASSAY, f\"True class ({ASSAY})\"]:\n",
    "    rna_df.loc[:, col].replace(\n",
    "        {\n",
    "            \"total RNA-seq\": \"rna_seq\",\n",
    "            \"polyA plus RNA-seq\": \"mrna_seq\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RNA-Seq assay accuracy, if mrna_seq != rna_seq\\n\")\n",
    "for min_pred in [0, 0.6, 0.8]:\n",
    "    df = rna_df[rna_df[f\"Max pred ({ASSAY})\"] >= min_pred]\n",
    "    acc = len(df[df[f\"True class ({ASSAY})\"] == df[f\"Predicted class ({ASSAY})\"]]) / len(\n",
    "        df\n",
    "    )\n",
    "    print(\n",
    "        f\"Min pred: {min_pred}, Accuracy: {acc:.4f}. Samples: {len(df)}/{rna_df.shape[0]}\\n\"\n",
    "    )\n",
    "    groupby = (\n",
    "        df.groupby([ASSAY, f\"Predicted class ({ASSAY})\"])\n",
    "        .size()\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"Count\"})\n",
    "        .sort_values([ASSAY, \"Count\"], ascending=False)\n",
    "    )\n",
    "    print(groupby, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RNA-Seq assay accuracy, if mrna_seq == rna_seq\\n\")\n",
    "df = rna_df.copy()\n",
    "for cat in [ASSAY, f\"Predicted class ({ASSAY})\", f\"True class ({ASSAY})\"]:\n",
    "    df.loc[df[cat] == \"mrna_seq\", cat] = \"rna_seq\"\n",
    "\n",
    "for min_pred in [0, 0.6, 0.8]:\n",
    "    sub_df = df[df[f\"Max pred ({ASSAY})\"] >= min_pred]\n",
    "    acc = len(\n",
    "        sub_df[sub_df[f\"True class ({ASSAY})\"] == sub_df[f\"Predicted class ({ASSAY})\"]]\n",
    "    ) / len(sub_df)\n",
    "    print(\n",
    "        f\"Min pred: {min_pred}, Accuracy: {acc:.4f}. Samples: {len(sub_df)}/{rna_df.shape[0]}\\n\"\n",
    "    )\n",
    "\n",
    "    groupby = (\n",
    "        sub_df.groupby([ASSAY, f\"Predicted class ({ASSAY})\"])\n",
    "        .size()\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"Count\"})\n",
    "        .sort_values(by=[ASSAY, \"Count\"], ascending=[True, False])\n",
    "    )\n",
    "    print(groupby, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
