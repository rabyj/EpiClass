{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Metadata for ChIP-Atlas datasets\n",
    "\"\"\"\n",
    "# pylint: disable=redefined-outer-name, import-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import display  # pylint: disable=unused-import\n",
    "\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import ASSAY_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "metadata_dir = base_data_dir / \"metadata\"\n",
    "\n",
    "predictions_dir = base_data_dir / \"training_results\" / \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_metadata_dir = metadata_dir / \"chip_atlas\"\n",
    "ca_pred_dir = predictions_dir / \"C-A\" / \"assay_epiclass\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detail `CA_metadata_4DB+all_pred.20240606.tsv` modifications\n",
    "\n",
    "See `paper/data/training_results/predictions/C-A/assay_epiclass/README.txt`\n",
    "\n",
    "Starting from `CA_metadata_4DB+all_pred.20240606_mod2.tsv`, since 1.0 -> 2.0 involved a manual modification of the life (shifting some rows that got mangled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df_path = ca_pred_dir / \"CA_metadata_4DB+all_pred.20240606_mod2.tsv\"\n",
    "initial_df = pd.read_csv(initial_df_path, sep=\"\\t\", low_memory=False)\n",
    "assert \"is_EpiAtlas_EpiRR\" not in initial_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.0 -> 2.1: Add `is_EpiAtlas_EpiRR` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_dir = base_data_dir / \"metadata\" / \"encode\" / \"old_meta\"\n",
    "encode_epiatlas_mapping_path = encode_metadata_dir / \"ENCODE_IHEC_keys.tsv\"\n",
    "encode_epiatlas_mapping_df = pd.read_csv(encode_epiatlas_mapping_path, sep=\"\\t\")\n",
    "print(encode_epiatlas_mapping_df.shape)\n",
    "\n",
    "enc_df = encode_epiatlas_mapping_df[[\"is_EpiAtlas_EpiRR\", \"accession\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_df = initial_df.merge(\n",
    "    enc_df, left_on=\"ENCODE_GSE\", right_on=\"accession\", how=\"left\"\n",
    ").drop_duplicates()\n",
    "\n",
    "new_pred_df.drop(columns=[\"accession\"], inplace=True)\n",
    "new_pred_df[\"is_EpiAtlas_EpiRR\"].fillna(\"0\", inplace=True)\n",
    "\n",
    "assert initial_df.shape[0] == new_pred_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = str(initial_df_path).replace(\"mod2.tsv\", \"mod2.1.tsv\")\n",
    "new_pred_df_2 = pd.read_csv(new_path, sep=\"\\t\", low_memory=False)\n",
    "assert new_pred_df.shape[0] == new_pred_df_2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_df_2 = new_pred_df_2.fillna(\"unknown\")\n",
    "new_pred_df = new_pred_df.fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (col1, col2) in enumerate(zip(new_pred_df.columns, new_pred_df_2.columns)):\n",
    "    if col1 != col2:\n",
    "        print(col1, col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in new_pred_df.columns:\n",
    "    if not np.equal(new_pred_df[col].values, new_pred_df_2[col].values).all():  # type: ignore\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_pred_df_2, enc_df, encode_epiatlas_mapping_df, initial_df\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_2_1 = new_pred_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 -> 2.2:  Add `core7_DBs_consensus` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAME_TARGET = \"core7_DBs_consensus\"\n",
    "DB_COLS = [\"GEO_mod\", \"C-A\", \"Cistrome\", \"NGS_mod\"]\n",
    "new_pred_df.loc[:, DB_COLS] = new_pred_df[DB_COLS].apply(lambda x: x.str.lower())\n",
    "\n",
    "CORE_ASSAYS = ASSAY_ORDER[0:7]\n",
    "print(CORE_ASSAYS)\n",
    "\n",
    "non_core_labels = [\"non-core\", \"CTCF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_core_consensus_column(\n",
    "    df: pd.DataFrame, verbose=False\n",
    ") -> tuple[pd.DataFrame, Counter, Counter]:\n",
    "    \"\"\"Create or replaces consensus column for core7 assays.\n",
    "\n",
    "    First column is presumed to be ID column.\n",
    "    \"\"\"\n",
    "    id_col = df.columns[0]\n",
    "    df = df.copy(deep=True)\n",
    "    try:\n",
    "        df.drop(columns=[SAME_TARGET], inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    core_df = df[~df[\"manual_target_consensus\"].isin((non_core_labels))].copy(deep=True)\n",
    "    if core_df[\"manual_target_consensus\"].isna().sum() > 0:\n",
    "        raise ValueError(\"There are missing values in the target column.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Input shape: {df.shape}. Core7 shape: {core_df.shape}.\")\n",
    "\n",
    "    tmp_df = core_df.loc[:, DB_COLS].copy(deep=True)\n",
    "    tmp_df[\"C-A\"].replace(\"unclassified\", \"----\", inplace=True)\n",
    "    if verbose:\n",
    "        display(tmp_df.value_counts(dropna=False))\n",
    "\n",
    "    id_db_target = []\n",
    "    unique_labels = Counter()\n",
    "    different_labels = Counter()\n",
    "\n",
    "    for labels in tmp_df.values:\n",
    "        missing_N = sum(label == \"----\" for label in labels)\n",
    "        db_labels = set(labels)\n",
    "\n",
    "        try:\n",
    "            db_labels.remove(\"----\")\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if any(label not in CORE_ASSAYS + [\"ctrl\"] for label in db_labels):\n",
    "            id_db_target.append(\"Ignored - Potential non-core\")\n",
    "        elif missing_N == 3:\n",
    "            id_db_target.append(\"1 source\")\n",
    "        elif len(db_labels) == 1:\n",
    "            id_db_target.append(\"Identical\")\n",
    "        else:\n",
    "            id_db_target.append(\"Different\")\n",
    "            different_labels[tuple(db_labels)] += 1\n",
    "\n",
    "        unique_labels[tuple(db_labels)] += 1\n",
    "\n",
    "    core_df.loc[:, SAME_TARGET] = id_db_target\n",
    "\n",
    "    df = pd.merge(df, core_df[[id_col, SAME_TARGET]], on=id_col, how=\"left\")\n",
    "    df.loc[df[SAME_TARGET].isna(), SAME_TARGET] = \"non-core/CTCF\"\n",
    "\n",
    "    return df, unique_labels, different_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_df, unique_labels, different_labels = create_core_consensus_column(new_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \"non-core/CTCF\" never there!^!^!\n",
    "new_pred_df[SAME_TARGET].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_counts = new_pred_df[SAME_TARGET].value_counts(dropna=False)\n",
    "display(new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_2_2_path = str(initial_df_path).replace(\"mod2.tsv\", \"mod2.2.tsv\")\n",
    "pred_df_2_2 = pd.read_csv(pred_df_2_2_path, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "assert new_pred_df.shape[0] == pred_df_2_2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_2_2 = pred_df_2_2[SAME_TARGET].value_counts(dropna=False)\n",
    "display(counts_2_2)\n",
    "display(new_counts - counts_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if making the 2.2 -> 3.0 target corrections takes the difference to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`h3.3k27m` is not a target, it's a cell line name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct some NGS_mod annotation errors using the file titles (GSE78801).\n",
    "# They took h3.3k27m as the target when it is related to the cell line (SF8628 Human DIPG H3.3-K27M Cell Line).\n",
    "to_replace = {\n",
    "    \"GSM2265634\": \"h3k27me3\",\n",
    "    \"GSM2265635\": \"h3k27me3\",\n",
    "    \"GSM2265642\": \"h3k4me1\",\n",
    "}\n",
    "idx = new_pred_df[\"GSM\"].isin(to_replace.keys())\n",
    "new_pred_df.loc[idx, \"NGS_mod\"] = new_pred_df.loc[idx, \"GSM\"].map(to_replace)\n",
    "\n",
    "if new_pred_df[new_pred_df.isin([\"h3.3k27m\"])].notna().sum().sum() != 0:\n",
    "    raise ValueError(\"h3.3k27m is still present in the dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old target mislabeling error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_correction_path = ca_pred_dir / \"CA_metadata_correction.tsv\"\n",
    "ca_correction_df = pd.read_csv(ca_correction_path, sep=\"\\t\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_df = new_pred_df.merge(ca_correction_df, on=\"Experimental-id\", how=\"left\")\n",
    "new_pred_df[[\"manual_target_consensus\", \"GEO_mod\"]] = new_pred_df[\n",
    "    [\"manual_target_consensus2\", \"GEO_mod2\"]\n",
    "]\n",
    "new_pred_df = new_pred_df.drop(columns=[\"manual_target_consensus2\", \"GEO_mod2\"])\n",
    "new_pred_df[\"GEO_mod\"] = new_pred_df[\"GEO_mod\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform all \"revxlinkchromatin\" target into \"input\" so they're not counted as different targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_df.loc[:, DB_COLS] = new_pred_df[DB_COLS].replace(\"revxlinkchromatin\", \"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redoing core7 consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_df, unique_labels, different_labels = create_core_consensus_column(new_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_counts = new_pred_df[SAME_TARGET].value_counts(dropna=False)\n",
    "assert sum(new_counts - counts_2_2) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in new_pred_df.columns:\n",
    "    if \"Same\" in col:\n",
    "        new_pred_df.drop(columns=[col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_3_path = str(initial_df_path).replace(\"mod2.tsv\", \"mod3.0.tsv\")\n",
    "df_3 = pd.read_csv(current_3_path, sep=\"\\t\", low_memory=False)\n",
    "df_3 = df_3.fillna(\"unknown\")\n",
    "\n",
    "assert new_pred_df.shape[0] == df_3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in new_pred_df.columns:\n",
    "    if not np.equal(new_pred_df[col].astype(str).values, df_3[col].astype(str).values).all():  # type: ignore\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_df.to_csv(current_3_path, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTCF details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 13c classifier will tend to classify CTCF as input, so we cannot trust it to differentiate between CTCF and input signals.  \n",
    "Possibly CTCF samples COULD be excluded from the prediction pool, since core assays classifier have never seen CTCF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_core_df = new_pred_df[~new_pred_df[\"manual_target_consensus\"].isin(non_core_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    ca_core_df[ca_core_df[DB_COLS].isin([\"ctcf\"]).any(axis=1)][\n",
    "        [\"Experimental-id\", \"Gse-geo\", \"GSM\"]\n",
    "        + DB_COLS\n",
    "        + [\"manual_target_consensus\", SAME_TARGET]\n",
    "    ].sort_values([\"Gse-geo\", \"GSM\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a review of the experiment descriptions on GEO, it seems GSE102237, GSE108869 and GSE38411 samples marked as CTCF by cistrome have an uncertain target, and so could be excluded from our core samples.\n",
    "\n",
    "As for GSE183379 samples marked as ctcf by C-A (7 samples), it seems none of them are actually CTCF, according to the original files names on GEO, so they don't need to be excluded.\n",
    "\n",
    "For simplicity's sake, they were all left as \"Ignored - Potential non-core\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_pred_df, ca_correction_df, ca_core_df, df_3, pred_df_2_2, pred_df_2_1\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChIP-Atlas website download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to obtain some of the metadata:\n",
    "\n",
    "```bash\n",
    "wget https://chip-atlas.dbcls.jp/data/metadata/experimentList.tab\n",
    "grep -E \"^[DESRX]{3}[0-9]{4,8}\\shg38\\s\" experimentList.tab > experimentList_hg38.tab\n",
    "grep -vE \"^[DESRX]{3}[0-9]{4,8}\\shg38\\s[ATAC,DNASE,Bisulfate,RNA]\" experimentList_hg38.tab > experimentList_hg38_chip.tab\n",
    "cut -f1,3-7,9- experimentList_hg38_chip.tab | sponge experimentList_hg38_chip.tab # Removing col 2 and 8.\n",
    "```\n",
    "\n",
    "Following columns given at the [wiki](https://github.com/inutano/chip-atlas/wiki#tables-summarizing-metadata-and-files), `assembly` and `Processing_logs_of[...]` columns were removed.  \n",
    "metadata submitted by authors was chunked into the last column, instead of dealing with varying column lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = ca_metadata_dir / \"experimentList_hg38_chip_20250306.tab\"\n",
    "new_file_name = ca_metadata_dir / (metadata_path.stem + \"_formatted.tab\")\n",
    "\n",
    "if not new_file_name.exists():\n",
    "    with open(metadata_path, \"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Merging all lines past the title\n",
    "    new_file = []\n",
    "    for line in lines:\n",
    "        elems = line.split(\"\\t\")\n",
    "        core = elems[0:7]\n",
    "        rest = elems[7:]\n",
    "\n",
    "        rest = [x.strip() for x in rest]\n",
    "        rest = [x for x in rest if x]\n",
    "\n",
    "        new_line = \"\\t\".join(core) + \"\\t\" + str(rest)\n",
    "        new_file.append(new_line)\n",
    "\n",
    "    new_file.insert(\n",
    "        0,\n",
    "        \"Experimental_ID\\tTrack_type_class\\tTrack_type\\tCell_type_class\\tCell_type\\tCell_type_description\\tTitle\\tMeta_data_submitted_by_authors\",\n",
    "    )\n",
    "\n",
    "    with open(new_file_name, \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(\"\\n\".join(new_file))\n",
    "\n",
    "ca_metadata_df = pd.read_csv(new_file_name, sep=\"\\t\", low_memory=False)\n",
    "print(ca_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal DB matching metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimal metadata created from `CA_metadata_4DB+all_pred.20240606_mod3.0.tsv`.  \n",
    "\n",
    "I mostly kepts the ids and targets from different databases, renamed+moved columns for easier understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_metadata_path = ca_metadata_dir / \"CA_minimal_metadata_20240606.tsv\"\n",
    "ca_minimal_metadata_df = pd.read_csv(minimal_metadata_path, sep=\"\\t\", low_memory=False)\n",
    "print(ca_minimal_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some GSM title were missing from the old work, so I redownloaded metadata from GEO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_minimal_metadata_df.replace(\"-\", None, inplace=True)\n",
    "missing_mask = ca_minimal_metadata_df[\"GEO_gsm-title\"].isna()\n",
    "print(\"Missing GSM titles:\", sum(missing_mask))\n",
    "\n",
    "missing_titles = ca_minimal_metadata_df[missing_mask][\"GEO_GSM\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_GEO_file(\n",
    "    GEO: str, logdir: str | Path, amount: str = \"quick\", verbose: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads a GEO (GSM) accession file and saves it to the specified log directory.\n",
    "\n",
    "    Args:\n",
    "        GEO (str): The GEO accession number (e.g., \"GSM123456\").\n",
    "        logdir (str): Directory to save the downloaded file.\n",
    "        amount (str): Level of detail for the file. Options: 'full', 'brief', 'quick', 'data'.\n",
    "                      Default is 'full'.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the saved file.\n",
    "    \"\"\"\n",
    "    # Ensure GEO is uppercase\n",
    "    GEO = GEO.upper()\n",
    "\n",
    "    # Validate the accession type\n",
    "    if not GEO.startswith(\"GSM\"):\n",
    "        raise ValueError(\"Only GSM accession numbers are supported.\")\n",
    "\n",
    "    # Ensure logdir exists\n",
    "    os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "    # Construct the URL\n",
    "    gseurl = \"https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi\"\n",
    "    myurl = f\"{gseurl}?targ=self&acc={GEO}&form=text&view={amount}\"\n",
    "\n",
    "    # Define the destination file path\n",
    "    destfile = os.path.join(logdir, f\"{GEO}.soft\")\n",
    "    if os.path.exists(destfile):\n",
    "        if verbose:\n",
    "            print(f\"File already exists: {destfile}\")\n",
    "        return destfile\n",
    "\n",
    "    try:\n",
    "        # Download the file\n",
    "        response = requests.get(myurl, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for bad responses (4xx, 5xx)\n",
    "\n",
    "        # Save the file\n",
    "        with open(destfile, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"File saved: {destfile}\")\n",
    "        return destfile\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {GEO}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = ca_metadata_dir / \"GSM_metadata\"\n",
    "logdir.mkdir(exist_ok=True)\n",
    "\n",
    "meta_paths = []\n",
    "for GEO in missing_titles:\n",
    "    filepath = download_GEO_file(GEO, logdir, amount=\"quick\", verbose=False)\n",
    "    if filepath:\n",
    "        meta_paths.append(Path(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_title_dict = {}\n",
    "for filepath in meta_paths:\n",
    "    gsm = filepath.stem\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        title_line = lines[1]\n",
    "        if not title_line.startswith(\"!Sample_title\"):\n",
    "            raise ValueError(f\"Title not found for {gsm}\")\n",
    "\n",
    "        title = title_line.split(\"=\")[1].strip()\n",
    "        missing_title_dict[gsm] = title\n",
    "\n",
    "with open(logdir / \"GSM_title.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(missing_title_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_minimal_metadata_df[\"GEO_gsm-title\"] = (\n",
    "    ca_minimal_metadata_df[\"GEO_GSM\"]\n",
    "    .map(missing_title_dict)\n",
    "    .fillna(ca_minimal_metadata_df[\"GEO_gsm-title\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_minimal_metadata_df.to_csv(\n",
    "    ca_metadata_dir / \"CA_minimal_metadata_20240606_mod.tsv\", sep=\"\\t\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancer / Sex / Age metadata categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CA_metadata_FW_20240917` contains new metadata categories (cancer/sex/age) created from analyzing more complete metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_custom_metadata_path = metadata_dir / \"chip_atlas\" / \"CA_metadata_FW_20240917.tsv\"\n",
    "ca_custom_metadata_df = pd.read_csv(ca_custom_metadata_path, sep=\"\\t\", low_memory=False)\n",
    "print(ca_custom_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Integrate how CA_metadata_FW_20240917 was created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = ca_minimal_metadata_df.columns[0]\n",
    "col2 = ca_metadata_df.columns[0]\n",
    "\n",
    "meta_df = ca_minimal_metadata_df.merge(\n",
    "    ca_metadata_df, how=\"left\", left_on=col1, right_on=col2\n",
    ")\n",
    "meta_df.drop(col2, axis=1, inplace=True)\n",
    "\n",
    "col2 = ca_custom_metadata_df.columns[0]\n",
    "meta_df = meta_df.merge(ca_custom_metadata_df, how=\"left\", left_on=col1, right_on=col2)\n",
    "meta_df.drop(col2, axis=1, inplace=True)\n",
    "meta_df.rename({\"Title\": \"C-A_title\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = meta_df.fillna(\"unknown\")\n",
    "meta_df = meta_df.replace(\"Unclassified\", \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.to_csv(ca_metadata_dir / \"CA_metadata_joined_20250306.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit assay and biospecimen counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_epiatlas = meta_df[meta_df[\"is_EpiAtlas_EpiRR\"] == \"0\"].copy()\n",
    "print(meta_df.shape, df_no_epiatlas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biospecimens = df_no_epiatlas.copy(deep=True)\n",
    "\n",
    "# Count occurrences of each \"Cell_type\" within \"Cell_type_class\"\n",
    "group_sizes = df_biospecimens.groupby(\"Cell_type_class\")[\"Cell_type\"].count()\n",
    "\n",
    "# Sort \"Cell_type_class\" by descending count of \"Cell_type\"\n",
    "sorted_classes = group_sizes.sort_values(ascending=False).index\n",
    "\n",
    "# Apply sorted order to the original grouping\n",
    "sorted_groupby = (\n",
    "    df_biospecimens.groupby([\"Cell_type_class\", \"Cell_type\"], dropna=False)\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    "    .set_index(\"Cell_type_class\")\n",
    "    .loc[sorted_classes]\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_groupby.columns = [\"Cell_type_class\", \"Cell_type\", \"count\"]\n",
    "\n",
    "output_dir = base_dir / \"tables\" / \"datasets_composition\"\n",
    "sorted_groupby.to_csv(output_dir / \"ChIP-Atlas_biospecimens.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assays_df = (\n",
    "    df_no_epiatlas.groupby(\"manual_target_consensus\", dropna=False)\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    "    .to_frame(name=\"count\")\n",
    ")\n",
    "assays_df.to_csv(output_dir / \"ChIP-Atlas_assays.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
