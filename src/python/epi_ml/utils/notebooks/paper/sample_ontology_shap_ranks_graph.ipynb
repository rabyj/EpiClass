{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Graph results from sample_ontology_shap_ranks.py\"\"\"\n",
    "\n",
    "# pylint: disable=import-error,redefined-outer-name,use-dict-literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Set\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display  # pylint: disable=unused-import\n",
    "\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import CELL_TYPE, IHECColorMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "base_fig_dir = paper_dir / \"figures\"\n",
    "base_data_dir = base_dir / \"data\" / \"SHAP\" / \"hg38_100kb_all_none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "cell_type_colors = IHECColorMap.cell_type_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_dir = base_data_dir / f\"{CELL_TYPE}_1l_3000n\" / \"10fold-oversampling\"\n",
    "if not cell_type_dir.exists():\n",
    "    raise ValueError(f\"Directory {cell_type_dir} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_folder = cell_type_dir / \"shap_ranks\" / \"merge_samplings\"\n",
    "if not ranks_folder.exists():\n",
    "    raise ValueError(f\"Directory {ranks_folder} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_folder_assay_level = cell_type_dir / \"global_shap_analysis\" / \"shap_ranks\" / \"core7\"\n",
    "if not ranks_folder_assay_level.exists():\n",
    "    raise ValueError(f\"Directory {ranks_folder_assay_level} does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results: Dict[str, pd.DataFrame] = {}\n",
    "for median_rank_file in ranks_folder.glob(\"*median_ranks*\"):\n",
    "    median_ranks_df = pd.read_csv(median_rank_file, sep=\"\\t\")\n",
    "\n",
    "    filename = median_rank_file.stem\n",
    "    cell_type = filename.replace(\"merge_samplings_\", \"\").replace(\n",
    "        \"_feature_set_median_ranks\", \"\"\n",
    "    )\n",
    "    results[cell_type] = median_ranks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_assay_level: Dict[str, pd.DataFrame] = {}\n",
    "for median_rank_file in ranks_folder_assay_level.glob(\"*median_ranks*\"):\n",
    "    median_ranks_df = pd.read_csv(median_rank_file, sep=\"\\t\")\n",
    "\n",
    "    name = median_rank_file.stem.replace(\"_feature_set_median_ranks\", \"\")\n",
    "    results_assay_level[name] = median_ranks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph median ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_median_ranks(\n",
    "    ranks: Dict[str, pd.DataFrame],\n",
    "    colors: Dict[str, str],\n",
    "    logdir: Path,\n",
    "    name: str = \"non-unique features\",\n",
    ") -> None:\n",
    "    \"\"\"Graphs the average median rank for important features in each cell type.\n",
    "\n",
    "    Args:\n",
    "        results Dict[str, pd.DataFrame]: Dictionary of cell type to DataFrame, containing median ranks for each feature and cell type subset.\n",
    "        colors (Dict[str, str]): Dictionary of cell type to color.\n",
    "        logdir (Path): Directory to save the figure.\n",
    "        name (str): Name of the global results set. Default is \"non-unique features\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Get the number of features for each cell type\n",
    "    set_size: Dict[str, int] = {\n",
    "        ct: len([col for col in df.columns if \"med\" in col]) for ct, df in ranks.items()\n",
    "    }\n",
    "\n",
    "    if len(ranks) > 16:\n",
    "        inner_trace_order = list(ranks.values())[0][[\"Assay\", \"CellType\"]]\n",
    "        inner_trace_order = inner_trace_order.apply(\"_\".join, axis=1)\n",
    "        if not all(\n",
    "            inner_trace_order.equals(df[[\"Assay\", \"CellType\"]].apply(\"_\".join, axis=1))\n",
    "            for df in ranks.values()\n",
    "        ):\n",
    "            raise ValueError(\"Assay+Cell type order is not the same for all dataframes\")\n",
    "\n",
    "    # Sort cell types by number of features\n",
    "    trace_names = []\n",
    "    sorted_set = sorted(set_size.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for i, (set_name, N) in enumerate(sorted_set):\n",
    "        df = ranks[set_name]\n",
    "\n",
    "        # Average median rank for all features, for each cell type\n",
    "        df = df.drop(columns=[col for col in df.columns if \"iqr\" in col])\n",
    "        avg_median_ranks = df.mean(axis=1, numeric_only=True)\n",
    "        ct_order = df[\"CellType\"]\n",
    "\n",
    "        # print(f\"set_name: {set_name}\")\n",
    "        cell_type = set_name if set_name in colors else set_name.split(\"_\", maxsplit=1)[1]\n",
    "\n",
    "        # for x-ticks\n",
    "        trace_name = f\"{set_name} ({N} features)\"\n",
    "        trace_names.append(trace_name)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                x=[i] * len(avg_median_ranks),\n",
    "                y=avg_median_ranks,\n",
    "                name=trace_name,\n",
    "                boxpoints=False,\n",
    "                boxmean=True,\n",
    "                line=dict(color=colors[cell_type]),\n",
    "                showlegend=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        marker_sizes = None\n",
    "        if set_name in colors:\n",
    "            marker_sizes = [6 if name == set_name else 3 for name in ct_order]\n",
    "        else:\n",
    "            marker_sizes = [6 if name == set_name else 3 for name in inner_trace_order]\n",
    "\n",
    "        hovertext = None\n",
    "        if set_name in colors:\n",
    "            hovertext = [f\"{ct}: {val}\" for ct, val in zip(ct_order, avg_median_ranks)]\n",
    "        else:\n",
    "            hovertext = [\n",
    "                f\"{set_name}: {val}\"\n",
    "                for set_name, val in zip(inner_trace_order, avg_median_ranks)\n",
    "            ]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[i - 0.4] * len(avg_median_ranks),\n",
    "                y=avg_median_ranks,\n",
    "                name=trace_name,\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    color=[colors[ct] for ct in ct_order],\n",
    "                    size=marker_sizes,\n",
    "                ),\n",
    "                hoverinfo=\"text\",\n",
    "                hovertext=hovertext,\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Modify integer ticks to cell type names\n",
    "    fig.update_xaxes(tickvals=list(range(len(set_size))), ticktext=trace_names)\n",
    "\n",
    "    width = 800\n",
    "    if len(set_size) > 16:\n",
    "        width = 50 * len(set_size)\n",
    "    fig.update_layout(\n",
    "        title=f\"Average median SHAP rank for each important cell type features<br>({name})\",\n",
    "        xaxis_title=\"Reference cell type\",\n",
    "        yaxis_title=\"Average median SHAP rank\",\n",
    "        height=800,\n",
    "        width=width,\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    figname = f\"global_shap_ranks_{name}\"\n",
    "    fig.write_html(logdir / f\"{figname}.html\")\n",
    "    fig.write_image(logdir / f\"{figname}.png\")\n",
    "    fig.write_image(logdir / f\"{figname}.svg\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_unique_features(ranks: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Return rank columns for unique subset features only.\n",
    "\n",
    "    Args:\n",
    "        ranks (Dict[str, pd.DataFrame]): Dictionary of set name to DataFrame, containing median ranks for each important feature in each set.\n",
    "    \"\"\"\n",
    "    ranks = ranks.copy()\n",
    "\n",
    "    # ignore iqr\n",
    "    for set_name, df in list(ranks.items()):\n",
    "        ranks[set_name] = df.drop(columns=[col for col in df.columns if \"iqr\" in col])\n",
    "\n",
    "    features_by_set = {\n",
    "        set_name: [col for col in df.columns if \"med\" in col]\n",
    "        for set_name, df in results.items()\n",
    "    }\n",
    "    other_features: Dict[str, set[str]] = {}\n",
    "    for main_set in features_by_set:\n",
    "        other_features[main_set] = set()\n",
    "        for set_name, features in features_by_set.items():\n",
    "            if set_name == main_set:\n",
    "                continue\n",
    "            other_features[main_set].update(set(features))\n",
    "\n",
    "    unique_set_features: Dict[str, Set[str]] = {}\n",
    "    for set_name, features in features_by_set.items():\n",
    "        unique_set_features[set_name] = set(features) - other_features[set_name]\n",
    "\n",
    "    unique_features_results: Dict[str, pd.DataFrame] = {}\n",
    "    for set_name, df in ranks.items():\n",
    "        col_to_drop = [\n",
    "            col for col in df.columns if col not in unique_set_features[set_name]\n",
    "        ]\n",
    "        col_to_drop.remove(\"CellType\")\n",
    "        unique_features_results[set_name] = df.drop(columns=col_to_drop)\n",
    "\n",
    "    return unique_features_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph merge_samplings results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_median_ranks(\n",
    "    results, cell_type_colors, logdir=ranks_folder, name=\"non-unique (all) features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_features_results = return_unique_features(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_median_ranks(\n",
    "    results, cell_type_colors, logdir=ranks_folder, name=\"non-unique features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_median_ranks(\n",
    "    unique_features_results, cell_type_colors, logdir=ranks_folder, name=\"unique features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph assay+ct subsets results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-core rows\n",
    "for set_name, df in results_assay_level.items():\n",
    "    results_assay_level[set_name] = df[~df[\"Assay\"].str.contains(\"rna|wgb\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_median_ranks(\n",
    "    results_assay_level,\n",
    "    cell_type_colors,\n",
    "    logdir=ranks_folder_assay_level,\n",
    "    name=\"non-unique (all) features\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_features_results_assay_level = return_unique_features(results_assay_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_median_ranks(\n",
    "#     unique_features_results_assay_level, cell_type_colors, logdir=ranks_folder_assay_level, name=\"unique features\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find 3 most important features (median-wise) per cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mapping_dir = paper_dir / \"data\" / \"regions\" / \"hg38.noy.100kb.bed\"\n",
    "\n",
    "feature_mapping = pd.read_csv(feature_mapping_dir, sep=\"\\t\", header=None)\n",
    "feature_mapping.columns = [\"chrom\", \"start\", \"end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_per_cell_type = []\n",
    "for cell_type, df in results.items():\n",
    "    relevant_row = df[df[\"CellType\"] == cell_type]\n",
    "    relevant_row = relevant_row.drop(columns=[\"CellType\"])\n",
    "\n",
    "    cols_median = [col for col in relevant_row.columns if \"med\" in col]\n",
    "\n",
    "    median_values = pd.Series(\n",
    "        [relevant_row[col].values[0] for col in relevant_row.columns if \"med\" in col]\n",
    "    )\n",
    "    iqr_values = pd.Series(\n",
    "        [relevant_row[col].values[0] for col in relevant_row.columns if \"iqr\" in col]\n",
    "    )\n",
    "\n",
    "    ordered_idx = median_values.argsort()\n",
    "    top_3_idx = ordered_idx[0:3].tolist()\n",
    "\n",
    "    top3_med = median_values[top_3_idx].tolist()\n",
    "    top3_iqr = iqr_values[top_3_idx].to_list()\n",
    "\n",
    "    feature_names = [cols_median[idx].split(\"_\")[1] for idx in top_3_idx]\n",
    "    feature_regions = [feature_mapping.iloc[int(idx_str), :] for idx_str in feature_names]\n",
    "    formatted_regions = [\n",
    "        f\"{chrom}:{start}-{end}\" for chrom, start, end in feature_regions\n",
    "    ]\n",
    "    for feature, med, iqr in zip(formatted_regions, top3_med, top3_iqr):\n",
    "        top3_per_cell_type.append((cell_type, feature, med, iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top3 = pd.DataFrame(\n",
    "    top3_per_cell_type,\n",
    "    columns=[\"cell_type\", \"feature\", \"rank median\", \"rank IQR\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top3.to_csv(ranks_folder / \"top3_median_per_cell_type.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
