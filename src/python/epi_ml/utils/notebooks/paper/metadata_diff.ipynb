{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Workbooks to analyze metadata differences.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Workbooks to analyze metadata differences.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython.display import display\n",
    "\n",
    "from epi_ml.core.metadata import Metadata\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    BIOMATERIAL_TYPE,\n",
    "    CELL_TYPE,\n",
    "    DISEASE,\n",
    "    LIFE_STAGE,\n",
    "    SEX,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_dir = Path().home() / \"Projects/epiclass/output/paper\"\n",
    "\n",
    "paper_meta_dir = paper_dir / \"data\" / \"metadata\"\n",
    "official_metadata_dir = paper_meta_dir / \"official\"\n",
    "\n",
    "table_dir = paper_dir / \"tables\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our training metadata VS official metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata we use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = paper_meta_dir / \"hg38_2023-epiatlas-dfreeze-pospurge-nodup_filterCtl.json\"\n",
    "training_metadata = Metadata(path)\n",
    "files_df = training_metadata.to_df()\n",
    "training_df = files_df.copy(deep=True)\n",
    "\n",
    "# keeping biological samples only, not track types or assays (EpiRR level)\n",
    "training_df = training_df.drop_duplicates(subset=[\"epirr_id_without_version\"])\n",
    "my_epirrs = set(training_df[\"epirr_id_without_version\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevants_cols = [CELL_TYPE, BIOMATERIAL_TYPE, SEX, DISEASE, LIFE_STAGE]\n",
    "training_df = training_df[[\"epirr_id_without_version\"] + relevants_cols]\n",
    "training_df = training_df.set_index(\"epirr_id_without_version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Official metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading version v1.0: https://raw.githubusercontent.com/IHEC/epiATLAS-metadata-harmonization/refs/heads/main/openrefine/v1.0/IHEC_metadata_harmonization.v1.0.extended.csv\n",
      "Downloading version v1.1: https://raw.githubusercontent.com/IHEC/epiATLAS-metadata-harmonization/refs/heads/main/openrefine/v1.1/IHEC_metadata_harmonization.v1.1.extended.csv\n",
      "Downloading version v1.2: https://raw.githubusercontent.com/IHEC/epiATLAS-metadata-harmonization/refs/heads/main/openrefine/v1.2/IHEC_metadata_harmonization.v1.2.extended.csv\n",
      "Downloading version v1.3: https://raw.githubusercontent.com/IHEC/epiATLAS-metadata-harmonization/refs/heads/main/openrefine/v1.3/IHEC_sample_metadata_harmonization.v1.3_extended.csv\n",
      "Downloading version v1.4: https://raw.githubusercontent.com/IHEC/epiATLAS-metadata-harmonization/refs/heads/main/openrefine/v1.4/IHEC_sample_metadata_harmonization.v1.4_extended.csv\n",
      "Downloading version v2.0: https://raw.githubusercontent.com/IHEC/epiATLAS-metadata-harmonization/refs/heads/main/openrefine/v2.0/IHEC_sample_metadata_harmonization.v2.0_extended.csv\n"
     ]
    }
   ],
   "source": [
    "dfs = {}\n",
    "url_template = \"https://raw.githubusercontent.com/IHEC/epiATLAS-metadata-harmonization/refs/heads/main/openrefine/{version}/IHEC_metadata_harmonization.{version}.extended.csv\"\n",
    "for version in [\"v1.0\", \"v1.1\", \"v1.2\", \"v1.3\", \"v1.4\", \"v2.0\"]:\n",
    "    myurl = url_template.format(version=version)\n",
    "\n",
    "    # naming convention changed starting v1.3\n",
    "    if version in [\"v1.3\", \"v1.4\", \"v2.0\"]:\n",
    "        myurl = myurl.replace(\".extended\", \"_extended\").replace(\n",
    "            \"IHEC_metadata\", \"IHEC_sample_metadata\"\n",
    "        )\n",
    "\n",
    "    print(f\"Downloading version {version}: {myurl}\")\n",
    "    try:\n",
    "        # Download the file\n",
    "        response = requests.get(myurl, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for bad responses (4xx, 5xx)\n",
    "\n",
    "        # Load file as a DataFrame\n",
    "        content = response.content\n",
    "        df = pd.read_csv(io.StringIO(content.decode(\"utf-8\")))\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {myurl}: {e}\")\n",
    "\n",
    "    dfs[version] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify dataframes to fit with our metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding epirr_id_without_version column to version v1.0\n",
      "version v1.1 already has epirr_id_without_version column\n",
      "version v1.2 already has epirr_id_without_version column\n",
      "version v1.3 already has epirr_id_without_version column\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20668/3148674733.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"unknown\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version v1.4 already has epirr_id_without_version column\n",
      "version v2.0 already has epirr_id_without_version column\n"
     ]
    }
   ],
   "source": [
    "for v, df in dfs.items():\n",
    "    if \"epirr_id_without_version\" in df.columns:\n",
    "        print(f\"version {v} already has epirr_id_without_version column\")\n",
    "    else:\n",
    "        print(f\"Adding epirr_id_without_version column to version {v}\")\n",
    "        df[\"epirr_id_without_version\"] = df[\"EpiRR\"].str.split(\".\").str[0]\n",
    "\n",
    "    df.fillna(\"unknown\", inplace=True)\n",
    "    df = df.set_index(\"epirr_id_without_version\")\n",
    "    df.to_csv(\n",
    "        official_metadata_dir / f\"IHEC_sample_metadata_harmonization.{v}.extended.csv\"\n",
    "    )\n",
    "\n",
    "    dfs[v] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unknown_ls_epirrs = training_df[(training_df[LIFE_STAGE] == \"unknown\") & (training_df[BIOMATERIAL_TYPE] == \"cell line\")].index.unique()\n",
    "# unknown_ls_epirrs = pd.Series(unknown_ls_epirrs)\n",
    "# unknown_ls_epirrs.to_csv(paper_meta_dir / \"training_metadata_unknown_LS_cell_line.list\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating json of differences (Our metadata VS official v1.1+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_idxs = defaultdict(set)\n",
    "for version in [\"v1.0\", \"v1.1\"]:\n",
    "    meta = dfs[version]\n",
    "    common_epirr = my_epirrs.intersection(meta.index)\n",
    "\n",
    "    # Order by epirr\n",
    "    common_epirr = [epirr for epirr in training_df.index if epirr in common_epirr]\n",
    "    meta = meta.loc[common_epirr, :]\n",
    "    training = training_df.loc[common_epirr, :]\n",
    "    for cat in relevants_cols:\n",
    "        # find idx where value is different\n",
    "        diff = meta[cat] != training[cat]\n",
    "        diff_idxs = diff[diff].index\n",
    "\n",
    "        if not diff_idxs.empty:\n",
    "            problematic_idxs[cat].update(diff_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_changes = {col: {} for col in relevants_cols if col in problematic_idxs}\n",
    "for col in relevants_cols:\n",
    "    cat_idxs = problematic_idxs[col]\n",
    "    for idx in cat_idxs:\n",
    "        values = {\n",
    "            \"training\": training_df.loc[idx, col],\n",
    "            \"v1.0-official\": dfs[\"v1.0\"].loc[idx, col],\n",
    "            \"v1.1-official\": dfs[\"v1.1\"].loc[idx, col],\n",
    "            \"v1.2-official\": dfs[\"v1.2\"].loc[idx, col],\n",
    "            \"v1.3-official\": dfs[\"v1.3\"].loc[idx, col],\n",
    "            \"v1.4-official\": dfs[\"v1.4\"].loc[idx, col],\n",
    "            \"v2.0-official\": dfs[\"v2.0\"].loc[idx, col],\n",
    "        }\n",
    "        all_changes[col][idx] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes in harmonized_sample_ontology_intermediate: 29\n",
      "Changes in harmonized_biomaterial_type: 6\n",
      "Changes in harmonized_donor_sex: 0\n",
      "Changes in harmonized_sample_disease_high: 14\n",
      "Changes in harmonized_donor_life_stage: 0\n"
     ]
    }
   ],
   "source": [
    "for col in relevants_cols:\n",
    "    if col in problematic_idxs:\n",
    "        print(f\"Changes in {col}: {len(problematic_idxs[col])}\")\n",
    "    else:\n",
    "        print(f\"No changes in {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"training_metadata_vs_official.json\"\n",
    "path = paper_meta_dir / filename\n",
    "\n",
    "with open(path, \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(all_changes, f, indent=4, allow_nan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harmonized_sample_ontology_intermediate\n",
      "IHECRE00003725: neural progenitor cell != stem cell derived cell line (v1.0=neural progenitor cell, v1.2=stem cell derived cell line)\n",
      "IHECRE00003726: neural cell != stem cell derived cell line (v1.0=unknown, v1.2=stem cell derived cell line)\n",
      "IHECRE00003728: neural progenitor cell != stem cell derived cell line (v1.0=neural progenitor cell, v1.2=stem cell derived cell line)\n",
      "IHECRE00003729: neural cell != stem cell derived cell line (v1.0=unknown, v1.2=stem cell derived cell line)\n",
      "harmonized_biomaterial_type\n",
      "IHECRE00003724: primary cell != cell line (v1.0=primary cell, v1.2=cell line)\n",
      "IHECRE00003725: primary cell != cell line (v1.0=primary cell, v1.2=cell line)\n",
      "IHECRE00003726: primary cell != cell line (v1.0=primary cell, v1.2=cell line)\n",
      "IHECRE00003727: primary cell != cell line (v1.0=primary cell, v1.2=cell line)\n",
      "IHECRE00003728: primary cell != cell line (v1.0=primary cell, v1.2=cell line)\n",
      "IHECRE00003729: primary cell != cell line (v1.0=primary cell, v1.2=cell line)\n",
      "harmonized_sample_disease_high\n",
      "Unique EpiRRs with changes: 6\n"
     ]
    }
   ],
   "source": [
    "diff_epirrs = set()\n",
    "for cat_label, diff_dict in all_changes.items():\n",
    "    print(cat_label)\n",
    "    for epirr, values_dict in sorted(diff_dict.items()):\n",
    "        training_val = values_dict[\"training\"]\n",
    "        official_val = values_dict[\"v1.1-official\"]\n",
    "        v1_0_val = values_dict[\"v1.0-official\"]\n",
    "        v1_2_val = values_dict[\"v1.2-official\"]\n",
    "        if training_val != official_val:\n",
    "            print(\n",
    "                f\"{epirr}: {training_val} != {official_val} (v1.0={v1_0_val}, v1.2={v1_2_val})\"\n",
    "            )\n",
    "            diff_epirrs.add(epirr)\n",
    "\n",
    "print(f\"Unique EpiRRs with changes: {len(diff_epirrs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_vals = []\n",
    "cols = [\n",
    "    \"epirr\",\n",
    "    \"category\",\n",
    "    \"training\",\n",
    "    \"v1.0-official\",\n",
    "    \"v1.1-official\",\n",
    "    \"v1.2-official\",\n",
    "    \"v1.3-official\",\n",
    "    \"v1.4-official\",\n",
    "    \"v2.0-official\",\n",
    "]\n",
    "for cat_label, diff_dict in all_changes.items():\n",
    "    for epirr, values_dict in sorted(diff_dict.items()):\n",
    "        if epirr in diff_epirrs:\n",
    "            row_vals.append(\n",
    "                [\n",
    "                    epirr,\n",
    "                    cat_label,\n",
    "                    values_dict[\"training\"],\n",
    "                    values_dict[\"v1.0-official\"],\n",
    "                    values_dict[\"v1.1-official\"],\n",
    "                    values_dict[\"v1.2-official\"],\n",
    "                    values_dict[\"v1.3-official\"],\n",
    "                    values_dict[\"v1.4-official\"],\n",
    "                    values_dict[\"v2.0-official\"],\n",
    "                ]\n",
    "            )\n",
    "df = pd.DataFrame(row_vals, columns=cols)\n",
    "df.to_csv(table_dir / \"training_metadata_vs_official_v1.1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: SEX v1.2 = SEX v1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_metadata_dfs = {}\n",
    "for version in [\"v1.1\", \"v1.2\", \"v1.3\"]:\n",
    "    path = (\n",
    "        official_metadata_dir\n",
    "        / f\"IHEC_sample_metadata_harmonization.{version}.extended.csv\"\n",
    "    )\n",
    "    df = pd.read_csv(path, sep=\",\")\n",
    "    official_metadata_dfs[version] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEX = \"harmonized_donor_sex\"\n",
    "sex_mislabels_path = (\n",
    "    official_metadata_dir / \"BadQual-mislabels\" / \"official_Sex_mislabeled.csv\"\n",
    ")\n",
    "sex_mislabels_df = pd.read_csv(sex_mislabels_path, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_epirrs = {}\n",
    "subset_df = sex_mislabels_df\n",
    "for version, df in official_metadata_dfs.items():\n",
    "    relevant_df = df.loc[:, [\"epirr_id_without_version\", SEX]]\n",
    "    subset_df = relevant_df.merge(\n",
    "        subset_df,\n",
    "        left_on=\"epirr_id_without_version\",\n",
    "        right_on=\"EpiRR_no-v\",\n",
    "        how=\"right\",\n",
    "        suffixes=(f\"_{version}\", \"\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = subset_df.drop(\n",
    "    columns=[col for col in subset_df.columns if col.startswith(\"epirr_id\")]\n",
    ")\n",
    "subset_df = subset_df.drop(columns=[SEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (subset_df[f\"{SEX}_v1.3\"] != subset_df[f\"{SEX}_v1.2\"]).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = official_metadata_dfs[\"v1.2\"].merge(\n",
    "    official_metadata_dfs[\"v1.3\"],\n",
    "    on=\"epirr_id_without_version\",\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_v1.2\", \"_v1.3\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (merged_df[f\"{SEX}_v1.3\"] != merged_df[f\"{SEX}_v1.2\"]).sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: How much RNA Unique_raw tracks (unstranded data) in the final training metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20922, 71)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "track_type\n",
       "fc                 5337\n",
       "pval               5337\n",
       "raw                5337\n",
       "Unique_minusRaw    1435\n",
       "Unique_plusRaw     1435\n",
       "ctl_raw             777\n",
       "gembs_neg           572\n",
       "gembs_pos           572\n",
       "Unique_raw          120\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outfile = table_dir / \"experiments_including_unique_raw_files.list\"\n",
    "outfile.unlink(missing_ok=True)\n",
    "outfile.touch()\n",
    "\n",
    "v2_meta_df = files_df\n",
    "print(v2_meta_df.shape)\n",
    "display(v2_meta_df[\"track_type\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique_raw md5sums: 120\n"
     ]
    }
   ],
   "source": [
    "md5_unique_raw = v2_meta_df[v2_meta_df[\"track_type\"] == \"Unique_raw\"][\"md5sum\"].tolist()\n",
    "print(f\"Total Unique_raw md5sums: {len(md5_unique_raw)}\")\n",
    "\n",
    "md5_unique_raw = set(md5_unique_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rabj2301/Projects/epiclass/output/paper/tables/2023-01-epiatlas-freeze/hg38_100kb_all_none_10fold_predictions_harmonized_donor_sex_10fold-binary.csv: 0\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/2023-01-epiatlas-freeze/hg38_100kb_all_none_0blklst_winsorized_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 0\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/2023-01-epiatlas-freeze/hg38_100kb_all_none_0blklst_winsorized_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/2023-01-epiatlas-freeze/hg38_100kb_all_none_0blklst_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 0\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/2023-01-epiatlas-freeze/hg38_100kb_all_none_0blklst_winsorized_10fold_predictions_harmonized_biomaterial_type.csv: 0\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/2023-01-epiatlas-freeze/hg38_100kb_all_none_10fold_predictions_harmonized_biomaterial_type.csv: 0\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/2023-01-epiatlas-freeze/hg38_100kb_all_none_10fold_predictions_assay_epiclass.csv: 0\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/2023-01-epiatlas-freeze/hg38_100kb_all_none_0blklst_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/2023-01-epiatlas-freeze/hg38_100kb_all_none_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 0\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/2023-01-epiatlas-freeze/hg38_100kb_all_none_0blklst_10fold_predictions_harmonized_donor_sex_10fold-binary.csv: 0\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/2023-01-epiatlas-freeze/hg38_100kb_all_none_0blklst_10fold_predictions_harmonized_biomaterial_type.csv: 0\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/2023-01-epiatlas-freeze/hg38_100kb_all_none_0blklst_winsorized_10fold_predictions_harmonized_donor_sex_10fold-binary.csv: 0\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_1kb_random_n30321_none_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_100kb_random_n316_none_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_1kb_random_n303114_none_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_10mb_all_none_1mb_coord_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_10kb_all_none_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_10kb_random_n30321_none_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_10kb_all_none_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_10mb_all_none_1mb_coord_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_regulatory_regions_n303114_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_cpg_topvar_200bp_10kb_coord_n30k_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_gene_regions_100kb_coord_n19864_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_1kb_random_n30321_none_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_100kb_random_n3044_none_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_1mb_all_none_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_cpg_topvar_200bp_10kb_coord_n300k_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_regulatory_regions_n303114_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_cpg_topvar_200bp_10kb_coord_n300k_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_1mb_all_none_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_1kb_random_n303114_none_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_cpg_topvar_200bp_10kb_coord_n30k_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_10kb_random_n30321_none_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_gene_regions_100kb_coord_n19864_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_regulatory_regions_n30321_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_regulatory_regions_n30321_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_100kb_random_n316_none_10fold_predictions_assay_epiclass.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/other_feature_sets/hg38_100kb_random_n3044_none_10fold_predictions_harmonized_sample_ontology_intermediate.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/all_10fold_predictions_MLP.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_harmonized_sample_ontology_intermediate_MLP.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_harmonized_sample_ontology_intermediate_LR.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_harmonized_biomaterial_type_MLP.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_assay_epiclass_7c_MLP.csv: 0\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_harmonized_sample_ontology_intermediate_RF.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_assay_epiclass_11c_MLP.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_assay_epiclass_LinearSVC.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_harmonized_sample_cancer_high_MLP.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_assay_epiclass_LR.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_harmonized_sample_ontology_intermediate_LGBM.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_project_MLP.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_paired_end_MLP.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_harmonized_donor_sex_w-mixed_MLP.csv: 98\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_assay_epiclass_RF.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_harmonized_sample_ontology_intermediate_LinearSVC.csv: 70\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_assay_epiclass_MLP.csv: 120\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_harmonized_donor_life_stage_MLP.csv: 108\n",
      "/home/rabj2301/Projects/epiclass/output/paper/tables/dfreeze_v2/100kb_all_none/10fold_predictions_assay_epiclass_LGBM.csv: 120\n"
     ]
    }
   ],
   "source": [
    "with outfile.open(\"w\", encoding=\"utf-8\") as out:\n",
    "    print(f\"Total Unique_raw md5sums: {len(md5_unique_raw)}\", file=out)\n",
    "    for pred_file in table_dir.rglob(\"*pred*.csv\"):\n",
    "        if any(label in str(pred_file) for label in [\"recount3\", \"encode\"]):\n",
    "            continue\n",
    "        df = pd.read_csv(pred_file, sep=\",\", low_memory=False)\n",
    "\n",
    "        # Get md5sums\n",
    "        try:\n",
    "            md5sums = set(df[\"md5sum\"])\n",
    "        except KeyError:\n",
    "            if isinstance(df.index[0], str) and len(df.index[0]) == 32:\n",
    "                md5sums = set(df.index)\n",
    "            else:\n",
    "                print(f\"Could not find md5sum column in {pred_file}\", file=out)\n",
    "                continue\n",
    "\n",
    "        shared_md5sums = md5sums.intersection(md5_unique_raw)\n",
    "\n",
    "        pred_file_relpath = pred_file.relative_to(table_dir)\n",
    "        print(f\"{pred_file}: {len(shared_md5sums)}\")\n",
    "        print(f\"{pred_file_relpath}: {len(shared_md5sums)}\", file=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diff sex/life stage mislabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and v1.1 are the same for sex and life stage categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_before = \"v1.1\"\n",
    "version_after = \"v1.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_metadata_dfs = {}\n",
    "for version in [version_before, version_after]:\n",
    "    path = (\n",
    "        official_metadata_dir\n",
    "        / f\"IHEC_sample_metadata_harmonization.{version}.extended.csv\"\n",
    "    )\n",
    "    df = pd.read_csv(path, sep=\",\", index_col=0)\n",
    "    official_metadata_dfs[version] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_epirrs = set(official_metadata_dfs[version_before].index).intersection(\n",
    "    set(official_metadata_dfs[version_after].index)\n",
    ")\n",
    "common_epirrs = list(common_epirrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2216\n"
     ]
    }
   ],
   "source": [
    "print(len(common_epirrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_before = official_metadata_dfs[version_before]\n",
    "df_after = official_metadata_dfs[version_after]\n",
    "cell_line_epirrs = set()\n",
    "for df in [df_before, df_after]:\n",
    "    cell_line_epirrs = cell_line_epirrs.union(\n",
    "        set(df[df[\"harmonized_biomaterial_type\"] == \"cell line\"].index)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Category: harmonized_donor_sex --\n",
      "\n",
      "Mislabels\n",
      "harmonized_donor_sex: 23\n",
      "\n",
      "Unknown to known (resolved)\n",
      "harmonized_donor_sex: 314 -> 46\n",
      "Resolved: 268\n",
      "\n",
      "-- Category: harmonized_donor_life_stage --\n",
      "\n",
      "Mislabels\n",
      "harmonized_donor_life_stage: 17\n",
      "\n",
      "Unknown to known (resolved)\n",
      "harmonized_donor_life_stage: 517 -> 113\n",
      "Resolved: 404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info = {}\n",
    "for col in [SEX, LIFE_STAGE]:\n",
    "    print(f\"-- Category: {col} --\\n\")\n",
    "    df_before = official_metadata_dfs[version_before].loc[common_epirrs, :]\n",
    "    df_after = official_metadata_dfs[version_after].loc[common_epirrs, :]\n",
    "\n",
    "    ref_known_epirr = df_before[df_before[col] != \"unknown\"].index\n",
    "\n",
    "    # Which samples had values changed? (mislabels)\n",
    "    print(\"Mislabels\")\n",
    "    values_before = df_before.loc[ref_known_epirr, col]\n",
    "    values_after = df_after.loc[ref_known_epirr, col]\n",
    "\n",
    "    changed = values_before != values_after\n",
    "    changed_epirrs = ref_known_epirr[changed]\n",
    "    if col == LIFE_STAGE:\n",
    "        assert set(changed_epirrs).intersection(cell_line_epirrs) == set()\n",
    "\n",
    "    print(f\"{col}: {len(changed_epirrs)}\\n\")\n",
    "    info[f\"changed_{col}\"] = changed_epirrs\n",
    "\n",
    "    # Which unknown samples were given a value in v1.4?\n",
    "    print(\"Unknown to known (resolved)\")\n",
    "    unknown_before = df_before[df_before[col] == \"unknown\"].index\n",
    "    unknown_after = df_after[df_after[col] == \"unknown\"].index\n",
    "    print(f\"{col}: {len(unknown_before)} -> {len(unknown_after)}\")\n",
    "\n",
    "    resolved = set(unknown_before) - set(unknown_after)\n",
    "    resolved_epirrs = [epirr for epirr in common_epirrs if epirr in resolved]\n",
    "    if col == LIFE_STAGE:\n",
    "        assert set(resolved_epirrs).intersection(cell_line_epirrs) == set()\n",
    "\n",
    "    print(f\"Resolved: {len(resolved_epirrs)}\\n\")\n",
    "    info[f\"resolved_{col}\"] = resolved_epirrs\n",
    "\n",
    "    unresolved = set(unknown_before) & set(unknown_after)\n",
    "    unresolved_epirrs = [epirr for epirr in common_epirrs if epirr in unresolved]\n",
    "    info[f\"unresolved_{col}\"] = unresolved_epirrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = table_dir / \"metadata_diff\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for col in [SEX, LIFE_STAGE]:\n",
    "    vals_before = official_metadata_dfs[version_before].loc[common_epirrs, col]\n",
    "    vals_after = official_metadata_dfs[version_after].loc[common_epirrs, col]\n",
    "    partial_name = f\"{version_before}_to_{version_after}\"\n",
    "    for status_name in [\"changed\", \"resolved\", \"unresolved\"]:\n",
    "        epirrs = info[f\"{status_name}_{col}\"]\n",
    "\n",
    "        status_df = pd.DataFrame(\n",
    "            {\n",
    "                version_before: vals_before.loc[epirrs],\n",
    "                version_after: vals_after.loc[epirrs],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        status_df.to_csv(save_dir / f\"{status_name}_{partial_name}_{col}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding missing uuids/datasets to general cross-val results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_path = (\n",
    "    paper_dir\n",
    "    / \"data\"\n",
    "    / \"training_results\"\n",
    "    / \"hg38_100kb_all_none\"\n",
    "    / \"merged_pred_results_all_2.1_chrY_zscores.csv\"\n",
    ")\n",
    "all_preds_df = pd.read_csv(all_preds_path, low_memory=False)\n",
    "print(all_preds_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_df = all_preds_df[all_preds_df[\"md5sum_encode\"].isnull()]\n",
    "print(all_preds_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_df = all_preds_df[all_preds_df[\"assay_type\"] == \"ChIP-Seq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_chip_uuids = set(chip_df[\"uuid\"])\n",
    "all_our_uuids = set(all_preds_df[\"uuid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_meta_dir = paper_meta_dir / \"official\"\n",
    "\n",
    "official_exp_metadata_path = (\n",
    "    official_meta_dir / \"EpiATLAS_experiment_metadata_11032024.csv\"\n",
    ")\n",
    "official_exp_metadata_df = pd.read_csv(official_exp_metadata_path, low_memory=False)\n",
    "print(official_exp_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_all_uuids = set(official_exp_metadata_df[\"uuid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_uuids = sorted(official_all_uuids - all_our_uuids)\n",
    "print(len(diff_uuids))\n",
    "filename = \"uuids_diff_hg38_2023-epiatlas-dfreeze-pospurge-nodup_filterCtl_VS_EpiATLAS_experiment_metadata_11032024.list\"\n",
    "pd.DataFrame(diff_uuids).to_csv(official_meta_dir / filename, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = official_exp_metadata_df[\"uuid\"].isin(diff_uuids)\n",
    "for col in [\"data_generating_centre\", \"experiment_type\", \"assay_type\"]:\n",
    "    display(official_exp_metadata_df[mask][col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_epirrs_path = official_meta_dir / \"pruned_wbgs_1.1_epirr.list\"\n",
    "pruned_epirrs = pd.read_csv(pruned_epirrs_path, header=None)[0].tolist()\n",
    "pruned_uuids = set(\n",
    "    official_exp_metadata_df[\n",
    "        official_exp_metadata_df[\"epirr_id_without_version\"].isin(pruned_epirrs)\n",
    "    ][\"uuid\"]\n",
    ")\n",
    "assert len(pruned_uuids) == len(pruned_epirrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_uuids = sorted(official_all_uuids - all_our_uuids - pruned_uuids)\n",
    "filename = \"uuids_diff_no_pruned_wgbs_hg38_2023-epiatlas-dfreeze-pospurge-nodup_filterCtl_VS_EpiATLAS_experiment_metadata_11032024.list\"\n",
    "pd.DataFrame(diff_uuids).to_csv(official_meta_dir / filename, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = official_exp_metadata_df[\"uuid\"].isin(diff_uuids)\n",
    "for col in [\"data_generating_centre\", \"experiment_type\", \"assay_type\"]:\n",
    "    display(official_exp_metadata_df[mask][col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"diff_no_pruned_wgbs_hg38_2023-epiatlas-dfreeze-pospurge-nodup_filterCtl_VS_EpiATLAS_experiment_metadata_11032024.csv\"\n",
    "\n",
    "diff_uuids = sorted(official_all_uuids - all_our_uuids - pruned_uuids)\n",
    "mask = official_exp_metadata_df[\"uuid\"].isin(diff_uuids)\n",
    "\n",
    "missing_files_df = official_exp_metadata_df[mask]\n",
    "print(missing_files_df.shape)\n",
    "\n",
    "missing_files_df.to_csv(official_meta_dir / filename, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_sample_meta = (\n",
    "    official_meta_dir / \"IHEC_sample_metadata_harmonization.v1.4.extended.csv\"\n",
    ")\n",
    "official_sample_meta_df = pd.read_csv(official_sample_meta, low_memory=False)\n",
    "print(official_sample_meta_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_files_df = missing_files_df.merge(\n",
    "    official_sample_meta_df, how=\"left\", on=\"epirr_id_without_version\"\n",
    ")\n",
    "print(missing_files_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_files_df[\"experiment_type\"].value_counts(dropna=False)\n",
    "missing_files_df[ASSAY] = missing_files_df[\"experiment_type\"].replace(\n",
    "    {\n",
    "        \"standard\": \"wgbs-standard\",\n",
    "        \"PBAT\": \"wgbs-pbat\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = set(missing_files_df.columns) & set(all_preds_df.columns)\n",
    "print(len(common_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding missing datasets info.\n",
    "preds_w_missing_df = pd.concat(\n",
    "    [all_preds_df, missing_files_df[list(common_cols)]],\n",
    "    axis=0,\n",
    "    join=\"outer\",\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "if preds_w_missing_df.shape[0] != all_preds_df.shape[0] + missing_files_df.shape[0]:\n",
    "    raise ValueError(\"Merge failed, expected concatenation of rows\")\n",
    "if preds_w_missing_df.shape[1] != all_preds_df.shape[1]:\n",
    "    raise ValueError(\"Merge failed, expected no new columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_v2_path = str(all_preds_path).replace(\".csv\", \"_with_missing.csv\")\n",
    "preds_w_missing_df.to_csv(preds_v2_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass_py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
