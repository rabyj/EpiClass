{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to analyse Chip-Atlas predictions, destined for the paper.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches, pointless-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import ASSAY_ORDER\n",
    "\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSAY_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_pred_path = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"C-A\"\n",
    "    / \"CA_metadata_4DB+all_pred_subset.20240606.tsv\"\n",
    ")\n",
    "ca_pred_df = pd.read_csv(ca_pred_path, sep=\"\\t\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Assay | Exp Key                               | Nb Files | Training Size | Oversampling | Expected Nb Files                      |\n",
    "|-------|---------------------------------------|----------|---------------|--------------|---------------------------------------|\n",
    "| 13c   | dd3710b73c0341af85a17ce1998362d0      | 24989    | 116550        | true         | 24989                                 |\n",
    "| 11c   | 0f8e5eb996114868a17057bebe64f87c      | 20922    | 46128         | true         | 20922                                 |\n",
    "| 7c    | 69488630801b4a05a53b5d9e572f0aaa      | 16788    | 34413         | true         | 16788 (contre-vérifié)                |\n",
    "\n",
    "*using hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_pred_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_assays = ASSAY_ORDER[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_cols = [\"GEO_mod\", \"C-A\", \"Cistrome\", \"NGS_mod\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ca_pred_df.shape)\n",
    "min_pred = 0.8\n",
    "ca_pred_df = ca_pred_df[\n",
    "    (ca_pred_df[\"Max_pred_assay13\"].astype(float) > min_pred)\n",
    "    | (ca_pred_df[\"Max_pred_assay7\"].astype(float) > min_pred)\n",
    "]\n",
    "print(ca_pred_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_high_level_pred_info(df: pd.DataFrame) -> None:\n",
    "    \"\"\"High level information about the predictions.\"\"\"\n",
    "    for assay in core_assays:\n",
    "        print(f\"{assay}\")\n",
    "        assay_df = df[df[\"manual_target_consensus\"] == assay]\n",
    "        for col in [\n",
    "            \"Predicted_class_assay7\",\n",
    "            \"Predicted_class_assay11\",\n",
    "            \"Predicted_class_assay13\",\n",
    "        ]:\n",
    "            assay_number = col.rsplit(\"_\", maxsplit=1)[-1]\n",
    "            display(assay_df[col].value_counts() / len(assay_df) * 100)\n",
    "            if any(label in col for label in [\"11\", \"13\"]):\n",
    "                wrong_pred = assay_df[assay_df[col] != assay]\n",
    "\n",
    "                display(\n",
    "                    wrong_pred[f\"2nd_pred_class_{assay_number}\"].value_counts()\n",
    "                    / len(wrong_pred)\n",
    "                    * 100\n",
    "                )\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"What is the actual target when wgbs-standard is predicted?\")\n",
    "    for assay_number in [\"assay11\", \"assay13\"]:\n",
    "        print(f\"{assay_number}\")\n",
    "        wgbs_dist = ca_pred_df[\n",
    "            ca_pred_df[f\"Predicted_class_{assay_number}\"] == \"wgbs-standard\"\n",
    "        ][\"manual_target_consensus\"]\n",
    "        display(wgbs_dist.value_counts())\n",
    "        display(wgbs_dist.value_counts() / len(wgbs_dist) * 100)\n",
    "\n",
    "    print(\"What is the actual target when non-core is predicted?\")\n",
    "    col = \"Predicted_class_assay13\"\n",
    "    wgbs_dist = ca_pred_df[ca_pred_df[col] == \"non-core\"][\"manual_target_consensus\"]\n",
    "    display(wgbs_dist.value_counts())\n",
    "    display(wgbs_dist.value_counts() / len(wgbs_dist) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_high_level_pred_info(ca_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_pred_df[\"manual_target_consensus\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_core_labels = [\"no_consensus\", \"non-core\", \"CTCF\"]\n",
    "ca_core_df = ca_pred_df[~ca_pred_df.isin(non_core_labels)]\n",
    "ca_core_df = ca_core_df.dropna(subset=[\"manual_target_consensus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_core_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_consensus_df = ca_pred_df[ca_pred_df[\"manual_target_consensus\"] == \"no_consensus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_consensus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_pred in [0.6, 0.8]:\n",
    "    # for min_pred in np.arange(0.6, 1.05, 0.05):\n",
    "    break_tie_mask = no_consensus_df[\"Max_pred_assay7\"] >= min_pred\n",
    "    nb_break_tie = break_tie_mask.sum()\n",
    "    print(\n",
    "        f\"Break no_consensus (minPred >= {min_pred:.02f}): {nb_break_tie/ len(no_consensus_df) * 100:.02f}% ({nb_break_tie}/{len(no_consensus_df)})\"\n",
    "    )\n",
    "    df = no_consensus_df[break_tie_mask]\n",
    "    # display(df[no_consensus_df.columns[2:10]])\n",
    "    display(df.value_counts(\"Predicted_class_assay7\"))\n",
    "    nb_not_input = (df[\"Predicted_class_assay7\"] != \"input\").sum()\n",
    "    print(\n",
    "        f\"non-input tie breakers: {nb_not_input}/{nb_break_tie} ({nb_not_input/len(df) * 100:.02f}%\\n\"\n",
    "    )\n",
    "    print(df[\"ENCODE\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Keeping only core 7.\n",
    "\n",
    "WITH ENCODE\n",
    "\n",
    "Nb pred assay7 (pred score >= 0.60): 93.41% (41323/44236) VS 89% of total predictions (including non-core7)  \n",
    "Nb match assay7: 96.08% (39704/41323)  \n",
    "Nb error assay7: 3.92% (1619/41323)  \n",
    "\n",
    "Of pred score >= 0.60:\n",
    "- Predictions as input: 39.36% (16266/41323)  \n",
    "- Correct predictions as input: 93.08% (15141/16266) OR 36.64% (15141/41323)  \n",
    "- Incorrect predictions as input: 6.92% (1125/16266) OR 2.72% (1125/41323) OR 69.49% (1125/1619)  \n",
    "\n",
    "NO ENCODE  \n",
    "\n",
    "Nb pred assay7 (pred score >= 0.60): 92.70%  \n",
    "Nb match assay7: 95.61% (33303/34832)  \n",
    "Nb error assay7: 4.39% (1529/34832)  \n",
    "\n",
    "Of pred score >= 0.60:  \n",
    "- Predictions as input: 40.61% (14145/34832)  \n",
    "- Correct predictions as input: 92.60% (13098/14145) OR 37.60% (13098/34832)\n",
    "- Incorrect predictions as input: 7.40% (1047/14145) OR 3.01% (1047/34832) OR 68.48% (1047/1529)\n",
    "\n",
    "BREAK CONSENSUS (does not contain any ENCODE data)  \n",
    "\n",
    "- Break no_consensus (minPred >= 0.60): 92.59% (462/499) & non-input 189/462 (40.91%)  \n",
    "- Break no_consensus (minPred >= 0.80): 80.56% (402/499) & non-input 163/402 (40.55%)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_core_df[\"ENCODE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ca_core_df = ca_core_df[ca_core_df[\"ENCODE\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_pred in np.arange(0, 1.05, 0.05):\n",
    "    nb_pred = (ca_core_df[\"Max_pred_assay7\"] >= min_pred).sum()\n",
    "    print(\n",
    "        f\"Nb pred assay7 (pred score >= {min_pred:.02f}): {nb_pred/ len(ca_core_df):.2%}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"fig_C-A\"\n",
    "for min_pred in [0.6, 0.8, 0.9]:\n",
    "    print(\"Min pred score:\", min_pred)\n",
    "    sub_df = ca_core_df[ca_core_df[\"Max_pred_assay7\"] >= min_pred]\n",
    "    match_consensus = (\n",
    "        sub_df[\"manual_target_consensus\"] == sub_df[\"Predicted_class_assay7\"]\n",
    "    )\n",
    "    nb_match = match_consensus.sum()\n",
    "    nb_error = (~match_consensus).sum()\n",
    "    print(f\"Nb match assay7: {nb_match/ len(sub_df):.2%} ({nb_match}/{len(sub_df)})\")\n",
    "    print(f\"Nb error assay7: {nb_error/ len(sub_df):.2%} ({nb_error}/{len(sub_df)})\\n\")\n",
    "\n",
    "    correct_pred_df = sub_df[match_consensus]\n",
    "    incorrect_pred_df = sub_df[~match_consensus]\n",
    "\n",
    "    confusion_mat = sk_cm(\n",
    "        sub_df[\"manual_target_consensus\"],\n",
    "        sub_df[\"Predicted_class_assay7\"],\n",
    "        labels=core_assays,\n",
    "    )\n",
    "\n",
    "    mat_writer = ConfusionMatrixWriter(labels=core_assays, confusion_matrix=confusion_mat)\n",
    "    name = f\"confusion_matrix_assay7_core7_minPred{min_pred:.02f}\"\n",
    "    if sub_df[\"ENCODE\"].sum() == 0:\n",
    "        name += \"_noENCODE\"\n",
    "    mat_writer.to_all_formats(logdir=fig_dir, name=name)\n",
    "\n",
    "    print(\n",
    "        r\"Following ratios: % of assay subset OR % of all predictions OR % of all incorrect predictions (potential mislabels).\",\n",
    "        \"\\n\",\n",
    "    )\n",
    "    for assay in core_assays:\n",
    "        assay_df = sub_df[sub_df[\"Predicted_class_assay7\"] == assay]\n",
    "        nb_assay = len(assay_df)\n",
    "\n",
    "        nb_assay_correct = len(\n",
    "            correct_pred_df[correct_pred_df[\"Predicted_class_assay7\"] == assay]\n",
    "        )\n",
    "        nb_assay_incorrect = len(\n",
    "            incorrect_pred_df[incorrect_pred_df[\"Predicted_class_assay7\"] == assay]\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Predictions as {assay}: {nb_assay / len(sub_df):.2%} ({nb_assay}/{len(sub_df)})\"\n",
    "        )\n",
    "        perc_cor = nb_assay_correct / nb_assay\n",
    "        perc_cor2 = nb_assay_correct / len(sub_df)\n",
    "        perc_inc = nb_assay_incorrect / nb_assay\n",
    "        perc_inc2 = nb_assay_incorrect / len(sub_df)\n",
    "        perc_inc3 = nb_assay_incorrect / len(incorrect_pred_df)\n",
    "\n",
    "        print(\n",
    "            f\"Correct predictions as {assay}: {perc_cor:.2%} ({nb_assay_correct}/{nb_assay}) OR {perc_cor2:.2%} ({nb_assay_correct}/{len(sub_df)})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Incorrect predictions as {assay}: \"\n",
    "            f\"{perc_inc:.2%} ({nb_assay_incorrect}/{nb_assay}) OR \"\n",
    "            f\"{perc_inc2:.2%} ({nb_assay_incorrect}/{len(sub_df)}) OR \"\n",
    "            f\"{perc_inc3:.2%} ({nb_assay_incorrect}/{len(incorrect_pred_df)})\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_core_df.groupby(db_cols + [\"manual_target_consensus\", \"Predicted_class_assay7\"]).size()\n",
    "\n",
    "reference_column = \"manual_target_consensus\"\n",
    "columns_to_check = db_cols\n",
    "ca_core_df[\"manual_target_consensus_size\"] = (\n",
    "    ca_core_df[columns_to_check].eq(ca_core_df[reference_column], axis=0)\n",
    ").sum(axis=1)\n",
    "\n",
    "for col in [\"manual_target_consensus\", \"manual_target_consensus_size\"]:\n",
    "    print(ca_core_df[col].value_counts(dropna=False))\n",
    "\n",
    "wut = ca_core_df[ca_core_df[\"manual_target_consensus_size\"] == 0]\n",
    "display(wut)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
