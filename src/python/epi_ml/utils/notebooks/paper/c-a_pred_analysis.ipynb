{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to analyse Chip-Atlas predictions, destined for the paper.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches, pointless-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import upsetplot\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY,\n",
    "    ASSAY_ORDER,\n",
    "    IHECColorMap,\n",
    "    display_perc,\n",
    ")\n",
    "\n",
    "# import plotly.express as px\n",
    "# from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSAY_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_dir = base_data_dir / \"training_results\" / \"predictions\" / \"C-A\" / \"assay_epiclass\"\n",
    "\n",
    "# mod2 = add 24NA to 1rst/2nd_prob_diff_cancer, 1rst/2nd_prob_ratio_cancer, 1rst/2nd_prob_diff_disease et 1rst/2nd_prob_ratio_disease\n",
    "# for samples that have column shift leading to Max_pred_donorlife (and others) having incoherent/string values\n",
    "ca_filename = \"CA_metadata_4DB+all_pred.20240606_mod2.1.tsv\"\n",
    "ca_pred_path = ca_dir / ca_filename\n",
    "ca_pred_df = pd.read_csv(ca_pred_path, sep=\"\\t\", low_memory=False)\n",
    "print(ca_pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Assay | Exp Key                               | Nb Files | Training Size | Oversampling | Expected Nb Files                      |\n",
    "|-------|---------------------------------------|----------|---------------|--------------|---------------------------------------|\n",
    "| 13c   | dd3710b73c0341af85a17ce1998362d0      | 24989    | 116550        | true         | 24989                                 |\n",
    "| 11c   | 0f8e5eb996114868a17057bebe64f87c      | 20922    | 46128         | true         | 20922                                 |\n",
    "| 7c    | 69488630801b4a05a53b5d9e572f0aaa      | 16788    | 34413         | true         | 16788 (contre-vérifié)                |\n",
    "\n",
    "*using hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = ca_pred_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_ASSAYS = ASSAY_ORDER[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_COLS = [\"GEO_mod\", \"C-A\", \"Cistrome\", \"NGS_mod\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_COLS = [\n",
    "    \"Predicted_class_assay7\",\n",
    "    \"Predicted_class_assay11\",\n",
    "    \"Predicted_class_assay13\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File correction\n",
    "\n",
    "CA_metadata_4DB+all_pred.20240606.tsv has some mistakes in GEO_mod and manual_target_consensus. Using values in CA_metadata_mod2.tsv to overwrite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_mod_path = ca_dir / \"CA_metadata_mod2.tsv\"\n",
    "ca_mod_df = pd.read_csv(ca_mod_path, sep=\"\\t\", low_memory=False)\n",
    "ca_mod_df.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_pred_df = ca_pred_df.merge(ca_mod_df, on=\"Experimental-id\", how=\"left\")\n",
    "ca_pred_df[[\"manual_target_consensus\", \"GEO_mod\"]] = ca_pred_df[\n",
    "    [\"manual_target_consensus2\", \"GEO_mod2\"]\n",
    "]\n",
    "ca_pred_df = ca_pred_df.drop(columns=[\"manual_target_consensus2\", \"GEO_mod2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_pred_df[\"manual_target_consensus\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercase all targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_pred_df.loc[:, DB_COLS] = (\n",
    "    ca_pred_df[DB_COLS].astype(str).apply(lambda x: x.str.lower())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform all \"revxlinkchromatin\" target into \"input\" so they're not counted as different targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_pred_df.loc[:, DB_COLS] = ca_pred_df[DB_COLS].replace(\"revxlinkchromatin\", \"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking samples with \"ctrl\" target. Definition is ambiguous. Will be treated as \"core\" sample.\n",
    "# display(ca_pred_df[ca_pred_df[DB_COLS].isin([\"ctrl\"]).any(axis=1)][[\"Experimental-id\", \"Gse-geo\", \"GSM\"] + DB_COLS + [\"manual_target_consensus\"] + PRED_COLS].sort_values(\"Gse-geo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct some NGS_mod annotation errors using the file titles (GSE78801). \n",
    "\n",
    "They took h3.3k27m as the target when it is related to the cell line (SF8628 Human DIPG H3.3-K27M Cell Line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replace = {\n",
    "    \"GSM2265634\": \"h3k27me3\",\n",
    "    \"GSM2265635\": \"h3k27me3\",\n",
    "    \"GSM2265642\": \"h3k4me1\",\n",
    "}\n",
    "\n",
    "idx = ca_pred_df[\"GSM\"].isin(to_replace.keys())\n",
    "ca_pred_df.loc[idx, \"NGS_mod\"] = ca_pred_df.loc[idx, \"GSM\"].map(to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ca_pred_df[ca_pred_df.isin([\"h3.3k27m\"])].notna().sum().sum() != 0:\n",
    "    raise ValueError(\"h3.3k27m is still present in the dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining CTCF labels, to exclude some samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier will tend to classify CTCF as input, so we cannot trust it to differentiate between CTCF and input signals.  \n",
    "Likely CTCF samples need to be excluded from the prediction pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_core_labels = [\"non-core\", \"CTCF\"]\n",
    "ca_core_df = ca_pred_df[~ca_pred_df[\"manual_target_consensus\"].isin((non_core_labels))]\n",
    "\n",
    "if ca_core_df[\"manual_target_consensus\"].isna().sum() > 0:\n",
    "    raise ValueError(\"There are missing values in the target column.\")\n",
    "\n",
    "assert ca_pred_df.shape[0] > ca_core_df.shape[0]\n",
    "\n",
    "print(ca_core_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_core_df[\"manual_target_consensus\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    ca_core_df[ca_core_df[DB_COLS].isin([\"ctcf\"]).any(axis=1)][\n",
    "        [\"Experimental-id\", \"Gse-geo\", \"GSM\"]\n",
    "        + DB_COLS\n",
    "        + [\"manual_target_consensus\"]\n",
    "        + PRED_COLS\n",
    "    ].sort_values([\"Gse-geo\", \"GSM\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a review of the experiment descriptions on GEO, it seems GSE102237, GSE108869 and GSE38411 samples marked as CTCF by cistrome are not clearly input (uncertain conclusion), and so should be excluded from our core samples.\n",
    "\n",
    "As for GSE183379 samples marked as ctcf by C-A (7 samples), it seems none of them are actually CTCF, according to the original files names on GEO, so they don't need to be excluded.\n",
    "\n",
    "BUT to not have to explain all this in the main paper, they will be all be excluded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exclude_ctcf = [\"GSM2731525\", \"GSM2731526\", \"GSM3466332\", \"GSM3466333\"]\n",
    "to_include_ctct_mask = (ca_core_df[\"Gse-geo\"] == \"GSE183379\") & (\n",
    "    ca_core_df[\"C-A\"] == \"ctcf\"\n",
    ")\n",
    "assert to_include_ctct_mask.sum() == 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base dataset used: Chip-Atlas experiments where at least one of the BD declared the target in core7.\n",
    "\n",
    "Excluding: Samples where at least one the BG declared a target out of core7. (except for GSE183379, which seems to have some C-A annotation error.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_target_info(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Print information about the targets.\"\"\"\n",
    "    assay_count = df[\"manual_target_consensus\"].value_counts(dropna=False)\n",
    "    display(assay_count)\n",
    "    print(\"Size of the dataset: \", len(df))\n",
    "    display_perc(assay_count / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_pred_df[\"GEO_mod\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for db_col in DB_COLS:\n",
    "    col = ca_core_df[db_col]\n",
    "    if col.isna().sum():\n",
    "        print(\"Missing values: \", ca_core_df[col.isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_target_info(ca_core_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_consensus_df = ca_core_df[ca_core_df[\"manual_target_consensus\"] == \"no_consensus\"]\n",
    "# for db_col in DB_COLS:\n",
    "#     display(no_consensus_df[db_col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### manual_target_consensus / DBs target details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new category called \"identical_DBs_target\" that specifies when, depending available sources, DBs give the same assay/target\n",
    "\n",
    "Treat \"Unclassified\" from Chip-Atlas as absent samples for the target consensus evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAME_TARGET = \"core7_DBs_consensus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_db_target = []\n",
    "unique_labels = Counter()\n",
    "different_labels = Counter()\n",
    "\n",
    "tmp_df = ca_core_df.loc[:, DB_COLS].copy()\n",
    "tmp_df[\"C-A\"].replace(\"unclassified\", \"----\", inplace=True)\n",
    "\n",
    "for labels in tmp_df.values:\n",
    "    missing_N = sum(label == \"----\" for label in labels)\n",
    "    db_labels = set(labels)\n",
    "\n",
    "    try:\n",
    "        db_labels.remove(\"----\")\n",
    "    except KeyError:\n",
    "        pass\n",
    "    if any(label not in CORE_ASSAYS + [\"ctrl\"] for label in db_labels):\n",
    "        id_db_target.append(\"Ignored - Potential non-core\")\n",
    "    elif missing_N == 3:\n",
    "        id_db_target.append(\"1 source\")\n",
    "    elif len(db_labels) == 1:\n",
    "        id_db_target.append(\"Identical\")\n",
    "    else:\n",
    "        id_db_target.append(\"Different\")\n",
    "        different_labels[tuple(db_labels)] += 1\n",
    "\n",
    "    unique_labels[tuple(db_labels)] += 1\n",
    "\n",
    "ca_core_df.loc[:, SAME_TARGET] = id_db_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ca pred 2.2 / with \"identical_DBs_target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new metadata that includes a column for target_consensus description overlap\n",
    "\n",
    "# unmodified_ca_metadata = pd.read_csv(ca_dir / \"CA_metadata_4DB+all_pred.20240606_mod2.1.tsv\", sep=\"\\t\", low_memory=False)\n",
    "# new_meta = ca_core_df[[\"Experimental-id\", SAME_TARGET]].merge(unmodified_ca_metadata, on=\"Experimental-id\", how=\"right\")\n",
    "# new_meta.loc[new_meta[SAME_TARGET].isna(), SAME_TARGET] = \"non-core/CTCF\"\n",
    "# assert new_meta[SAME_TARGET].isna().sum() == 0\n",
    "\n",
    "# display(new_meta[SAME_TARGET].value_counts(dropna=False))\n",
    "\n",
    "# new_path = ca_pred_path.parent / ca_pred_path.name.replace(\"mod2.1.tsv\", \"mod2.2.tsv\")\n",
    "# new_meta.to_csv(new_path, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for labels, count in h3_labels.most_common():\n",
    "#     print(\"\\t\".join([labels])+f\"\\t{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for labels, count in unique_labels.items():\n",
    "#     if len(labels) > 1 and \"input\" in labels:\n",
    "#         print(labels, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_DB_count = ca_core_df[SAME_TARGET].value_counts(dropna=False)\n",
    "# display(target_DB_count)\n",
    "# print()\n",
    "# display_perc(target_DB_count / target_DB_count.sum() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pd.option_context(\"display.max_rows\", None):\n",
    "#     display(ca_core_df[ca_core_df[SAME_TARGET] == \"Different\"][DB_COLS].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ca_core_df[SAME_TARGET].value_counts(dropna=False))\n",
    "print(f\"Total core7 samples: {ca_core_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = ca_dir / (\n",
    "    str(Path(ca_filename).stem) + \"_core7_DB_not_identical_target_counts.tsv\"\n",
    ")\n",
    "\n",
    "# ca_core_df[ca_core_df[SAME_TARGET] != \"Identical\"][\n",
    "#     DB_COLS + [\"manual_target_consensus\", SAME_TARGET]\n",
    "# ].value_counts(dropna=False).to_csv(outpath, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upset plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"fig_C-A\" / \"DB_upset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_db_upsetplot(\n",
    "    df: pd.DataFrame, db_cols: List[str], title: str\n",
    ") -> upsetplot.UpSet:\n",
    "    \"\"\"Make an upsetplot of the sample presence in the different databases.\"\"\"\n",
    "    df = df.copy()\n",
    "    if SAME_TARGET not in df.columns:\n",
    "        raise ValueError(\"Column 'identical_DBs_target' not found in DataFrame.\")\n",
    "\n",
    "    # Create a new DataFrame with boolean columns for each database\n",
    "    upset_df = pd.DataFrame()\n",
    "    for col in db_cols:\n",
    "        upset_df[col] = df[col] != \"----\"\n",
    "    upset_df[SAME_TARGET] = df[SAME_TARGET]\n",
    "\n",
    "    # Set the index for the UpSet plot\n",
    "    upset_df = upset_df.set_index(db_cols)\n",
    "\n",
    "    # Create the UpSet plot\n",
    "    upset = upsetplot.UpSet(\n",
    "        upset_df,\n",
    "        intersection_plot_elements=0,  # disable the default bar chart\n",
    "        sort_by=\"cardinality\",\n",
    "        show_counts=True,\n",
    "        orientation=\"horizontal\",\n",
    "    )\n",
    "\n",
    "    # Add stacked bars\n",
    "    upset.add_stacked_bars(by=SAME_TARGET, elements=15)\n",
    "\n",
    "    # Plot and set title\n",
    "    axes = upset.plot()\n",
    "    plt.suptitle(title)\n",
    "    axes[\"totals\"].set_title(\"Total\")\n",
    "    plt.legend(loc=\"center left\")\n",
    "    return upset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"All core7 ChIP-Atlas samples presence in used DBs\\nTarget consensus\"\n",
    "upset = make_db_upsetplot(ca_core_df, DB_COLS, title=title)\n",
    "# plt.savefig(fig_dir / \"upsetplot_DB_core7_samples.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ca_core_df[\"is_EpiAtlas_EpiRR\"] != \"0\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no encode\n",
    "no_encode_df = ca_core_df[ca_core_df[\"ENCODE\"] == 0]\n",
    "title = \"ChIP-Atlas samples presence in used DBs\\nTarget Consensus - No ENCODE\"\n",
    "upset = make_db_upsetplot(no_encode_df, DB_COLS, title=title)\n",
    "# plt.savefig(fig_dir / \"upsetplot_DB_core7_samples_noENC.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yes encode, no epiatlas\n",
    "no_epiatlas_df = ca_core_df[ca_core_df[\"is_EpiAtlas_EpiRR\"] == \"0\"]\n",
    "title = \"ChIP-Atlas core 7 samples presence in used DBs\\nTarget Consensus - No EpiAtlas overlap\"\n",
    "upset = make_db_upsetplot(no_epiatlas_df, DB_COLS, title=title)\n",
    "# plt.savefig(fig_dir / \"upsetplot_DB_core7_samples_noEpiAtlas.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prediction_resolved(row, pred_col: str, db_cols: List[str]) -> bool:\n",
    "    \"\"\"Check if the prediction matches any of the database columns.\"\"\"\n",
    "    pred_val = row[pred_col]\n",
    "    db_vals = [row[col] for col in db_cols]\n",
    "    return pred_val in db_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier was able to resolve xx% of the cases where the target was not identical between the sources\n",
    "different_targets_df = ca_core_df[ca_core_df[SAME_TARGET] == \"Different\"]\n",
    "\n",
    "for min_pred_score in [0, 0.6]:\n",
    "    filtered_df = different_targets_df[\n",
    "        different_targets_df[\"Max_pred_assay7\"] >= min_pred_score\n",
    "    ]\n",
    "\n",
    "    pred_col = PRED_COLS[0]\n",
    "\n",
    "    num_resolved = filtered_df.apply(\n",
    "        is_prediction_resolved, axis=1, args=(pred_col, DB_COLS)\n",
    "    ).sum()\n",
    "\n",
    "    print(\n",
    "        f\"Resolved (min_predScore >= {min_pred_score}): \"\n",
    "        f\"{num_resolved} / {len(filtered_df)} \"\n",
    "        f\"({num_resolved / len(filtered_df) * 100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    # Exclude rows where the prediction is labeled as 'input'\n",
    "    non_input_df = filtered_df[filtered_df[PRED_COLS[0]] != \"input\"]\n",
    "    num_resolved = non_input_df.apply(\n",
    "        is_prediction_resolved, axis=1, args=(pred_col, DB_COLS)\n",
    "    ).sum()\n",
    "\n",
    "    print(\n",
    "        f\"Resolved (min_predScore >= {min_pred_score}, excluding 'input' predictions): \"\n",
    "        f\"{num_resolved} / {len(non_input_df)} \"\n",
    "        f\"({num_resolved / len(non_input_df) * 100:.2f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level prediction accuracy breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_high_level_pred_info(df: pd.DataFrame) -> None:\n",
    "    \"\"\"High level information about the predictions.\"\"\"\n",
    "    for assay in CORE_ASSAYS:\n",
    "        print(f\"{assay}\")\n",
    "        assay_df = df[df[\"manual_target_consensus\"] == assay]\n",
    "        for col in [\n",
    "            \"Predicted_class_assay7\",\n",
    "            \"Predicted_class_assay11\",\n",
    "            \"Predicted_class_assay13\",\n",
    "        ]:\n",
    "            assay_number = col.rsplit(\"_\", maxsplit=1)[-1]\n",
    "            display(assay_df[col].value_counts() / len(assay_df) * 100)\n",
    "            if any(label in col for label in [\"11\", \"13\"]):\n",
    "                wrong_pred = assay_df[assay_df[col] != assay]\n",
    "\n",
    "                display(\n",
    "                    wrong_pred[f\"2nd_pred_class_{assay_number}\"].value_counts()\n",
    "                    / len(wrong_pred)\n",
    "                    * 100\n",
    "                )\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"What is the actual target when wgbs-standard is predicted?\")\n",
    "    for assay_number in [\"assay11\", \"assay13\"]:\n",
    "        print(f\"{assay_number}\")\n",
    "        wgbs_dist = ca_pred_df[\n",
    "            ca_pred_df[f\"Predicted_class_{assay_number}\"] == \"wgbs-standard\"\n",
    "        ][\"manual_target_consensus\"]\n",
    "        display(wgbs_dist.value_counts())\n",
    "        display(wgbs_dist.value_counts() / len(wgbs_dist) * 100)\n",
    "\n",
    "    print(\"What is the actual target when non-core is predicted?\")\n",
    "    col = \"Predicted_class_assay13\"\n",
    "    wgbs_dist = ca_pred_df[ca_pred_df[col] == \"non-core\"][\"manual_target_consensus\"]\n",
    "    display(wgbs_dist.value_counts())\n",
    "    display(wgbs_dist.value_counts() / len(wgbs_dist) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_high_level_pred_info(ca_pred_df)\n",
    "# print_target_info(ca_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_pred in [0.6, 0.8]:\n",
    "    # for min_pred in np.arange(0.6, 1.05, 0.05):\n",
    "    break_tie_mask = no_consensus_df[\"Max_pred_assay7\"] >= min_pred\n",
    "    nb_break_tie = break_tie_mask.sum()\n",
    "    print(\n",
    "        f\"Break no_consensus (minPred >= {min_pred:.02f}): {nb_break_tie/ len(no_consensus_df) * 100:.02f}% ({nb_break_tie}/{len(no_consensus_df)})\"\n",
    "    )\n",
    "    df = no_consensus_df[break_tie_mask]\n",
    "    # display(df[no_consensus_df.columns[2:10]])\n",
    "    # display(df.value_counts(\"Predicted_class_assay7\"))\n",
    "    nb_not_input = (df[\"Predicted_class_assay7\"] != \"input\").sum()\n",
    "    print(\n",
    "        f\"non-input tie breakers: {nb_not_input}/{nb_break_tie} ({nb_not_input/len(df) * 100:.02f}%)\\n\"\n",
    "    )\n",
    "    # print(df[\"ENCODE\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_count = ca_core_df[\"ENCODE\"].value_counts()\n",
    "display(enc_count)\n",
    "display_perc(enc_count / len(ca_core_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_within_threshold(\n",
    "    df: pd.DataFrame, min_pred: float = 0.6, col: str = \"Max_pred_assay7\"\n",
    ") -> None:\n",
    "    \"\"\"Print the predictions percentage within a threshold.\"\"\"\n",
    "    try:\n",
    "        mask = df[col].astype(float) >= min_pred\n",
    "    except KeyError:\n",
    "        print(f\"Column {col} not found.\")\n",
    "        return\n",
    "    nb_pred = mask.sum()\n",
    "    print(\n",
    "        f\"Nb pred {col.split('_')[-1]} (pred score >= {min_pred:.02f}): {nb_pred/len(df) * 100:.02f}% ({nb_pred}/{len(df)})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for min_pred in np.arange(0, 1.05, 0.05):\n",
    "#     df = ca_core_df[ca_core_df[\"ENCODE\"] == 0]\n",
    "#     print_pred_within_threshold(df, min_pred=min_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_confusion_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    fig_dir: Path | str,\n",
    "    nb_classes: int | str = 7,\n",
    "    min_pred: float = 0.6,\n",
    "):\n",
    "    \"\"\"Save the confusion matrix for core assays predictions. Does not filter.\"\"\"\n",
    "    col = f\"Predicted_class_assay{nb_classes}\"\n",
    "    cm = sk_cm(df[\"manual_target_consensus\"], df[col])\n",
    "    cm_writer = ConfusionMatrixWriter(labels=CORE_ASSAYS, confusion_matrix=cm)\n",
    "\n",
    "    name = f\"confusion_matrix_assay{nb_classes}_core7_minPred{min_pred:.02f}\"\n",
    "    if df[\"ENCODE\"].sum() == 0:\n",
    "        name += \"_noENCODE\"\n",
    "\n",
    "    cm_writer.to_all_formats(logdir=fig_dir, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_breakdown_predictions(\n",
    "    df: pd.DataFrame, min_pred: float = 0.6, nb_classes: int | str = 7\n",
    ") -> None:\n",
    "    \"\"\"Breakdown the predictions, print results.\"\"\"\n",
    "    df = df[df[f\"Max_pred_assay{nb_classes}\"] >= min_pred]\n",
    "\n",
    "    pred_col = f\"Predicted_class_assay{nb_classes}\"\n",
    "    match_consensus = df[\"manual_target_consensus\"] == df[pred_col]\n",
    "    nb_match = match_consensus.sum()\n",
    "    nb_error = (~match_consensus).sum()\n",
    "    print(f\"Nb match assay{nb_classes}: {nb_match/ len(df):.2%} ({nb_match}/{len(df)})\")\n",
    "    print(f\"Nb error assay{nb_classes}: {nb_error/ len(df):.2%} ({nb_error}/{len(df)})\\n\")\n",
    "\n",
    "    correct_pred_df = df[match_consensus]\n",
    "    incorrect_pred_df = df[~match_consensus]\n",
    "\n",
    "    print(\n",
    "        r\"Following ratios: % of assay subset OR % of all predictions OR % of all incorrect predictions (potential mislabels).\",\n",
    "        \"\\n\",\n",
    "    )\n",
    "    for assay in CORE_ASSAYS:\n",
    "        assay_df = df[df[pred_col] == assay]\n",
    "        nb_assay = len(assay_df)\n",
    "\n",
    "        nb_assay_correct = len(correct_pred_df[correct_pred_df[pred_col] == assay])\n",
    "        nb_assay_incorrect = len(incorrect_pred_df[incorrect_pred_df[pred_col] == assay])\n",
    "\n",
    "        print(f\"Predictions as {assay}: {nb_assay / len(df):.2%} ({nb_assay}/{len(df)})\")\n",
    "        perc_cor = nb_assay_correct / nb_assay\n",
    "        perc_cor2 = nb_assay_correct / len(df)\n",
    "        perc_inc = nb_assay_incorrect / nb_assay\n",
    "        perc_inc2 = nb_assay_incorrect / len(df)\n",
    "        perc_inc3 = nb_assay_incorrect / len(incorrect_pred_df)\n",
    "\n",
    "        print(\n",
    "            f\"Correct predictions as {assay}: {perc_cor:.2%} ({nb_assay_correct}/{nb_assay}) OR {perc_cor2:.2%} ({nb_assay_correct}/{len(df)})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Incorrect predictions as {assay}: \"\n",
    "            f\"{perc_inc:.2%} ({nb_assay_incorrect}/{nb_assay}) OR \"\n",
    "            f\"{perc_inc2:.2%} ({nb_assay_incorrect}/{len(df)}) OR \"\n",
    "            f\"{perc_inc3:.2%} ({nb_assay_incorrect}/{len(incorrect_pred_df)})\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"fig_C-A\" / \"confusion_matrices\"\n",
    "for subset in [[0], [0, 1]]:\n",
    "    if subset == [0]:\n",
    "        print(\"Subset: no ENCODE\")\n",
    "    else:\n",
    "        print(\"Subset: Include ENCODE\")\n",
    "    df = ca_core_df[ca_core_df[\"ENCODE\"].isin(subset)]\n",
    "    for min_pred in [0.6, 0.8, 0.9]:\n",
    "        # print(\"Min pred score:\", min_pred)\n",
    "        # print_pred_within_threshold(df, min_pred=min_pred)\n",
    "        # print_breakdown_predictions(df, min_pred=min_pred)\n",
    "\n",
    "        # df = ca_core_df[ca_core_df[\"Max_pred_assay7\"] >= min_pred]\n",
    "        # save_confusion_matrix(df, fig_dir, min_pred=min_pred)\n",
    "        # print(\"miaw\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mislabels by GSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = [\n",
    "    col for col in df_cols if any(string in col.lower() for string in [\"id\", \"gse\"])\n",
    "]\n",
    "id_cols.remove(\"Gse-title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSE = \"Gse-geo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 7\n",
    "min_pred = 0.6\n",
    "df = df[df[f\"Max_pred_assay{nb_classes}\"] >= min_pred]\n",
    "\n",
    "pred_col = f\"Predicted_class_assay{nb_classes}\"\n",
    "match_consensus = df[\"manual_target_consensus\"] == df[pred_col]\n",
    "nb_match = match_consensus.sum()\n",
    "nb_error = (~match_consensus).sum()\n",
    "print(f\"Nb match assay{nb_classes}: {nb_match/ len(df):.2%} ({nb_match}/{len(df)})\")\n",
    "print(f\"Nb error assay{nb_classes}: {nb_error/ len(df):.2%} ({nb_error}/{len(df)})\\n\")\n",
    "\n",
    "incorrect_pred_df = df[~match_consensus]\n",
    "incorrect_pred_df = incorrect_pred_df[incorrect_pred_df[pred_col] != \"input\"]\n",
    "\n",
    "desired_cols = [\"manual_target_consensus\", pred_col]\n",
    "\n",
    "# with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "#     print(\"Incorrect predictions, breakdown by GSE count.\")\n",
    "#     gse_count = incorrect_pred_df.groupby(GSE).size().sort_values(ascending=False)  # type: ignore\n",
    "#     display(gse_count)\n",
    "#     gse_count.to_csv(\n",
    "#         ca_dir / \"gse_count_incorrect_pred_no_input_20240606_mod2.tsv\", sep=\"\\t\"\n",
    "#     )\n",
    "\n",
    "#     print(\"Incorrect predictions, breakdown by GSE and target.\")\n",
    "#     gse_target_count = incorrect_pred_df.groupby(GSE)[desired_cols].value_counts()\n",
    "#     display(gse_target_count)\n",
    "#     gse_target_count.to_csv(\n",
    "#         ca_dir / \"gse_target_count_incorrect_pred_no_input_20240606_mod2.tsv\", sep=\"\\t\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Keeping only core 7.\n",
    "\n",
    "WITH ENCODE\n",
    "\n",
    "- Nb pred assay7 (pred score >= 0.60): 89.60% (42314/47226)\n",
    "- Nb match assay7: 95.74% (40511/42314)\n",
    "- Nb error assay7: 4.26% (1803/42314)\n",
    "\n",
    "Of pred score >= 0.60:\n",
    "- Predictions as input: 39.32% (16636/42314)\n",
    "- Correct predictions as input: 92.45% (15380/16636) OR 36.35% (15380/42314)\n",
    "- Incorrect predictions as input: 7.55% (1256/16636) OR 2.97% (1256/42314) OR 69.66% (1256/1803)\n",
    "\n",
    "NO ENCODE  \n",
    "\n",
    "- Nb pred assay7 (pred score >= 0.60): 88.56% (35631/40232)\n",
    "- Nb match assay7: 95.24% (33935/35631)\n",
    "- Nb error assay7: 4.76% (1696/35631)\n",
    "\n",
    "Of pred score >= 0.60:\n",
    "- Predictions as input: 40.69% (14499/35631)\n",
    "- Correct predictions as input: 91.96% (13334/14499) OR 37.42% (13334/35631)\n",
    "- Incorrect predictions as input: 8.04% (1165/14499) OR 3.27% (1165/35631) OR 68.69% (1165/1696)\n",
    "\n",
    "BREAK CONSENSUS (does not contain any ENCODE data)\n",
    "\n",
    "- Break no_consensus (minPred >= 0.60): 92.59% (462/499) & non-input 189/462 (40.91%)  \n",
    "- Break no_consensus (minPred >= 0.80): 76.57% (402/525) & non-input 163/402 (40.55%)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying consensus criterion (nb DB agreeing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ca_core_df.groupby(DB_COLS + [\"manual_target_consensus\", \"Predicted_class_assay7\"]).size()\n",
    "\n",
    "reference_column = \"manual_target_consensus\"\n",
    "columns_to_check = DB_COLS\n",
    "ca_core_df[\"manual_target_consensus_size\"] = (\n",
    "    ca_core_df[columns_to_check].eq(ca_core_df[reference_column], axis=0)\n",
    ").sum(axis=1)\n",
    "\n",
    "for col in [\"manual_target_consensus\", \"manual_target_consensus_size\"]:\n",
    "    val_count = ca_core_df[col].value_counts(dropna=False).sort_index()\n",
    "    display(val_count)\n",
    "    display_perc(val_count / len(ca_core_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Global:\")\n",
    "print_pred_within_threshold(ca_core_df, min_pred=0.6)\n",
    "print()\n",
    "print_breakdown_predictions(ca_core_df, min_pred=0.6, nb_classes=7)\n",
    "sub_df = ca_core_df[ca_core_df[\"Max_pred_assay7\"] >= 0.6]\n",
    "assay_count = sub_df[\"manual_target_consensus\"].value_counts().sort_index()\n",
    "# display_perc(assay_count / len(sub_df))\n",
    "\n",
    "\n",
    "for i in range(1, 5):\n",
    "    df = ca_core_df[ca_core_df[\"manual_target_consensus_size\"] == i]\n",
    "    print(f\"Consensus defined with {i} DB: {len(df)} files.\")\n",
    "\n",
    "    # Display % assay\n",
    "    sub_df = df[df[\"Max_pred_assay7\"] >= 0.6]\n",
    "    assay_count = df[\"manual_target_consensus\"].value_counts()\n",
    "\n",
    "    # display_perc(assay_count / len(df))\n",
    "\n",
    "    input_val = (assay_count / len(df))[\"input\"]\n",
    "    print(f\"input: {input_val:.2%}\")\n",
    "\n",
    "    print_pred_within_threshold(df, min_pred=0.6)\n",
    "    print()\n",
    "\n",
    "    print_breakdown_predictions(df, min_pred=0.6, nb_classes=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be big differences in accuracy when looking at consensus defined by a different number of DB.  \n",
    "\n",
    "\n",
    "Core 7 files, including ENCODE\n",
    "\n",
    "| Consensus size | Nb files | Nb files (%) | Input size (%) | Accuracy (min_pred > 0.6) |\n",
    "|----------------|----------|--------------|----------------|--------------------|\n",
    "| 1              | 3324     | 6.98%        | 61.85%         | 95.46%             |\n",
    "| 2              | 25551    | 53.81%       | 42.27%         | 95.83%             |\n",
    "| 3              | 10275    | 21.60%       | 37.38%         | 95.36%             |\n",
    "| 4              | 8076     | 17.61%       | 17.62%         | 96.03%             |\n",
    "| global          | 47226    | 100%         | 38.37%         | 95.74%             |\n",
    "\n",
    "\n",
    "For assay7 core7, min_pred 0.6:  \n",
    "\n",
    "Global average\n",
    "- acc: 95.74%\n",
    "- %err=input (lowQual): 69.66%\n",
    "- consensus input = 36.77%\n",
    "\n",
    "1 to 4 DB consensus\n",
    "\n",
    "- acc within [95.36%, 96.03%]\n",
    "- %err=input (lowQual): within [63.43%, 79.39%] = [63.43%, 69.55%, 65.09%, 79.39%]\n",
    "- consensus input = [60.03%, 41.01%, 35.73%, 15.84%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE core no EpiAtlas + Chip-Atlas (no ENCODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_dir = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"predictions\"\n",
    "    / \"encode\"\n",
    "    / \"assay_epiclass_1l_3000n\"\n",
    ")\n",
    "encode_pred_path = encode_dir / \"encode_only-core-7c_predictions.csv\"\n",
    "encode_pred = pd.read_csv(encode_pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_pred[\"Experimental-id\"] = encode_pred[\"md5sum\"]\n",
    "encode_pred[\"Predicted_class_assay7\"] = encode_pred[\"Predicted class\"]\n",
    "encode_pred[\"manual_target_consensus\"] = encode_pred[\"True class\"]\n",
    "encode_pred[\"Max_pred_assay7\"] = encode_pred[\"Max pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pred_df = pd.concat([ca_core_df, encode_pred], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pred_df = global_pred_df[~(global_pred_df[\"ENCODE\"] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(global_pred_df[\"ENCODE\"].value_counts(dropna=False))\n",
    "display(global_pred_df[\"manual_target_consensus\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pred_df = global_pred_df[\n",
    "    global_pred_df[\"manual_target_consensus\"].isin(CORE_ASSAYS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pred = 0.6\n",
    "print_pred_within_threshold(global_pred_df, min_pred=min_pred)\n",
    "print_breakdown_predictions(global_pred_df, min_pred=min_pred, nb_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = base_data_dir / \"training_results\" / \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabels = global_pred_df[\n",
    "    global_pred_df[\"Predicted_class_assay7\"] != global_pred_df[\"manual_target_consensus\"]\n",
    "]\n",
    "# mislabels.to_csv(output_dir / \"mislabels_C-A&ENCODE_assay7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main stats\n",
    "\n",
    "- Nb pred assay7 (pred score >= 0.60): 89.49% (39910/44599)\n",
    "- Nb match assay7: 95.65% (38172/39910)\n",
    "- Nb error assay7: 4.35% (1738/39910)\n",
    "\n",
    "Following ratios: % of assay subset OR % of all predictions OR % of all incorrect predictions (potential mislabels).   \n",
    "\n",
    "- Predictions as input: 40.39% (16119/39910)\n",
    "- Correct predictions as input: 92.75% (14951/16119) OR 37.46% (14951/39910)\n",
    "- Incorrect predictions as input: 7.25% (1168/16119) OR 2.93% (1168/39910) OR 67.20% (1168/1738)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chip-Atlas with ENCODE, excluding intersection of ENCODE and EpiAtlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_metadata_dir = base_data_dir / \"metadata\" / \"encode\"\n",
    "encode_epiatlas_mapping_path = encode_metadata_dir / \"ENCODE_IHEC_keys.tsv\"\n",
    "encode_epiatlas_mapping_df = pd.read_csv(encode_epiatlas_mapping_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_epiatlas_mapping_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ca pred 2.1 / with \"is_EpiAtlas_EpiRR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new metadata that includes a column for EpiAtlas_EpiRR overlap\n",
    "# enc_df = encode_epiatlas_mapping_df[[\"is_EpiAtlas_EpiRR\", \"accession\"]]\n",
    "\n",
    "# current_ca_filename = \"CA_metadata_4DB+all_pred.20240606_mod2.tsv\"\n",
    "# mod2_ca_metadata = pd.read_csv(ca_dir / current_ca_filename, sep=\"\\t\", low_memory=False)\n",
    "# new_pred_df = mod2_ca_metadata.merge(enc_df, left_on=\"ENCODE_GSE\", right_on=\"accession\", how=\"left\").drop_duplicates()\n",
    "\n",
    "# new_pred_df.drop(columns=[\"accession\"], inplace=True)\n",
    "# new_pred_df[\"is_EpiAtlas_EpiRR\"].fillna(0, inplace=True)\n",
    "\n",
    "# assert ca_pred_df.shape[0] == new_pred_df.shape[0]\n",
    "\n",
    "# new_path = ca_dir / current_ca_filename.replace(\"mod2.tsv\", \"mod2.1.tsv\")\n",
    "# new_pred_df.to_csv(ca_dir / new_path, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_exclude_encode = encode_epiatlas_mapping_df[\n",
    "    encode_epiatlas_mapping_df[\"is_EpiAtlas_EpiRR\"].notna()\n",
    "][\"accession\"].to_list()\n",
    "\n",
    "print(len(to_exclude_encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_core_df_no_epiatlas = ca_core_df[~ca_core_df[\"ENCODE_GSE\"].isin(to_exclude_encode)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ca_core_df_no_epiatlas.shape[0] + len(to_exclude_encode) == ca_core_df.shape[0]:\n",
    "    print(\"Mismatch in the number of files.\")\n",
    "    print(\n",
    "        f\"{ca_core_df_no_epiatlas.shape[0]} + {len(to_exclude_encode)} = {ca_core_df_no_epiatlas.shape[0] + len(to_exclude_encode)} != {ca_core_df.shape[0]}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Only {ca_core_df.shape[0] - ca_core_df_no_epiatlas.shape[0]}/{len(to_exclude_encode)} files excluded.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pred = 0.6\n",
    "print_pred_within_threshold(ca_core_df_no_epiatlas, min_pred=min_pred)\n",
    "print_breakdown_predictions(ca_core_df_no_epiatlas, min_pred=min_pred, nb_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_core_df_no_epiatlas.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add 9n-nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_9c_nc_preds_path = ca_dir / \"C-A_predictions_9c-nc.csv\"\n",
    "new_9c_nc_preds = pd.read_csv(new_9c_nc_preds_path, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_cols = new_9c_nc_preds.columns.to_list()\n",
    "new_cols_names = current_cols[0:1] + [\n",
    "    f\"{label}_assay9nc\".replace(\" \", \"_\") for label in current_cols[1:]\n",
    "]\n",
    "new_9c_nc_preds.columns = new_cols_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ca_core_df_no_epiatlas.shape)\n",
    "ca_core_df_no_epiatlas_2 = ca_core_df_no_epiatlas.merge(\n",
    "    new_9c_nc_preds,\n",
    "    left_on=\"Experimental-id\",\n",
    "    right_on=\"md5sum\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_9nc\"),\n",
    ")\n",
    "ca_core_df_no_epiatlas_2 = ca_core_df_no_epiatlas_2.drop(labels=\"md5sum_9nc\", axis=1)\n",
    "print(ca_core_df_no_epiatlas_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_core_df_no_epiatlas_2.columns[144:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core7 stats\n",
    "print_pred_within_threshold(\n",
    "    ca_core_df_no_epiatlas_2, min_pred=min_pred, col=\"Max_pred_assay9nc\"\n",
    ")\n",
    "print_breakdown_predictions(ca_core_df_no_epiatlas_2, min_pred=min_pred, nb_classes=\"9nc\")\n",
    "display(ca_core_df_no_epiatlas_2[\"Predicted_class_assay9nc\"].value_counts())\n",
    "display(ca_core_df_no_epiatlas_2[\"manual_target_consensus\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_min_pred = {}\n",
    "\n",
    "sub_df = ca_core_df_no_epiatlas_2[\n",
    "    ca_core_df_no_epiatlas_2[\"Predicted_class_assay9nc\"] == \"non-core\"\n",
    "].copy()\n",
    "\n",
    "for min_pred in np.arange(0, 1, 0.05):\n",
    "    filtered_df = sub_df[sub_df[\"Max_pred_assay9nc\"] >= min_pred]\n",
    "\n",
    "    groupby = filtered_df.groupby([\"manual_target_consensus\"])\n",
    "    for target, df in groupby:\n",
    "        if target not in results_min_pred:\n",
    "            results_min_pred[target] = {\n",
    "                \"min_pred\": [],\n",
    "                \"input\": [],\n",
    "                \"target\": [],\n",
    "                \"Total ok\": [],\n",
    "                r\"%correct_excluding_input\": [],\n",
    "            }\n",
    "\n",
    "        pred_count = df[\"2nd_pred_class_assay9nc\"].value_counts()\n",
    "        pred_count = pred_count / pred_count.sum()\n",
    "        input_perc = pred_count[\"input\"]\n",
    "        target_perc = pred_count[target] if target in pred_count else 0\n",
    "\n",
    "        results_min_pred[target][\"min_pred\"].append(min_pred)\n",
    "        results_min_pred[target][\"input\"].append(input_perc * 100)\n",
    "        results_min_pred[target][\"target\"].append(target_perc * 100)\n",
    "\n",
    "        if target != \"input\":\n",
    "            results_min_pred[target][\"Total ok\"].append((input_perc + target_perc) * 100)\n",
    "            results_min_pred[target][r\"%correct_excluding_input\"].append(\n",
    "                (target_perc / (1 - input_perc)) * 100\n",
    "            )\n",
    "        else:\n",
    "            results_min_pred[target][\"Total ok\"].append(None)\n",
    "            results_min_pred[target][r\"%correct_excluding_input\"].append(None)\n",
    "\n",
    "# Optionally, create a table with all the data\n",
    "table_data = []\n",
    "for target, data in results_min_pred.items():\n",
    "    for i in range(len(data[\"min_pred\"])):\n",
    "        row = [\n",
    "            target,\n",
    "            data[\"min_pred\"][i],\n",
    "            data[\"input\"][i],\n",
    "            data[\"target\"][i],\n",
    "            data[\"Total ok\"][i],\n",
    "            data[r\"%correct_excluding_input\"][i],\n",
    "        ]\n",
    "        table_data.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(\n",
    "    table_data,\n",
    "    columns=[\n",
    "        \"Target\",\n",
    "        \"min_pred\",\n",
    "        \"input\",\n",
    "        \"target\",\n",
    "        \"Total ok\",\n",
    "        r\"%correct_excluding_input\",\n",
    "    ],\n",
    ")\n",
    "cols = df_results.columns.to_list()\n",
    "cols.remove(\"min_pred\")\n",
    "\n",
    "# pylint: disable=consider-using-f-string\n",
    "with pd.option_context(\"display.float_format\", \"{:.2f}\".format):\n",
    "    display(df_results[df_results[\"min_pred\"] < 0.05][cols].sort_values(by=\"Target\"))  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pred_diff = {}\n",
    "\n",
    "for min_pred_diff in np.arange(0, 0.6, 0.05):\n",
    "    sub_df = ca_core_df_no_epiatlas_2[\n",
    "        ca_core_df_no_epiatlas_2[\"Predicted_class_assay9nc\"] == \"non-core\"\n",
    "    ].copy()\n",
    "    sub_df = sub_df[sub_df[\"1rst/2nd_prob_diff_assay9nc\"] >= min_pred_diff]\n",
    "\n",
    "    groupby = sub_df.groupby([\"manual_target_consensus\"])\n",
    "    for target, df in groupby:\n",
    "        if target not in results_pred_diff:\n",
    "            results_pred_diff[target] = {\n",
    "                \"min_pred_diff\": [],\n",
    "                r\"%correct_excluding_input\": [],\n",
    "            }\n",
    "\n",
    "        pred_count = df[\"2nd_pred_class_assay9nc\"].value_counts()\n",
    "        pred_count = pred_count / pred_count.sum()\n",
    "        input_perc = pred_count[\"input\"]\n",
    "        target_perc = pred_count[target] if target in pred_count else 0\n",
    "\n",
    "        if target != \"input\":\n",
    "            correct_excluding_input = target_perc / (1 - input_perc) * 100\n",
    "            results_pred_diff[target][\"min_pred_diff\"].append(min_pred_diff)\n",
    "            results_pred_diff[target][r\"%correct_excluding_input\"].append(\n",
    "                correct_excluding_input\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df, name in [(results_min_pred, \"min_pred\"), (results_pred_diff, \"min_pred_diff\")]:\n",
    "#     fig = go.Figure()\n",
    "\n",
    "#     for target, data in df.items():\n",
    "#         if target != \"input\":\n",
    "#             fig.add_trace(\n",
    "#                 go.Scatter(\n",
    "#                     x=data[name],\n",
    "#                     y=data[r\"%correct_excluding_input\"],\n",
    "#                     mode=\"lines+markers\",\n",
    "#                     marker=dict(color=assay_colors[target]),\n",
    "#                     name=target,\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     fig.update_layout(\n",
    "#         title=f\"Performance by Target and {name}\",\n",
    "#         xaxis_title=name,\n",
    "#         yaxis_title=r\"%correct_excluding_input\",\n",
    "#         legend_title=\"Target\",\n",
    "#         hovermode=\"x unified\",\n",
    "#     )\n",
    "\n",
    "#     fig.update_yaxes(range=[80, 95])\n",
    "\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "min_pred = 0.6\n",
    "col = \"Predicted_class_assay9nc\"\n",
    "labels = sorted(ca_core_df_no_epiatlas_2[col].unique().tolist())\n",
    "\n",
    "df = ca_core_df_no_epiatlas_2[ca_core_df_no_epiatlas_2[\"Max_pred_assay9nc\"] >= min_pred]\n",
    "fig_dir = base_fig_dir / \"fig_C-A\" / \"confusion_matrices\"\n",
    "\n",
    "# cm = sk_cm(df[\"manual_target_consensus\"], df[col])\n",
    "# cm_writer = ConfusionMatrixWriter(labels=labels, confusion_matrix=cm)\n",
    "# name = f\"confusion_matrix_assay9nc_core7_minPred{min_pred:.02f}\"\n",
    "# cm_writer.to_all_formats(logdir=fig_dir, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main stats  \n",
    "\n",
    "assay7  \n",
    "\n",
    "- Nb pred assay7 (pred score >= 0.60): 89.49% (41326/46179)\n",
    "- Nb match assay7: 95.66% (39532/41326)\n",
    "- Nb error assay7: 4.34% (1794/41326)\n",
    "\n",
    "Following ratios: % of assay subset OR % of all predictions OR % of all incorrect predictions (potential mislabels).   \n",
    "\n",
    "- Predictions as input: 39.79% (16442/41326)\n",
    "- Correct predictions as input: 92.42% (15195/16442) OR 36.77% (15195/41326)\n",
    "- Incorrect predictions as input: 7.58% (1247/16442) OR 3.02% (1247/41326) OR 69.51% (1247/1794)\n",
    "\n",
    "assay9nc\n",
    "\n",
    "- Nb pred assay9nc (pred score >= 0.60): 92.10% (42531/46179)\n",
    "- Nb match assay9nc: 25.88% (11007/42531)\n",
    "- Nb error assay9nc: 74.12% (31524/42531)  --> mostly non-core preds\n",
    "\n",
    "Following ratios: % of assay subset OR % of all predictions OR % of all incorrect predictions (potential mislabels).   \n",
    "\n",
    "- Predictions as input: 0.50% (211/42531)\n",
    "- Correct predictions as input: 93.36% (197/211) OR 0.46% (197/42531)\n",
    "- Incorrect predictions as input: 6.64% (14/211) OR 0.03% (14/42531) OR 0.04% (14/31524)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata complementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_categories = []\n",
    "for name in ca_core_df.columns:\n",
    "    if \"True_class\" in name and \"assay\" not in name:\n",
    "        print(name)\n",
    "        other_categories.append(name.split(\"_\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [ca_core_df.copy(), ca_core_df_no_epiatlas.copy()]:\n",
    "    print(df.shape)\n",
    "    for name in other_categories:\n",
    "        col = f\"Max_pred_{name}\"\n",
    "        try:\n",
    "            df.loc[:, col] = df[col].astype(float)\n",
    "        except ValueError:\n",
    "            print(f\"Problem with {col}\")\n",
    "            continue\n",
    "        # print(col)\n",
    "        # col_pred = f\"Predicted_class_{name}\"\n",
    "        # preds = df[col_pred].value_counts(dropna=False)\n",
    "        # display(preds)\n",
    "\n",
    "        # print_pred_within_threshold(df, min_pred=0.6, col=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chip-Atlas all core7\n",
    "\n",
    "Predictions with pred score >= 0.60 (of total 47226)\n",
    "| Category    | % Samples | Nb Samples |\n",
    "|-------------|-----------|------------|\n",
    "| cancer2      | 93.95%    | 44371      |\n",
    "| donor_life_stage   | 94.91%    | 44823  |\n",
    "| sex3         | 87.15%    | 41156      |\n",
    "| biomaterial_type      | 79.39%    | 37493      |\n",
    "| paired_end      | 95.94%    | 45310      |\n",
    "\n",
    "\n",
    "Chip-Atlas all core 7, no epiatlas encode\n",
    "Predictions with pred score >= 0.60 (of total 46179)\n",
    "| Category             | % Samples | Nb Samples |\n",
    "|----------------------|-----------|------------|\n",
    "| cancer2              | 93.93%    | 43375      |\n",
    "| donor_life_stage     | 94.88%    | 43815      |\n",
    "| sex3                 | 87.02%    | 40186      |\n",
    "| biomaterial_type     | 79.17%    | 36562      |\n",
    "| paired_end           | 95.96%    | 44313      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### track type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_type_pred_path = (\n",
    "    base_data_dir\n",
    "    / \"training_results\"\n",
    "    / \"predictions\"\n",
    "    / \"C-A\"\n",
    "    / \"track_type\"\n",
    "    / \"split0_test_prediction_C-A_100kb_all_none.csv\"\n",
    ")\n",
    "track_type_pred_df = pd.read_csv(track_type_pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_type_pred_df[\"Max_pred_track_type\"] = track_type_pred_df.loc[\n",
    "    :, track_type_pred_df.columns[3:]\n",
    "].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_type_df = track_type_pred_df.merge(\n",
    "    ca_pred_df, left_on=\"Unnamed: 0\", right_on=\"Experimental-id\", how=\"inner\"\n",
    ")\n",
    "\n",
    "print(track_type_df.shape, ca_pred_df.shape, track_type_pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write each table in a separate excel sheet\n",
    "output = track_type_pred_path.parent / \"track_type_predictions_pivot.csv\"\n",
    "output.unlink(missing_ok=True)\n",
    "\n",
    "with open(output, \"a\", encoding=\"utf8\") as csv_stream:\n",
    "    for min_pred in [0, 0.6, 0.8]:\n",
    "        df = track_type_df[track_type_df[\"Max_pred_track_type\"] >= min_pred]\n",
    "        pivot = df.pivot_table(\n",
    "            index=\"manual_target_consensus\",\n",
    "            columns=\"Predicted class\",\n",
    "            values=\"Max_pred_track_type\",\n",
    "            aggfunc=\"count\",\n",
    "            fill_value=0,\n",
    "            margins=True,\n",
    "        ).astype(int)\n",
    "        relative_pivot = pivot.div(pivot[\"All\"], axis=0) * 100\n",
    "\n",
    "        csv_stream.write(f\"Count Pivot - Min pred: {min_pred}\\n\")\n",
    "        pivot.to_csv(csv_stream)\n",
    "        csv_stream.write(\"\\n\")\n",
    "\n",
    "        csv_stream.write(f\"Relative Pivot - Min pred: {min_pred}\\n\")\n",
    "        relative_pivot.to_csv(csv_stream)\n",
    "        csv_stream.write(\"\\n\")\n",
    "\n",
    "        # display(pivot)\n",
    "        # with pd.option_context(\"display.float_format\", \"{:.2f}\".format):\n",
    "        #     display(relative_pivot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
