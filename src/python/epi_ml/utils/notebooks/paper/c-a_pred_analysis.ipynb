{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to analyse classifier predictions on ChIP-Atlas data.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches, pointless-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import ast\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import upsetplot\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.metrics_per_assay import MetricsPerAssay\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import ASSAY_ORDER, IHECColorMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ASSAY_ORDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_dir = base_data_dir / \"training_results\" / \"predictions\" / \"C-A\" / \"assay_epiclass\"\n",
    "\n",
    "ca_filename = \"CA_metadata_4DB+all_pred.20240606_mod3.0.tsv\"\n",
    "ca_pred_path = ca_dir / ca_filename\n",
    "\n",
    "ca_pred_df = pd.read_csv(ca_pred_path, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "print(ca_pred_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Assay | Exp Key                               | Nb Files | Training Size | Oversampling |\n",
    "|-------|---------------------------------------|----------|---------------|--------------|\n",
    "| 13c*   | dd3710b73c0341af85a17ce1998362d0      | 24989    | 116550        | true         |\n",
    "| 11c   | 0f8e5eb996114868a17057bebe64f87c      | 20922    | 46128         | true         |\n",
    "| 7c    | 69488630801b4a05a53b5d9e572f0aaa      | 16788    | 34413         | true         |\n",
    "\n",
    "*using hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = ca_pred_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_ASSAYS = ASSAY_ORDER[0:7]\n",
    "\n",
    "DB_COLS = [\"GEO_mod\", \"C-A\", \"Cistrome\", \"NGS_mod\"]\n",
    "\n",
    "PRED_COLS = [\n",
    "    \"Predicted_class_assay7\",\n",
    "    \"Predicted_class_assay11\",\n",
    "    \"Predicted_class_assay13\",\n",
    "]\n",
    "\n",
    "SAME_TARGET = \"core7_DBs_consensus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_column_content(df: pd.DataFrame, col: str) -> None:\n",
    "    \"\"\"Print information about the targets.\"\"\"\n",
    "    label_count_df = df[col].value_counts(dropna=False).to_frame()\n",
    "\n",
    "    label_count_df[\"relative\"] = label_count_df[\"count\"] / len(df)\n",
    "\n",
    "    label_count_df.loc[\"Total\", \"count\"] = len(df)\n",
    "    label_count_df.loc[\"Total\", \"relative\"] = 1\n",
    "\n",
    "    style_map = {\n",
    "        \"count\": \"{:.0f}\",\n",
    "        \"relative\": \"{:.2%}\",\n",
    "    }\n",
    "\n",
    "    display(label_count_df.style.format(style_map))  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base dataset used: Chip-Atlas experiments where at least one of the BD declared the target in core7.\n",
    "\n",
    "Excluding: \n",
    "- Samples where at least one the DB declared a target out of core7.\n",
    "- samples overlapping with EpiATLAS dataset (different file creation pipeline, same base bam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = ca_pred_df.loc[:, DB_COLS].copy(deep=True)\n",
    "tmp_df[\"C-A\"].replace(\"unclassified\", \"----\", inplace=True)\n",
    "\n",
    "id_db_target = []\n",
    "unique_labels = Counter()\n",
    "different_labels = Counter()\n",
    "\n",
    "for labels in tmp_df.values:\n",
    "    missing_N = sum(label == \"----\" for label in labels)\n",
    "    db_labels = set(labels)\n",
    "\n",
    "    try:\n",
    "        db_labels.remove(\"----\")\n",
    "    except KeyError:\n",
    "        pass\n",
    "    if missing_N == 3:\n",
    "        id_db_target.append(\"1 source\")\n",
    "    elif len(db_labels) == 1:\n",
    "        id_db_target.append(\"Identical\")\n",
    "    else:\n",
    "        id_db_target.append(\"Different\")\n",
    "        different_labels[tuple(db_labels)] += 1\n",
    "\n",
    "    unique_labels[tuple(db_labels)] += 1\n",
    "\n",
    "\n",
    "display(pd.Series(id_db_target).value_counts(dropna=False, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_core_labels = [\"non-core\", \"CTCF\", \"ctcf\"]\n",
    "non_core_labels_2 = [\"Ignored - Potential non-core\", \"non-core/CTCF\"]\n",
    "\n",
    "print(f\"Starting with {len(ca_pred_df)} rows.\")\n",
    "ca_core_df = ca_pred_df[~ca_pred_df[SAME_TARGET].isin(non_core_labels_2)]\n",
    "diff_N = len(ca_pred_df) - len(ca_core_df)\n",
    "print(\n",
    "    f\"Removed {diff_N} rows with {SAME_TARGET} in {non_core_labels_2}.\\nAfter this, {len(ca_core_df)} rows remain.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_diff = len(ca_core_df)\n",
    "ca_core_df = ca_core_df[ca_core_df[\"is_EpiAtlas_EpiRR\"].astype(str) == \"0\"].copy()\n",
    "N_diff -= len(ca_core_df)\n",
    "print(\n",
    "    f\"Removed {N_diff} rows with EpiATLAS EpiRR overlap. After this, {len(ca_core_df)} rows remain.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for db_col in DB_COLS:\n",
    "    col = ca_core_df[db_col]\n",
    "    if col.isna().sum():\n",
    "        print(\"Missing values: \", ca_core_df[col.isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_column_content(ca_core_df, \"manual_target_consensus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_column_content(ca_core_df, SAME_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_consensus_df = ca_core_df[ca_core_df[\"manual_target_consensus\"] == \"no_consensus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upset plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"fig_C-A\" / \"DB_upset\" / \"no_EpiATLAS\"\n",
    "fig_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_db_upsetplot(\n",
    "    df: pd.DataFrame, db_cols: List[str], title: str\n",
    ") -> upsetplot.UpSet:\n",
    "    \"\"\"Make an upsetplot of the sample presence in the different databases.\"\"\"\n",
    "    df = df.copy()\n",
    "    if SAME_TARGET not in df.columns:\n",
    "        raise ValueError(\"Column 'identical_DBs_target' not found in DataFrame.\")\n",
    "\n",
    "    # Create a new DataFrame with boolean columns for each database\n",
    "    upset_df = pd.DataFrame()\n",
    "    for col in db_cols:\n",
    "        upset_df[col] = df[col] != \"----\"\n",
    "    upset_df[SAME_TARGET] = df[SAME_TARGET]\n",
    "\n",
    "    # Set the index for the UpSet plot\n",
    "    upset_df = upset_df.set_index(db_cols)\n",
    "\n",
    "    # Create the UpSet plot\n",
    "    upset = upsetplot.UpSet(\n",
    "        upset_df,\n",
    "        intersection_plot_elements=0,  # disable the default bar chart\n",
    "        sort_by=\"cardinality\",\n",
    "        show_counts=True,  # type: ignore\n",
    "        orientation=\"horizontal\",\n",
    "    )\n",
    "\n",
    "    # Add stacked bars\n",
    "    upset.add_stacked_bars(by=SAME_TARGET, elements=15)\n",
    "\n",
    "    # Plot and set title\n",
    "    axes = upset.plot()\n",
    "    plt.suptitle(title)\n",
    "    axes[\"totals\"].set_title(\"Total\")\n",
    "    plt.legend(loc=\"center left\")\n",
    "    return upset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"All core7 ChIP-Atlas samples presence in used DBs)\\nTarget consensus\"\n",
    "upset = make_db_upsetplot(ca_core_df, DB_COLS, title=title)\n",
    "\n",
    "plt.savefig(fig_dir / \"upsetplot_DB_core7_samples.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No ENCODE EpiRR overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_column_content(ca_core_df, \"ENCODE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no encode\n",
    "no_encode_df = ca_core_df[ca_core_df[\"ENCODE\"] == 0]\n",
    "title = \"ChIP-Atlas samples presence in used DBs\\nTarget Consensus - No ENCODE\"\n",
    "\n",
    "upset = make_db_upsetplot(no_encode_df, DB_COLS, title=title)\n",
    "\n",
    "plt.savefig(fig_dir / \"upsetplot_DB_core7_samples_noENC.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prediction_resolved(row, pred_col: str, db_cols: List[str]) -> bool:\n",
    "    \"\"\"Check if the prediction matches any of the database columns.\"\"\"\n",
    "    pred_val = row[pred_col]\n",
    "    db_vals = [row[col] for col in db_cols]\n",
    "    return pred_val in db_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier was able to resolve xx% of the cases where the target was not identical between the sources\n",
    "different_targets_df = ca_core_df[ca_core_df[SAME_TARGET] == \"Different\"]\n",
    "\n",
    "for min_pred_score in [0, 0.6]:\n",
    "    filtered_df = different_targets_df[\n",
    "        different_targets_df[\"Max_pred_assay7\"] >= min_pred_score\n",
    "    ]\n",
    "\n",
    "    pred_col = PRED_COLS[0]\n",
    "\n",
    "    num_resolved = filtered_df.apply(\n",
    "        is_prediction_resolved, axis=1, args=(pred_col, DB_COLS)\n",
    "    ).sum()\n",
    "\n",
    "    print(\n",
    "        f\"Resolved (min_predScore >= {min_pred_score}): \"\n",
    "        f\"{num_resolved} / {len(filtered_df)} \"\n",
    "        f\"({num_resolved / len(filtered_df) * 100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    # Exclude rows where the prediction is labeled as 'input'\n",
    "    non_input_df = filtered_df[filtered_df[PRED_COLS[0]] != \"input\"]\n",
    "    num_resolved = non_input_df.apply(\n",
    "        is_prediction_resolved, axis=1, args=(pred_col, DB_COLS)\n",
    "    ).sum()\n",
    "\n",
    "    print(\n",
    "        f\"Resolved (min_predScore >= {min_pred_score}, excluding 'input' predictions): \"\n",
    "        f\"{num_resolved} / {len(non_input_df)} \"\n",
    "        f\"({num_resolved / len(non_input_df) * 100:.2f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level prediction accuracy breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `epiclass_match_status` column and join it to predictions.\n",
    "\n",
    "This category represents the agreement between the databases labels and \n",
    "the classifier prediction.  \n",
    "\n",
    "If there is a database consensus and our prediction matches, it's a complete match.  \n",
    "If there is no database consensus, but our prediction matches one of the databases labels, it's a partial match.  \n",
    "Otherwise, no match.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epiclass_match_status = []\n",
    "for _, row in ca_core_df.iterrows():\n",
    "    target_vals = [row[col] for col in DB_COLS]\n",
    "    consensus: str = row[\"manual_target_consensus\"]\n",
    "\n",
    "    epiclass_target: str = row[\"Predicted_class_assay7\"]\n",
    "\n",
    "    if epiclass_target == consensus:\n",
    "        epiclass_match_status.append(\"Complete match\")\n",
    "        continue\n",
    "\n",
    "    if epiclass_target in target_vals:\n",
    "        epiclass_match_status.append(\"Partial match\")\n",
    "        continue\n",
    "\n",
    "    epiclass_match_status.append(\"No match\")\n",
    "\n",
    "ca_core_df[\"epiclass_match_status\"] = epiclass_match_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_pred_score in [0, 0.6]:\n",
    "    print(f\"Prediction agreement, minimum prediction score >= {min_pred_score:.2f}\")\n",
    "    subset_df = ca_core_df[ca_core_df[\"Max_pred_assay7\"] >= min_pred_score]\n",
    "    print_column_content(subset_df, \"epiclass_match_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dir = base_data_dir / \"training_results\" / \"predictions\"\n",
    "pred_path = predictions_dir / \"C-A\" / \"CA_only_pred_20240606.tsv\"\n",
    "pred_df = pd.read_csv(pred_path, sep=\"\\t\", low_memory=False)\n",
    "print(pred_df.shape, pred_df.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = \"epiclass_match_status\"\n",
    "if new_col not in pred_df.columns:\n",
    "    index_1 = pred_df.columns[0]\n",
    "    index_2 = ca_core_df.columns[0]\n",
    "    pred_df = pd.merge(\n",
    "        pred_df,\n",
    "        ca_core_df[[index_2, new_col]],\n",
    "        how=\"left\",\n",
    "        left_on=pred_df.columns[0],\n",
    "        right_on=ca_core_df.columns[0],\n",
    "        suffixes=(\"\", \"_DROP\"),\n",
    "    )\n",
    "    pred_df = pred_df.drop(\n",
    "        columns=[col for col in pred_df.columns if col.endswith(\"_DROP\")]\n",
    "    )\n",
    "    pred_df[\"epiclass_match_status\"].fillna(\"NA\", inplace=True)\n",
    "    pred_df.to_csv(pred_path, sep=\"\\t\", index=False)\n",
    "\n",
    "del pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details prediction stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_high_level_pred_info(df: pd.DataFrame, save_conf_matrix: bool = False) -> None:\n",
    "    \"\"\"High level information about the predictions.\"\"\"\n",
    "    for assay in CORE_ASSAYS:\n",
    "        print(f\"{assay}\")\n",
    "        assay_df = df[df[\"manual_target_consensus\"] == assay]\n",
    "        for col in [\n",
    "            \"Predicted_class_assay7\",\n",
    "            \"Predicted_class_assay11\",\n",
    "            \"Predicted_class_assay13\",\n",
    "        ]:\n",
    "            assay_number = col.rsplit(\"_\", maxsplit=1)[-1]\n",
    "            display(assay_df[col].value_counts() / len(assay_df) * 100)\n",
    "            if any(label in col for label in [\"11\", \"13\"]):\n",
    "                wrong_pred = assay_df[assay_df[col] != assay]\n",
    "\n",
    "                display(\n",
    "                    wrong_pred[f\"2nd_pred_class_{assay_number}\"].value_counts()\n",
    "                    / len(wrong_pred)\n",
    "                    * 100\n",
    "                )\n",
    "        print(\"\\n\")\n",
    "\n",
    "    if save_conf_matrix:\n",
    "        for col in [\n",
    "            \"Predicted_class_assay7\",\n",
    "            \"Predicted_class_assay11\",\n",
    "            \"Predicted_class_assay13\",\n",
    "        ]:\n",
    "            labels = sorted(df[col].unique().tolist())\n",
    "            cm = sk_cm(\n",
    "                df[\"manual_target_consensus\"],\n",
    "                df[col],\n",
    "                labels=labels,\n",
    "            )\n",
    "            cm_writer = ConfusionMatrixWriter(labels=labels, confusion_matrix=cm)\n",
    "            cm_writer.to_png(\n",
    "                Path.home() / \"Downloads\" / f\"C-A_confusion_matrix_{col}.png\"\n",
    "            )\n",
    "\n",
    "    print(\"What is the actual target when wgbs-standard is predicted?\")\n",
    "    for assay_number in [\"assay11\", \"assay13\"]:\n",
    "        print(f\"{assay_number}\")\n",
    "        wgbs_dist = ca_pred_df[\n",
    "            ca_pred_df[f\"Predicted_class_{assay_number}\"] == \"wgbs-standard\"\n",
    "        ][\"manual_target_consensus\"]\n",
    "        display(wgbs_dist.value_counts())\n",
    "        display(wgbs_dist.value_counts() / len(wgbs_dist) * 100)\n",
    "\n",
    "    print(\"What is the actual target when non-core is predicted?\")\n",
    "    col = \"Predicted_class_assay13\"\n",
    "    wgbs_dist = ca_pred_df[ca_pred_df[col] == \"non-core\"][\"manual_target_consensus\"]\n",
    "    display(wgbs_dist.value_counts())\n",
    "    display(wgbs_dist.value_counts() / len(wgbs_dist) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "if verbose:\n",
    "    print_column_content(ca_pred_df, \"manual_target_consensus\")\n",
    "    print_high_level_pred_info(ca_pred_df, save_conf_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_pred in [0.6, 0.8]:\n",
    "    break_tie_mask = no_consensus_df[\"Max_pred_assay7\"] >= min_pred\n",
    "    nb_break_tie = break_tie_mask.sum()\n",
    "    print(\n",
    "        f\"Break no_consensus (minPred >= {min_pred:.02f}): {nb_break_tie/ len(no_consensus_df) * 100:.02f}% ({nb_break_tie}/{len(no_consensus_df)})\"\n",
    "    )\n",
    "    df = no_consensus_df[break_tie_mask]\n",
    "\n",
    "    nb_not_input = (df[\"Predicted_class_assay7\"] != \"input\").sum()\n",
    "    print(\n",
    "        f\"non-input tie breakers: {nb_not_input}/{nb_break_tie} ({nb_not_input/len(df) * 100:.02f}%)\\n\"\n",
    "    )\n",
    "    print(df[\"ENCODE\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_within_threshold(\n",
    "    df: pd.DataFrame, min_pred: float = 0.6, col: str = \"Max_pred_assay7\"\n",
    ") -> None:\n",
    "    \"\"\"Print the predictions percentage within a threshold.\"\"\"\n",
    "    try:\n",
    "        mask = df[col].astype(float) >= min_pred\n",
    "    except KeyError:\n",
    "        print(f\"Column {col} not found.\")\n",
    "        return\n",
    "    nb_pred = mask.sum()\n",
    "    print(\n",
    "        f\"Nb pred {col.split('_')[-1]} (pred score >= {min_pred:.02f}): {nb_pred/len(df) * 100:.02f}% ({nb_pred}/{len(df)})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_confusion_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    fig_dir: Path | str,\n",
    "    nb_classes: int | str = 7,\n",
    "    min_pred: float = 0.6,\n",
    "):\n",
    "    \"\"\"Save the confusion matrix for core assays predictions. Does not filter.\"\"\"\n",
    "    col = f\"Predicted_class_assay{nb_classes}\"\n",
    "    cm = sk_cm(df[\"manual_target_consensus\"], df[col], labels=CORE_ASSAYS)\n",
    "    cm_writer = ConfusionMatrixWriter(labels=CORE_ASSAYS, confusion_matrix=cm)\n",
    "\n",
    "    name = f\"confusion_matrix_assay{nb_classes}_core7_minPred{min_pred:.02f}\"\n",
    "    if df[\"ENCODE\"].sum() == 0:\n",
    "        name += \"_noENCODE\"\n",
    "\n",
    "    cm_writer.to_all_formats(logdir=fig_dir, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_breakdown_predictions(\n",
    "    df: pd.DataFrame,\n",
    "    min_pred: float = 0.6,\n",
    "    nb_classes: int | str = 7,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Breakdown the predictions, print results.\"\"\"\n",
    "    df = df[df[f\"Max_pred_assay{nb_classes}\"] >= min_pred]\n",
    "\n",
    "    pred_col = f\"Predicted_class_assay{nb_classes}\"\n",
    "    match_consensus = df[\"manual_target_consensus\"] == df[pred_col]\n",
    "    nb_match = match_consensus.sum()\n",
    "    nb_error = (~match_consensus).sum()\n",
    "    print(f\"Nb match assay{nb_classes}: {nb_match/ len(df):.2%} ({nb_match}/{len(df)})\")\n",
    "    print(f\"Nb error assay{nb_classes}: {nb_error/ len(df):.2%} ({nb_error}/{len(df)})\\n\")\n",
    "\n",
    "    correct_pred_df = df[match_consensus]\n",
    "    incorrect_pred_df = df[~match_consensus]\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            r\"Following ratios: % of assay subset OR % of all predictions OR % of all incorrect predictions (potential mislabels).\",\n",
    "            \"\\n\",\n",
    "        )\n",
    "    acc_per_class = {}\n",
    "    for assay in CORE_ASSAYS:\n",
    "        assay_df = df[df[pred_col] == assay]\n",
    "        nb_assay = len(assay_df)\n",
    "\n",
    "        nb_assay_correct = len(correct_pred_df[correct_pred_df[pred_col] == assay])\n",
    "        nb_assay_incorrect = len(incorrect_pred_df[incorrect_pred_df[pred_col] == assay])\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Predictions as {assay}: {nb_assay / len(df):.2%} ({nb_assay}/{len(df)})\"\n",
    "            )\n",
    "        perc_cor = nb_assay_correct / nb_assay\n",
    "        perc_cor2 = nb_assay_correct / len(df)\n",
    "        perc_inc = nb_assay_incorrect / nb_assay\n",
    "        perc_inc2 = nb_assay_incorrect / len(df)\n",
    "        perc_inc3 = nb_assay_incorrect / len(incorrect_pred_df)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Correct predictions as {assay}: {perc_cor:.2%} ({nb_assay_correct}/{nb_assay}) OR {perc_cor2:.2%} ({nb_assay_correct}/{len(df)})\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Incorrect predictions as {assay}: \"\n",
    "                f\"{perc_inc:.2%} ({nb_assay_incorrect}/{nb_assay}) OR \"\n",
    "                f\"{perc_inc2:.2%} ({nb_assay_incorrect}/{len(df)}) OR \"\n",
    "                f\"{perc_inc3:.2%} ({nb_assay_incorrect}/{len(incorrect_pred_df)})\\n\"\n",
    "            )\n",
    "        acc_per_class[assay] = perc_cor\n",
    "\n",
    "    return acc_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "fig_dir = base_fig_dir / \"fig_C-A\" / \"confusion_matrices\"\n",
    "for subset in [[0], [0, 1]]:\n",
    "    # continue\n",
    "    if verbose:\n",
    "        if subset == [0]:\n",
    "            print(\"Subset: no ENCODE\")\n",
    "        else:\n",
    "            print(\"Subset: Include ENCODE\")\n",
    "\n",
    "    df = ca_core_df[ca_core_df[\"ENCODE\"].isin(subset)]\n",
    "\n",
    "    for min_pred in [0.6, 0.8, 0.9]:\n",
    "        # continue\n",
    "        if verbose:\n",
    "            print(\"Min pred score:\", min_pred)\n",
    "            print_pred_within_threshold(df, min_pred=min_pred)\n",
    "            print_breakdown_predictions(df, min_pred=min_pred)\n",
    "\n",
    "        sub_df = df[df[\"Max_pred_assay7\"] >= min_pred]\n",
    "        save_confusion_matrix(sub_df, fig_dir, min_pred=min_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mislabels by GSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = ca_dir / \"GSE_mispred\"\n",
    "logdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSE = \"Gse-geo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "nb_classes = 7\n",
    "min_pred = 0.6\n",
    "pred_col = f\"Predicted_class_assay{nb_classes}\"\n",
    "max_pred_col = f\"Max_pred_assay{nb_classes}\"\n",
    "\n",
    "excluding_no_consensus = True\n",
    "excluding_ENCODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ca_core_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if excluding_no_consensus:\n",
    "    N_diff = len(df)\n",
    "    df = df[df[\"manual_target_consensus\"] != \"no_consensus\"]\n",
    "    N_diff -= len(df)\n",
    "    print(f\"Removed {N_diff} rows with no consensus.\\nLeft with {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if excluding_ENCODE:\n",
    "    N_diff = len(df)\n",
    "    df = df[df[\"ENCODE\"] == 0]\n",
    "    N_diff -= len(df)\n",
    "    print(f\"Removed {N_diff} rows with ENCODE.\")\n",
    "\n",
    "    this_logdir = logdir / \"excluding_ENCODE\"\n",
    "else:\n",
    "    this_logdir = logdir / \"including_ENCODE\"\n",
    "\n",
    "this_logdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_total = len(df)\n",
    "N_diff = len(df)\n",
    "df = df[df[max_pred_col] >= min_pred]\n",
    "N_diff -= len(df)\n",
    "\n",
    "print(\n",
    "    f\"Removed {N_diff}/{N_total} ({N_diff/N_total:.2%}) rows with pred score < {min_pred}\\nLeft with {len(df)} rows.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"no_consensus\" in df[\"manual_target_consensus\"].unique():\n",
    "    raise ValueError(\"'no_consensus' present in df, cannot compute accuracy.\")\n",
    "\n",
    "no_match = df[\"epiclass_match_status\"] == \"No match\"\n",
    "nb_match = (~no_match).sum()\n",
    "nb_error = (no_match).sum()\n",
    "print(f\"Nb match assay{nb_classes}: {nb_match/ len(df):.2%} ({nb_match}/{len(df)})\")\n",
    "print(f\"Nb mismatch assay{nb_classes}: {nb_error/ len(df):.2%} ({nb_error}/{len(df)})\\n\")\n",
    "\n",
    "incorrect_pred_df = df[no_match]\n",
    "\n",
    "if verbose:\n",
    "    print(\"Incorrect predictions, breakdown by predicted class:\")\n",
    "    display(incorrect_pred_df[pred_col].value_counts(normalize=True))\n",
    "\n",
    "incorrect_pred_df = incorrect_pred_df[incorrect_pred_df[pred_col] != \"input\"]\n",
    "\n",
    "print(\n",
    "    f\"Excluding input predictions. Left with {len(incorrect_pred_df)} complete mismatches.\\n\"\n",
    ")\n",
    "\n",
    "desired_cols = [\"manual_target_consensus\", pred_col]\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    gse_count = incorrect_pred_df.groupby(GSE).size().sort_values(ascending=False)\n",
    "    gse_count = gse_count.to_frame()\n",
    "    gse_count.columns = [\"Nb of mismatches\"]\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Incorrect predictions, breakdown by GSE count ({len(gse_count)} unique GSE)\"\n",
    "        )\n",
    "\n",
    "    gse_count[\"cumsum\"] = gse_count.cumsum()\n",
    "    gse_count[\"cumsum (%)\"] = (\n",
    "        gse_count[\"cumsum\"] * 100 / sum(gse_count[\"Nb of mismatches\"])\n",
    "    )\n",
    "    if verbose:\n",
    "        display(gse_count.reset_index())\n",
    "\n",
    "    gse_count.to_csv(\n",
    "        this_logdir / \"gse_count_incorrect_pred_no_input_20240606_mod3.tsv\", sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    gse_target_count = incorrect_pred_df.groupby(GSE)[desired_cols].value_counts(dropna=False)  # type: ignore\n",
    "    if verbose:\n",
    "        print(\"Incorrect predictions, breakdown by GSE and target.\")\n",
    "        display(gse_target_count)\n",
    "\n",
    "    gse_target_count = gse_target_count.to_frame()\n",
    "    gse_target_count.columns = [\"Nb of mismatches\"]\n",
    "\n",
    "    gse_target_count.to_csv(\n",
    "        this_logdir / \"gse_target_count_incorrect_pred_no_input_20240606_mod3.tsv\",\n",
    "        sep=\"\\t\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unclassified files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclassified = ca_core_df[ca_core_df[\"C-A\"] == \"unclassified\"]\n",
    "print(f\"Nb unclassified: {len(unclassified)} ({len(unclassified) / len(ca_core_df):.2%})\")\n",
    "\n",
    "high_pred = unclassified[unclassified[max_pred_col] >= min_pred]\n",
    "print(\n",
    "    f\"Nb high pred unclassified: {len(high_pred)} ({len(high_pred) / len(unclassified):.2%})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Summary\n",
    "\n",
    "-- Assay7 classifier, ENCODE effect --\n",
    "\n",
    "Keeping only core 7 targets, excludes no consensus (since it's always going to not match)\n",
    "\n",
    "NO ENCODE\n",
    "\n",
    "- Removed 4526/39668 (11.41%) rows with pred score < 0.6\n",
    "- Nb match assay7: 95.54% (33575/35142)\n",
    "- Nb mismatch assay7: 4.46% (1567/35142)\n",
    "\n",
    "WITH ENCODE\n",
    "\n",
    "- Removed 4778/45615 (10.47%) rows with pred score < 0.6\n",
    "- Nb match assay7: 95.92% (39172/40837)\n",
    "- Nb mismatch assay7: 4.08% (1665/40837)\n",
    "  - Mismatch predicted as input: 68.83%\n",
    "<br><br>\n",
    "\n",
    "-- Resolving different predictions --\n",
    "\n",
    "Total different predictions: 706\n",
    "\n",
    "min_predScore >= 0.6 (87.68%)\n",
    "- Resolved: 612 / 619 (98.87%)\n",
    "- Resolved, excluding 'input' predictions: 214 / 217 (98.62%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying consensus criterion (nb DB agreeing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ca_core_df.copy(deep=True)\n",
    "\n",
    "reference_column = \"manual_target_consensus\"\n",
    "columns_to_check = DB_COLS\n",
    "df[\"manual_target_consensus_size\"] = (\n",
    "    df[columns_to_check].eq(df[reference_column], axis=0)\n",
    ").sum(axis=1)\n",
    "\n",
    "for col in [\"manual_target_consensus\", \"manual_target_consensus_size\"]:\n",
    "    print_column_content(df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Global:\")\n",
    "\n",
    "sub_df = df[df[\"Max_pred_assay7\"] >= 0.6]\n",
    "assay_count = sub_df[\"manual_target_consensus\"].value_counts(normalize=True).sort_index()\n",
    "print(f\"input pred: {assay_count['input']:.2%}\")\n",
    "print_pred_within_threshold(df, min_pred=0.6)\n",
    "\n",
    "acc_per_class = print_breakdown_predictions(df, min_pred=0.6, nb_classes=7, verbose=False)\n",
    "avg_acc_per_class = np.mean(list(acc_per_class.values()))\n",
    "print(f\"Average acc per class: {avg_acc_per_class:.2%}\")\n",
    "print()\n",
    "\n",
    "verbose = True\n",
    "\n",
    "N_global = len(df)\n",
    "for i in range(1, 5):\n",
    "    con_df = df[df[\"manual_target_consensus_size\"] == i]\n",
    "    print(\n",
    "        f\"Consensus defined with {i} DB: {len(con_df)} files. ({len(con_df)/N_global:.2%})\"\n",
    "    )\n",
    "\n",
    "    # Display % assay\n",
    "    if verbose:\n",
    "        sub_df = con_df[con_df[\"Max_pred_assay7\"] >= 0.6]\n",
    "        assay_count = sub_df[\"manual_target_consensus\"].value_counts(normalize=True)\n",
    "        print(f\"input: {assay_count['input']:.2%}\")\n",
    "\n",
    "        print_pred_within_threshold(con_df, min_pred=0.6)\n",
    "\n",
    "        acc_per_class = print_breakdown_predictions(\n",
    "            con_df, min_pred=0.6, nb_classes=7, verbose=False\n",
    "        )\n",
    "        avg_acc_per_class = np.mean(list(acc_per_class.values()))\n",
    "        print(f\"Average acc per class: {avg_acc_per_class:.2%}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be big differences in accuracy when looking at consensus defined by a different number of DB.  \n",
    "There biggest difference is consensus size 1 (only 1 DB source): 3.27% less good than global for avg accuracy per class. (global acc is very similar)\n",
    "\n",
    "Core 7 files, including ENCODE\n",
    "\n",
    "| Consensus size | Nb files | Nb files   | Input size     | Files with min_pred > 0.6| Global accuracy (Nb match assay7) | Average acc per class |\n",
    "|---------------|---------|--------------|----------------|--------------------------|-----------------------------------|-----------------------|\n",
    "| 1             | 3320    | 7.05%        | 61.81%         | 88.80%                   | 95.45%                            | 92.87%                |\n",
    "| 2             | 25248   | 53.64%       | 42.08%         | 89.17%                   | 95.80%                            | 96.93%                |\n",
    "| 3             | 9976    | 21.20%       | 36.96%         | 88.99%                   | 95.31%                            | 95.28%                |\n",
    "| 4             | 8118    | 17.25%       | 18.05%         | 92.21%                   | 96.05%                            | 96.14%                |\n",
    "| Global        | 46018   | 100%         | 36.30          | 89.64%                   | 96.05%                            | 96.14%                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_metadata_dir = base_data_dir / \"metadata\" / \"chip_atlas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_meta_df = pd.read_csv(\n",
    "    ca_metadata_dir / \"CA_extracted_metadata_FW_20250314.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "other_meta_df.fillna(\"unknown\", inplace=True)\n",
    "display(other_meta_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_metadata_path = ca_metadata_dir / \"CA_metadata_joined_20250306.tsv\"\n",
    "general_metadata_df = pd.read_csv(general_metadata_path, sep=\"\\t\", low_memory=False)\n",
    "print(general_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_label = \"Meta_data_submitted_by_authors\"\n",
    "temp_df = general_metadata_df[general_metadata_df[col_label] != \"unknown\"]\n",
    "\n",
    "other_meta_vals = {}\n",
    "for key, vals in temp_df[[\"Experimental-id\", col_label]].values:\n",
    "    values_list = ast.literal_eval(vals)\n",
    "    values_dict = {val.split(\"=\")[0]: val.split(\"=\")[1] for val in values_list}\n",
    "    values_dict[\"Experimental-id\"] = key\n",
    "    other_meta_vals[key] = values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_meta_df = pd.DataFrame.from_dict(other_meta_vals, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomaterial_related_cols = [\n",
    "    \"strain\",\n",
    "    \"cell type\",\n",
    "    \"cell_type\",\n",
    "    \"cell line\",\n",
    "    \"cell_line\",\n",
    "    \"sample type\",\n",
    "    \"sample_type\",\n",
    "    \"tissue\",\n",
    "    \"biomaterial_type\",\n",
    "    \"cell\",\n",
    "    \"cells\",\n",
    "    \"cells/tissue\",\n",
    "    \"cell line/type\",\n",
    "    \"tissue source/type\",\n",
    "    \"tissue source\",\n",
    "    \"cell line id\",\n",
    "    \"parental cell line\",\n",
    "    \"tissue origin\",\n",
    "    \"biosample type\",\n",
    "    \"cell line source\",\n",
    "    \"clone name\",\n",
    "    \"cell-type/cell line\",\n",
    "    \"cell line background\",\n",
    "    \"cell-line\",\n",
    "    \"histology\",\n",
    "    \"tissue/cell type\",\n",
    "    \"source cell type\",\n",
    "    \"cell types\",\n",
    "    \"cell_line/tissue\",\n",
    "    \"cell lines\",\n",
    "    \"cell line or tissue\",\n",
    "    \"cell or tissue type\",\n",
    "    \"line name\",\n",
    "    \"tissue/cells\",\n",
    "    \"cell line/strain\",\n",
    "    \"cell strain\",\n",
    "    \"biosample\",\n",
    "    \"cell lin\",\n",
    "    \"cell line of origin\",\n",
    "    \"cell line name\",\n",
    "    \"tissue/cell line\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomaterial_df = other_meta_df[biomaterial_related_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomaterial_df = (\n",
    "    biomaterial_df.replace(\"n/a\", np.nan).replace(\"unknown\", np.nan).replace(\"NA\", np.nan)\n",
    ")\n",
    "relevant_cols = []\n",
    "\n",
    "for col in biomaterial_df.columns:\n",
    "    col_content = biomaterial_df[col]\n",
    "    counts = col_content.value_counts(dropna=True)\n",
    "    if counts.sum() > 100:\n",
    "        relevant_cols.append(col)\n",
    "\n",
    "        # with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        #     print(col.upper(), \"\\n\")\n",
    "        #     print(counts)\n",
    "        #     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biomaterial_df = biomaterial_df[relevant_cols]\n",
    "biomaterial_df.to_csv(ca_metadata_dir / \"biomaterial_metadata_20250306.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ca_core_df.copy(deep=True)\n",
    "df = pd.merge(df, other_meta_df, how=\"left\", on=\"Experimental-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    df,\n",
    "    general_metadata_df,\n",
    "    how=\"left\",\n",
    "    on=\"Experimental-id\",\n",
    "    suffixes=(\"\", \"_DROP\"),\n",
    ")\n",
    "df = df.drop([col for col in df.columns if col.endswith(\"_DROP\")], axis=1)\n",
    "df = df.drop(\"ID\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Predicted_class_donorlife\"] = df[\"Predicted_class_donorlife\"].replace(\n",
    "    {\n",
    "        \"newborn\": \"perinatal\",\n",
    "        \"fetal\": \"perinatal\",\n",
    "        \"embryonic\": \"perinatal\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a DF with no cell line\n",
    "\n",
    "Life stage classifier was not trained on any cell line, and also the notion of life stage makes less sense for cell lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_cell_line = df[\"Cell_type_description\"].str.lower().str.contains(\"cell line\")\n",
    "\n",
    "no_cell_line_df = df[~mask_cell_line]\n",
    "print(f\"{len(df) - len(no_cell_line_df)} rows removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check, are there some cell lines with life stages, if so what are the samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line_df = df[mask_cell_line]\n",
    "display(cell_line_df[\"expected_donorlife\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cell lines with perinatal status:\")\n",
    "display(\n",
    "    cell_line_df[cell_line_df[\"expected_donorlife\"] == \"perinatal\"][\n",
    "        [\"Cell_type\", \"Cell_type_description\"]\n",
    "    ].drop_duplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think dropping \"HEK293-T-REx\" and \"NT2-D1\" from life stage predictions is justified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(df, min_pred: float = 0.6):\n",
    "    \"\"\"Prints metrics for the given df.\n",
    "    Classification report and confusion matrix for sex, cancer and donorlife.\"\"\"\n",
    "    df = df.copy(deep=True)\n",
    "    for name in [\"sex\", \"cancer\", \"donorlife\"]:\n",
    "        print(f\"--- {name} ---\")\n",
    "        col_max_pred = f\"Max_pred_{name}\"\n",
    "        col_pred = f\"Predicted_class_{name}\"\n",
    "        col_true = f\"expected_{name}\"\n",
    "\n",
    "        no_unknown_df = df[df[col_true] != \"unknown\"]\n",
    "        print(\n",
    "            f\"Removing {len(df) - len(no_unknown_df)} rows with unknown.\\nLeft with {len(no_unknown_df)} rows.\"\n",
    "        )\n",
    "\n",
    "        high_conf_df = no_unknown_df[no_unknown_df[col_max_pred] >= min_pred]\n",
    "        print(\n",
    "            f\"Removing {len(no_unknown_df) - len(high_conf_df)} rows with low confidence.\\nLeft with {len(high_conf_df)} rows\\n\"\n",
    "        )\n",
    "\n",
    "        preds = high_conf_df[col_pred]\n",
    "        true = high_conf_df[col_true]\n",
    "        labels = sorted(set(true.unique()) | set(preds.unique()))\n",
    "\n",
    "        print(classification_report(true, preds, zero_division=0, digits=4))\n",
    "\n",
    "        cm = sk_cm(true, preds, labels=labels)\n",
    "        print(labels)\n",
    "        print(str(cm) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in [df, no_cell_line_df]:\n",
    "    print_metrics(dataframe, min_pred=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about life stage performance for similar cell types? Difficult to know without extensive labeling, but ENCODE results suggest it has a significant effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary metrics by assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_handler = MetricsPerAssay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"in_epiatlas\"] = df[\"is_EpiAtlas_EpiRR\"].astype(str) != \"0\"\n",
    "display(df[\"in_epiatlas\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"assay7\", \"sex\", \"cancer\", \"donorlife\"]\n",
    "column_templates = {\n",
    "    \"True\": \"expected_{}\",\n",
    "    \"Predicted\": \"Predicted_class_{}\",\n",
    "    \"Max pred\": \"Max_pred_{}\",\n",
    "}\n",
    "df[\"expected_assay7\"] = df[\"manual_target_consensus\"]\n",
    "\n",
    "compute_fct_kwargs = {\n",
    "    \"no_epiatlas\": True,\n",
    "    \"merge_assays\": False,\n",
    "    \"categories\": categories,\n",
    "    \"column_templates\": column_templates,\n",
    "    \"assay_label\": \"manual_target_consensus\",\n",
    "    \"core_assays\": CORE_ASSAYS + [\"no_consensus\"],\n",
    "    \"non_core_assays\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filename = \"C-A_acc_per_assay\"\n",
    "\n",
    "metrics_handler.compute_multiple_metric_formats(\n",
    "    preds=df,\n",
    "    general_filename=base_filename,\n",
    "    folders_to_save=[ca_pred_path.parent.parent],\n",
    "    verbose=False,\n",
    "    return_df=False,\n",
    "    compute_fct_kwargs=compute_fct_kwargs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
