{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Workbook to analyse classifier predictions on ChIP-Atlas data.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Workbook to analyse classifier predictions on ChIP-Atlas data.\"\"\"\n",
    "\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches, pointless-statement, duplicate-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import ast\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import upsetplot\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.metrics_per_assay import MetricsPerAssay\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY_ORDER,\n",
    "    IHECColorMap,\n",
    "    print_column_content,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h3k4me3',\n",
       " 'h3k27ac',\n",
       " 'h3k4me1',\n",
       " 'h3k36me3',\n",
       " 'h3k27me3',\n",
       " 'h3k9me3',\n",
       " 'input',\n",
       " 'rna_seq',\n",
       " 'wgbs']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ASSAY_ORDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "table_dir = base_dir / \"tables\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48669, 145)\n"
     ]
    }
   ],
   "source": [
    "ca_dir = base_data_dir / \"training_results\" / \"predictions\" / \"C-A\" / \"assay_epiclass\"\n",
    "\n",
    "ca_filename = \"CA_metadata_4DB+all_pred.20240606_mod3.0.tsv\"\n",
    "ca_pred_path = ca_dir / ca_filename\n",
    "\n",
    "ca_pred_df = pd.read_csv(ca_pred_path, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "print(ca_pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = ca_pred_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_ASSAYS = ASSAY_ORDER[0:7]\n",
    "\n",
    "DB_COLS = [\"GEO_mod\", \"C-A\", \"Cistrome\", \"NGS_mod\"]\n",
    "\n",
    "PRED_COLS = [\n",
    "    \"Predicted_class_assay7\",\n",
    "    \"Predicted_class_assay11\",\n",
    "    \"Predicted_class_assay13\",\n",
    "]\n",
    "\n",
    "SAME_TARGET = \"core7_DBs_consensus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base dataset used: Chip-Atlas experiments where at least one of the BD declared the target in core7.\n",
    "\n",
    "Excluding: \n",
    "- Samples where at least one the DB declared a target out of core7.\n",
    "- samples overlapping with EpiATLAS dataset (different file creation pipeline, same base bam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Identical    0.885122\n",
       "1 source     0.069654\n",
       "Different    0.045224\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_df = ca_pred_df.loc[:, DB_COLS].copy(deep=True)\n",
    "tmp_df[\"C-A\"].replace(\"unclassified\", \"----\", inplace=True)\n",
    "\n",
    "id_db_target = []\n",
    "unique_labels = Counter()\n",
    "different_labels = Counter()\n",
    "\n",
    "for labels in tmp_df.values:\n",
    "    missing_N = sum(label == \"----\" for label in labels)\n",
    "    db_labels = set(labels)\n",
    "\n",
    "    try:\n",
    "        db_labels.remove(\"----\")\n",
    "    except KeyError:\n",
    "        pass\n",
    "    if missing_N == 3:\n",
    "        id_db_target.append(\"1 source\")\n",
    "    elif len(db_labels) == 1:\n",
    "        id_db_target.append(\"Identical\")\n",
    "    else:\n",
    "        id_db_target.append(\"Different\")\n",
    "        different_labels[tuple(db_labels)] += 1\n",
    "\n",
    "    unique_labels[tuple(db_labels)] += 1\n",
    "\n",
    "\n",
    "display(pd.Series(id_db_target).value_counts(dropna=False, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 48669 rows.\n",
      "Removed 1604 rows with core7_DBs_consensus in ['Ignored - Potential non-core', 'non-core/CTCF'].\n",
      "After this, 47065 rows remain.\n"
     ]
    }
   ],
   "source": [
    "non_core_labels = [\"non-core\", \"CTCF\", \"ctcf\"]\n",
    "non_core_labels_2 = [\"Ignored - Potential non-core\", \"non-core/CTCF\"]\n",
    "\n",
    "print(f\"Starting with {len(ca_pred_df)} rows.\")\n",
    "ca_core_df = ca_pred_df[~ca_pred_df[SAME_TARGET].isin(non_core_labels_2)]\n",
    "diff_N = len(ca_pred_df) - len(ca_core_df)\n",
    "print(\n",
    "    f\"Removed {diff_N} rows with {SAME_TARGET} in {non_core_labels_2}.\\nAfter this, {len(ca_core_df)} rows remain.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1047 rows with EpiATLAS EpiRR overlap. After this, 46018 rows remain.\n"
     ]
    }
   ],
   "source": [
    "N_diff = len(ca_core_df)\n",
    "ca_core_df = ca_core_df[ca_core_df[\"is_EpiAtlas_EpiRR\"].astype(str) == \"0\"].copy()\n",
    "N_diff -= len(ca_core_df)\n",
    "print(\n",
    "    f\"Removed {N_diff} rows with EpiATLAS EpiRR overlap. After this, {len(ca_core_df)} rows remain.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for db_col in DB_COLS:\n",
    "    col = ca_core_df[db_col]\n",
    "    if col.isna().sum():\n",
    "        print(\"Missing values: \", ca_core_df[col.isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_61812\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_61812_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_61812_level0_col1\" class=\"col_heading level0 col1\" >relative</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >manual_target_consensus</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_61812_level0_row0\" class=\"row_heading level0 row0\" >input</th>\n",
       "      <td id=\"T_61812_row0_col0\" class=\"data row0 col0\" >17637</td>\n",
       "      <td id=\"T_61812_row0_col1\" class=\"data row0 col1\" >38.33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61812_level0_row1\" class=\"row_heading level0 row1\" >h3k27ac</th>\n",
       "      <td id=\"T_61812_row1_col0\" class=\"data row1 col0\" >11087</td>\n",
       "      <td id=\"T_61812_row1_col1\" class=\"data row1 col1\" >24.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61812_level0_row2\" class=\"row_heading level0 row2\" >h3k4me3</th>\n",
       "      <td id=\"T_61812_row2_col0\" class=\"data row2 col0\" >6213</td>\n",
       "      <td id=\"T_61812_row2_col1\" class=\"data row2 col1\" >13.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61812_level0_row3\" class=\"row_heading level0 row3\" >h3k27me3</th>\n",
       "      <td id=\"T_61812_row3_col0\" class=\"data row3 col0\" >4171</td>\n",
       "      <td id=\"T_61812_row3_col1\" class=\"data row3 col1\" >9.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61812_level0_row4\" class=\"row_heading level0 row4\" >h3k4me1</th>\n",
       "      <td id=\"T_61812_row4_col0\" class=\"data row4 col0\" >3000</td>\n",
       "      <td id=\"T_61812_row4_col1\" class=\"data row4 col1\" >6.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61812_level0_row5\" class=\"row_heading level0 row5\" >h3k9me3</th>\n",
       "      <td id=\"T_61812_row5_col0\" class=\"data row5 col0\" >2086</td>\n",
       "      <td id=\"T_61812_row5_col1\" class=\"data row5 col1\" >4.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61812_level0_row6\" class=\"row_heading level0 row6\" >h3k36me3</th>\n",
       "      <td id=\"T_61812_row6_col0\" class=\"data row6 col0\" >1421</td>\n",
       "      <td id=\"T_61812_row6_col1\" class=\"data row6 col1\" >3.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61812_level0_row7\" class=\"row_heading level0 row7\" >no_consensus</th>\n",
       "      <td id=\"T_61812_row7_col0\" class=\"data row7 col0\" >403</td>\n",
       "      <td id=\"T_61812_row7_col1\" class=\"data row7 col1\" >0.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_61812_level0_row8\" class=\"row_heading level0 row8\" >Total</th>\n",
       "      <td id=\"T_61812_row8_col0\" class=\"data row8 col0\" >46018</td>\n",
       "      <td id=\"T_61812_row8_col1\" class=\"data row8 col1\" >100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc61eddc1f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_column_content(ca_core_df, \"manual_target_consensus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_caa7c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_caa7c_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_caa7c_level0_col1\" class=\"col_heading level0 col1\" >relative</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >core7_DBs_consensus</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_caa7c_level0_row0\" class=\"row_heading level0 row0\" >Identical</th>\n",
       "      <td id=\"T_caa7c_row0_col0\" class=\"data row0 col0\" >41992</td>\n",
       "      <td id=\"T_caa7c_row0_col1\" class=\"data row0 col1\" >91.25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_caa7c_level0_row1\" class=\"row_heading level0 row1\" >1 source</th>\n",
       "      <td id=\"T_caa7c_row1_col0\" class=\"data row1 col0\" >3320</td>\n",
       "      <td id=\"T_caa7c_row1_col1\" class=\"data row1 col1\" >7.21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_caa7c_level0_row2\" class=\"row_heading level0 row2\" >Different</th>\n",
       "      <td id=\"T_caa7c_row2_col0\" class=\"data row2 col0\" >706</td>\n",
       "      <td id=\"T_caa7c_row2_col1\" class=\"data row2 col1\" >1.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_caa7c_level0_row3\" class=\"row_heading level0 row3\" >Total</th>\n",
       "      <td id=\"T_caa7c_row3_col0\" class=\"data row3 col0\" >46018</td>\n",
       "      <td id=\"T_caa7c_row3_col1\" class=\"data row3 col1\" >100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc61eddc850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_column_content(ca_core_df, SAME_TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_consensus_df = ca_core_df[ca_core_df[\"manual_target_consensus\"] == \"no_consensus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upset plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"fig_C-A\" / \"DB_upset\" / \"no_EpiATLAS\"\n",
    "fig_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_db_upsetplot(\n",
    "    df: pd.DataFrame, db_cols: List[str], title: str\n",
    ") -> upsetplot.UpSet:\n",
    "    \"\"\"Make an upsetplot of the sample presence in the different databases.\"\"\"\n",
    "    df = df.copy()\n",
    "    if SAME_TARGET not in df.columns:\n",
    "        raise ValueError(\"Column 'identical_DBs_target' not found in DataFrame.\")\n",
    "\n",
    "    # Create a new DataFrame with boolean columns for each database\n",
    "    upset_df = pd.DataFrame()\n",
    "    for col in db_cols:\n",
    "        upset_df[col] = df[col] != \"----\"\n",
    "    upset_df[SAME_TARGET] = df[SAME_TARGET]\n",
    "\n",
    "    # Set the index for the UpSet plot\n",
    "    upset_df = upset_df.set_index(db_cols)\n",
    "\n",
    "    # Create the UpSet plot\n",
    "    upset = upsetplot.UpSet(\n",
    "        upset_df,\n",
    "        intersection_plot_elements=0,  # disable the default bar chart\n",
    "        sort_by=\"cardinality\",\n",
    "        show_counts=True,  # type: ignore\n",
    "        orientation=\"horizontal\",\n",
    "    )\n",
    "\n",
    "    # Add stacked bars\n",
    "    upset.add_stacked_bars(by=SAME_TARGET, elements=15)\n",
    "\n",
    "    # Plot and set title\n",
    "    axes = upset.plot()\n",
    "    plt.suptitle(title)\n",
    "    axes[\"totals\"].set_title(\"Total\")\n",
    "    plt.legend(loc=\"center left\")\n",
    "    return upset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"All core7 ChIP-Atlas samples presence in used DBs)\\nTarget consensus\"\n",
    "upset = make_db_upsetplot(ca_core_df, DB_COLS, title=title)\n",
    "\n",
    "plt.savefig(fig_dir / \"upsetplot_DB_core7_samples.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No ENCODE EpiRR overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_8d6c9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8d6c9_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_8d6c9_level0_col1\" class=\"col_heading level0 col1\" >relative</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >ENCODE</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8d6c9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8d6c9_row0_col0\" class=\"data row0 col0\" >40071</td>\n",
       "      <td id=\"T_8d6c9_row0_col1\" class=\"data row0 col1\" >87.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8d6c9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_8d6c9_row1_col0\" class=\"data row1 col0\" >5947</td>\n",
       "      <td id=\"T_8d6c9_row1_col1\" class=\"data row1 col1\" >12.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8d6c9_level0_row2\" class=\"row_heading level0 row2\" >Total</th>\n",
       "      <td id=\"T_8d6c9_row2_col0\" class=\"data row2 col0\" >46018</td>\n",
       "      <td id=\"T_8d6c9_row2_col1\" class=\"data row2 col1\" >100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc61ef06df0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_column_content(ca_core_df, \"ENCODE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no encode\n",
    "no_encode_df = ca_core_df[ca_core_df[\"ENCODE\"] == 0]\n",
    "title = \"ChIP-Atlas samples presence in used DBs\\nTarget Consensus - No ENCODE\"\n",
    "\n",
    "upset = make_db_upsetplot(no_encode_df, DB_COLS, title=title)\n",
    "\n",
    "plt.savefig(fig_dir / \"upsetplot_DB_core7_samples_noENC.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prediction_resolved(row, pred_col: str, db_cols: List[str]) -> bool:\n",
    "    \"\"\"Check if the prediction matches any of the database columns.\"\"\"\n",
    "    pred_val = row[pred_col]\n",
    "    db_vals = [row[col] for col in db_cols]\n",
    "    return pred_val in db_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolved (min_predScore >= 0): 682 / 706 (96.60%)\n",
      "Resolved (min_predScore >= 0, excluding 'input' predictions): 229 / 247 (92.71%)\n",
      "Resolved (min_predScore >= 0.6): 612 / 619 (98.87%)\n",
      "Resolved (min_predScore >= 0.6, excluding 'input' predictions): 214 / 217 (98.62%)\n"
     ]
    }
   ],
   "source": [
    "# the classifier was able to resolve xx% of the cases where the target was not identical between the sources\n",
    "different_targets_df = ca_core_df[ca_core_df[SAME_TARGET] == \"Different\"]\n",
    "\n",
    "for min_pred_score in [0, 0.6]:\n",
    "    filtered_df = different_targets_df[\n",
    "        different_targets_df[\"Max_pred_assay7\"] >= min_pred_score\n",
    "    ]\n",
    "\n",
    "    pred_col = PRED_COLS[0]\n",
    "\n",
    "    num_resolved = filtered_df.apply(\n",
    "        is_prediction_resolved, axis=1, args=(pred_col, DB_COLS)\n",
    "    ).sum()\n",
    "\n",
    "    print(\n",
    "        f\"Resolved (min_predScore >= {min_pred_score}): \"\n",
    "        f\"{num_resolved} / {len(filtered_df)} \"\n",
    "        f\"({num_resolved / len(filtered_df) * 100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    # Exclude rows where the prediction is labeled as 'input'\n",
    "    non_input_df = filtered_df[filtered_df[PRED_COLS[0]] != \"input\"]\n",
    "    num_resolved = non_input_df.apply(\n",
    "        is_prediction_resolved, axis=1, args=(pred_col, DB_COLS)\n",
    "    ).sum()\n",
    "\n",
    "    print(\n",
    "        f\"Resolved (min_predScore >= {min_pred_score}, excluding 'input' predictions): \"\n",
    "        f\"{num_resolved} / {len(non_input_df)} \"\n",
    "        f\"({num_resolved / len(non_input_df) * 100:.2f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level prediction accuracy breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `epiclass_match_status` column and join it to predictions.\n",
    "\n",
    "This category represents the agreement between the databases labels and \n",
    "the classifier prediction.  \n",
    "\n",
    "If there is a database consensus and our prediction matches, it's a complete match.  \n",
    "If there is no database consensus, but our prediction matches one of the databases labels, it's a partial match.  \n",
    "Otherwise, no match.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epiclass_match_status = []\n",
    "for _, row in ca_core_df.iterrows():\n",
    "    target_vals = [row[col] for col in DB_COLS]\n",
    "    consensus: str = row[\"manual_target_consensus\"]\n",
    "\n",
    "    epiclass_target: str = row[\"Predicted_class_assay7\"]\n",
    "\n",
    "    if epiclass_target == consensus:\n",
    "        epiclass_match_status.append(\"Complete match\")\n",
    "        continue\n",
    "\n",
    "    if epiclass_target in target_vals:\n",
    "        epiclass_match_status.append(\"Partial match\")\n",
    "        continue\n",
    "\n",
    "    epiclass_match_status.append(\"No match\")\n",
    "\n",
    "ca_core_df[\"epiclass_match_status\"] = epiclass_match_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction agreement, minimum prediction score >= 0.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_096db\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_096db_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_096db_level0_col1\" class=\"col_heading level0 col1\" >relative</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >epiclass_match_status</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_096db_level0_row0\" class=\"row_heading level0 row0\" >Complete match</th>\n",
       "      <td id=\"T_096db_row0_col0\" class=\"data row0 col0\" >42266</td>\n",
       "      <td id=\"T_096db_row0_col1\" class=\"data row0 col1\" >91.85%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_096db_level0_row1\" class=\"row_heading level0 row1\" >No match</th>\n",
       "      <td id=\"T_096db_row1_col0\" class=\"data row1 col0\" >3217</td>\n",
       "      <td id=\"T_096db_row1_col1\" class=\"data row1 col1\" >6.99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_096db_level0_row2\" class=\"row_heading level0 row2\" >Partial match</th>\n",
       "      <td id=\"T_096db_row2_col0\" class=\"data row2 col0\" >535</td>\n",
       "      <td id=\"T_096db_row2_col1\" class=\"data row2 col1\" >1.16%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_096db_level0_row3\" class=\"row_heading level0 row3\" >Total</th>\n",
       "      <td id=\"T_096db_row3_col0\" class=\"data row3 col0\" >46018</td>\n",
       "      <td id=\"T_096db_row3_col1\" class=\"data row3 col1\" >100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc61ef06df0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction agreement, minimum prediction score >= 0.60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_893e9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_893e9_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_893e9_level0_col1\" class=\"col_heading level0 col1\" >relative</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >epiclass_match_status</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_893e9_level0_row0\" class=\"row_heading level0 row0\" >Complete match</th>\n",
       "      <td id=\"T_893e9_row0_col0\" class=\"data row0 col0\" >39054</td>\n",
       "      <td id=\"T_893e9_row0_col1\" class=\"data row0 col1\" >94.78%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_893e9_level0_row1\" class=\"row_heading level0 row1\" >No match</th>\n",
       "      <td id=\"T_893e9_row1_col0\" class=\"data row1 col0\" >1666</td>\n",
       "      <td id=\"T_893e9_row1_col1\" class=\"data row1 col1\" >4.04%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_893e9_level0_row2\" class=\"row_heading level0 row2\" >Partial match</th>\n",
       "      <td id=\"T_893e9_row2_col0\" class=\"data row2 col0\" >483</td>\n",
       "      <td id=\"T_893e9_row2_col1\" class=\"data row2 col1\" >1.17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_893e9_level0_row3\" class=\"row_heading level0 row3\" >Total</th>\n",
       "      <td id=\"T_893e9_row3_col0\" class=\"data row3 col0\" >41203</td>\n",
       "      <td id=\"T_893e9_row3_col1\" class=\"data row3 col1\" >100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc63b956af0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for min_pred_score in [0, 0.6]:\n",
    "    print(f\"Prediction agreement, minimum prediction score >= {min_pred_score:.2f}\")\n",
    "    subset_df = ca_core_df[ca_core_df[\"Max_pred_assay7\"] >= min_pred_score]\n",
    "    print_column_content(subset_df, \"epiclass_match_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48669, 36) Experimental-id\n"
     ]
    }
   ],
   "source": [
    "predictions_dir = base_data_dir / \"training_results\" / \"predictions\"\n",
    "pred_path = predictions_dir / \"C-A\" / \"CA_only_pred_20240606.tsv\"\n",
    "pred_df = pd.read_csv(pred_path, sep=\"\\t\", low_memory=False)\n",
    "print(pred_df.shape, pred_df.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = \"epiclass_match_status\"\n",
    "if new_col not in pred_df.columns:\n",
    "    index_1 = pred_df.columns[0]\n",
    "    index_2 = ca_core_df.columns[0]\n",
    "    pred_df = pd.merge(\n",
    "        pred_df,\n",
    "        ca_core_df[[index_2, new_col]],\n",
    "        how=\"left\",\n",
    "        left_on=pred_df.columns[0],\n",
    "        right_on=ca_core_df.columns[0],\n",
    "        suffixes=(\"\", \"_DROP\"),\n",
    "    )\n",
    "    pred_df = pred_df.drop(\n",
    "        columns=[col for col in pred_df.columns if col.endswith(\"_DROP\")]\n",
    "    )\n",
    "    pred_df[\"epiclass_match_status\"].fillna(\"NA\", inplace=True)\n",
    "    pred_df.to_csv(pred_path, sep=\"\\t\", index=False)\n",
    "\n",
    "del pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details prediction stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_high_level_pred_info(df: pd.DataFrame, save_conf_matrix: bool = False) -> None:\n",
    "    \"\"\"High level information about the predictions.\"\"\"\n",
    "    for assay in CORE_ASSAYS:\n",
    "        print(f\"{assay}\")\n",
    "        assay_df = df[df[\"manual_target_consensus\"] == assay]\n",
    "        for col in [\n",
    "            \"Predicted_class_assay7\",\n",
    "            \"Predicted_class_assay11\",\n",
    "            \"Predicted_class_assay13\",\n",
    "        ]:\n",
    "            assay_number = col.rsplit(\"_\", maxsplit=1)[-1]\n",
    "            display(assay_df[col].value_counts() / len(assay_df) * 100)\n",
    "            if any(label in col for label in [\"11\", \"13\"]):\n",
    "                wrong_pred = assay_df[assay_df[col] != assay]\n",
    "\n",
    "                display(\n",
    "                    wrong_pred[f\"2nd_pred_class_{assay_number}\"].value_counts()\n",
    "                    / len(wrong_pred)\n",
    "                    * 100\n",
    "                )\n",
    "        print(\"\\n\")\n",
    "\n",
    "    if save_conf_matrix:\n",
    "        for col in [\n",
    "            \"Predicted_class_assay7\",\n",
    "            \"Predicted_class_assay11\",\n",
    "            \"Predicted_class_assay13\",\n",
    "        ]:\n",
    "            labels = sorted(df[col].unique().tolist())\n",
    "            cm = sk_cm(\n",
    "                df[\"manual_target_consensus\"],\n",
    "                df[col],\n",
    "                labels=labels,\n",
    "            )\n",
    "            cm_writer = ConfusionMatrixWriter(labels=labels, confusion_matrix=cm)\n",
    "            cm_writer.to_png(\n",
    "                Path.home() / \"Downloads\" / f\"C-A_confusion_matrix_{col}.png\"\n",
    "            )\n",
    "\n",
    "    print(\"What is the actual target when wgbs-standard is predicted?\")\n",
    "    for assay_number in [\"assay11\", \"assay13\"]:\n",
    "        print(f\"{assay_number}\")\n",
    "        wgbs_dist = ca_pred_df[\n",
    "            ca_pred_df[f\"Predicted_class_{assay_number}\"] == \"wgbs-standard\"\n",
    "        ][\"manual_target_consensus\"]\n",
    "        display(wgbs_dist.value_counts())\n",
    "        display(wgbs_dist.value_counts() / len(wgbs_dist) * 100)\n",
    "\n",
    "    print(\"What is the actual target when non-core is predicted?\")\n",
    "    col = \"Predicted_class_assay13\"\n",
    "    wgbs_dist = ca_pred_df[ca_pred_df[col] == \"non-core\"][\"manual_target_consensus\"]\n",
    "    display(wgbs_dist.value_counts())\n",
    "    display(wgbs_dist.value_counts() / len(wgbs_dist) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "if verbose:\n",
    "    print_column_content(ca_pred_df, \"manual_target_consensus\")\n",
    "    print_high_level_pred_info(ca_pred_df, save_conf_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break no_consensus (minPred >= 0.60): 90.82% (366/403)\n",
      "non-input tie breakers: 130/366 (35.52%)\n",
      "\n",
      "ENCODE\n",
      "0    366\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Break no_consensus (minPred >= 0.80): 80.15% (323/403)\n",
      "non-input tie breakers: 115/323 (35.60%)\n",
      "\n",
      "ENCODE\n",
      "0    323\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for min_pred in [0.6, 0.8]:\n",
    "    break_tie_mask = no_consensus_df[\"Max_pred_assay7\"] >= min_pred\n",
    "    nb_break_tie = break_tie_mask.sum()\n",
    "    print(\n",
    "        f\"Break no_consensus (minPred >= {min_pred:.02f}): {nb_break_tie/ len(no_consensus_df) * 100:.02f}% ({nb_break_tie}/{len(no_consensus_df)})\"\n",
    "    )\n",
    "    df = no_consensus_df[break_tie_mask]\n",
    "\n",
    "    nb_not_input = (df[\"Predicted_class_assay7\"] != \"input\").sum()\n",
    "    print(\n",
    "        f\"non-input tie breakers: {nb_not_input}/{nb_break_tie} ({nb_not_input/len(df) * 100:.02f}%)\\n\"\n",
    "    )\n",
    "    print(df[\"ENCODE\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_within_threshold(\n",
    "    df: pd.DataFrame, min_pred: float = 0.6, col: str = \"Max_pred_assay7\"\n",
    ") -> None:\n",
    "    \"\"\"Print the predictions percentage within a threshold.\"\"\"\n",
    "    try:\n",
    "        mask = df[col].astype(float) >= min_pred\n",
    "    except KeyError:\n",
    "        print(f\"Column {col} not found.\")\n",
    "        return\n",
    "    nb_pred = mask.sum()\n",
    "    print(\n",
    "        f\"Nb pred {col.split('_')[-1]} (pred score >= {min_pred:.02f}): {nb_pred/len(df) * 100:.02f}% ({nb_pred}/{len(df)})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_confusion_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    fig_dir: Path | str,\n",
    "    nb_classes: int | str = 7,\n",
    "    min_pred: float = 0.6,\n",
    "):\n",
    "    \"\"\"Save the confusion matrix for core assays predictions. Does not filter.\"\"\"\n",
    "    col = f\"Predicted_class_assay{nb_classes}\"\n",
    "    cm = sk_cm(df[\"manual_target_consensus\"], df[col], labels=CORE_ASSAYS)\n",
    "    cm_writer = ConfusionMatrixWriter(labels=CORE_ASSAYS, confusion_matrix=cm)\n",
    "\n",
    "    name = f\"confusion_matrix_assay{nb_classes}_core7_minPred{min_pred:.02f}\"\n",
    "    if df[\"ENCODE\"].sum() == 0:\n",
    "        name += \"_noENCODE\"\n",
    "\n",
    "    cm_writer.to_all_formats(logdir=fig_dir, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_breakdown_predictions(\n",
    "    df: pd.DataFrame,\n",
    "    min_pred: float = 0.6,\n",
    "    nb_classes: int | str = 7,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Breakdown the predictions, print results.\"\"\"\n",
    "    df = df[df[f\"Max_pred_assay{nb_classes}\"] >= min_pred]\n",
    "\n",
    "    pred_col = f\"Predicted_class_assay{nb_classes}\"\n",
    "    match_consensus = df[\"manual_target_consensus\"] == df[pred_col]\n",
    "    nb_match = match_consensus.sum()\n",
    "    nb_error = (~match_consensus).sum()\n",
    "    print(f\"Nb match assay{nb_classes}: {nb_match/ len(df):.2%} ({nb_match}/{len(df)})\")\n",
    "    print(f\"Nb error assay{nb_classes}: {nb_error/ len(df):.2%} ({nb_error}/{len(df)})\\n\")\n",
    "\n",
    "    correct_pred_df = df[match_consensus]\n",
    "    incorrect_pred_df = df[~match_consensus]\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            r\"Following ratios: % of assay subset OR % of all predictions OR % of all incorrect predictions (potential mislabels).\",\n",
    "            \"\\n\",\n",
    "        )\n",
    "    acc_per_class = {}\n",
    "    for assay in CORE_ASSAYS:\n",
    "        assay_df = df[df[pred_col] == assay]\n",
    "        nb_assay = len(assay_df)\n",
    "\n",
    "        nb_assay_correct = len(correct_pred_df[correct_pred_df[pred_col] == assay])\n",
    "        nb_assay_incorrect = len(incorrect_pred_df[incorrect_pred_df[pred_col] == assay])\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Predictions as {assay}: {nb_assay / len(df):.2%} ({nb_assay}/{len(df)})\"\n",
    "            )\n",
    "        perc_cor = nb_assay_correct / nb_assay\n",
    "        perc_cor2 = nb_assay_correct / len(df)\n",
    "        perc_inc = nb_assay_incorrect / nb_assay\n",
    "        perc_inc2 = nb_assay_incorrect / len(df)\n",
    "        perc_inc3 = nb_assay_incorrect / len(incorrect_pred_df)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Correct predictions as {assay}: {perc_cor:.2%} ({nb_assay_correct}/{nb_assay}) OR {perc_cor2:.2%} ({nb_assay_correct}/{len(df)})\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Incorrect predictions as {assay}: \"\n",
    "                f\"{perc_inc:.2%} ({nb_assay_incorrect}/{nb_assay}) OR \"\n",
    "                f\"{perc_inc2:.2%} ({nb_assay_incorrect}/{len(df)}) OR \"\n",
    "                f\"{perc_inc3:.2%} ({nb_assay_incorrect}/{len(incorrect_pred_df)})\\n\"\n",
    "            )\n",
    "        acc_per_class[assay] = perc_cor\n",
    "\n",
    "    return acc_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "fig_dir = base_fig_dir / \"fig_C-A\" / \"confusion_matrices\"\n",
    "for subset in [[0], [0, 1]]:\n",
    "    # continue\n",
    "    if verbose:\n",
    "        if subset == [0]:\n",
    "            print(\"Subset: no ENCODE\")\n",
    "        else:\n",
    "            print(\"Subset: Include ENCODE\")\n",
    "\n",
    "    df = ca_core_df[ca_core_df[\"ENCODE\"].isin(subset)]\n",
    "\n",
    "    for min_pred in [0.6, 0.8, 0.9]:\n",
    "        # continue\n",
    "        if verbose:\n",
    "            print(\"Min pred score:\", min_pred)\n",
    "            print_pred_within_threshold(df, min_pred=min_pred)\n",
    "            print_breakdown_predictions(df, min_pred=min_pred)\n",
    "\n",
    "        sub_df = df[df[\"Max_pred_assay7\"] >= min_pred]\n",
    "        save_confusion_matrix(sub_df, fig_dir, min_pred=min_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mislabels by GSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = ca_dir / \"GSE_mispred\"\n",
    "logdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSE = \"Gse-geo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "nb_classes = 7\n",
    "min_pred = 0.6\n",
    "pred_col = f\"Predicted_class_assay{nb_classes}\"\n",
    "max_pred_col = f\"Max_pred_assay{nb_classes}\"\n",
    "\n",
    "excluding_no_consensus = True\n",
    "excluding_ENCODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ca_core_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 403 rows with no consensus.\n",
      "Left with 45615 rows.\n"
     ]
    }
   ],
   "source": [
    "if excluding_no_consensus:\n",
    "    N_diff = len(df)\n",
    "    df = df[df[\"manual_target_consensus\"] != \"no_consensus\"]\n",
    "    N_diff -= len(df)\n",
    "    print(f\"Removed {N_diff} rows with no consensus.\\nLeft with {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if excluding_ENCODE:\n",
    "    N_diff = len(df)\n",
    "    df = df[df[\"ENCODE\"] == 0]\n",
    "    N_diff -= len(df)\n",
    "    print(f\"Removed {N_diff} rows with ENCODE.\")\n",
    "\n",
    "    this_logdir = logdir / \"excluding_ENCODE\"\n",
    "else:\n",
    "    this_logdir = logdir / \"including_ENCODE\"\n",
    "\n",
    "this_logdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 4778/45615 (10.47%) rows with pred score < 0.6\n",
      "Left with 40837 rows.\n"
     ]
    }
   ],
   "source": [
    "N_total = len(df)\n",
    "N_diff = len(df)\n",
    "df = df[df[max_pred_col] >= min_pred]\n",
    "N_diff -= len(df)\n",
    "\n",
    "print(\n",
    "    f\"Removed {N_diff}/{N_total} ({N_diff/N_total:.2%}) rows with pred score < {min_pred}\\nLeft with {len(df)} rows.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb match assay7: 95.92% (39172/40837)\n",
      "Nb mismatch assay7: 4.08% (1665/40837)\n",
      "\n",
      "Excluding input predictions. Left with 519 complete mismatches.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if \"no_consensus\" in df[\"manual_target_consensus\"].unique():\n",
    "    raise ValueError(\"'no_consensus' present in df, cannot compute accuracy.\")\n",
    "\n",
    "no_match = df[\"epiclass_match_status\"] == \"No match\"\n",
    "nb_match = (~no_match).sum()\n",
    "nb_error = (no_match).sum()\n",
    "print(f\"Nb match assay{nb_classes}: {nb_match/ len(df):.2%} ({nb_match}/{len(df)})\")\n",
    "print(f\"Nb mismatch assay{nb_classes}: {nb_error/ len(df):.2%} ({nb_error}/{len(df)})\\n\")\n",
    "\n",
    "incorrect_pred_df = df[no_match]\n",
    "\n",
    "if verbose:\n",
    "    print(\"Incorrect predictions, breakdown by predicted class:\")\n",
    "    display(incorrect_pred_df[pred_col].value_counts(normalize=True))\n",
    "\n",
    "incorrect_pred_df = incorrect_pred_df[incorrect_pred_df[pred_col] != \"input\"]\n",
    "\n",
    "print(\n",
    "    f\"Excluding input predictions. Left with {len(incorrect_pred_df)} complete mismatches.\\n\"\n",
    ")\n",
    "\n",
    "desired_cols = [\"manual_target_consensus\", pred_col]\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    gse_count = incorrect_pred_df.groupby(GSE).size().sort_values(ascending=False)\n",
    "    gse_count = gse_count.to_frame()\n",
    "    gse_count.columns = [\"Nb of mismatches\"]\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Incorrect predictions, breakdown by GSE count ({len(gse_count)} unique GSE)\"\n",
    "        )\n",
    "\n",
    "    gse_count[\"cumsum\"] = gse_count.cumsum()\n",
    "    gse_count[\"cumsum (%)\"] = (\n",
    "        gse_count[\"cumsum\"] * 100 / sum(gse_count[\"Nb of mismatches\"])\n",
    "    )\n",
    "    if verbose:\n",
    "        display(gse_count.reset_index())\n",
    "\n",
    "    gse_count.to_csv(\n",
    "        this_logdir / \"gse_count_incorrect_pred_no_input_20240606_mod3.tsv\", sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    gse_target_count = incorrect_pred_df.groupby(GSE)[desired_cols].value_counts(dropna=False)  # type: ignore\n",
    "    if verbose:\n",
    "        print(\"Incorrect predictions, breakdown by GSE and target.\")\n",
    "        display(gse_target_count)\n",
    "\n",
    "    gse_target_count = gse_target_count.to_frame()\n",
    "    gse_target_count.columns = [\"Nb of mismatches\"]\n",
    "\n",
    "    gse_target_count.to_csv(\n",
    "        this_logdir / \"gse_target_count_incorrect_pred_no_input_20240606_mod3.tsv\",\n",
    "        sep=\"\\t\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unclassified files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb unclassified: 5115 (11.12%)\n",
      "Nb high pred unclassified: 4548 (88.91%)\n"
     ]
    }
   ],
   "source": [
    "unclassified = ca_core_df[ca_core_df[\"C-A\"] == \"unclassified\"]\n",
    "print(f\"Nb unclassified: {len(unclassified)} ({len(unclassified) / len(ca_core_df):.2%})\")\n",
    "\n",
    "high_pred = unclassified[unclassified[max_pred_col] >= min_pred]\n",
    "print(\n",
    "    f\"Nb high pred unclassified: {len(high_pred)} ({len(high_pred) / len(unclassified):.2%})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying consensus criterion (nb DB agreeing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f166e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f166e_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_f166e_level0_col1\" class=\"col_heading level0 col1\" >relative</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >manual_target_consensus</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f166e_level0_row0\" class=\"row_heading level0 row0\" >input</th>\n",
       "      <td id=\"T_f166e_row0_col0\" class=\"data row0 col0\" >17637</td>\n",
       "      <td id=\"T_f166e_row0_col1\" class=\"data row0 col1\" >38.33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f166e_level0_row1\" class=\"row_heading level0 row1\" >h3k27ac</th>\n",
       "      <td id=\"T_f166e_row1_col0\" class=\"data row1 col0\" >11087</td>\n",
       "      <td id=\"T_f166e_row1_col1\" class=\"data row1 col1\" >24.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f166e_level0_row2\" class=\"row_heading level0 row2\" >h3k4me3</th>\n",
       "      <td id=\"T_f166e_row2_col0\" class=\"data row2 col0\" >6213</td>\n",
       "      <td id=\"T_f166e_row2_col1\" class=\"data row2 col1\" >13.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f166e_level0_row3\" class=\"row_heading level0 row3\" >h3k27me3</th>\n",
       "      <td id=\"T_f166e_row3_col0\" class=\"data row3 col0\" >4171</td>\n",
       "      <td id=\"T_f166e_row3_col1\" class=\"data row3 col1\" >9.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f166e_level0_row4\" class=\"row_heading level0 row4\" >h3k4me1</th>\n",
       "      <td id=\"T_f166e_row4_col0\" class=\"data row4 col0\" >3000</td>\n",
       "      <td id=\"T_f166e_row4_col1\" class=\"data row4 col1\" >6.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f166e_level0_row5\" class=\"row_heading level0 row5\" >h3k9me3</th>\n",
       "      <td id=\"T_f166e_row5_col0\" class=\"data row5 col0\" >2086</td>\n",
       "      <td id=\"T_f166e_row5_col1\" class=\"data row5 col1\" >4.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f166e_level0_row6\" class=\"row_heading level0 row6\" >h3k36me3</th>\n",
       "      <td id=\"T_f166e_row6_col0\" class=\"data row6 col0\" >1421</td>\n",
       "      <td id=\"T_f166e_row6_col1\" class=\"data row6 col1\" >3.09%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f166e_level0_row7\" class=\"row_heading level0 row7\" >no_consensus</th>\n",
       "      <td id=\"T_f166e_row7_col0\" class=\"data row7 col0\" >403</td>\n",
       "      <td id=\"T_f166e_row7_col1\" class=\"data row7 col1\" >0.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f166e_level0_row8\" class=\"row_heading level0 row8\" >Total</th>\n",
       "      <td id=\"T_f166e_row8_col0\" class=\"data row8 col0\" >46018</td>\n",
       "      <td id=\"T_f166e_row8_col1\" class=\"data row8 col1\" >100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc61e28b430>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_04954\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_04954_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_04954_level0_col1\" class=\"col_heading level0 col1\" >relative</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >manual_target_consensus_size</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_04954_level0_row0\" class=\"row_heading level0 row0\" >2</th>\n",
       "      <td id=\"T_04954_row0_col0\" class=\"data row0 col0\" >24927</td>\n",
       "      <td id=\"T_04954_row0_col1\" class=\"data row0 col1\" >54.17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_04954_level0_row1\" class=\"row_heading level0 row1\" >3</th>\n",
       "      <td id=\"T_04954_row1_col0\" class=\"data row1 col0\" >9557</td>\n",
       "      <td id=\"T_04954_row1_col1\" class=\"data row1 col1\" >20.77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_04954_level0_row2\" class=\"row_heading level0 row2\" >4</th>\n",
       "      <td id=\"T_04954_row2_col0\" class=\"data row2 col0\" >7811</td>\n",
       "      <td id=\"T_04954_row2_col1\" class=\"data row2 col1\" >16.97%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_04954_level0_row3\" class=\"row_heading level0 row3\" >1</th>\n",
       "      <td id=\"T_04954_row3_col0\" class=\"data row3 col0\" >3320</td>\n",
       "      <td id=\"T_04954_row3_col1\" class=\"data row3 col1\" >7.21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_04954_level0_row4\" class=\"row_heading level0 row4\" >0</th>\n",
       "      <td id=\"T_04954_row4_col0\" class=\"data row4 col0\" >403</td>\n",
       "      <td id=\"T_04954_row4_col1\" class=\"data row4 col1\" >0.88%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_04954_level0_row5\" class=\"row_heading level0 row5\" >Total</th>\n",
       "      <td id=\"T_04954_row5_col0\" class=\"data row5 col0\" >46018</td>\n",
       "      <td id=\"T_04954_row5_col1\" class=\"data row5 col1\" >100.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc61eddc880>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = ca_core_df.copy(deep=True)\n",
    "\n",
    "reference_column = \"manual_target_consensus\"\n",
    "columns_to_check = DB_COLS\n",
    "df[\"manual_target_consensus_size\"] = (\n",
    "    df[columns_to_check].eq(df[reference_column], axis=0)\n",
    ").sum(axis=1)\n",
    "\n",
    "for col in [\"manual_target_consensus\", \"manual_target_consensus_size\"]:\n",
    "    print_column_content(df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global:\n",
      "input pred: 36.72%\n",
      "Nb pred assay7 (pred score >= 0.60): 89.54% (41203/46018)\n",
      "Nb match assay7: 94.78% (39054/41203)\n",
      "Nb error assay7: 5.22% (2149/41203)\n",
      "\n",
      "Average acc per class: 95.67%\n",
      "\n",
      "Consensus defined with 1 DB: 3320 files. (7.21%)\n",
      "input: 59.97%\n",
      "Nb pred assay7 (pred score >= 0.60): 88.80% (2948/3320)\n",
      "Nb match assay7: 95.45% (2814/2948)\n",
      "Nb error assay7: 4.55% (134/2948)\n",
      "\n",
      "Average acc per class: 92.87%\n",
      "\n",
      "Consensus defined with 2 DB: 24927 files. (54.17%)\n",
      "input: 41.20%\n",
      "Nb pred assay7 (pred score >= 0.60): 89.06% (22200/24927)\n",
      "Nb match assay7: 95.75% (21256/22200)\n",
      "Nb error assay7: 4.25% (944/22200)\n",
      "\n",
      "Average acc per class: 96.86%\n",
      "\n",
      "Consensus defined with 3 DB: 9557 files. (20.77%)\n",
      "input: 35.35%\n",
      "Nb pred assay7 (pred score >= 0.60): 88.84% (8490/9557)\n",
      "Nb match assay7: 95.14% (8077/8490)\n",
      "Nb error assay7: 4.86% (413/8490)\n",
      "\n",
      "Average acc per class: 94.86%\n",
      "\n",
      "Consensus defined with 4 DB: 7811 files. (16.97%)\n",
      "input: 16.88%\n",
      "Nb pred assay7 (pred score >= 0.60): 92.16% (7199/7811)\n",
      "Nb match assay7: 95.94% (6907/7199)\n",
      "Nb error assay7: 4.06% (292/7199)\n",
      "\n",
      "Average acc per class: 95.96%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Global:\")\n",
    "\n",
    "sub_df = df[df[\"Max_pred_assay7\"] >= 0.6]\n",
    "assay_count = sub_df[\"manual_target_consensus\"].value_counts(normalize=True).sort_index()\n",
    "print(f\"input pred: {assay_count['input']:.2%}\")\n",
    "print_pred_within_threshold(df, min_pred=0.6)\n",
    "\n",
    "acc_per_class = print_breakdown_predictions(df, min_pred=0.6, nb_classes=7, verbose=False)\n",
    "avg_acc_per_class = np.mean(list(acc_per_class.values()))\n",
    "print(f\"Average acc per class: {avg_acc_per_class:.2%}\")\n",
    "print()\n",
    "\n",
    "verbose = True\n",
    "\n",
    "N_global = len(df)\n",
    "for i in range(1, 5):\n",
    "    con_df = df[df[\"manual_target_consensus_size\"] == i]\n",
    "    print(\n",
    "        f\"Consensus defined with {i} DB: {len(con_df)} files. ({len(con_df)/N_global:.2%})\"\n",
    "    )\n",
    "\n",
    "    # Display % assay\n",
    "    if verbose:\n",
    "        sub_df = con_df[con_df[\"Max_pred_assay7\"] >= 0.6]\n",
    "        assay_count = sub_df[\"manual_target_consensus\"].value_counts(normalize=True)\n",
    "        print(f\"input: {assay_count['input']:.2%}\")\n",
    "\n",
    "        print_pred_within_threshold(con_df, min_pred=0.6)\n",
    "\n",
    "        acc_per_class = print_breakdown_predictions(\n",
    "            con_df, min_pred=0.6, nb_classes=7, verbose=False\n",
    "        )\n",
    "        avg_acc_per_class = np.mean(list(acc_per_class.values()))\n",
    "        print(f\"Average acc per class: {avg_acc_per_class:.2%}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be big differences in accuracy when looking at consensus defined by a different number of DB.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_metadata_dir = base_data_dir / \"metadata\" / \"chip_atlas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48669, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "biomat_df = pd.read_csv(\n",
    "    ca_metadata_dir / \"CA_expected_biomat.20250514.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "biomat_df.fillna(\"unknown\", inplace=True)\n",
    "display(biomat_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48669, 26)\n"
     ]
    }
   ],
   "source": [
    "general_metadata_path = ca_metadata_dir / \"CA_metadata_joined_20250306.tsv\"\n",
    "general_metadata_df = pd.read_csv(general_metadata_path, sep=\"\\t\", low_memory=False)\n",
    "print(general_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48669, 27)\n"
     ]
    }
   ],
   "source": [
    "general_metadata_df = pd.merge(\n",
    "    general_metadata_df,\n",
    "    biomat_df,\n",
    "    how=\"left\",\n",
    "    left_on=\"Experimental-id\",\n",
    "    right_on=\"ID\",\n",
    "    suffixes=(\"_DROP\", \"\"),\n",
    ")\n",
    "general_metadata_df.drop(columns=[\"ID\"], inplace=True)\n",
    "for col in general_metadata_df.columns:\n",
    "    if col.endswith(\"_DROP\"):\n",
    "        general_metadata_df.drop(columns=col, inplace=True)\n",
    "\n",
    "print(general_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ca_core_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    df,\n",
    "    general_metadata_df,\n",
    "    how=\"left\",\n",
    "    on=\"Experimental-id\",\n",
    "    suffixes=(\"\", \"_DROP\"),\n",
    ")\n",
    "df = df.drop([col for col in df.columns if col.endswith(\"_DROP\")], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Predicted_class_donorlife\"] = df[\"Predicted_class_donorlife\"].replace(\n",
    "    {\n",
    "        \"newborn\": \"perinatal\",\n",
    "        \"fetal\": \"perinatal\",\n",
    "        \"embryonic\": \"perinatal\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a DF with no cell line\n",
    "\n",
    "Life stage classifier was not trained on any cell line, and also the notion of life stage makes less sense for cell lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790 rows removed.\n"
     ]
    }
   ],
   "source": [
    "mask_cell_line = df[\"Cell_type_description\"].str.lower().str.contains(\"cell line\")\n",
    "\n",
    "no_cell_line_df = df[~mask_cell_line]\n",
    "print(f\"{len(df) - len(no_cell_line_df)} rows removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check, are there some cell lines with life stages, if so what are the samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expected_donorlife\n",
       "unknown      779\n",
       "perinatal     11\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_line_df = df[mask_cell_line]\n",
    "display(cell_line_df[\"expected_donorlife\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell lines with perinatal status:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Cell_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Cell_type_description",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "e4912e73-29fe-4d15-b735-88033362242b",
       "rows": [
        [
         "8082",
         "HEK293-T-REx",
         "Tissue=kidney|Lineage=mesoderm|Description=embryonic kidney cells transformed with Adenovirus 5 DNA stably expressing tetracycline repressor, HEK293 (ATCC number CRL-1573) is the parental cell line, hypotriploid, XXX"
        ],
        [
         "23653",
         "NT2-D1",
         "Tissue=testis|Lineage=inner cell mass|Description=malignant pluripotent embryonal carcinoma (NTera-2), \"The NTERA-2 cl.D1 cell line is a pluripotent human testicular embryonal carcinoma cell line derived by cloning the NTERA-2 cell line.\" - ATCC. (PMID: 6694356)"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cell_type</th>\n",
       "      <th>Cell_type_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8082</th>\n",
       "      <td>HEK293-T-REx</td>\n",
       "      <td>Tissue=kidney|Lineage=mesoderm|Description=emb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23653</th>\n",
       "      <td>NT2-D1</td>\n",
       "      <td>Tissue=testis|Lineage=inner cell mass|Descript...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Cell_type                              Cell_type_description\n",
       "8082   HEK293-T-REx  Tissue=kidney|Lineage=mesoderm|Description=emb...\n",
       "23653        NT2-D1  Tissue=testis|Lineage=inner cell mass|Descript..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Cell lines with perinatal status:\")\n",
    "display(\n",
    "    cell_line_df[cell_line_df[\"expected_donorlife\"] == \"perinatal\"][\n",
    "        [\"Cell_type\", \"Cell_type_description\"]\n",
    "    ].drop_duplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think dropping \"HEK293-T-REx\" and \"NT2-D1\" from life stage predictions is justified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(df, min_pred: float = 0.6):\n",
    "    \"\"\"Prints metrics for the given df.\n",
    "    Classification report and confusion matrix for sex, cancer and donorlife.\"\"\"\n",
    "    df = df.copy(deep=True)\n",
    "    for name in [\"sex\", \"cancer\", \"donorlife\"]:\n",
    "        print(f\"--- {name} ---\")\n",
    "        col_max_pred = f\"Max_pred_{name}\"\n",
    "        col_pred = f\"Predicted_class_{name}\"\n",
    "        col_true = f\"expected_{name}\"\n",
    "\n",
    "        no_unknown_df = df[df[col_true] != \"unknown\"]\n",
    "        print(\n",
    "            f\"Removing {len(df) - len(no_unknown_df)} rows with unknown.\\nLeft with {len(no_unknown_df)} rows.\"\n",
    "        )\n",
    "\n",
    "        high_conf_df = no_unknown_df[no_unknown_df[col_max_pred] >= min_pred]\n",
    "        print(\n",
    "            f\"Removing {len(no_unknown_df) - len(high_conf_df)} rows with low confidence.\\nLeft with {len(high_conf_df)} rows\\n\"\n",
    "        )\n",
    "\n",
    "        preds = high_conf_df[col_pred]\n",
    "        true = high_conf_df[col_true]\n",
    "        labels = sorted(set(true.unique()) | set(preds.unique()))\n",
    "\n",
    "        print(classification_report(true, preds, zero_division=0, digits=4))\n",
    "\n",
    "        cm = sk_cm(true, preds, labels=labels)\n",
    "        print(labels)\n",
    "        print(str(cm) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- with cell line ---\n",
      "--- sex ---\n",
      "Removing 39163 rows with unknown.\n",
      "Left with 6855 rows.\n",
      "Removing 528 rows with low confidence.\n",
      "Left with 6327 rows\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female     0.9520    0.6799    0.7932      3005\n",
      "        male     0.9789    0.9239    0.9506      3311\n",
      "       mixed     0.0104    1.0000    0.0206        11\n",
      "\n",
      "    accuracy                         0.8081      6327\n",
      "   macro avg     0.6471    0.8679    0.5882      6327\n",
      "weighted avg     0.9644    0.8081    0.8742      6327\n",
      "\n",
      "['female', 'male', 'mixed']\n",
      "[[2043   66  896]\n",
      " [ 103 3059  149]\n",
      " [   0    0   11]]\n",
      "\n",
      "--- cancer ---\n",
      "Removing 24702 rows with unknown.\n",
      "Left with 21316 rows.\n",
      "Removing 951 rows with low confidence.\n",
      "Left with 20365 rows\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      cancer     0.9603    0.9001    0.9292     17946\n",
      "  non-cancer     0.4942    0.7243    0.5875      2419\n",
      "\n",
      "    accuracy                         0.8792     20365\n",
      "   macro avg     0.7273    0.8122    0.7584     20365\n",
      "weighted avg     0.9050    0.8792    0.8887     20365\n",
      "\n",
      "['cancer', 'non-cancer']\n",
      "[[16153  1793]\n",
      " [  667  1752]]\n",
      "\n",
      "--- donorlife ---\n",
      "Removing 39016 rows with unknown.\n",
      "Left with 7002 rows.\n",
      "Removing 293 rows with low confidence.\n",
      "Left with 6709 rows\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       adult     0.9792    0.2771    0.4319      4746\n",
      "       child     0.2500    0.0183    0.0340       219\n",
      "   perinatal     0.3211    0.9851    0.4844      1744\n",
      "\n",
      "    accuracy                         0.4527      6709\n",
      "   macro avg     0.5168    0.4268    0.3168      6709\n",
      "weighted avg     0.7843    0.4527    0.4326      6709\n",
      "\n",
      "['adult', 'child', 'perinatal']\n",
      "[[1315    5 3426]\n",
      " [   9    4  206]\n",
      " [  19    7 1718]]\n",
      "\n",
      "--- without cell line ---\n",
      "--- sex ---\n",
      "Removing 38470 rows with unknown.\n",
      "Left with 6758 rows.\n",
      "Removing 522 rows with low confidence.\n",
      "Left with 6236 rows\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female     0.9513    0.6846    0.7962      2939\n",
      "        male     0.9788    0.9249    0.9510      3287\n",
      "       mixed     0.0099    1.0000    0.0195        10\n",
      "\n",
      "    accuracy                         0.8117      6236\n",
      "   macro avg     0.6466    0.8698    0.5889      6236\n",
      "weighted avg     0.9643    0.8117    0.8766      6236\n",
      "\n",
      "['female', 'male', 'mixed']\n",
      "[[2012   66  861]\n",
      " [ 103 3040  144]\n",
      " [   0    0   10]]\n",
      "\n",
      "--- cancer ---\n",
      "Removing 24034 rows with unknown.\n",
      "Left with 21194 rows.\n",
      "Removing 951 rows with low confidence.\n",
      "Left with 20243 rows\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      cancer     0.9601    0.8995    0.9288     17824\n",
      "  non-cancer     0.4945    0.7243    0.5877      2419\n",
      "\n",
      "    accuracy                         0.8786     20243\n",
      "   macro avg     0.7273    0.8119    0.7583     20243\n",
      "weighted avg     0.9044    0.8786    0.8880     20243\n",
      "\n",
      "['cancer', 'non-cancer']\n",
      "[[16033  1791]\n",
      " [  667  1752]]\n",
      "\n",
      "--- donorlife ---\n",
      "Removing 38237 rows with unknown.\n",
      "Left with 6991 rows.\n",
      "Removing 293 rows with low confidence.\n",
      "Left with 6698 rows\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       adult     0.9792    0.2771    0.4319      4746\n",
      "       child     0.2500    0.0183    0.0340       219\n",
      "   perinatal     0.3197    0.9850    0.4827      1733\n",
      "\n",
      "    accuracy                         0.4518      6698\n",
      "   macro avg     0.5163    0.4268    0.3162      6698\n",
      "weighted avg     0.7847    0.4518    0.4321      6698\n",
      "\n",
      "['adult', 'child', 'perinatal']\n",
      "[[1315    5 3426]\n",
      " [   9    4  206]\n",
      " [  19    7 1707]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, dataframe in zip(\n",
    "    [\"with cell line\", \"without cell line\"], [df, no_cell_line_df]\n",
    "):\n",
    "    print(f\"--- {name} ---\")\n",
    "    print_metrics(dataframe, min_pred=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about life stage performance for similar cell types? Difficult to know without extensive labeling, but ENCODE results suggest it has a significant effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary metrics by assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_handler = MetricsPerAssay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "in_epiatlas\n",
       "False    46018\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"in_epiatlas\"] = df[\"is_EpiAtlas_EpiRR\"].astype(str) != \"0\"\n",
    "display(df[\"in_epiatlas\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"assay7\", \"sex\", \"cancer\", \"donorlife\"]\n",
    "column_templates = {\n",
    "    \"True\": \"expected_{}\",\n",
    "    \"Predicted\": \"Predicted_class_{}\",\n",
    "    \"Max pred\": \"Max_pred_{}\",\n",
    "}\n",
    "df[\"expected_assay7\"] = df[\"manual_target_consensus\"]\n",
    "\n",
    "compute_fct_kwargs = {\n",
    "    \"no_epiatlas\": True,\n",
    "    \"merge_assays\": False,\n",
    "    \"categories\": categories,\n",
    "    \"column_templates\": column_templates,\n",
    "    \"assay_label\": \"manual_target_consensus\",\n",
    "    \"core_assays\": CORE_ASSAYS + [\"no_consensus\"],\n",
    "    \"non_core_assays\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filename = \"C-A_acc_per_assay\"\n",
    "output_dir = table_dir / \"dfreeze_v2\" / \"predictions\" / \"metrics\"\n",
    "\n",
    "metrics_handler.compute_multiple_metric_formats(\n",
    "    preds=df,\n",
    "    general_filename=base_filename,\n",
    "    folders_to_save=[output_dir],\n",
    "    verbose=False,\n",
    "    return_df=False,\n",
    "    compute_fct_kwargs=compute_fct_kwargs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
