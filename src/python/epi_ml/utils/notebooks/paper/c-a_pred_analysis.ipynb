{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to analyse Chip-Atlas predictions, destined for the paper.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches, pointless-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import ASSAY_ORDER\n",
    "\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSAY_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_dir = base_data_dir / \"training_results\" / \"C-A\"\n",
    "ca_pred_path = ca_dir / \"CA_metadata_4DB+all_pred_subset.20240606.tsv\"\n",
    "ca_pred_df = pd.read_csv(ca_pred_path, sep=\"\\t\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Assay | Exp Key                               | Nb Files | Training Size | Oversampling | Expected Nb Files                      |\n",
    "|-------|---------------------------------------|----------|---------------|--------------|---------------------------------------|\n",
    "| 13c   | dd3710b73c0341af85a17ce1998362d0      | 24989    | 116550        | true         | 24989                                 |\n",
    "| 11c   | 0f8e5eb996114868a17057bebe64f87c      | 20922    | 46128         | true         | 20922                                 |\n",
    "| 7c    | 69488630801b4a05a53b5d9e572f0aaa      | 16788    | 34413         | true         | 16788 (contre-vérifié)                |\n",
    "\n",
    "*using hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_pred_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_assays = ASSAY_ORDER[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_cols = [\"GEO_mod\", \"C-A\", \"Cistrome\", \"NGS_mod\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_perc(df: pd.DataFrame | pd.Series):\n",
    "    \"\"\"Display a DataFrame with percentages.\"\"\"\n",
    "    # pylint: disable=consider-using-f-string\n",
    "    with pd.option_context(\"display.float_format\", \"{:.2f}\".format):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File correction\n",
    "\n",
    "CA_metadata_4DB+all_pred.20240606.tsv has some mistakes in GEO_mod and manual_target_consensus. Using values in CA_metadata_mod2.tsv to overwrite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_mod_path = ca_dir / \"CA_metadata_mod2.tsv\"\n",
    "ca_mod_df = pd.read_csv(ca_mod_path, sep=\"\\t\", low_memory=False)\n",
    "ca_mod_df.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_pred_df = ca_pred_df.merge(ca_mod_df, on=\"Experimental-id\", how=\"left\")\n",
    "ca_pred_df[[\"manual_target_consensus\", \"GEO_mod\"]] = ca_pred_df[\n",
    "    [\"manual_target_consensus2\", \"GEO_mod2\"]\n",
    "]\n",
    "ca_pred_df = ca_pred_df.drop(columns=[\"manual_target_consensus2\", \"GEO_mod2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_high_level_pred_info(df: pd.DataFrame) -> None:\n",
    "    \"\"\"High level information about the predictions.\"\"\"\n",
    "    for assay in core_assays:\n",
    "        print(f\"{assay}\")\n",
    "        assay_df = df[df[\"manual_target_consensus\"] == assay]\n",
    "        for col in [\n",
    "            \"Predicted_class_assay7\",\n",
    "            \"Predicted_class_assay11\",\n",
    "            \"Predicted_class_assay13\",\n",
    "        ]:\n",
    "            assay_number = col.rsplit(\"_\", maxsplit=1)[-1]\n",
    "            display(assay_df[col].value_counts() / len(assay_df) * 100)\n",
    "            if any(label in col for label in [\"11\", \"13\"]):\n",
    "                wrong_pred = assay_df[assay_df[col] != assay]\n",
    "\n",
    "                display(\n",
    "                    wrong_pred[f\"2nd_pred_class_{assay_number}\"].value_counts()\n",
    "                    / len(wrong_pred)\n",
    "                    * 100\n",
    "                )\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"What is the actual target when wgbs-standard is predicted?\")\n",
    "    for assay_number in [\"assay11\", \"assay13\"]:\n",
    "        print(f\"{assay_number}\")\n",
    "        wgbs_dist = ca_pred_df[\n",
    "            ca_pred_df[f\"Predicted_class_{assay_number}\"] == \"wgbs-standard\"\n",
    "        ][\"manual_target_consensus\"]\n",
    "        display(wgbs_dist.value_counts())\n",
    "        display(wgbs_dist.value_counts() / len(wgbs_dist) * 100)\n",
    "\n",
    "    print(\"What is the actual target when non-core is predicted?\")\n",
    "    col = \"Predicted_class_assay13\"\n",
    "    wgbs_dist = ca_pred_df[ca_pred_df[col] == \"non-core\"][\"manual_target_consensus\"]\n",
    "    display(wgbs_dist.value_counts())\n",
    "    display(wgbs_dist.value_counts() / len(wgbs_dist) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_high_level_pred_info(ca_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_target_info(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Print information about the targets.\"\"\"\n",
    "    assay_count = df[\"manual_target_consensus\"].value_counts(dropna=False)\n",
    "    display(assay_count)\n",
    "    print(\"Size of the dataset: \", len(df))\n",
    "    display_perc(assay_count / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_target_info(ca_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_core_labels = [\"no_consensus\", \"non-core\", \"CTCF\"]\n",
    "ca_core_df = ca_pred_df[~ca_pred_df.isin(non_core_labels)]\n",
    "ca_core_df = ca_core_df.dropna(subset=[\"manual_target_consensus\"])\n",
    "print(ca_core_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_target_info(ca_core_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_consensus_df = ca_pred_df[ca_pred_df[\"manual_target_consensus\"] == \"no_consensus\"]\n",
    "print(no_consensus_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_pred in [0.6, 0.8]:\n",
    "    # for min_pred in np.arange(0.6, 1.05, 0.05):\n",
    "    break_tie_mask = no_consensus_df[\"Max_pred_assay7\"] >= min_pred\n",
    "    nb_break_tie = break_tie_mask.sum()\n",
    "    print(\n",
    "        f\"Break no_consensus (minPred >= {min_pred:.02f}): {nb_break_tie/ len(no_consensus_df) * 100:.02f}% ({nb_break_tie}/{len(no_consensus_df)})\"\n",
    "    )\n",
    "    df = no_consensus_df[break_tie_mask]\n",
    "    # display(df[no_consensus_df.columns[2:10]])\n",
    "    # display(df.value_counts(\"Predicted_class_assay7\"))\n",
    "    nb_not_input = (df[\"Predicted_class_assay7\"] != \"input\").sum()\n",
    "    print(\n",
    "        f\"non-input tie breakers: {nb_not_input}/{nb_break_tie} ({nb_not_input/len(df) * 100:.02f}%)\\n\"\n",
    "    )\n",
    "    # print(df[\"ENCODE\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_count = ca_core_df[\"ENCODE\"].value_counts()\n",
    "display(enc_count)\n",
    "display_perc(enc_count / len(ca_core_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_within_threshold(\n",
    "    df: pd.DataFrame, min_pred: float = 0.6, nb_classes: str | int = 7\n",
    ") -> None:\n",
    "    \"\"\"Print the predictions within a threshold.\"\"\"\n",
    "    col = f\"Max_pred_assay{nb_classes}\"\n",
    "    mask = df[col] >= min_pred\n",
    "    nb_pred = mask.sum()\n",
    "    print(\n",
    "        f\"Nb pred assay{nb_classes} (pred score >= {min_pred:.02f}): {nb_pred/len(df) * 100:.02f}% ({nb_pred}/{len(df)})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_pred in np.arange(0, 1.05, 0.05):\n",
    "    df = ca_core_df[ca_core_df[\"ENCODE\"] == 0]\n",
    "    print_pred_within_threshold(df, min_pred=min_pred, nb_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_confusion_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    fig_dir: Path | str,\n",
    "    nb_classes: int | str = 7,\n",
    "    min_pred: float = 0.6,\n",
    "):\n",
    "    \"\"\"Save the confusion matrix for core assays predictions. Does not filter.\"\"\"\n",
    "    col = f\"Predicted_class_assay{nb_classes}\"\n",
    "    cm = sk_cm(df[\"manual_target_consensus\"], df[col])\n",
    "    cm_writer = ConfusionMatrixWriter(labels=core_assays, confusion_matrix=cm)\n",
    "\n",
    "    name = f\"confusion_matrix_assay{nb_classes}_core7_minPred{min_pred:.02f}\"\n",
    "    if df[\"ENCODE\"].sum() == 0:\n",
    "        name += \"_noENCODE\"\n",
    "\n",
    "    cm_writer.to_all_formats(logdir=fig_dir, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_breakdown_predictions(\n",
    "    df: pd.DataFrame, min_pred: float = 0.6, nb_classes: int | str = 7\n",
    ") -> None:\n",
    "    \"\"\"Breakdown the predictions, print results.\"\"\"\n",
    "    df = df[df[f\"Max_pred_assay{nb_classes}\"] >= min_pred]\n",
    "\n",
    "    pred_col = f\"Predicted_class_assay{nb_classes}\"\n",
    "    match_consensus = df[\"manual_target_consensus\"] == df[pred_col]\n",
    "    nb_match = match_consensus.sum()\n",
    "    nb_error = (~match_consensus).sum()\n",
    "    print(f\"Nb match assay{nb_classes}: {nb_match/ len(df):.2%} ({nb_match}/{len(df)})\")\n",
    "    print(f\"Nb error assay{nb_classes}: {nb_error/ len(df):.2%} ({nb_error}/{len(df)})\\n\")\n",
    "\n",
    "    correct_pred_df = df[match_consensus]\n",
    "    incorrect_pred_df = df[~match_consensus]\n",
    "\n",
    "    print(\n",
    "        r\"Following ratios: % of assay subset OR % of all predictions OR % of all incorrect predictions (potential mislabels).\",\n",
    "        \"\\n\",\n",
    "    )\n",
    "    for assay in core_assays:\n",
    "        assay_df = df[df[pred_col] == assay]\n",
    "        nb_assay = len(assay_df)\n",
    "\n",
    "        nb_assay_correct = len(correct_pred_df[correct_pred_df[pred_col] == assay])\n",
    "        nb_assay_incorrect = len(incorrect_pred_df[incorrect_pred_df[pred_col] == assay])\n",
    "\n",
    "        print(f\"Predictions as {assay}: {nb_assay / len(df):.2%} ({nb_assay}/{len(df)})\")\n",
    "        perc_cor = nb_assay_correct / nb_assay\n",
    "        perc_cor2 = nb_assay_correct / len(df)\n",
    "        perc_inc = nb_assay_incorrect / nb_assay\n",
    "        perc_inc2 = nb_assay_incorrect / len(df)\n",
    "        perc_inc3 = nb_assay_incorrect / len(incorrect_pred_df)\n",
    "\n",
    "        print(\n",
    "            f\"Correct predictions as {assay}: {perc_cor:.2%} ({nb_assay_correct}/{nb_assay}) OR {perc_cor2:.2%} ({nb_assay_correct}/{len(df)})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Incorrect predictions as {assay}: \"\n",
    "            f\"{perc_inc:.2%} ({nb_assay_incorrect}/{nb_assay}) OR \"\n",
    "            f\"{perc_inc2:.2%} ({nb_assay_incorrect}/{len(df)}) OR \"\n",
    "            f\"{perc_inc3:.2%} ({nb_assay_incorrect}/{len(incorrect_pred_df)})\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"fig_C-A\"\n",
    "for subset in [[0], [0, 1]]:\n",
    "    if subset == [0]:\n",
    "        print(\"Subset: no ENCODE\")\n",
    "    else:\n",
    "        print(\"Subset: Include ENCODE\")\n",
    "    df = ca_core_df[ca_core_df[\"ENCODE\"].isin(subset)]\n",
    "    for min_pred in [0.6, 0.8, 0.9]:\n",
    "        print(\"Min pred score:\", min_pred)\n",
    "        print_pred_within_threshold(df, min_pred=min_pred, nb_classes=7)\n",
    "        print_breakdown_predictions(df, min_pred=min_pred)\n",
    "\n",
    "        # df = ca_core_df[ca_core_df[\"Max_pred_assay7\"] >= min_pred]\n",
    "        # save_confusion_matrix(df, fig_dir, min_pred=min_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Keeping only core 7.\n",
    "\n",
    "WITH ENCODE\n",
    "\n",
    "- Nb pred assay7 (pred score >= 0.60): 89.60% (42314/47226)\n",
    "- Nb match assay7: 95.74% (40511/42314)\n",
    "- Nb error assay7: 4.26% (1803/42314)\n",
    "\n",
    "Of pred score >= 0.60:\n",
    "- Predictions as input: 39.32% (16636/42314)\n",
    "- Correct predictions as input: 92.45% (15380/16636) OR 36.35% (15380/42314)\n",
    "- Incorrect predictions as input: 7.55% (1256/16636) OR 2.97% (1256/42314) OR 69.66% (1256/1803)\n",
    "\n",
    "NO ENCODE  \n",
    "\n",
    "- Nb pred assay7 (pred score >= 0.60): 88.56% (35631/40232)\n",
    "- Nb match assay7: 95.24% (33935/35631)\n",
    "- Nb error assay7: 4.76% (1696/35631)\n",
    "\n",
    "Of pred score >= 0.60:\n",
    "- Predictions as input: 40.69% (14499/35631)\n",
    "- Correct predictions as input: 91.96% (13334/14499) OR 37.42% (13334/35631)\n",
    "- Incorrect predictions as input: 8.04% (1165/14499) OR 3.27% (1165/35631) OR 68.69% (1165/1696)\n",
    "\n",
    "BREAK CONSENSUS (does not contain any ENCODE data)\n",
    "\n",
    "- Break no_consensus (minPred >= 0.60): 92.59% (462/499) & non-input 189/462 (40.91%)  \n",
    "- Break no_consensus (minPred >= 0.80): 76.57% (402/525) & non-input 163/402 (40.55%)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying consensus criterion (nb DB agreeing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ca_core_df.groupby(db_cols + [\"manual_target_consensus\", \"Predicted_class_assay7\"]).size()\n",
    "\n",
    "reference_column = \"manual_target_consensus\"\n",
    "columns_to_check = db_cols\n",
    "ca_core_df[\"manual_target_consensus_size\"] = (\n",
    "    ca_core_df[columns_to_check].eq(ca_core_df[reference_column], axis=0)\n",
    ").sum(axis=1)\n",
    "\n",
    "for col in [\"manual_target_consensus\", \"manual_target_consensus_size\"]:\n",
    "    val_count = ca_core_df[col].value_counts(dropna=False).sort_index()\n",
    "    display(val_count)\n",
    "    display_perc(val_count / len(ca_core_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Global:\")\n",
    "# print_pred_within_threshold(ca_core_df, min_pred=0.6, nb_classes=7)\n",
    "# print()\n",
    "# print_breakdown_predictions(ca_core_df, min_pred=0.6, nb_classes=7)\n",
    "# sub_df = ca_core_df[ca_core_df[\"Max_pred_assay7\"] >= 0.6]\n",
    "# assay_count = sub_df[\"manual_target_consensus\"].value_counts().sort_index()\n",
    "# display_perc(assay_count / len(sub_df))\n",
    "\n",
    "\n",
    "for i in range(1, 5):\n",
    "    df = ca_core_df[ca_core_df[\"manual_target_consensus_size\"] == i]\n",
    "    print(f\"Consensus defined with {i} DB: {len(df)} files.\")\n",
    "\n",
    "    # Display % assay\n",
    "    # sub_df = df[df[\"Max_pred_assay7\"] >= 0.6]\n",
    "    # assay_count = df[\"manual_target_consensus\"].value_counts()\n",
    "\n",
    "    # display_perc(assay_count / len(df))\n",
    "\n",
    "    #     input_val = (assay_count / len(df))[\"input\"]\n",
    "    #     print(f\"input: {input_val:.2%}\")\n",
    "\n",
    "    print_pred_within_threshold(df, min_pred=0.6, nb_classes=7)\n",
    "    # print()\n",
    "\n",
    "    print_breakdown_predictions(df, min_pred=0.6, nb_classes=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be big differences in accuracy when looking at consensus defined by a different number of DB.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Core 7 files, including ENCODE\n",
    "| Consensus size | Nb files | Nb files (%) | Input size (%) |\n",
    "|-------------|------------------------------|------------|------------|\n",
    "| NA           | 47226                         | 100%      |  38.37%      |\n",
    "| 1           | 3324                         | 6.98%      | 61.85%     |\n",
    "| 2           | 25551                        | 53.81%     | 42.27%     |\n",
    "| 3           | 10275                        | 21.60%     | 37.38%     |\n",
    "| 4           | 8076                         | 17.61%     | 17.62%     |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For assay7 core7, min_pred 0.6:  \n",
    "\n",
    "Global average\n",
    "- acc: 95.74%\n",
    "- %err=input (lowQual): 69.66%\n",
    "- consensus input = 36.77%\n",
    "\n",
    "1 to 4 DB consensus\n",
    "\n",
    "- acc within [95.36%, 96.03%]\n",
    "- %err=input (lowQual): within [63.43%, 79.39%] = [63.43%, 69.55%, 65.09%, 79.39%]\n",
    "- consensus input = [60.03%, 41.01%, 35.73%, 15.84%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENCODE core no EpiAtlas + Chip-Atlas (no ENCODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_dir = base_data_dir / \"training_results\" / \"encode_predictions\" / \"assay_epiclass\"\n",
    "encode_pred_path = encode_dir / \"encode_only-core-7c_predictions.csv\"\n",
    "encode_pred = pd.read_csv(encode_pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_pred[\"Experimental-id\"] = encode_pred[\"md5sum\"]\n",
    "encode_pred[\"Predicted_class_assay7\"] = encode_pred[\"Predicted class\"]\n",
    "encode_pred[\"manual_target_consensus\"] = encode_pred[\"True class\"]\n",
    "encode_pred[\"Max_pred_assay7\"] = encode_pred[\"Max pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pred_df = pd.concat([ca_core_df, encode_pred], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pred_df = global_pred_df[~(global_pred_df[\"ENCODE\"] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(global_pred_df[\"ENCODE\"].value_counts(dropna=False))\n",
    "display(global_pred_df[\"manual_target_consensus\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pred_df = global_pred_df[\n",
    "    global_pred_df[\"manual_target_consensus\"].isin(core_assays)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pred = 0.6\n",
    "print_pred_within_threshold(global_pred_df, min_pred=min_pred, nb_classes=7)\n",
    "print_breakdown_predictions(global_pred_df, min_pred=min_pred, nb_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = base_data_dir / \"training_results\" / \"predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mislabels = global_pred_df[\n",
    "    global_pred_df[\"Predicted_class_assay7\"] != global_pred_df[\"manual_target_consensus\"]\n",
    "]\n",
    "# mislabels.to_csv(output_dir / \"mislabels_C-A&ENCODE_assay7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nb pred assay7 (pred score >= 0.60): 89.49% (39910/44599)\n",
    "- Nb match assay7: 95.65% (38172/39910)\n",
    "- Nb error assay7: 4.35% (1738/39910)\n",
    "\n",
    "Following ratios: % of assay subset OR % of all predictions OR % of all incorrect predictions (potential mislabels).   \n",
    "\n",
    "- Predictions as input: 40.39% (16119/39910)\n",
    "- Correct predictions as input: 92.75% (14951/16119) OR 37.46% (14951/39910)\n",
    "- Incorrect predictions as input: 7.25% (1168/16119) OR 2.93% (1168/39910) OR 67.20% (1168/1738)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
