{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workbook to analyse ChIP-Atlas classifier predictions.\n",
    "\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, use-dict-literal, too-many-lines, unused-import, unused-argument, too-many-branches, pointless-statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import upsetplot\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix as sk_cm\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.notebooks.paper.paper_utilities import (\n",
    "    ASSAY_ORDER,\n",
    "    IHECColorMap,\n",
    "    display_perc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSAY_ORDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.home() / \"Projects/epiclass/output/paper\"\n",
    "base_data_dir = base_dir / \"data\"\n",
    "base_fig_dir = base_dir / \"figures\"\n",
    "paper_dir = base_dir\n",
    "\n",
    "if not base_fig_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {base_fig_dir} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IHECColorMap = IHECColorMap(base_fig_dir)\n",
    "assay_colors = IHECColorMap.assay_color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_dir = base_data_dir / \"training_results\" / \"predictions\" / \"C-A\" / \"assay_epiclass\"\n",
    "\n",
    "ca_filename = \"CA_metadata_4DB+all_pred.20240606_mod3.0.tsv\"\n",
    "ca_pred_path = ca_dir / ca_filename\n",
    "\n",
    "ca_pred_df = pd.read_csv(ca_pred_path, sep=\"\\t\", low_memory=False)\n",
    "\n",
    "print(ca_pred_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Assay | Exp Key                               | Nb Files | Training Size | Oversampling |\n",
    "|-------|---------------------------------------|----------|---------------|--------------|\n",
    "| 13c*   | dd3710b73c0341af85a17ce1998362d0      | 24989    | 116550        | true         |\n",
    "| 11c   | 0f8e5eb996114868a17057bebe64f87c      | 20922    | 46128         | true         |\n",
    "| 7c    | 69488630801b4a05a53b5d9e572f0aaa      | 16788    | 34413         | true         |\n",
    "\n",
    "*using hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = ca_pred_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORE_ASSAYS = ASSAY_ORDER[0:7]\n",
    "\n",
    "DB_COLS = [\"GEO_mod\", \"C-A\", \"Cistrome\", \"NGS_mod\"]\n",
    "\n",
    "PRED_COLS = [\n",
    "    \"Predicted_class_assay7\",\n",
    "    \"Predicted_class_assay11\",\n",
    "    \"Predicted_class_assay13\",\n",
    "]\n",
    "\n",
    "SAME_TARGET = \"core7_DBs_consensus\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base dataset used: Chip-Atlas experiments where at least one of the BD declared the target in core7.\n",
    "\n",
    "Excluding: \n",
    "- Samples where at least one the DB declared a target out of core7.\n",
    "- samples overlapping with EpiATLAS dataset (different file creation pipeline, same base bam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_target_info(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Print information about the targets.\"\"\"\n",
    "    assay_count = df[\"manual_target_consensus\"].value_counts(dropna=False)\n",
    "    print(\"Size of the dataset: \", len(df))\n",
    "    display(assay_count)\n",
    "    display_perc(assay_count / len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_pred_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_core_labels = [\"non-core\", \"CTCF\", \"ctcf\"]\n",
    "non_core_labels_2 = [\"Ignored - Potential non-core\", \"non-core/CTCF\"]\n",
    "\n",
    "ca_core_df = ca_pred_df[~ca_pred_df[SAME_TARGET].isin(non_core_labels_2)]\n",
    "diff_N = len(ca_pred_df) - len(ca_core_df)\n",
    "print(\n",
    "    f\"Removed {diff_N} rows with {SAME_TARGET} in {non_core_labels_2}.\\nAfter this, {len(ca_core_df)} rows remain.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_diff = len(ca_core_df)\n",
    "ca_core_df = ca_core_df[ca_core_df[\"is_EpiAtlas_EpiRR\"].astype(str) == \"0\"].copy()\n",
    "N_diff -= len(ca_core_df)\n",
    "print(f\"Removed {N_diff} rows with EpiATLAS EpiRR overlap.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for db_col in DB_COLS:\n",
    "    col = ca_core_df[db_col]\n",
    "    if col.isna().sum():\n",
    "        print(\"Missing values: \", ca_core_df[col.isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_target_info(ca_core_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_consensus_df = ca_core_df[ca_core_df[\"manual_target_consensus\"] == \"no_consensus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upset plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = base_fig_dir / \"fig_C-A\" / \"DB_upset\" / \"no_EpiATLAS\"\n",
    "fig_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_db_upsetplot(\n",
    "    df: pd.DataFrame, db_cols: List[str], title: str\n",
    ") -> upsetplot.UpSet:\n",
    "    \"\"\"Make an upsetplot of the sample presence in the different databases.\"\"\"\n",
    "    df = df.copy()\n",
    "    if SAME_TARGET not in df.columns:\n",
    "        raise ValueError(\"Column 'identical_DBs_target' not found in DataFrame.\")\n",
    "\n",
    "    # Create a new DataFrame with boolean columns for each database\n",
    "    upset_df = pd.DataFrame()\n",
    "    for col in db_cols:\n",
    "        upset_df[col] = df[col] != \"----\"\n",
    "    upset_df[SAME_TARGET] = df[SAME_TARGET]\n",
    "\n",
    "    # Set the index for the UpSet plot\n",
    "    upset_df = upset_df.set_index(db_cols)\n",
    "\n",
    "    # Create the UpSet plot\n",
    "    upset = upsetplot.UpSet(\n",
    "        upset_df,\n",
    "        intersection_plot_elements=0,  # disable the default bar chart\n",
    "        sort_by=\"cardinality\",\n",
    "        show_counts=True,  # type: ignore\n",
    "        orientation=\"horizontal\",\n",
    "    )\n",
    "\n",
    "    # Add stacked bars\n",
    "    upset.add_stacked_bars(by=SAME_TARGET, elements=15)\n",
    "\n",
    "    # Plot and set title\n",
    "    axes = upset.plot()\n",
    "    plt.suptitle(title)\n",
    "    axes[\"totals\"].set_title(\"Total\")\n",
    "    plt.legend(loc=\"center left\")\n",
    "    return upset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ca_core_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ca_core_df[\"manual_target_consensus\"].value_counts(dropna=False))\n",
    "display(ca_core_df[SAME_TARGET].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"All core7 ChIP-Atlas samples presence in used DBs)\\nTarget consensus\"\n",
    "upset = make_db_upsetplot(ca_core_df, DB_COLS, title=title)\n",
    "\n",
    "plt.savefig(fig_dir / \"upsetplot_DB_core7_samples.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No ENCODE EpiRR overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no encode\n",
    "no_encode_df = ca_core_df[ca_core_df[\"ENCODE\"] == 0]\n",
    "title = \"ChIP-Atlas samples presence in used DBs\\nTarget Consensus - No ENCODE\"\n",
    "\n",
    "upset = make_db_upsetplot(no_encode_df, DB_COLS, title=title)\n",
    "\n",
    "plt.savefig(fig_dir / \"upsetplot_DB_core7_samples_noENC.svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prediction_resolved(row, pred_col: str, db_cols: List[str]) -> bool:\n",
    "    \"\"\"Check if the prediction matches any of the database columns.\"\"\"\n",
    "    pred_val = row[pred_col]\n",
    "    db_vals = [row[col] for col in db_cols]\n",
    "    return pred_val in db_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier was able to resolve xx% of the cases where the target was not identical between the sources\n",
    "different_targets_df = ca_core_df[ca_core_df[SAME_TARGET] == \"Different\"]\n",
    "\n",
    "for min_pred_score in [0, 0.6]:\n",
    "    filtered_df = different_targets_df[\n",
    "        different_targets_df[\"Max_pred_assay7\"] >= min_pred_score\n",
    "    ]\n",
    "\n",
    "    pred_col = PRED_COLS[0]\n",
    "\n",
    "    num_resolved = filtered_df.apply(\n",
    "        is_prediction_resolved, axis=1, args=(pred_col, DB_COLS)\n",
    "    ).sum()\n",
    "\n",
    "    print(\n",
    "        f\"Resolved (min_predScore >= {min_pred_score}): \"\n",
    "        f\"{num_resolved} / {len(filtered_df)} \"\n",
    "        f\"({num_resolved / len(filtered_df) * 100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    # Exclude rows where the prediction is labeled as 'input'\n",
    "    non_input_df = filtered_df[filtered_df[PRED_COLS[0]] != \"input\"]\n",
    "    num_resolved = non_input_df.apply(\n",
    "        is_prediction_resolved, axis=1, args=(pred_col, DB_COLS)\n",
    "    ).sum()\n",
    "\n",
    "    print(\n",
    "        f\"Resolved (min_predScore >= {min_pred_score}, excluding 'input' predictions): \"\n",
    "        f\"{num_resolved} / {len(non_input_df)} \"\n",
    "        f\"({num_resolved / len(non_input_df) * 100:.2f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level prediction accuracy breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_high_level_pred_info(df: pd.DataFrame, save_conf_matrix: bool = False) -> None:\n",
    "    \"\"\"High level information about the predictions.\"\"\"\n",
    "    for assay in CORE_ASSAYS:\n",
    "        print(f\"{assay}\")\n",
    "        assay_df = df[df[\"manual_target_consensus\"] == assay]\n",
    "        for col in [\n",
    "            \"Predicted_class_assay7\",\n",
    "            \"Predicted_class_assay11\",\n",
    "            \"Predicted_class_assay13\",\n",
    "        ]:\n",
    "            assay_number = col.rsplit(\"_\", maxsplit=1)[-1]\n",
    "            display(assay_df[col].value_counts() / len(assay_df) * 100)\n",
    "            if any(label in col for label in [\"11\", \"13\"]):\n",
    "                wrong_pred = assay_df[assay_df[col] != assay]\n",
    "\n",
    "                display(\n",
    "                    wrong_pred[f\"2nd_pred_class_{assay_number}\"].value_counts()\n",
    "                    / len(wrong_pred)\n",
    "                    * 100\n",
    "                )\n",
    "        print(\"\\n\")\n",
    "\n",
    "    if save_conf_matrix:\n",
    "        for col in [\n",
    "            \"Predicted_class_assay7\",\n",
    "            \"Predicted_class_assay11\",\n",
    "            \"Predicted_class_assay13\",\n",
    "        ]:\n",
    "            labels = sorted(df[col].unique().tolist())\n",
    "            cm = sk_cm(\n",
    "                df[\"manual_target_consensus\"],\n",
    "                df[col],\n",
    "                labels=labels,\n",
    "            )\n",
    "            cm_writer = ConfusionMatrixWriter(labels=labels, confusion_matrix=cm)\n",
    "            cm_writer.to_png(\n",
    "                Path.home() / \"Downloads\" / f\"C-A_confusion_matrix_{col}.png\"\n",
    "            )\n",
    "\n",
    "    print(\"What is the actual target when wgbs-standard is predicted?\")\n",
    "    for assay_number in [\"assay11\", \"assay13\"]:\n",
    "        print(f\"{assay_number}\")\n",
    "        wgbs_dist = ca_pred_df[\n",
    "            ca_pred_df[f\"Predicted_class_{assay_number}\"] == \"wgbs-standard\"\n",
    "        ][\"manual_target_consensus\"]\n",
    "        display(wgbs_dist.value_counts())\n",
    "        display(wgbs_dist.value_counts() / len(wgbs_dist) * 100)\n",
    "\n",
    "    print(\"What is the actual target when non-core is predicted?\")\n",
    "    col = \"Predicted_class_assay13\"\n",
    "    wgbs_dist = ca_pred_df[ca_pred_df[col] == \"non-core\"][\"manual_target_consensus\"]\n",
    "    display(wgbs_dist.value_counts())\n",
    "    display(wgbs_dist.value_counts() / len(wgbs_dist) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "if verbose:\n",
    "    print_target_info(ca_pred_df)\n",
    "    print_high_level_pred_info(ca_pred_df, save_conf_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_pred in [0.6, 0.8]:\n",
    "    break_tie_mask = no_consensus_df[\"Max_pred_assay7\"] >= min_pred\n",
    "    nb_break_tie = break_tie_mask.sum()\n",
    "    print(\n",
    "        f\"Break no_consensus (minPred >= {min_pred:.02f}): {nb_break_tie/ len(no_consensus_df) * 100:.02f}% ({nb_break_tie}/{len(no_consensus_df)})\"\n",
    "    )\n",
    "    df = no_consensus_df[break_tie_mask]\n",
    "\n",
    "    nb_not_input = (df[\"Predicted_class_assay7\"] != \"input\").sum()\n",
    "    print(\n",
    "        f\"non-input tie breakers: {nb_not_input}/{nb_break_tie} ({nb_not_input/len(df) * 100:.02f}%)\\n\"\n",
    "    )\n",
    "    print(df[\"ENCODE\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_count = ca_core_df[\"ENCODE\"].value_counts(dropna=False)\n",
    "display(enc_count)\n",
    "display_perc(enc_count / len(ca_core_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_within_threshold(\n",
    "    df: pd.DataFrame, min_pred: float = 0.6, col: str = \"Max_pred_assay7\"\n",
    ") -> None:\n",
    "    \"\"\"Print the predictions percentage within a threshold.\"\"\"\n",
    "    try:\n",
    "        mask = df[col].astype(float) >= min_pred\n",
    "    except KeyError:\n",
    "        print(f\"Column {col} not found.\")\n",
    "        return\n",
    "    nb_pred = mask.sum()\n",
    "    print(\n",
    "        f\"Nb pred {col.split('_')[-1]} (pred score >= {min_pred:.02f}): {nb_pred/len(df) * 100:.02f}% ({nb_pred}/{len(df)})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_confusion_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    fig_dir: Path | str,\n",
    "    nb_classes: int | str = 7,\n",
    "    min_pred: float = 0.6,\n",
    "):\n",
    "    \"\"\"Save the confusion matrix for core assays predictions. Does not filter.\"\"\"\n",
    "    col = f\"Predicted_class_assay{nb_classes}\"\n",
    "    cm = sk_cm(df[\"manual_target_consensus\"], df[col], labels=CORE_ASSAYS)\n",
    "    cm_writer = ConfusionMatrixWriter(labels=CORE_ASSAYS, confusion_matrix=cm)\n",
    "\n",
    "    name = f\"confusion_matrix_assay{nb_classes}_core7_minPred{min_pred:.02f}\"\n",
    "    if df[\"ENCODE\"].sum() == 0:\n",
    "        name += \"_noENCODE\"\n",
    "\n",
    "    cm_writer.to_all_formats(logdir=fig_dir, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_breakdown_predictions(\n",
    "    df: pd.DataFrame,\n",
    "    min_pred: float = 0.6,\n",
    "    nb_classes: int | str = 7,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Breakdown the predictions, print results.\"\"\"\n",
    "    df = df[df[f\"Max_pred_assay{nb_classes}\"] >= min_pred]\n",
    "\n",
    "    pred_col = f\"Predicted_class_assay{nb_classes}\"\n",
    "    match_consensus = df[\"manual_target_consensus\"] == df[pred_col]\n",
    "    nb_match = match_consensus.sum()\n",
    "    nb_error = (~match_consensus).sum()\n",
    "    print(f\"Nb match assay{nb_classes}: {nb_match/ len(df):.2%} ({nb_match}/{len(df)})\")\n",
    "    print(f\"Nb error assay{nb_classes}: {nb_error/ len(df):.2%} ({nb_error}/{len(df)})\\n\")\n",
    "\n",
    "    correct_pred_df = df[match_consensus]\n",
    "    incorrect_pred_df = df[~match_consensus]\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            r\"Following ratios: % of assay subset OR % of all predictions OR % of all incorrect predictions (potential mislabels).\",\n",
    "            \"\\n\",\n",
    "        )\n",
    "    acc_per_class = {}\n",
    "    for assay in CORE_ASSAYS:\n",
    "        assay_df = df[df[pred_col] == assay]\n",
    "        nb_assay = len(assay_df)\n",
    "\n",
    "        nb_assay_correct = len(correct_pred_df[correct_pred_df[pred_col] == assay])\n",
    "        nb_assay_incorrect = len(incorrect_pred_df[incorrect_pred_df[pred_col] == assay])\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Predictions as {assay}: {nb_assay / len(df):.2%} ({nb_assay}/{len(df)})\"\n",
    "            )\n",
    "        perc_cor = nb_assay_correct / nb_assay\n",
    "        perc_cor2 = nb_assay_correct / len(df)\n",
    "        perc_inc = nb_assay_incorrect / nb_assay\n",
    "        perc_inc2 = nb_assay_incorrect / len(df)\n",
    "        perc_inc3 = nb_assay_incorrect / len(incorrect_pred_df)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Correct predictions as {assay}: {perc_cor:.2%} ({nb_assay_correct}/{nb_assay}) OR {perc_cor2:.2%} ({nb_assay_correct}/{len(df)})\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Incorrect predictions as {assay}: \"\n",
    "                f\"{perc_inc:.2%} ({nb_assay_incorrect}/{nb_assay}) OR \"\n",
    "                f\"{perc_inc2:.2%} ({nb_assay_incorrect}/{len(df)}) OR \"\n",
    "                f\"{perc_inc3:.2%} ({nb_assay_incorrect}/{len(incorrect_pred_df)})\\n\"\n",
    "            )\n",
    "        acc_per_class[assay] = perc_cor\n",
    "\n",
    "    return acc_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True\n",
    "\n",
    "fig_dir = base_fig_dir / \"fig_C-A\" / \"confusion_matrices\"\n",
    "for subset in [[0], [0, 1]]:\n",
    "    if verbose:\n",
    "        if subset == [0]:\n",
    "            print(\"Subset: no ENCODE\")\n",
    "        else:\n",
    "            print(\"Subset: Include ENCODE\")\n",
    "\n",
    "    df = ca_core_df[ca_core_df[\"ENCODE\"].isin(subset)]\n",
    "\n",
    "    for min_pred in [0.6, 0.8, 0.9]:\n",
    "        if verbose:\n",
    "            print(\"Min pred score:\", min_pred)\n",
    "            print_pred_within_threshold(df, min_pred=min_pred)\n",
    "            print_breakdown_predictions(df, min_pred=min_pred)\n",
    "\n",
    "        sub_df = df[df[\"Max_pred_assay7\"] >= min_pred]\n",
    "        save_confusion_matrix(sub_df, fig_dir, min_pred=min_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mislabels by GSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = ca_dir / \"GSE_mispred\"\n",
    "logdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSE = \"Gse-geo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "nb_classes = 7\n",
    "min_pred = 0.6\n",
    "pred_col = f\"Predicted_class_assay{nb_classes}\"\n",
    "max_pred_col = f\"Max_pred_assay{nb_classes}\"\n",
    "\n",
    "excluding_no_consensus = True\n",
    "excluding_ENCODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ca_core_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "if excluding_no_consensus:\n",
    "    N_diff = len(df)\n",
    "    df = df[df[\"manual_target_consensus\"] != \"no_consensus\"]\n",
    "    N_diff -= len(df)\n",
    "    print(f\"Removed {N_diff} rows with no consensus.\\nLeft with {len(df)} rows.\")\n",
    "\n",
    "    this_logdir = logdir / \"excluding_no_consensus\"\n",
    "else:\n",
    "    this_logdir = logdir / \"including_no_consensus\"\n",
    "\n",
    "this_logdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if excluding_ENCODE:\n",
    "    N_diff = len(df)\n",
    "    df = df[df[\"ENCODE\"] == 0]\n",
    "    N_diff -= len(df)\n",
    "    print(f\"Removed {N_diff} rows with ENCODE.\")\n",
    "\n",
    "    this_logdir = this_logdir / \"excluding_ENCODE\"\n",
    "else:\n",
    "    this_logdir = this_logdir / \"including_ENCODE\"\n",
    "\n",
    "this_logdir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_total = len(df)\n",
    "N_diff = len(df)\n",
    "df = df[df[max_pred_col] >= min_pred]\n",
    "N_diff -= len(df)\n",
    "\n",
    "print(\n",
    "    f\"Removed {N_diff}/{N_total} ({N_diff/N_total:.2%}) rows with pred score < {min_pred}\\nLeft with {len(df)} rows.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_consensus = df[\"manual_target_consensus\"] == df[pred_col]\n",
    "nb_match = match_consensus.sum()\n",
    "nb_error = (~match_consensus).sum()\n",
    "print(f\"Nb match assay{nb_classes}: {nb_match/ len(df):.2%} ({nb_match}/{len(df)})\")\n",
    "print(f\"Nb mismatch assay{nb_classes}: {nb_error/ len(df):.2%} ({nb_error}/{len(df)})\\n\")\n",
    "\n",
    "incorrect_pred_df = df[~match_consensus]\n",
    "incorrect_pred_df = incorrect_pred_df[incorrect_pred_df[pred_col] != \"input\"]\n",
    "\n",
    "print(f\"Excluding input predictions. Left with {len(incorrect_pred_df)} mismatches.\\n\")\n",
    "\n",
    "desired_cols = [\"manual_target_consensus\", pred_col]\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    gse_count = incorrect_pred_df.groupby(GSE).size().sort_values(ascending=False)  # type: ignore\n",
    "    gse_count = gse_count.to_frame()\n",
    "    gse_count.columns = [\"Nb of mismatches\"]\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Incorrect predictions, breakdown by GSE count ({len(gse_count)} unique GSE)\"\n",
    "        )\n",
    "        gse_count[\"cumsum\"] = gse_count.cumsum()\n",
    "        gse_count[\"cumsum (%)\"] = (\n",
    "            gse_count[\"cumsum\"] * 100 / sum(gse_count[\"Nb of mismatches\"])\n",
    "        )\n",
    "        display(gse_count)\n",
    "\n",
    "    gse_count.to_csv(\n",
    "        this_logdir / \"gse_count_incorrect_pred_no_input_20240606_mod3.tsv\", sep=\"\\t\"\n",
    "    )\n",
    "\n",
    "    gse_target_count = incorrect_pred_df.groupby(GSE)[desired_cols].value_counts(dropna=False)  # type: ignore\n",
    "    if verbose:\n",
    "        print(\"Incorrect predictions, breakdown by GSE and target.\")\n",
    "        display(gse_target_count)\n",
    "\n",
    "    gse_target_count.to_csv(\n",
    "        this_logdir / \"gse_target_count_incorrect_pred_no_input_20240606_mod3.tsv\",\n",
    "        sep=\"\\t\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unclassified files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclassified = ca_core_df[ca_core_df[\"C-A\"] == \"unclassified\"]\n",
    "print(f\"Nb unclassified: {len(unclassified)} ({len(unclassified) / len(ca_core_df):.2%})\")\n",
    "\n",
    "high_pred = unclassified[unclassified[max_pred_col] >= min_pred]\n",
    "print(\n",
    "    f\"Nb high pred unclassified: {len(high_pred)} ({len(high_pred) / len(unclassified):.2%})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_pred[pred_col].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Keeping only core 7, excludes no consensus (since it's always going to not match)\n",
    "\n",
    "NO ENCODE\n",
    "\n",
    "- Nb pred assay7 (pred score >= 0.60): 88.61% (35508/40071)\n",
    "- Nb match assay7: 94.22% (33457/35508)\n",
    "- Nb error assay7: 5.78% (2051/35508)\n",
    "\n",
    "WITH ENCODE\n",
    "\n",
    "- Nb pred assay7 (pred score >= 0.60): 89.64% (42191/47065)\n",
    "- Nb match assay7: 94.89% (40033/42191)\n",
    "- Nb error assay7: 5.11% (2158/42191)\n",
    "\n",
    "\n",
    "BREAK CONSENSUS (does not contain any ENCODE data)\n",
    "\n",
    "minPred >= 0.60  \n",
    "- Break no_consensus: 90.82% (366/403)\n",
    "- non-input tie breakers: 130/366 (35.52%)\n",
    "\n",
    "minPred >= 0.80\n",
    "- Break no_consensus: 80.15% (323/403)\n",
    "- non-input tie breakers: 115/323 (35.60%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying consensus criterion (nb DB agreeing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ca_core_df.copy(deep=True)\n",
    "\n",
    "reference_column = \"manual_target_consensus\"\n",
    "columns_to_check = DB_COLS\n",
    "df[\"manual_target_consensus_size\"] = (\n",
    "    df[columns_to_check].eq(df[reference_column], axis=0)\n",
    ").sum(axis=1)\n",
    "\n",
    "for col in [\"manual_target_consensus\", \"manual_target_consensus_size\"]:\n",
    "    val_count = df[col].value_counts(dropna=False).sort_index()\n",
    "    display(val_count)\n",
    "    display_perc(val_count / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Global:\")\n",
    "\n",
    "sub_df = df[df[\"Max_pred_assay7\"] >= 0.6]\n",
    "assay_count = sub_df[\"manual_target_consensus\"].value_counts().sort_index()\n",
    "input_val = (assay_count / len(sub_df))[\"input\"]\n",
    "print(f\"input: {input_val:.2%}\")\n",
    "print_pred_within_threshold(df, min_pred=0.6)\n",
    "\n",
    "acc_per_class = print_breakdown_predictions(df, min_pred=0.6, nb_classes=7, verbose=False)\n",
    "avg_acc_per_class = np.mean(list(acc_per_class.values()))\n",
    "print(f\"Average acc per class: {avg_acc_per_class:.2%}\")\n",
    "print()\n",
    "\n",
    "N_global = len(df)\n",
    "for i in range(1, 5):\n",
    "    con_df = df[df[\"manual_target_consensus_size\"] == i]\n",
    "    print(\n",
    "        f\"Consensus defined with {i} DB: {len(con_df)} files. ({len(con_df)/N_global:.2%})\"\n",
    "    )\n",
    "\n",
    "    # Display % assay\n",
    "    sub_df = con_df[con_df[\"Max_pred_assay7\"] >= 0.6]\n",
    "    assay_count = sub_df[\"manual_target_consensus\"].value_counts()\n",
    "\n",
    "    # display_perc(assay_count / len(df))\n",
    "\n",
    "    # input_val = (assay_count / len(df))[\"input\"]\n",
    "    # print(f\"input: {input_val:.2%}\")\n",
    "\n",
    "    # print_pred_within_threshold(con_df, min_pred=0.6)\n",
    "    # print()\n",
    "\n",
    "    # acc_per_class = print_breakdown_predictions(con_df, min_pred=0.6, nb_classes=7, verbose=False)\n",
    "    # avg_acc_per_class = np.mean(list(acc_per_class.values()))\n",
    "    # print(f\"Average acc per class: {avg_acc_per_class:.2%}\")\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be big differences in accuracy when looking at consensus defined by a different number of DB.  \n",
    "There biggest difference is consensus size 1 (only 1 DB source): 3.27% less good than global for avg accuracy per class. (global acc is very similar)\n",
    "\n",
    "Core 7 files, including ENCODE\n",
    "\n",
    "| Consensus size | Nb files | Nb files   | Input size     | Files with min_pred > 0.6| Global accuracy (Nb match assay7) | Average acc per class |\n",
    "|---------------|---------|--------------|----------------|--------------------------|-----------------------------------|-----------------------|\n",
    "| 1             | 3320    | 7.05%        | 61.81%         | 88.80%                   | 95.45%                            | 92.87%                |\n",
    "| 2             | 25248   | 53.64%       | 42.08%         | 89.17%                   | 95.80%                            | 96.93%                |\n",
    "| 3             | 9976    | 21.20%       | 36.96%         | 88.99%                   | 95.31%                            | 95.28%                |\n",
    "| 4             | 8118    | 17.25%       | 18.05%         | 92.21%                   | 96.05%                            | 96.14%                |\n",
    "| Global        | 46018   | 100%         | 36.30          | 89.64%                   | 96.05%                            | 96.14%                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_metadata_dir = base_data_dir / \"metadata\" / \"chip_atlas\"\n",
    "other_meta_df = pd.read_csv(\n",
    "    ca_metadata_dir / \"CA_metadata_FW_20240917.tsv\",\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "other_meta_df.fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ca_core_df.copy(deep=True)\n",
    "df = pd.merge(\n",
    "    df, other_meta_df, how=\"left\", left_on=\"Experimental-id\", right_on=\"Experimental.id\"\n",
    ")\n",
    "df = df.drop(\"Experimental.id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_mapper = {\n",
    "    \"sex\": \"sex\",\n",
    "    \"cancer\": \"cancer\",\n",
    "    \"age\": \"donorlife\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name1, name2 in cat_mapper.items():\n",
    "    print(name1)\n",
    "    col_max_pred = f\"Max_pred_{name2}\"\n",
    "    col_pred = f\"Predicted_class_{name2}\"\n",
    "    col_true = f\"True_class_FW_{name1}\"\n",
    "\n",
    "    no_unknown_df = df[df[col_true] != \"unknown\"]\n",
    "    print(\n",
    "        f\"Removing {len(df) - len(no_unknown_df)} rows with unknown.\\nLeft with {len(no_unknown_df)} rows.\"\n",
    "    )\n",
    "\n",
    "    high_conf_df = no_unknown_df[no_unknown_df[col_max_pred] >= 0.6]\n",
    "    print(\n",
    "        f\"Removing {len(no_unknown_df) - len(high_conf_df)} rows with low confidence.\\nLeft with {len(high_conf_df)} rows\\n\"\n",
    "    )\n",
    "\n",
    "    preds = high_conf_df[col_pred]\n",
    "    true = high_conf_df[col_true]\n",
    "\n",
    "    print(classification_report(true, preds, zero_division=0, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
