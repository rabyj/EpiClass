{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Collect from comet-ml results important for the paper, e.g. epiatlas dfreeze '2.1' data.\"\"\"\n",
    "# pylint: disable=import-error, redefined-outer-name, too-many-branches, unnecessary-lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Set\n",
    "\n",
    "import pandas as pd\n",
    "from comet_ml.api import API\n",
    "import numpy as np\n",
    "\n",
    "from epi_ml.utils.time import seconds_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = API()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_run_information(api: API) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"Collect NN training metadata+metrics from comet.ml.\n",
    "\n",
    "    Returns:\n",
    "        - results (Dict[str, Dict[str, Any]])\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for experiment in api.get(\"rabyj/epilap\"):  # type: ignore\n",
    "        hyperparams = experiment.get_parameters_summary()\n",
    "        for hparam_dict in hyperparams:\n",
    "            if hparam_dict[\"name\"] == \"run_arg_0\":\n",
    "                label_category = hparam_dict[\"valueMax\"]\n",
    "                break\n",
    "\n",
    "        meta = experiment.get_metadata()\n",
    "        exp_key = meta[\"experimentKey\"]\n",
    "\n",
    "        exp_dict = {\n",
    "            \"name\": experiment.name,\n",
    "            \"label_category\": label_category,\n",
    "            \"metadata\": meta,\n",
    "            \"hyperparameters\": hyperparams,\n",
    "            \"metrics\": experiment.get_metrics_summary(),\n",
    "            \"other_metadata\": experiment.get_others_summary(),\n",
    "            \"tags\": experiment.get_tags(),\n",
    "        }\n",
    "\n",
    "        results[exp_key] = exp_dict\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results_dict = collect_run_information(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_dir = Path().home() / \"projects\" / \"epiclass\" / \"output\"\n",
    "# json_path = json_dir / \"all_results_cometml.json\"\n",
    "# with open(json_path, \"w\", encoding=\"utf8\") as f:\n",
    "#     json.dump(all_results_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_info(all_results_dict: Dict[str, Dict[str, Any]]):\n",
    "    \"\"\"Remove unwanted info from all_results_dict. Mostly system metrics info.\"\"\"\n",
    "    all_info_names = set()\n",
    "    for experiment_dict in all_results_dict.values():\n",
    "        for info_name, an_info_item in experiment_dict.items():\n",
    "            if isinstance(an_info_item, list) and info_name != \"tags\":\n",
    "                for info_dict in an_info_item:\n",
    "                    all_info_names.add(info_dict[\"name\"])\n",
    "\n",
    "    undesired_info_names = set(name for name in all_info_names if \"sys\" in name)\n",
    "    undesired_info_names.update(\n",
    "        [\"offline_experiment\", \"storage_size_bytes\", \"throttled_by_params\"]\n",
    "    )\n",
    "\n",
    "    for exp_key, experiment_dict in list(all_results_dict.items()):\n",
    "        for info_name, info_item in experiment_dict.items():\n",
    "            if isinstance(info_item, list) and info_name != \"tags\":\n",
    "                info_item = [\n",
    "                    info_dict\n",
    "                    for info_dict in info_item\n",
    "                    if info_dict[\"name\"] not in undesired_info_names\n",
    "                ]\n",
    "                all_results_dict[exp_key][info_name] = info_item\n",
    "\n",
    "    return all_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results_dict = remove_unwanted_info(all_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_path = json_dir / \"all_results_cometml_filtered.json\"\n",
    "# with open(json_path, \"w\", encoding=\"utf8\") as f:\n",
    "#     json.dump(all_results_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_time_slice(experiments: Dict[str, Any], date1: str, date2: str) -> List[str]:\n",
    "    \"\"\"Select experiments within a time slice.\n",
    "\n",
    "    Args:\n",
    "        experiments (): object loaded from custom cometml json file\n",
    "        date1 (str): start date, ISO format\n",
    "        date2 (str): end date, ISO format\n",
    "    Returns:\n",
    "        List[str]: List of experiment keys\n",
    "    \"\"\"\n",
    "    valid_list = []\n",
    "    for exp_key, experiment in experiments.items():  # type: ignore\n",
    "        meta = experiment[\"metadata\"]\n",
    "        time = int(meta[\"startTimeMillis\"]) / 1000\n",
    "        time = datetime.utcfromtimestamp(time)\n",
    "        is_within_date = (\n",
    "            datetime.fromisoformat(date1) < time < datetime.fromisoformat(date2)\n",
    "        )\n",
    "        if is_within_date:\n",
    "            valid_list.append(exp_key)\n",
    "\n",
    "    return valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_path = (\n",
    "    Path().home()\n",
    "    / \"projects\"\n",
    "    / \"epiclass\"\n",
    "    / \"output\"\n",
    "    / \"all_results_cometml_filtered.json\"\n",
    ")\n",
    "with open(experiments_path, \"r\", encoding=\"utf8\") as f:\n",
    "    experiments = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_info_keys(experiments: Dict[str, Any]) -> Dict[str, Set[str]]:\n",
    "    \"\"\"Find the label for many information categories.\"\"\"\n",
    "    names = defaultdict(set)\n",
    "    for experiment in experiments.values():  # type: ignore\n",
    "        for k, v in experiment.items():\n",
    "            if isinstance(v, list) and k not in [\"tags\"]:\n",
    "                for item in v:\n",
    "                    try:\n",
    "                        name = item[\"name\"]\n",
    "                        names[k].add(name)\n",
    "                    except TypeError:\n",
    "                        print(k, v)\n",
    "\n",
    "    return names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dict_keys(['name', 'label_category', 'metadata', 'hyperparameters', 'metrics', 'other_metadata', 'tags'])\n",
    "\n",
    "'metadata' entries\n",
    "\n",
    "  experimentKey\n",
    "  experimentName\n",
    "  optimizationId\n",
    "  userName\n",
    "  projectId\n",
    "  projectName\n",
    "  workspaceName\n",
    "  filePath\n",
    "  fileName\n",
    "  throttle\n",
    "  throttleMessage\n",
    "  throttlingReasons\n",
    "  durationMillis\n",
    "  startTimeMillis\n",
    "  endTimeMillis\n",
    "  running\n",
    "  error\n",
    "  hasCrashed\n",
    "  archived\n",
    "\n",
    "'hyperparameters' entries\n",
    "\n",
    "  curr_step\n",
    "  hl_units\n",
    "  hparams/batch_size\n",
    "  hparams/early_stop_limit\n",
    "  hparams/is_training\n",
    "  hparams/keep_prob\n",
    "  hparams/l1_scale\n",
    "  hparams/l2_scale\n",
    "  hparams/learning_rate\n",
    "  hparams/measure_frequency\n",
    "  hparams/n_fold\n",
    "  hparams/oversample\n",
    "  hparams/oversampling\n",
    "  hparams/training_epochs\n",
    "  input_size\n",
    "  mapping/*\n",
    "  nb_layer\n",
    "  output_size\n",
    "  run_arg_*\n",
    "  for training runs, run_arg details:\n",
    "    run_arg_0 category\n",
    "    run_arg_1 hyperparams_filepath\n",
    "    run_arg_2 hdf5_list_filepath\n",
    "    run_arg_3 chromsizes_filepath\n",
    "    run_arg_4 metadata_filepath\n",
    "    run_arg_5 logdir_filepath\n",
    "\n",
    "'metrics' entries\n",
    "  Last epoch\n",
    "  Loop time\n",
    "  Split_time\n",
    "  Training time\n",
    "  tes_Accuracy\n",
    "  tes_F1Score\n",
    "  tes_MatthewsCorrCoef\n",
    "  tes_Precision\n",
    "  tes_Recall\n",
    "  tra_Accuracy\n",
    "  tra_F1Score\n",
    "  tra_MatthewsCorrCoef\n",
    "  tra_Precision\n",
    "  tra_Recall\n",
    "  train_acc\n",
    "  train_loss\n",
    "  val_Accuracy\n",
    "  val_F1Score\n",
    "  val_MatthewsCorrCoef\n",
    "  val_Precision\n",
    "  val_Recall\n",
    "  valid_acc\n",
    "  valid_loss\n",
    "\n",
    "\n",
    "'other_metadata' entries\n",
    "  Category\n",
    "  Code version / commit\n",
    "  Data source\n",
    "  Experience key\n",
    "  HDF5 Resolution\n",
    "  Initial hdf5 loading time\n",
    "  Last epoch\n",
    "  Main duration\n",
    "  Name\n",
    "  SLURM_JOB_ID\n",
    "  Total nb of files\n",
    "  Training size\n",
    "  Training time\n",
    "  assembly\n",
    "  category\n",
    "  test size\n",
    "  train size\n",
    "  validation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_json_to_flat_df(experiments: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Convert nested json to flat DataFrame.\n",
    "\n",
    "    Args:\n",
    "        experiments (Dict[str, Any]): object loaded from custom cometml json file\n",
    "    Returns:\n",
    "        pd.DataFrame: flat DataFrame\n",
    "    \"\"\"\n",
    "    flat_list = []\n",
    "    for experiment in experiments.values():  # type: ignore\n",
    "        flat_dict = {}\n",
    "        for k, v in experiment.items():\n",
    "            if k == \"tags\":\n",
    "                if not isinstance(v, list):\n",
    "                    v = [v]\n",
    "                flat_dict[k] = v\n",
    "            elif k == \"metadata\":\n",
    "                for meta_key, meta_value in sorted(v.items()):\n",
    "                    flat_dict[meta_key] = meta_value\n",
    "            elif isinstance(v, list):\n",
    "                for item in v:\n",
    "                    try:\n",
    "                        name = item[\"name\"]\n",
    "                        value = item[\"valueMax\"]\n",
    "                        flat_dict[name] = value\n",
    "                    except TypeError:\n",
    "                        print(k, v)\n",
    "\n",
    "        flat_list.append(flat_dict)\n",
    "\n",
    "    cols_to_cat = set()\n",
    "    for flat_dict in flat_list:\n",
    "        cols_to_cat.update(key for key in flat_dict.keys() if \"mapping\" in key)\n",
    "    cols_to_cat = sorted(cols_to_cat, key=lambda x: int(x.split(\"/\")[-1]))\n",
    "\n",
    "    df = pd.DataFrame.from_records(flat_list, index=\"experimentKey\")\n",
    "\n",
    "    # Combine all mapping columns into one.\n",
    "    df[cols_to_cat] = df[cols_to_cat].fillna(\"\")\n",
    "    try:\n",
    "        df[\"mapping\"] = df[cols_to_cat].apply(lambda x: \";\".join(x), axis=1)\n",
    "    except TypeError as e:\n",
    "        df[cols_to_cat].apply(lambda x: print(x), axis=1)  # type: ignore\n",
    "        raise e\n",
    "    df[\"mapping\"] = df[\"mapping\"].str.replace(\"[;;]+\", \";\", regex=True)\n",
    "    df[\"mapping\"] = df[\"mapping\"].str.replace(\";$\", \"\", regex=True)\n",
    "\n",
    "    df = df.drop(columns=cols_to_cat)\n",
    "\n",
    "    # Remove useless columns.\n",
    "    to_drop = [\n",
    "        \"optimizationId\",\n",
    "        \"userName\",\n",
    "        \"projectId\",\n",
    "        \"projectName\",\n",
    "        \"workspaceName\",\n",
    "        \"throttle\",\n",
    "        \"throttleMessage\",\n",
    "        \"throttlingReasons\",\n",
    "        \"running\",\n",
    "        \"error\",\n",
    "        \"hasCrashed\",\n",
    "        \"archived\",\n",
    "        \"Category\",\n",
    "        \"Data source\",\n",
    "        \"Experience key\",\n",
    "    ]\n",
    "\n",
    "    df = df.drop(columns=to_drop)\n",
    "\n",
    "    # Combine oversampling status columns.\n",
    "    oversampling_replace = {\"TRUE\": True, \"FALSE\": False}\n",
    "    oversampling_cat = [\"hparams/oversampling\", \"hparams/oversample\"]\n",
    "    df[oversampling_cat] = df[oversampling_cat].replace(oversampling_replace)\n",
    "    df[\"hparams/oversampling\"] = df[\"hparams/oversampling\"].fillna(\n",
    "        df[\"hparams/oversample\"]\n",
    "    )\n",
    "    df = df.drop(columns=\"hparams/oversample\")\n",
    "\n",
    "    # Transform datetime.timedelta(seconds=X) into HH:MM:SS format\n",
    "    time_cols = [\"Training time\", \"Loop time\"]\n",
    "    for col in time_cols:\n",
    "        for col in time_cols:\n",
    "            df[col] = df[col].fillna(\"\")\n",
    "            for item in df[col].items():\n",
    "                time_value = item[1]\n",
    "                if isinstance(time_value, str):\n",
    "                    re_search = re.search(\n",
    "                        r\"datetime\\.timedelta\\(seconds=(.*)\\)\", time_value\n",
    "                    )\n",
    "                    try:\n",
    "                        seconds = int(re_search.group(1))  # type: ignore\n",
    "                    except AttributeError:\n",
    "                        continue\n",
    "                    time_value_str = seconds_to_str(seconds)\n",
    "                    df.at[item[0], col] = time_value_str\n",
    "\n",
    "    # Combine train size cols\n",
    "    df[\"train size\"].fillna(df[\"Training size\"], inplace=True)\n",
    "    df = df.drop(columns=\"Training size\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nested_json_to_flat_df(experiments)\n",
    "df = df.reindex(sorted(df.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(experiments_path.parent / \"all_results_cometml_filtered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify value of train_size when oversampling false vs true, for a given nb_files, I think there were some mistakes maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in df.columns:\n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"test size\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_cols = [\"Name\", \"Total nb of files\", \"train size\", \"validation size\", \"hparams/oversampling\"]\n",
    "\n",
    "\n",
    "# Is oversampling expected?\n",
    "expected_no_oversampling = (df[\"hparams/oversampling\"] == \"false\")\n",
    "expected_oversampling = (df[\"hparams/oversampling\"] == \"true\")\n",
    "print(f\"Expected no oversampling: {expected_no_oversampling.sum()}\")\n",
    "print(f\"Expected oversampling: {expected_oversampling.sum()}\")\n",
    "# display(df[expected_no_oversampling][relevant_cols].value_counts())\n",
    "\n",
    "# Is oversampling observed?\n",
    "nb_real_files = df[\"Total nb of files\"].fillna(0).astype(int)\n",
    "max_files = (nb_real_files * 1.01).astype(float)\n",
    "\n",
    "total_sample_size = df[[\"train size\", \"validation size\"]].astype(float).sum(axis=1, skipna=True)\n",
    "\n",
    "observed_oversampling = total_sample_size > max_files\n",
    "observed_no_oversampling = total_sample_size == max_files\n",
    "print(f\"Observed oversampling: {observed_oversampling.sum()}\")\n",
    "print(f\"Observed no oversampling: {observed_no_oversampling.sum()}\")\n",
    "\n",
    "\n",
    "should_be_true = expected_no_oversampling & observed_oversampling\n",
    "should_be_false = expected_oversampling & observed_no_oversampling\n",
    "print(f\"Should be true: {should_be_true.sum()}\")\n",
    "print(f\"Should be false: {should_be_false.sum()}\")\n",
    "\n",
    "# display(df[should_be_true][relevant_cols].value_counts())\n",
    "# display(df[should_be_false][relevant_cols].value_counts())\n",
    "\n",
    "df.loc[should_be_true, \"hparams/oversampling\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(experiments_path.parent / \"all_results_cometml_filtered_oversampling-fixed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect all general run parameters: fix oversampling when missing\n",
    "i.e. create a new all_results_cometml_filtered_oversampling-fixed.csv\n",
    "- get difference of content between different metadata groups (diff md5, create new meta obj with just diff, display labels the usual way)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
