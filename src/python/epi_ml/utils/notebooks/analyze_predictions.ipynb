{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyze full prediction vector values.\"\"\"\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, pointless-statement, use-dict-literal, expression-not-assigned, unused-import, too-many-lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import sklearn.metrics\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.general_utility import get_valid_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSAY = \"assay_epiclass\"\n",
    "TRACK = \"track_type\"\n",
    "SEX = \"harmonized_donor_sex\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_local_logdir = Path.home() / \"downloads\" / \"temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_logdir = (\n",
    "    Path.home()\n",
    "    / \"mounts/narval-mount/project-rabyj/epilap/output/logs/epiatlas-dfreeze-v2.1/hg38_100kb_all_none\"\n",
    ")\n",
    "pred_folders = [\n",
    "    base_logdir / name\n",
    "    for name in [\n",
    "        \"harmonized_donor_life_stage_1l_3000n/no-unknown/10fold-oversampling\",\n",
    "        # \"assay_epiclass_1l_3000n/11c/10fold-oversampling\",\n",
    "        # \"harmonized_donor_sex_1l_3000n/w-mixed/10fold-oversample\"\n",
    "    ]\n",
    "]\n",
    "pred_files = [\n",
    "    pred_folder / \"full-10fold-validation_prediction_augmented-all.csv\"\n",
    "    for pred_folder in pred_folders\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_file in pred_files:\n",
    "    assert pred_file.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per sample, different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pred_file in pred_files:\n",
    "#         df = pd.read_csv(\n",
    "#             pred_file,\n",
    "#             sep=\",\",\n",
    "#             usecols=[\"True class\", \"Predicted class\", \"Max pred\"],\n",
    "#         )\n",
    "\n",
    "#         for threshold in [0, 0.7, 0.9]:\n",
    "#             sub_df  = df[df[\"Max pred\"] >= threshold]\n",
    "\n",
    "#             true, pred = sub_df.iloc[:, 0], sub_df.iloc[:, 1]\n",
    "#             labels = sorted(set(true.unique().tolist() + pred.unique().tolist()))\n",
    "#             confusion_mat = sklearn.metrics.confusion_matrix(true, pred, labels=labels)\n",
    "\n",
    "#             writer = ConfusionMatrixWriter(labels=labels, confusion_matrix=confusion_mat)\n",
    "#             writer.to_all_formats(\n",
    "#                 logdir=pred_file.parent,\n",
    "#                 name=str(pred_file.stem) + f\"-confusion_matrix-{threshold*100}\",\n",
    "#             )\n",
    "#             plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per EpiRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_class(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Given a DataFrame representing a single EpiRR, determine the predicted majority class.\n",
    "\n",
    "    Uses max prediction value to break ties.\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame containing predictions for a single EpiRR.\n",
    "\n",
    "    Returns:\n",
    "        str: The majority class label for this EpiRR.\n",
    "    \"\"\"\n",
    "    class_counts = df[\"Predicted class\"].value_counts()\n",
    "    max_count = class_counts.max()\n",
    "    majority_classes = class_counts[class_counts == max_count].index.tolist()\n",
    "\n",
    "    if len(majority_classes) == 1:\n",
    "        return majority_classes[0]\n",
    "\n",
    "    avg_max_pred = df.groupby(\"Predicted class\")[\"Max pred\"].mean()\n",
    "    return avg_max_pred.loc[majority_classes].idxmax()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_file in pred_files:\n",
    "    df = pd.read_csv(\n",
    "        pred_file,\n",
    "        sep=\",\",\n",
    "    )\n",
    "\n",
    "    # px.box(df, x=\"True class\", y=\"Max pred\", color=\"Predicted class\", title=pred_file.stem).show()\n",
    "\n",
    "    # only one true class per epiRR\n",
    "    total_epirrs = df[\"EpiRR\"].nunique()\n",
    "    print(\"Total number of EpiRRs:\", total_epirrs)\n",
    "    assert (\n",
    "        df[[\"EpiRR\", \"True class\"]].value_counts().shape[0]\n",
    "        == df[[\"EpiRR\"]].value_counts().shape[0]\n",
    "    )\n",
    "\n",
    "    for threshold in [0, 0.7, 0.9]:\n",
    "        threshold_df = df[df[\"Max pred\"] >= threshold]\n",
    "\n",
    "        # Group by EpiRR and apply the function to find the majority class for each EpiRR\n",
    "        majority_class_series = threshold_df.groupby(\"EpiRR\").apply(get_majority_class)\n",
    "        majority_class_series.name = \"Predicted class\"\n",
    "\n",
    "        threshold_df = threshold_df[[\"EpiRR\", \"True class\"]].drop_duplicates()\n",
    "        epirr_df = threshold_df.join(majority_class_series, how=\"inner\", on=\"EpiRR\")\n",
    "        epirr_df = epirr_df.set_index(\"EpiRR\")\n",
    "\n",
    "        print(\n",
    "            epirr_df[\n",
    "                (epirr_df[\"True class\"] == \"adult\")\n",
    "                & (epirr_df[\"Predicted class\"] == \"embryonic\")\n",
    "            ]\n",
    "        )\n",
    "        # true, pred = epirr_df[\"True class\"], epirr_df[\"Predicted class\"]\n",
    "        # assert len(true) == len(pred)\n",
    "        # assert len(true) <= total_epirrs\n",
    "\n",
    "        # labels = sorted(set(true.unique().tolist() + pred.unique().tolist()))\n",
    "        # confusion_mat = sklearn.metrics.confusion_matrix(true, pred, labels=labels)\n",
    "\n",
    "        # writer = ConfusionMatrixWriter(labels=labels, confusion_matrix=confusion_mat)\n",
    "\n",
    "        # out_logdir = pred_file.parent / \"conf_per_epirr\"\n",
    "        # out_logdir.mkdir(exist_ok=True)\n",
    "        # paths = writer.to_all_formats(\n",
    "        #     logdir=out_logdir,\n",
    "        #     name=str(pred_file.stem) + f\"-confusion_matrix-epirr-t{threshold*100}\",\n",
    "        # )\n",
    "        # print(paths[-1])\n",
    "        # plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of predicted classes for each EpiRR\n",
    "# class_counts = threshold_df.groupby(\"EpiRR\")[\"Predicted class\"].nunique()\n",
    "\n",
    "# # Find EpiRRs with more than one predicted class\n",
    "# epiRRs_with_multiple_classes = class_counts[class_counts > 1].index\n",
    "\n",
    "# # Filter the DataFrame to only include these EpiRRs\n",
    "# final_df = threshold_df[threshold_df[\"EpiRR\"].isin(epiRRs_with_multiple_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_class_2(group: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Given a DataFrame, determine the majority class per EpiRR.\n",
    "\n",
    "    Args:\n",
    "        group (pd.DataFrame): A DataFrame containing aggregated data for each EpiRR.\n",
    "\n",
    "    Returns:\n",
    "        str: The majority class label.\n",
    "    \"\"\"\n",
    "    # Sorting by count and mean\n",
    "    sorted_group = group.sort_values(\n",
    "        by=[(\"Max pred\", \"count\"), (\"Max pred\", \"mean\")], ascending=[False, False]\n",
    "    )\n",
    "\n",
    "    # Select the first (majority) class\n",
    "    majority_class = sorted_group.index[0][\n",
    "        2\n",
    "    ]  # The third element in the index tuple should be \"Predicted class\"\n",
    "\n",
    "    return majority_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check specific EpiRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pred_file in pred_files:\n",
    "\n",
    "#     df = pd.read_csv(\n",
    "#         pred_file,\n",
    "#         sep=\",\",\n",
    "#     )\n",
    "\n",
    "#     px.box(df, x=\"True class\", y=\"Max pred\", color=\"Predicted class\", title=pred_file.stem).show()\n",
    "\n",
    "#     # Check that there's only one true class for each EpiRR\n",
    "#     assert df.groupby(\"EpiRR\")[\"True class\"].nunique().eq(1).all()\n",
    "#     print(\"Total number of EpiRRs:\", df[\"EpiRR\"].nunique())\n",
    "\n",
    "#     threshold_df = df.groupby([\"EpiRR\",\"harmonized_donor_type\", \"True class\",\"Predicted class\", \"assay_epiclass\"]).agg({'Max pred': ['mean', 'median', 'count']})\n",
    "#     display(threshold_df.loc[\"IHECRE00003713.7\"])\n",
    "#     # classes = threshold_df.groupby(\"EpiRR\").apply(get_majority_class_2)\n",
    "#     # display(classes)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction distributions (per cell of confusion matrix)\n",
    "\n",
    "Analyze prediction values of correct vs false predictions. Can we find a good prediction score threshold that lets us eliminate important errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logdir = Path.home() / \"downloads\" / \"temp\"\n",
    "\n",
    "# path = logdir / \"sex3_oversample_full-10fold-validation_prediction_augmented-all.csv\"\n",
    "# df = pd.read_csv(path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = df[\"True class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"harmonized_donor_sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in classes:\n",
    "#     df_label = df[df[\"True class\"] == label]\n",
    "#     fig = go.Figure()\n",
    "\n",
    "#     # Iterate classes each target and add a violin plot for it\n",
    "#     for target in classes:\n",
    "#         vals = df_label[df_label[\"Predicted class\"] == target][\"Max pred\"]\n",
    "#         print(df_label[\"assay_epiclass\"].value_counts())\n",
    "\n",
    "#         fig.add_trace(\n",
    "#             go.Violin(\n",
    "#                 y=vals,\n",
    "#                 name=f\"{target} ({len(vals)})\",\n",
    "#                 box_visible=True,\n",
    "#                 meanline_visible=True,\n",
    "#                 points=\"all\",\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     fig.update_layout(\n",
    "#         title_text=f\"Predicted value distribution for {label} ({df_label.shape[0]})\",\n",
    "#         yaxis_title=\"Prediction score\",\n",
    "#         xaxis_title=\"Target\",\n",
    "#     )\n",
    "#     fig.update_yaxes(range=[1 / len(classes), 1.01])\n",
    "\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assay_list(df: pd.DataFrame) -> List[List[str]]:\n",
    "    \"\"\"Return list of assay labels. Includes rna and wgb label pairs.\"\"\"\n",
    "    assay_labels = df[\"assay_epiclass\"].unique().tolist()\n",
    "    assay_labels = [[assay_label] for assay_label in assay_labels]\n",
    "    assay_labels = assay_labels + [\n",
    "        [\"mrna_seq\", \"rna_seq\"],\n",
    "        [\"wgbs-standard\", \"wgbs-pbat\"],\n",
    "    ]\n",
    "    return assay_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex chrY coverage information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = Path.home() / \"downloads\" / \"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = logdir / \"sex3_oversample_full-10fold-validation_prediction_augmented-all.csv\"\n",
    "sex_df = pd.read_csv(path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_path = logdir / \"chrXY_coverage_all.csv\"\n",
    "coverage_df = pd.read_csv(coverage_path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = sex_df.merge(coverage_df, left_index=True, right_index=True, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df.to_csv(\n",
    "#     logdir / \"sex3_oversample_full-10fold-validation_prediction_augmented-all-chrY.csv\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df, coverage_df, merged_df]:\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[~merged_df[TRACK].str.contains(pat=\"pval|fc\", case=False)]\n",
    "# merged_df = merged_df[~merged_df[ASSAY].str.contains(pat=\"wgb|input\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS_DICT = {\"female\": \"red\", \"male\": \"blue\", \"mixed\": \"purple\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_labels = get_assay_list(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All samples (1 sample = 1 data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir_10fold_per_assay = (\n",
    "    general_local_logdir\n",
    "    / \"chrY_coverage_results\"\n",
    "    / \"10fold_valid\"\n",
    "    / \"conf_matrix_per_assay\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a 3x3 subplot layout\n",
    "coverage_label = \"chrY\"\n",
    "classes = merged_df[\"True class\"].unique()\n",
    "\n",
    "# Iterate over each label to populate the subplots\n",
    "for assay_list in assay_labels:\n",
    "    assay_df = merged_df[merged_df[ASSAY].isin(assay_list)]\n",
    "    for threshold in [0, 0.7, 0.9]:\n",
    "        row = 1\n",
    "        col = 1\n",
    "        fig = make_subplots(\n",
    "            rows=3,\n",
    "            cols=3,\n",
    "            shared_yaxes=True,\n",
    "            x_title=\"Predicted class (nb of predictions)\",\n",
    "            y_title=\"Mean coverage\",\n",
    "            row_titles=list(classes),\n",
    "            column_titles=list(classes),\n",
    "            vertical_spacing=0.08,\n",
    "            horizontal_spacing=0.01,\n",
    "        )\n",
    "        threshold_df = assay_df[assay_df[\"Max pred\"] >= threshold]\n",
    "        for label in classes:\n",
    "            df_label = threshold_df[threshold_df[\"True class\"] == label]\n",
    "\n",
    "            # Iterate over each target and add a violin plot for it\n",
    "            for target in classes:\n",
    "                sub_df = df_label[df_label[\"Predicted class\"] == target]\n",
    "\n",
    "                if len(assay_list) == 1:\n",
    "                    hovertext = [\n",
    "                        f\"{md5sum}:(chrY={chrY_val:.3f}, pred={pred:.3f})\"\n",
    "                        for md5sum, pred, chrY_val in zip(\n",
    "                            sub_df.index, sub_df[\"Max pred\"], sub_df[coverage_label]\n",
    "                        )\n",
    "                    ]\n",
    "                else:\n",
    "                    hovertext = [\n",
    "                        f\"{md5sum},{assay}:(chrY={chrY_val:.3f}, pred={pred:.3f})\"\n",
    "                        for md5sum, pred, chrY_val, assay in zip(\n",
    "                            sub_df.index,\n",
    "                            sub_df[\"Max pred\"],\n",
    "                            sub_df[coverage_label],\n",
    "                            sub_df[ASSAY],\n",
    "                        )\n",
    "                    ]\n",
    "                fig.add_trace(\n",
    "                    go.Violin(\n",
    "                        y=sub_df[coverage_label],\n",
    "                        name=f\"{target} ({sub_df.shape[0]})\",\n",
    "                        box_visible=True,\n",
    "                        meanline_visible=True,\n",
    "                        points=\"all\",\n",
    "                        text=hovertext,\n",
    "                        line_color=COLORS_DICT[target],\n",
    "                        hovertemplate=\"%{text}\",\n",
    "                    ),\n",
    "                    row=row,\n",
    "                    col=col,\n",
    "                )\n",
    "\n",
    "                # Move to the next subplot position\n",
    "                col += 1\n",
    "                if col > 3:\n",
    "                    col = 1\n",
    "                    row += 1\n",
    "\n",
    "        # Update global layout and traces\n",
    "        fig.update_traces(marker=dict(size=1))\n",
    "        fig.update_yaxes(range=[-0.001, max(assay_df[coverage_label])])\n",
    "\n",
    "        # Directly using annotations param does not work with make_subplots\n",
    "        existing_annotations = fig.layout.annotations\n",
    "        new_annotation = dict(\n",
    "            x=1.01,  # Position on the x-axis\n",
    "            y=0.5,  # Position on the y-axis\n",
    "            showarrow=False,  # Do not show arrow\n",
    "            text=\"Reference class\",  # The text you want to display\n",
    "            xref=\"paper\",  # 'x' coordinate is set in relative coordinates\n",
    "            yref=\"paper\",  # 'y' coordinate is set in relative coordinates\n",
    "            xanchor=\"left\",  # Text starts from the left of the x-coordinate\n",
    "            yanchor=\"middle\",  # Middle aligned vertically\n",
    "            font=dict(size=16),\n",
    "            textangle=90,\n",
    "        )\n",
    "        updated_annotations = list(existing_annotations) + [new_annotation]\n",
    "\n",
    "        title = f\"Mean chrY coverage per file, {','.join(assay_list)} (pred>{threshold})<br>(no fc/pval)\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"{title} (n={threshold_df.shape[0]})\",\n",
    "            showlegend=False,\n",
    "            annotations=updated_annotations,\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "        title = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "        fig.write_html(logdir_10fold_per_assay / f\"{title}.html\")\n",
    "        fig.write_image(logdir_10fold_per_assay / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chrY + chrX + ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = merged_df[merged_df[\"Max pred\"] > 0.9]\n",
    "# merged_df = merged_df[\n",
    "#     ~merged_df[\"assay_epiclass\"].str.contains(pat=\"input|wgb\", case=False)\n",
    "# ]\n",
    "\n",
    "# for label in classes:\n",
    "#     df_label = merged_df[merged_df[\"Predicted class\"] == label]\n",
    "#     fig = go.Figure()\n",
    "\n",
    "#     # Iterate classes each target and add a violin plot for it\n",
    "#     for target in classes:\n",
    "#         for coverage_label in [\"chrY\", \"chrX\", \"chrY/chrX\"]:\n",
    "#             sub_df = df_label[df_label[\"True class\"] == target]\n",
    "\n",
    "#             fig.add_trace(\n",
    "#                 go.Violin(\n",
    "#                     y=sub_df[coverage_label],\n",
    "#                     name=f\"{target}: {coverage_label} ({sub_df.shape[0]})\",\n",
    "#                     box_visible=True,\n",
    "#                     meanline_visible=True,\n",
    "#                     points=\"all\",\n",
    "#                     text=sub_df.index,\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     # title = f\"Coverage distribution for prediction {label}\"\n",
    "#     title = f\"Coverage distribution for prediction {label}, max_pred > 0.9\"\n",
    "#     fig.update_layout(\n",
    "#         title_text=f\"{title} ({df_label.shape[0]})\",\n",
    "#         yaxis_title=\"Mean coverage\",\n",
    "#         xaxis_title=\"True class\"\n",
    "#     )\n",
    "#     fig.update_traces(marker=dict(size=1))\n",
    "#     fig.update_yaxes(range=[-0.001, 2])\n",
    "\n",
    "\n",
    "#     fig.show()\n",
    "\n",
    "#     title = get_valid_filename(title)\n",
    "#     # fig.write_html(logdir / f\"{title}.html\")\n",
    "#     # fig.write_image(logdir / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in classes:\n",
    "#     df_label = merged_df[merged_df[\"True class\"] == label]\n",
    "#     fig = go.Figure()\n",
    "\n",
    "#     # Iterate classes each target and add a violin plot for it\n",
    "#     for target in classes:\n",
    "#         for coverage_label in [\"chrY\", \"chrX\", \"chrY/chrX\"]:\n",
    "#             sub_df = df_label[df_label[\"Predicted class\"] == target]\n",
    "\n",
    "#             fig.add_trace(\n",
    "#                 go.Violin(\n",
    "#                     y=sub_df[coverage_label],\n",
    "#                     name=f\"{target}: {coverage_label} ({sub_df.shape[0]})\",\n",
    "#                     box_visible=True,\n",
    "#                     meanline_visible=True,\n",
    "#                     points=\"all\",\n",
    "#                     text=sub_df.index,\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     # title = f\"Coverage distribution for label {label}\"\n",
    "#     title = f\"Coverage distribution for label {label}, max_pred > 0.9\"\n",
    "#     fig.update_layout(\n",
    "#         title_text=f\"{title} ({df_label.shape[0]})\",\n",
    "#         yaxis_title=\"Mean coverage\",\n",
    "#         xaxis_title=\"Predicted class\",\n",
    "#     )\n",
    "#     fig.update_yaxes(range=[-0.001, 1.5])\n",
    "#     fig.update_traces(marker=dict(size=1))\n",
    "\n",
    "#     fig.show()\n",
    "\n",
    "#     title = get_valid_filename(title)\n",
    "#     # fig.write_html(logdir / f\"{title}.html\")\n",
    "#     # fig.write_image(logdir / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epiRR version (1 epiRR ~ 1 data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = merged_df[\"Predicted class\"].unique()\n",
    "\n",
    "epirr_df = (\n",
    "    merged_df.groupby([\"EpiRR\", \"True class\", \"Predicted class\"])\n",
    "    .agg({\"Max pred\": [\"mean\", \"median\"], \"chrY\": [\"mean\", \"median\"], \"EpiRR\": [\"count\"]})\n",
    "    .reset_index()\n",
    "    .set_index(\"EpiRR\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epirr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = epirr_df[\n",
    "#     (epirr_df[\"True class\"] == \"mixed\")\n",
    "#     & (epirr_df[\"Predicted class\"] == \"female\")\n",
    "#     & (~epirr_df[\"track_type\"].str.contains(\"fc|pval\"))\n",
    "# ]\n",
    "# display(test)\n",
    "# print(test.index.value_counts().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed_columns = [\"True class\", \"Predicted class\", \"EpiRR\", \"Max pred\"]\n",
    "# merged_df[\n",
    "#     (merged_df[\"True class\"] == \"mixed\")\n",
    "#     & (merged_df[\"Predicted class\"] == \"female\")\n",
    "#     & (~merged_df[\"track_type\"].str.contains(\"fc|pval\"))\n",
    "# ][needed_columns].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_label = \"chrY\"\n",
    "\n",
    "for metric, pred_threshold in itertools.product([\"mean\", \"median\"], [0, 0.7, 0.9]):\n",
    "    fig = make_subplots(\n",
    "        rows=3,\n",
    "        cols=3,\n",
    "        shared_yaxes=True,\n",
    "        x_title=\"Predicted class (nb of epiRR)\",\n",
    "        y_title=f\"{metric} coverage\",\n",
    "        row_titles=list(classes),\n",
    "        column_titles=list(classes),\n",
    "        vertical_spacing=0.08,\n",
    "        horizontal_spacing=0.01,\n",
    "    )\n",
    "\n",
    "    row = 1\n",
    "    col = 1\n",
    "    threshold_sub_df = epirr_df[epirr_df[\"Max pred\"][f\"{metric}\"] > pred_threshold]\n",
    "    for label in classes:\n",
    "        df_label = threshold_sub_df[threshold_sub_df[\"True class\"] == label]\n",
    "        for target in classes:\n",
    "            sub_df = df_label[df_label[\"Predicted class\"] == target]\n",
    "\n",
    "            hovertext = [\n",
    "                f\"{epirr} (n={count}) pred:{pred:.02f}\"\n",
    "                for (epirr, count), pred in zip(\n",
    "                    sub_df.index, sub_df[\"Max pred\"][f\"{metric}\"]\n",
    "                )\n",
    "            ]\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    y=sub_df[coverage_label][f\"{metric}\"],\n",
    "                    name=f\"{target}: {metric}({coverage_label}) ({sub_df.shape[0]})\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=\"all\",\n",
    "                    line_color=COLORS_DICT[target],\n",
    "                    text=hovertext,\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "            # Move to the next subplot position\n",
    "            col += 1\n",
    "            if col > 3:\n",
    "                col = 1\n",
    "                row += 1\n",
    "\n",
    "    # Update global layout and traces\n",
    "    fig.update_traces(marker=dict(size=1))\n",
    "    fig.update_yaxes(range=[-0.001, 1.5])\n",
    "\n",
    "    # Directly using annotations param does not work with make_subplots\n",
    "    existing_annotations = fig.layout.annotations\n",
    "    new_annotation = dict(\n",
    "        x=1.01,  # Position on the x-axis\n",
    "        y=0.5,  # Position on the y-axis\n",
    "        showarrow=False,  # Do not show arrow\n",
    "        text=\"Reference class\",  # The text you want to display\n",
    "        xref=\"paper\",  # 'x' coordinate is set in relative coordinates\n",
    "        yref=\"paper\",  # 'y' coordinate is set in relative coordinates\n",
    "        xanchor=\"left\",  # Text starts from the left of the x-coordinate\n",
    "        yanchor=\"middle\",  # Middle aligned vertically\n",
    "        font=dict(size=16),\n",
    "        textangle=90,\n",
    "    )\n",
    "    updated_annotations = list(existing_annotations) + [new_annotation]\n",
    "\n",
    "    title = f\"Coverage distribution of {metric}({coverage_label}) per epiRR<br>{metric}(max_pred) > {pred_threshold} (no fc/pval/input/wgb)\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{title} (n={threshold_sub_df.shape[0]})\",\n",
    "        showlegend=False,\n",
    "        annotations=updated_annotations,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # title = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "    # fig.write_html(logdir / f\"{title}.html\")\n",
    "    # fig.write_image(logdir / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chrY - unknown samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_path = general_local_logdir / \"chrY_coverage_results\" / \"chrXY_coverage_all.csv\"\n",
    "coverage_df = pd.read_csv(coverage_path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_predict_path = (\n",
    "    general_local_logdir\n",
    "    / \"sex3_complete_no_valid_oversample_test_prediction_100kb_all_none_dfreeze_v2.1_sex_mixed_unknown_augmented-all.csv\"\n",
    ")\n",
    "unknown_predict_df = pd.read_csv(unknown_predict_path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"unknown\"\n",
    "unknown_predict_df = unknown_predict_df[unknown_predict_df[\"True class\"] == label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_predict_df = unknown_predict_df[\n",
    "    ~unknown_predict_df[TRACK].str.contains(pat=\"pval|fc\", case=False)\n",
    "]\n",
    "unknown_predict_df = unknown_predict_df.merge(\n",
    "    coverage_df, left_index=True, right_index=True, how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All samples (1 sample = 1 data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_logdir = general_local_logdir / \"chrY_coverage_results\" / \"unknown_per_assay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = unknown_predict_df[\"Predicted class\"].unique()\n",
    "coverage_label = \"chrY\"\n",
    "\n",
    "assay_labels = get_assay_list(unknown_predict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for assay_list in assay_labels:\n",
    "    # Initialize subplots figure\n",
    "    fig = make_subplots(\n",
    "        rows=3,\n",
    "        cols=1,\n",
    "        subplot_titles=(\"pred>0\", \"pred>0.7\", \"pred>0.9\"),\n",
    "        vertical_spacing=0.075,\n",
    "        x_title=\"Predicted class (nb of predictions)\",\n",
    "        y_title=\"Mean coverage\",\n",
    "    )\n",
    "\n",
    "    assay_sub_df = unknown_predict_df[\n",
    "        unknown_predict_df[\"assay_epiclass\"].isin(assay_list)\n",
    "    ]\n",
    "\n",
    "    for idx, pred_threshold in enumerate([0, 0.7, 0.9]):\n",
    "        threshold_sub_df = assay_sub_df[assay_sub_df[\"Max pred\"] > pred_threshold]\n",
    "\n",
    "        for target in classes:\n",
    "            sub_df = threshold_sub_df[threshold_sub_df[\"Predicted class\"] == target]\n",
    "\n",
    "            if len(assay_list) == 1:\n",
    "                hovertext = [\n",
    "                    f\"{md5sum}:(chrY={chrY_val:.3f}, pred={pred:.3f})\"\n",
    "                    for md5sum, pred, chrY_val in zip(\n",
    "                        sub_df.index, sub_df[\"Max pred\"], sub_df[coverage_label]\n",
    "                    )\n",
    "                ]\n",
    "            else:\n",
    "                hovertext = [\n",
    "                    f\"{md5sum},{assay}:(chrY={chrY_val:.3f}, pred={pred:.3f})\"\n",
    "                    for md5sum, pred, chrY_val, assay in zip(\n",
    "                        sub_df.index,\n",
    "                        sub_df[\"Max pred\"],\n",
    "                        sub_df[coverage_label],\n",
    "                        sub_df[ASSAY],\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "            # Add traces with checks for empty subsets\n",
    "            if sub_df.shape[0] == 0:\n",
    "                y_values = [\n",
    "                    threshold_sub_df[coverage_label].mean()\n",
    "                ]  # Minimal synthetic data\n",
    "                sample_count = 0\n",
    "                hovertext = [\"PLACEHOLDER - NO DATA\"]\n",
    "            else:\n",
    "                y_values = sub_df[coverage_label]\n",
    "                sample_count = sub_df.shape[0]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    y=y_values,\n",
    "                    name=f\"{target}: {coverage_label} ({sample_count})\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=\"all\",\n",
    "                    text=hovertext,\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    line_color=COLORS_DICT[target],\n",
    "                    legendgroup=target,\n",
    "                ),\n",
    "                row=idx + 1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "    title = f\"Coverage distribution for {coverage_label} in {','.join(assay_list)} (no fc/pval)\"\n",
    "    fig.update_layout(title_text=f\"{title}\", height=1200)\n",
    "\n",
    "    # Update y-axis range\n",
    "    try:\n",
    "        fig.update_yaxes(range=[-0.001, max(assay_sub_df[coverage_label])])\n",
    "    except ValueError:\n",
    "        # Set a default y-axis range when no samples are available\n",
    "        fig.update_yaxes(range=[-0.001, 1])\n",
    "\n",
    "    fig.update_traces(marker=dict(size=1))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    title = get_valid_filename(title)\n",
    "    fig.write_html(unknown_logdir / f\"{title}.html\")\n",
    "    fig.write_image(unknown_logdir / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epiRR version (1 epiRR ~ 1 data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_predict_df = unknown_predict_df[\n",
    "    ~unknown_predict_df[ASSAY].str.contains(pat=\"wgb\", case=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = unknown_predict_df[\"Predicted class\"].unique()\n",
    "\n",
    "epirr_df = (\n",
    "    unknown_predict_df.groupby([\"EpiRR\", \"Predicted class\"])\n",
    "    .agg({\"Max pred\": [\"mean\", \"median\"], \"chrY\": [\"mean\", \"median\"], \"EpiRR\": [\"count\"]})\n",
    "    .reset_index()\n",
    "    .set_index(\"EpiRR\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unknown_predict_df.shape)\n",
    "print(epirr_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epirr_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir_unknown_epirr = (\n",
    "    general_local_logdir / \"chrY_coverage_results\" / \"unknown_per_epirr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_label = \"chrY\"\n",
    "thresholds = [0, 0.7, 0.9]\n",
    "\n",
    "for agg_metric in [\"mean\", \"median\"]:\n",
    "    subplot_titles = [f\"{agg_metric}(pred)>{threshold}\" for threshold in thresholds]\n",
    "    fig = make_subplots(\n",
    "        rows=3,\n",
    "        cols=1,\n",
    "        subplot_titles=subplot_titles,\n",
    "        vertical_spacing=0.075,\n",
    "        x_title=\"Predicted class (nb of predictions)\",\n",
    "        y_title=\"agg(Mean chrY coverage) \",\n",
    "    )\n",
    "\n",
    "    for row_idx, pred_threshold in enumerate(thresholds):\n",
    "        threshold_sub_df = epirr_df[\n",
    "            epirr_df[\"Max pred\"][f\"{agg_metric}\"] > pred_threshold\n",
    "        ]\n",
    "\n",
    "        for target in classes:\n",
    "            sub_df = threshold_sub_df[threshold_sub_df[\"Predicted class\"] == target]\n",
    "\n",
    "            # Add traces with checks for empty subsets\n",
    "            if sub_df.shape[0] == 0:\n",
    "                y_values = [\n",
    "                    threshold_sub_df[coverage_label][f\"{agg_metric}\"].mean()\n",
    "                ]  # Minimal synthetic data\n",
    "                sample_count = 0\n",
    "                sample_text = [\"PLACEHOLDER - NO DATA\"]\n",
    "            else:\n",
    "                y_values = sub_df[coverage_label][f\"{agg_metric}\"]\n",
    "                sample_count = sub_df.shape[0]\n",
    "                sample_text = [\n",
    "                    (f\"{value:.3f}\", epirr, f\"{agg_metric}={pred:.3f}(n={count})\")\n",
    "                    for value, (epirr, count), pred in zip(\n",
    "                        y_values, sub_df.index, sub_df[\"Max pred\"][f\"{agg_metric}\"]\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    y=y_values,\n",
    "                    name=f\"{target}: {agg_metric}({coverage_label}) ({sample_count})\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=\"all\",\n",
    "                    text=sample_text,\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    line_color=COLORS_DICT[target],\n",
    "                    legendgroup=target,\n",
    "                ),\n",
    "                row=row_idx + 1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "    title = f\"Coverage distribution of {agg_metric}({coverage_label}) for {label} per epiRR (no fc/pval/wgb)\"\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{title}\", height=1200  # Adjust the overall height of the figure\n",
    "    )\n",
    "\n",
    "    # Update y-axis range\n",
    "    try:\n",
    "        fig.update_yaxes(range=[-0.001, max(epirr_df[coverage_label][f\"{agg_metric}\"])])\n",
    "    except ValueError as e:\n",
    "        fig.update_yaxes(range=[-0.001, 1])\n",
    "\n",
    "    fig.update_traces(marker=dict(size=1))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    title = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "    fig.write_html(logdir_unknown_epirr / f\"{title}.html\")\n",
    "    fig.write_image(logdir_unknown_epirr / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence threshold impact on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    threshold: float,\n",
    "    true_col: str,\n",
    "    pred_col: str,\n",
    "    pred_prob_cols: List[str],\n",
    "    target_class: str | None,\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute accuracy, precision, and subset size for a given probability threshold and class.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the true labels, predicted labels, and predicted probabilities.\n",
    "    threshold (float): The probability threshold for filtering the DataFrame.\n",
    "    true_col (str): The column name containing the true labels.\n",
    "    pred_col (str): The column name containing the predicted labels.\n",
    "    pred_prob_cols (List[str]): List of column names containing the predicted probabilities.\n",
    "    target_class (str|None): The class for which precision is to be calculated. Return np.nan if None.\n",
    "\n",
    "    Considers target class for computations if given, otherwise considers all samples.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, float, float, float]: A tuple containing the threshold, the calculated accuracy (%), the calculated precision (%),\n",
    "                                       and the subset size (%) respectively.\n",
    "    \"\"\"\n",
    "    # Targeting a class or not\n",
    "    if target_class in [None, \"all\"]:\n",
    "        total_size = len(df)\n",
    "    else:\n",
    "        total_size = len(df[true_col] == target_class)\n",
    "\n",
    "    # Filter rows where the max predicted probability is above the threshold\n",
    "    subset_df = df[df[pred_prob_cols].max(axis=1) >= threshold]\n",
    "\n",
    "    if len(subset_df) == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    # Calculate the accuracy for this subset\n",
    "    if target_class in [None, \"all\"]:\n",
    "        correct_preds = np.sum(subset_df[true_col] == subset_df[pred_col])\n",
    "        subset_size = len(subset_df)\n",
    "    else:\n",
    "        correct_preds = np.sum(\n",
    "            (subset_df[true_col] == subset_df[pred_col])\n",
    "            & (subset_df[true_col] == target_class)\n",
    "        )\n",
    "        subset_size = np.sum(subset_df[true_col] == target_class)\n",
    "    accuracy = (correct_preds / subset_size) * 100\n",
    "    subset_size_percent = (subset_size / total_size) * 100\n",
    "\n",
    "    # Calculate precision for the target class\n",
    "    if target_class in [None, \"all\"]:\n",
    "        precision = np.nan\n",
    "        return threshold, accuracy, precision, subset_size_percent\n",
    "\n",
    "    true_positives = np.sum(\n",
    "        (subset_df[true_col] == target_class) & (subset_df[pred_col] == target_class)\n",
    "    )\n",
    "    false_positives = np.sum(\n",
    "        (subset_df[true_col] != target_class) & (subset_df[pred_col] == target_class)\n",
    "    )\n",
    "\n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = np.nan\n",
    "    else:\n",
    "        precision = (true_positives / (true_positives + false_positives)) * 100\n",
    "\n",
    "    return threshold, accuracy, precision, subset_size_percent\n",
    "\n",
    "\n",
    "accuracy_name = \"rec\"\n",
    "precision_name = \"prec\"\n",
    "subset_size_name = \"sz\"\n",
    "\n",
    "\n",
    "def evaluate_thresholds(\n",
    "    df: pd.DataFrame, thresholds: List[float]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy and subset size for different probability thresholds with improved automatic column detection.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe containing true labels and predicted probabilities.\n",
    "    thresholds (list): List of probability thresholds to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe containing the accuracy and subset size for each threshold.\n",
    "    \"\"\"\n",
    "    # Automatic column detection\n",
    "    likely_true_class_cols = [col for col in df.columns if \"true\" in col.lower()]\n",
    "    likely_pred_class_cols = [col for col in df.columns if \"pred\" in col.lower()]\n",
    "\n",
    "    if not likely_true_class_cols or not likely_pred_class_cols:\n",
    "        raise ValueError(\n",
    "            \"Could not automatically detect 'True class' or 'Predicted class' columns.\"\n",
    "        )\n",
    "\n",
    "    true_col = likely_true_class_cols[0]\n",
    "    pred_col = likely_pred_class_cols[0]\n",
    "    if df[true_col].dtype != object or df[pred_col].dtype != object:\n",
    "        print(f\"{true_col} and {pred_col} are not string columns. Could cause issues.\")\n",
    "\n",
    "    classes = df[true_col].unique().tolist() + [\"all\"]\n",
    "    pred_prob_cols = classes[0:-1]\n",
    "    # Evaluate each threshold over each class\n",
    "    results_dfs = {}\n",
    "    for class_label in classes:\n",
    "        results = []\n",
    "        filtered_df = (\n",
    "            df\n",
    "            if class_label == \"all\"\n",
    "            else df[(df[true_col] == class_label) | (df[pred_col] == class_label)]\n",
    "        )\n",
    "\n",
    "        for thresh in thresholds:\n",
    "            result = compute_metrics(\n",
    "                filtered_df,\n",
    "                thresh,\n",
    "                true_col,\n",
    "                pred_col,\n",
    "                pred_prob_cols,\n",
    "                target_class=class_label,\n",
    "            )\n",
    "            results.append(result)\n",
    "\n",
    "        # Convert to DataFrame for easier manipulation\n",
    "        short_class_label = class_label[0:10]\n",
    "        results_df = pd.DataFrame(\n",
    "            results,\n",
    "            columns=[\n",
    "                \"Threshold\",\n",
    "                f\"{accuracy_name}_{short_class_label} (%)\",\n",
    "                f\"{precision_name}_{short_class_label} (%)\",\n",
    "                f\"{subset_size_name}_{short_class_label} (%) ({filtered_df.shape[0]})\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        results_dfs[class_label] = results_df\n",
    "\n",
    "    return results_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thresholds_graph_plotly(threshold_dfs: Dict[str, pd.DataFrame], name: str):\n",
    "    \"\"\"\n",
    "    Return graph of the accuracy and subset size at different probability thresholds for all classes.\n",
    "\n",
    "    Parameters:\n",
    "    threshold_dfs (Dict[str, pd.DataFrame]): A dictionary containing dataframes for each class label and the general case.\n",
    "    name (str): Graph title.\n",
    "\n",
    "    Returns:\n",
    "    go.Figure: Plotly figure object with the plotted graph.\n",
    "    \"\"\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "    colors = px.colors.qualitative.Dark24\n",
    "    marker1 = \"circle\"\n",
    "    marker2 = \"cross-open\"\n",
    "    marker3 = \"circle-open\"\n",
    "\n",
    "    for idx, (_, threshold_df) in enumerate(threshold_dfs.items()):\n",
    "        color = colors[idx % len(colors)]\n",
    "\n",
    "        acc_label = threshold_df.filter(like=f\"{accuracy_name}\").columns[0]\n",
    "        acc_subset = threshold_df.filter(like=f\"{subset_size_name}\").columns[0]\n",
    "        prec_label = threshold_df.filter(like=f\"{precision_name}\").columns[0]\n",
    "\n",
    "        # Plot accuracy\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=threshold_df[\"Threshold\"],\n",
    "                y=threshold_df[acc_label],\n",
    "                name=acc_label,\n",
    "                line=dict(color=color),\n",
    "                marker_symbol=marker1,\n",
    "                mode=\"lines+markers\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Plot precision\n",
    "        prec_vals = threshold_df[prec_label]\n",
    "        if not prec_vals.isna().all():\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=threshold_df[\"Threshold\"],\n",
    "                    y=prec_vals,\n",
    "                    name=prec_label,\n",
    "                    line=dict(color=color, dash=\"dot\"),\n",
    "                    marker_symbol=marker2,\n",
    "                    mode=\"lines+markers\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Plot subset size on secondary Y-axis\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=threshold_df[\"Threshold\"],\n",
    "                y=threshold_df[acc_subset],\n",
    "                name=acc_subset,\n",
    "                line=dict(color=color, dash=\"dash\"),\n",
    "                marker_symbol=marker3,\n",
    "                yaxis=\"y2\",\n",
    "                mode=\"lines+markers\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Adjusting the layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Accuracy and Subset Size at Different Probability Thresholds<br>{name}\",\n",
    "        xaxis_title=\"Probability Threshold\",\n",
    "        xaxis=dict(\n",
    "            tickvals=np.linspace(0, 1, 11),\n",
    "            ticktext=[f\"{x:.1f}\" for x in np.linspace(0, 1, 11)],\n",
    "        ),\n",
    "        yaxis_title=\"Accuracy (%)\",\n",
    "        yaxis2=dict(title=\"Subset Size (%)\", overlaying=\"y\", side=\"right\"),\n",
    "        legend=dict(orientation=\"v\", x=1.05, y=1),\n",
    "        height=1000,\n",
    "        width=1600,\n",
    "    )\n",
    "    fig.update_xaxes(range=[-0.001, 1.001])\n",
    "    fig.update_traces(line={\"width\": 1})\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select files for analysis\n",
    "list_path = (\n",
    "    Path.home() / \"Projects/epilap/output/logs/epiatlas-dfreeze-v2.1/10fold_results.list\"\n",
    ")\n",
    "\n",
    "output2 = Path.home() / \"downloads\" / \"temp\" / \"threshold_impact_graph\"\n",
    "with open(list_path, \"r\", encoding=\"utf8\") as f:\n",
    "    files = [Path(line.strip()) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = list(np.arange(0, 1, 1 / 20)) + [0.99]\n",
    "for file in files:\n",
    "    print(file)\n",
    "    # compute\n",
    "    df = pd.read_csv(file, header=0, dtype={\"True class\": str, \"Predicted class\": str})\n",
    "    nb_samples = df.shape[0]\n",
    "    nb_classes = len(df.select_dtypes(include=[np.number]).columns.tolist())\n",
    "\n",
    "    threshold_dfs = evaluate_thresholds(df, thresholds)\n",
    "\n",
    "    # plot\n",
    "    name = f\"{file.parents[1].name} - {file.parents[0].name} - {nb_classes} classes\"\n",
    "    graph = create_thresholds_graph_plotly(threshold_dfs, f\"{name} - n={nb_samples}\")\n",
    "\n",
    "    # save\n",
    "    filename = f\"threshold_impact_graph_full_{get_valid_filename(name)}\".replace(\n",
    "        \"_-_\", \"-\"\n",
    "    )\n",
    "    print(filename)\n",
    "    hmtl_name = filename + \".html\"\n",
    "    png_name = filename + \".png\"\n",
    "\n",
    "    out1_html = file.parent / hmtl_name\n",
    "    out1_png = file.parent / png_name\n",
    "\n",
    "    graph.write_html(out1_html)\n",
    "    graph.write_image(out1_png, scale=3)\n",
    "\n",
    "    shutil.copy(out1_html, output2 / hmtl_name)\n",
    "    shutil.copy(out1_png, output2 / png_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
