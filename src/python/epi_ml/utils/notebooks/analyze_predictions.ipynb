{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyze full prediction vector values.\"\"\"\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, pointless-statement, use-dict-literal, expression-not-assigned, unused-import, too-many-lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import sklearn.metrics\n",
    "from IPython.display import display\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from epi_ml.core.confusion_matrix import ConfusionMatrixWriter\n",
    "from epi_ml.utils.general_utility import get_valid_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIOMATERIAL_TYPE = \"harmonized_biomaterial_type\"\n",
    "CELL_TYPE = \"harmonized_sample_ontology_intermediate\"\n",
    "ASSAY = \"assay_epiclass\"\n",
    "SEX = \"harmonized_donor_sex\"\n",
    "CANCER = \"harmonized_sample_cancer_high\"\n",
    "DISEASE = \"harmonized_sample_disease_high\"\n",
    "LIFE_STAGE = \"harmonized_donor_life_stage\"\n",
    "TRACK = \"track_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_local_logdir = Path.home() / \"downloads\" / \"temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot table on assay and cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = (\n",
    "    general_local_logdir\n",
    "    / \"sex3_oversample_full-10fold-validation_prediction_augmented-all.csv\"\n",
    ")\n",
    "sex_df = pd.read_csv(results_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_table(df: pd.DataFrame, category_label: str) -> pd.DataFrame:\n",
    "    \"\"\"Create a pivot table for predictions results.\"\"\"\n",
    "    index_columns = [category_label, ASSAY, CELL_TYPE]\n",
    "    pivot = df.pivot_table(\n",
    "        index=index_columns,\n",
    "        columns=\"Predicted class\",\n",
    "        values=\"Same?\",\n",
    "        aggfunc=[\"count\", \"mean\"],\n",
    "        margins=True,\n",
    "        margins_name=\"Total\",\n",
    "        fill_value=0,\n",
    "    )\n",
    "\n",
    "    mean_columns = [\n",
    "        (aggfunc, pred_class)\n",
    "        for aggfunc, pred_class in pivot.columns\n",
    "        if aggfunc == \"mean\"\n",
    "    ]\n",
    "    mean_columns.remove((\"mean\", \"Total\"))\n",
    "    pivot.drop(columns=mean_columns, inplace=True)\n",
    "\n",
    "    return pivot\n",
    "\n",
    "\n",
    "sex_pivot = pivot_table(sex_df, SEX)\n",
    "sex_pivot.to_csv(\n",
    "    general_local_logdir\n",
    "    / \"sex3_oversample_full-10fold-validation_prediction_augmented-all_pivot.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_logdir = (\n",
    "    Path.home()\n",
    "    / \"mounts/narval-mount/project-rabyj/epilap/output/logs/epiatlas-dfreeze-v2.1/hg38_100kb_all_none\"\n",
    ")\n",
    "pred_folders = [\n",
    "    base_logdir / name\n",
    "    for name in [\n",
    "        \"harmonized_donor_life_stage_1l_3000n/no-unknown/10fold-oversampling\",\n",
    "        # \"assay_epiclass_1l_3000n/11c/10fold-oversampling\",\n",
    "        # \"harmonized_donor_sex_1l_3000n/w-mixed/10fold-oversample\"\n",
    "    ]\n",
    "]\n",
    "pred_files = [\n",
    "    pred_folder / \"full-10fold-validation_prediction_augmented-all.csv\"\n",
    "    for pred_folder in pred_folders\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_file in pred_files:\n",
    "    assert pred_file.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per sample, different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pred_file in pred_files:\n",
    "#         df = pd.read_csv(\n",
    "#             pred_file,\n",
    "#             sep=\",\",\n",
    "#             usecols=[\"True class\", \"Predicted class\", \"Max pred\"],\n",
    "#         )\n",
    "\n",
    "#         for threshold in [0, 0.7, 0.9]:\n",
    "#             sub_df  = df[df[\"Max pred\"] >= threshold]\n",
    "\n",
    "#             true, pred = sub_df.iloc[:, 0], sub_df.iloc[:, 1]\n",
    "#             labels = sorted(set(true.unique().tolist() + pred.unique().tolist()))\n",
    "#             confusion_mat = sklearn.metrics.confusion_matrix(true, pred, labels=labels)\n",
    "\n",
    "#             writer = ConfusionMatrixWriter(labels=labels, confusion_matrix=confusion_mat)\n",
    "#             writer.to_all_formats(\n",
    "#                 logdir=pred_file.parent,\n",
    "#                 name=str(pred_file.stem) + f\"-confusion_matrix-{threshold*100}\",\n",
    "#             )\n",
    "#             plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per EpiRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_class(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Given a DataFrame representing a single EpiRR, determine the predicted majority class.\n",
    "\n",
    "    Uses max prediction value to break ties.\n",
    "    Args:\n",
    "        df (pd.DataFrame): A DataFrame containing predictions for a single EpiRR.\n",
    "\n",
    "    Returns:\n",
    "        str: The majority class label for this EpiRR.\n",
    "    \"\"\"\n",
    "    class_counts = df[\"Predicted class\"].value_counts()\n",
    "    max_count = class_counts.max()\n",
    "    majority_classes = class_counts[class_counts == max_count].index.tolist()\n",
    "\n",
    "    if len(majority_classes) == 1:\n",
    "        return majority_classes[0]\n",
    "\n",
    "    avg_max_pred = df.groupby(\"Predicted class\")[\"Max pred\"].mean()\n",
    "    return avg_max_pred.loc[majority_classes].idxmax()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred_file in pred_files:\n",
    "    df = pd.read_csv(\n",
    "        pred_file,\n",
    "        sep=\",\",\n",
    "    )\n",
    "\n",
    "    # px.box(df, x=\"True class\", y=\"Max pred\", color=\"Predicted class\", title=pred_file.stem).show()\n",
    "\n",
    "    # only one true class per epiRR\n",
    "    total_epirrs = df[\"EpiRR\"].nunique()\n",
    "    print(\"Total number of EpiRRs:\", total_epirrs)\n",
    "    assert (\n",
    "        df[[\"EpiRR\", \"True class\"]].value_counts().shape[0]\n",
    "        == df[[\"EpiRR\"]].value_counts().shape[0]\n",
    "    )\n",
    "\n",
    "    for threshold in [0, 0.7, 0.9]:\n",
    "        threshold_df = df[df[\"Max pred\"] >= threshold]\n",
    "\n",
    "        # Group by EpiRR and apply the function to find the majority class for each EpiRR\n",
    "        majority_class_series = threshold_df.groupby(\"EpiRR\").apply(get_majority_class)\n",
    "        majority_class_series.name = \"Predicted class\"\n",
    "\n",
    "        threshold_df = threshold_df[[\"EpiRR\", \"True class\"]].drop_duplicates()\n",
    "        epirr_df = threshold_df.join(majority_class_series, how=\"inner\", on=\"EpiRR\")\n",
    "        epirr_df = epirr_df.set_index(\"EpiRR\")\n",
    "\n",
    "        print(\n",
    "            epirr_df[\n",
    "                (epirr_df[\"True class\"] == \"adult\")\n",
    "                & (epirr_df[\"Predicted class\"] == \"embryonic\")\n",
    "            ]\n",
    "        )\n",
    "        # true, pred = epirr_df[\"True class\"], epirr_df[\"Predicted class\"]\n",
    "        # assert len(true) == len(pred)\n",
    "        # assert len(true) <= total_epirrs\n",
    "\n",
    "        # labels = sorted(set(true.unique().tolist() + pred.unique().tolist()))\n",
    "        # confusion_mat = sklearn.metrics.confusion_matrix(true, pred, labels=labels)\n",
    "\n",
    "        # writer = ConfusionMatrixWriter(labels=labels, confusion_matrix=confusion_mat)\n",
    "\n",
    "        # out_logdir = pred_file.parent / \"conf_per_epirr\"\n",
    "        # out_logdir.mkdir(exist_ok=True)\n",
    "        # paths = writer.to_all_formats(\n",
    "        #     logdir=out_logdir,\n",
    "        #     name=str(pred_file.stem) + f\"-confusion_matrix-epirr-t{threshold*100}\",\n",
    "        # )\n",
    "        # print(paths[-1])\n",
    "        # plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of predicted classes for each EpiRR\n",
    "# class_counts = threshold_df.groupby(\"EpiRR\")[\"Predicted class\"].nunique()\n",
    "\n",
    "# # Find EpiRRs with more than one predicted class\n",
    "# epiRRs_with_multiple_classes = class_counts[class_counts > 1].index\n",
    "\n",
    "# # Filter the DataFrame to only include these EpiRRs\n",
    "# final_df = threshold_df[threshold_df[\"EpiRR\"].isin(epiRRs_with_multiple_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_class_2(group: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Given a DataFrame, determine the majority class per EpiRR.\n",
    "\n",
    "    Args:\n",
    "        group (pd.DataFrame): A DataFrame containing aggregated data for each EpiRR.\n",
    "\n",
    "    Returns:\n",
    "        str: The majority class label.\n",
    "    \"\"\"\n",
    "    # Sorting by count and mean\n",
    "    sorted_group = group.sort_values(\n",
    "        by=[(\"Max pred\", \"count\"), (\"Max pred\", \"mean\")], ascending=[False, False]\n",
    "    )\n",
    "\n",
    "    # Select the first (majority) class\n",
    "    majority_class = sorted_group.index[0][\n",
    "        2\n",
    "    ]  # The third element in the index tuple should be \"Predicted class\"\n",
    "\n",
    "    return majority_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check specific EpiRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pred_file in pred_files:\n",
    "\n",
    "#     df = pd.read_csv(\n",
    "#         pred_file,\n",
    "#         sep=\",\",\n",
    "#     )\n",
    "\n",
    "#     px.box(df, x=\"True class\", y=\"Max pred\", color=\"Predicted class\", title=pred_file.stem).show()\n",
    "\n",
    "#     # Check that there's only one true class for each EpiRR\n",
    "#     assert df.groupby(\"EpiRR\")[\"True class\"].nunique().eq(1).all()\n",
    "#     print(\"Total number of EpiRRs:\", df[\"EpiRR\"].nunique())\n",
    "\n",
    "#     threshold_df = df.groupby([\"EpiRR\",\"harmonized_donor_type\", \"True class\",\"Predicted class\", \"assay_epiclass\"]).agg({'Max pred': ['mean', 'median', 'count']})\n",
    "#     display(threshold_df.loc[\"IHECRE00003713.7\"])\n",
    "#     # classes = threshold_df.groupby(\"EpiRR\").apply(get_majority_class_2)\n",
    "#     # display(classes)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction distributions (per cell of confusion matrix)\n",
    "\n",
    "Analyze prediction values of correct vs false predictions. Can we find a good prediction score threshold that lets us eliminate important errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logdir = Path.home() / \"downloads\" / \"temp\"\n",
    "\n",
    "# path = logdir / \"sex3_oversample_full-10fold-validation_prediction_augmented-all.csv\"\n",
    "# df = pd.read_csv(path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = df[\"True class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"harmonized_donor_sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in classes:\n",
    "#     df_label = df[df[\"True class\"] == label]\n",
    "#     fig = go.Figure()\n",
    "\n",
    "#     # Iterate classes each target and add a violin plot for it\n",
    "#     for target in classes:\n",
    "#         vals = df_label[df_label[\"Predicted class\"] == target][\"Max pred\"]\n",
    "#         print(df_label[\"assay_epiclass\"].value_counts())\n",
    "\n",
    "#         fig.add_trace(\n",
    "#             go.Violin(\n",
    "#                 y=vals,\n",
    "#                 name=f\"{target} ({len(vals)})\",\n",
    "#                 box_visible=True,\n",
    "#                 meanline_visible=True,\n",
    "#                 points=\"all\",\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     fig.update_layout(\n",
    "#         title_text=f\"Predicted value distribution for {label} ({df_label.shape[0]})\",\n",
    "#         yaxis_title=\"Prediction score\",\n",
    "#         xaxis_title=\"Target\",\n",
    "#     )\n",
    "#     fig.update_yaxes(range=[1 / len(classes), 1.01])\n",
    "\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assay_list(df: pd.DataFrame) -> List[List[str]]:\n",
    "    \"\"\"Return list of assay labels. Includes rna and wgb label pairs.\"\"\"\n",
    "    assay_labels = df[\"assay_epiclass\"].unique().tolist()\n",
    "    assay_labels = [[assay_label] for assay_label in assay_labels]\n",
    "    assay_labels = assay_labels + [\n",
    "        [\"mrna_seq\", \"rna_seq\"],\n",
    "        [\"wgbs-standard\", \"wgbs-pbat\"],\n",
    "    ]\n",
    "    return assay_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex chrY coverage information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = Path.home() / \"downloads\" / \"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = logdir / \"sex3_oversample_full-10fold-validation_prediction_augmented-all.csv\"\n",
    "sex_df = pd.read_csv(path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_path = logdir / \"chrXY_coverage_all.csv\"\n",
    "coverage_df = pd.read_csv(coverage_path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = sex_df.merge(coverage_df, left_index=True, right_index=True, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df.to_csv(\n",
    "#     logdir / \"sex3_oversample_full-10fold-validation_prediction_augmented-all-chrY.csv\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df, coverage_df, merged_df]:\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[~merged_df[TRACK].str.contains(pat=\"pval|fc\", case=False)]\n",
    "# merged_df = merged_df[~merged_df[ASSAY].str.contains(pat=\"wgb|input\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS_DICT = {\"female\": \"red\", \"male\": \"blue\", \"mixed\": \"purple\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_labels = get_assay_list(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All samples (1 sample = 1 data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir_10fold_per_assay = (\n",
    "    general_local_logdir\n",
    "    / \"chrY_coverage_results\"\n",
    "    / \"10fold_valid\"\n",
    "    / \"conf_matrix_per_assay\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a 3x3 subplot layout\n",
    "coverage_label = \"chrY\"\n",
    "classes = merged_df[\"True class\"].unique()\n",
    "\n",
    "# Iterate over each label to populate the subplots\n",
    "for assay_list in assay_labels:\n",
    "    assay_df = merged_df[merged_df[ASSAY].isin(assay_list)]\n",
    "    for threshold in [0, 0.7, 0.9]:\n",
    "        row = 1\n",
    "        col = 1\n",
    "        fig = make_subplots(\n",
    "            rows=3,\n",
    "            cols=3,\n",
    "            shared_yaxes=True,\n",
    "            x_title=\"Predicted class (nb of predictions)\",\n",
    "            y_title=\"Mean coverage\",\n",
    "            row_titles=list(classes),\n",
    "            column_titles=list(classes),\n",
    "            vertical_spacing=0.08,\n",
    "            horizontal_spacing=0.01,\n",
    "        )\n",
    "        threshold_df = assay_df[assay_df[\"Max pred\"] >= threshold]\n",
    "        for label in classes:\n",
    "            df_label = threshold_df[threshold_df[\"True class\"] == label]\n",
    "\n",
    "            # Iterate over each target and add a violin plot for it\n",
    "            for target in classes:\n",
    "                sub_df = df_label[df_label[\"Predicted class\"] == target]\n",
    "\n",
    "                if len(assay_list) == 1:\n",
    "                    hovertext = [\n",
    "                        f\"{md5sum}:(chrY={chrY_val:.3f}, pred={pred:.3f})\"\n",
    "                        for md5sum, pred, chrY_val in zip(\n",
    "                            sub_df.index, sub_df[\"Max pred\"], sub_df[coverage_label]\n",
    "                        )\n",
    "                    ]\n",
    "                else:\n",
    "                    hovertext = [\n",
    "                        f\"{md5sum},{assay}:(chrY={chrY_val:.3f}, pred={pred:.3f})\"\n",
    "                        for md5sum, pred, chrY_val, assay in zip(\n",
    "                            sub_df.index,\n",
    "                            sub_df[\"Max pred\"],\n",
    "                            sub_df[coverage_label],\n",
    "                            sub_df[ASSAY],\n",
    "                        )\n",
    "                    ]\n",
    "                fig.add_trace(\n",
    "                    go.Violin(\n",
    "                        y=sub_df[coverage_label],\n",
    "                        name=f\"{target} ({sub_df.shape[0]})\",\n",
    "                        box_visible=True,\n",
    "                        meanline_visible=True,\n",
    "                        points=\"all\",\n",
    "                        text=hovertext,\n",
    "                        line_color=COLORS_DICT[target],\n",
    "                        hovertemplate=\"%{text}\",\n",
    "                    ),\n",
    "                    row=row,\n",
    "                    col=col,\n",
    "                )\n",
    "\n",
    "                # Move to the next subplot position\n",
    "                col += 1\n",
    "                if col > 3:\n",
    "                    col = 1\n",
    "                    row += 1\n",
    "\n",
    "        # Update global layout and traces\n",
    "        fig.update_traces(marker=dict(size=1))\n",
    "        fig.update_yaxes(range=[-0.001, max(assay_df[coverage_label])])\n",
    "\n",
    "        # Directly using annotations param does not work with make_subplots\n",
    "        existing_annotations = fig.layout.annotations\n",
    "        new_annotation = dict(\n",
    "            x=1.01,  # Position on the x-axis\n",
    "            y=0.5,  # Position on the y-axis\n",
    "            showarrow=False,  # Do not show arrow\n",
    "            text=\"Reference class\",  # The text you want to display\n",
    "            xref=\"paper\",  # 'x' coordinate is set in relative coordinates\n",
    "            yref=\"paper\",  # 'y' coordinate is set in relative coordinates\n",
    "            xanchor=\"left\",  # Text starts from the left of the x-coordinate\n",
    "            yanchor=\"middle\",  # Middle aligned vertically\n",
    "            font=dict(size=16),\n",
    "            textangle=90,\n",
    "        )\n",
    "        updated_annotations = list(existing_annotations) + [new_annotation]\n",
    "\n",
    "        title = f\"Mean chrY coverage per file, {','.join(assay_list)} (pred>{threshold})<br>(no fc/pval)\"\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"{title} (n={threshold_df.shape[0]})\",\n",
    "            showlegend=False,\n",
    "            annotations=updated_annotations,\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "        title = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "        fig.write_html(logdir_10fold_per_assay / f\"{title}.html\")\n",
    "        fig.write_image(logdir_10fold_per_assay / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chrY + chrX + ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = merged_df[merged_df[\"Max pred\"] > 0.9]\n",
    "# merged_df = merged_df[\n",
    "#     ~merged_df[\"assay_epiclass\"].str.contains(pat=\"input|wgb\", case=False)\n",
    "# ]\n",
    "\n",
    "# for label in classes:\n",
    "#     df_label = merged_df[merged_df[\"Predicted class\"] == label]\n",
    "#     fig = go.Figure()\n",
    "\n",
    "#     # Iterate classes each target and add a violin plot for it\n",
    "#     for target in classes:\n",
    "#         for coverage_label in [\"chrY\", \"chrX\", \"chrY/chrX\"]:\n",
    "#             sub_df = df_label[df_label[\"True class\"] == target]\n",
    "\n",
    "#             fig.add_trace(\n",
    "#                 go.Violin(\n",
    "#                     y=sub_df[coverage_label],\n",
    "#                     name=f\"{target}: {coverage_label} ({sub_df.shape[0]})\",\n",
    "#                     box_visible=True,\n",
    "#                     meanline_visible=True,\n",
    "#                     points=\"all\",\n",
    "#                     text=sub_df.index,\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     # title = f\"Coverage distribution for prediction {label}\"\n",
    "#     title = f\"Coverage distribution for prediction {label}, max_pred > 0.9\"\n",
    "#     fig.update_layout(\n",
    "#         title_text=f\"{title} ({df_label.shape[0]})\",\n",
    "#         yaxis_title=\"Mean coverage\",\n",
    "#         xaxis_title=\"True class\"\n",
    "#     )\n",
    "#     fig.update_traces(marker=dict(size=1))\n",
    "#     fig.update_yaxes(range=[-0.001, 2])\n",
    "\n",
    "\n",
    "#     fig.show()\n",
    "\n",
    "#     title = get_valid_filename(title)\n",
    "#     # fig.write_html(logdir / f\"{title}.html\")\n",
    "#     # fig.write_image(logdir / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in classes:\n",
    "#     df_label = merged_df[merged_df[\"True class\"] == label]\n",
    "#     fig = go.Figure()\n",
    "\n",
    "#     # Iterate classes each target and add a violin plot for it\n",
    "#     for target in classes:\n",
    "#         for coverage_label in [\"chrY\", \"chrX\", \"chrY/chrX\"]:\n",
    "#             sub_df = df_label[df_label[\"Predicted class\"] == target]\n",
    "\n",
    "#             fig.add_trace(\n",
    "#                 go.Violin(\n",
    "#                     y=sub_df[coverage_label],\n",
    "#                     name=f\"{target}: {coverage_label} ({sub_df.shape[0]})\",\n",
    "#                     box_visible=True,\n",
    "#                     meanline_visible=True,\n",
    "#                     points=\"all\",\n",
    "#                     text=sub_df.index,\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     # title = f\"Coverage distribution for label {label}\"\n",
    "#     title = f\"Coverage distribution for label {label}, max_pred > 0.9\"\n",
    "#     fig.update_layout(\n",
    "#         title_text=f\"{title} ({df_label.shape[0]})\",\n",
    "#         yaxis_title=\"Mean coverage\",\n",
    "#         xaxis_title=\"Predicted class\",\n",
    "#     )\n",
    "#     fig.update_yaxes(range=[-0.001, 1.5])\n",
    "#     fig.update_traces(marker=dict(size=1))\n",
    "\n",
    "#     fig.show()\n",
    "\n",
    "#     title = get_valid_filename(title)\n",
    "#     # fig.write_html(logdir / f\"{title}.html\")\n",
    "#     # fig.write_image(logdir / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epiRR version (1 epiRR ~ 1 data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = merged_df[\"Predicted class\"].unique()\n",
    "\n",
    "epirr_df = (\n",
    "    merged_df.groupby([\"EpiRR\", \"True class\", \"Predicted class\"])\n",
    "    .agg({\"Max pred\": [\"mean\", \"median\"], \"chrY\": [\"mean\", \"median\"], \"EpiRR\": [\"count\"]})\n",
    "    .reset_index()\n",
    "    .set_index(\"EpiRR\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epirr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = epirr_df[\n",
    "#     (epirr_df[\"True class\"] == \"mixed\")\n",
    "#     & (epirr_df[\"Predicted class\"] == \"female\")\n",
    "#     & (~epirr_df[\"track_type\"].str.contains(\"fc|pval\"))\n",
    "# ]\n",
    "# display(test)\n",
    "# print(test.index.value_counts().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed_columns = [\"True class\", \"Predicted class\", \"EpiRR\", \"Max pred\"]\n",
    "# merged_df[\n",
    "#     (merged_df[\"True class\"] == \"mixed\")\n",
    "#     & (merged_df[\"Predicted class\"] == \"female\")\n",
    "#     & (~merged_df[\"track_type\"].str.contains(\"fc|pval\"))\n",
    "# ][needed_columns].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_label = \"chrY\"\n",
    "\n",
    "for metric, pred_threshold in itertools.product([\"mean\", \"median\"], [0, 0.7, 0.9]):\n",
    "    fig = make_subplots(\n",
    "        rows=3,\n",
    "        cols=3,\n",
    "        shared_yaxes=True,\n",
    "        x_title=\"Predicted class (nb of epiRR)\",\n",
    "        y_title=f\"{metric} coverage\",\n",
    "        row_titles=list(classes),\n",
    "        column_titles=list(classes),\n",
    "        vertical_spacing=0.08,\n",
    "        horizontal_spacing=0.01,\n",
    "    )\n",
    "\n",
    "    row = 1\n",
    "    col = 1\n",
    "    threshold_sub_df = epirr_df[epirr_df[\"Max pred\"][f\"{metric}\"] > pred_threshold]\n",
    "    for label in classes:\n",
    "        df_label = threshold_sub_df[threshold_sub_df[\"True class\"] == label]\n",
    "        for target in classes:\n",
    "            sub_df = df_label[df_label[\"Predicted class\"] == target]\n",
    "\n",
    "            hovertext = [\n",
    "                f\"{epirr} (n={count}) pred:{pred:.02f}\"\n",
    "                for (epirr, count), pred in zip(\n",
    "                    sub_df.index, sub_df[\"Max pred\"][f\"{metric}\"]\n",
    "                )\n",
    "            ]\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    y=sub_df[coverage_label][f\"{metric}\"],\n",
    "                    name=f\"{target}: {metric}({coverage_label}) ({sub_df.shape[0]})\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=\"all\",\n",
    "                    line_color=COLORS_DICT[target],\n",
    "                    text=hovertext,\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                ),\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "\n",
    "            # Move to the next subplot position\n",
    "            col += 1\n",
    "            if col > 3:\n",
    "                col = 1\n",
    "                row += 1\n",
    "\n",
    "    # Update global layout and traces\n",
    "    fig.update_traces(marker=dict(size=1))\n",
    "    fig.update_yaxes(range=[-0.001, 1.5])\n",
    "\n",
    "    # Directly using annotations param does not work with make_subplots\n",
    "    existing_annotations = fig.layout.annotations\n",
    "    new_annotation = dict(\n",
    "        x=1.01,  # Position on the x-axis\n",
    "        y=0.5,  # Position on the y-axis\n",
    "        showarrow=False,  # Do not show arrow\n",
    "        text=\"Reference class\",  # The text you want to display\n",
    "        xref=\"paper\",  # 'x' coordinate is set in relative coordinates\n",
    "        yref=\"paper\",  # 'y' coordinate is set in relative coordinates\n",
    "        xanchor=\"left\",  # Text starts from the left of the x-coordinate\n",
    "        yanchor=\"middle\",  # Middle aligned vertically\n",
    "        font=dict(size=16),\n",
    "        textangle=90,\n",
    "    )\n",
    "    updated_annotations = list(existing_annotations) + [new_annotation]\n",
    "\n",
    "    title = f\"Coverage distribution of {metric}({coverage_label}) per epiRR<br>{metric}(max_pred) > {pred_threshold} (no fc/pval/input/wgb)\"\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{title} (n={threshold_sub_df.shape[0]})\",\n",
    "        showlegend=False,\n",
    "        annotations=updated_annotations,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # title = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "    # fig.write_html(logdir / f\"{title}.html\")\n",
    "    # fig.write_image(logdir / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chrY - unknown samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_path = general_local_logdir / \"chrY_coverage_results\" / \"chrXY_coverage_all.csv\"\n",
    "coverage_df = pd.read_csv(coverage_path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_predict_path = (\n",
    "    general_local_logdir\n",
    "    / \"sex3_complete_no_valid_oversample_test_prediction_100kb_all_none_dfreeze_v2.1_sex_mixed_unknown_augmented-all.csv\"\n",
    ")\n",
    "unknown_predict_df = pd.read_csv(unknown_predict_path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"unknown\"\n",
    "unknown_predict_df = unknown_predict_df[unknown_predict_df[\"True class\"] == label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_predict_df = unknown_predict_df[\n",
    "    ~unknown_predict_df[TRACK].str.contains(pat=\"pval|fc\", case=False)\n",
    "]\n",
    "unknown_predict_df = unknown_predict_df.merge(\n",
    "    coverage_df, left_index=True, right_index=True, how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All samples (1 sample = 1 data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_logdir = general_local_logdir / \"chrY_coverage_results\" / \"unknown_per_assay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = unknown_predict_df[\"Predicted class\"].unique()\n",
    "coverage_label = \"chrY\"\n",
    "\n",
    "assay_labels = get_assay_list(unknown_predict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for assay_list in assay_labels:\n",
    "    # Initialize subplots figure\n",
    "    fig = make_subplots(\n",
    "        rows=3,\n",
    "        cols=1,\n",
    "        subplot_titles=(\"pred>0\", \"pred>0.7\", \"pred>0.9\"),\n",
    "        vertical_spacing=0.075,\n",
    "        x_title=\"Predicted class (nb of predictions)\",\n",
    "        y_title=\"Mean coverage\",\n",
    "    )\n",
    "\n",
    "    assay_sub_df = unknown_predict_df[\n",
    "        unknown_predict_df[\"assay_epiclass\"].isin(assay_list)\n",
    "    ]\n",
    "\n",
    "    for idx, pred_threshold in enumerate([0, 0.7, 0.9]):\n",
    "        threshold_sub_df = assay_sub_df[assay_sub_df[\"Max pred\"] > pred_threshold]\n",
    "\n",
    "        for target in classes:\n",
    "            sub_df = threshold_sub_df[threshold_sub_df[\"Predicted class\"] == target]\n",
    "\n",
    "            if len(assay_list) == 1:\n",
    "                hovertext = [\n",
    "                    f\"{md5sum}:(chrY={chrY_val:.3f}, pred={pred:.3f})\"\n",
    "                    for md5sum, pred, chrY_val in zip(\n",
    "                        sub_df.index, sub_df[\"Max pred\"], sub_df[coverage_label]\n",
    "                    )\n",
    "                ]\n",
    "            else:\n",
    "                hovertext = [\n",
    "                    f\"{md5sum},{assay}:(chrY={chrY_val:.3f}, pred={pred:.3f})\"\n",
    "                    for md5sum, pred, chrY_val, assay in zip(\n",
    "                        sub_df.index,\n",
    "                        sub_df[\"Max pred\"],\n",
    "                        sub_df[coverage_label],\n",
    "                        sub_df[ASSAY],\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "            # Add traces with checks for empty subsets\n",
    "            if sub_df.shape[0] == 0:\n",
    "                y_values = [\n",
    "                    threshold_sub_df[coverage_label].mean()\n",
    "                ]  # Minimal synthetic data\n",
    "                sample_count = 0\n",
    "                hovertext = [\"PLACEHOLDER - NO DATA\"]\n",
    "            else:\n",
    "                y_values = sub_df[coverage_label]\n",
    "                sample_count = sub_df.shape[0]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    y=y_values,\n",
    "                    name=f\"{target}: {coverage_label} ({sample_count})\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=\"all\",\n",
    "                    text=hovertext,\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    line_color=COLORS_DICT[target],\n",
    "                    legendgroup=target,\n",
    "                ),\n",
    "                row=idx + 1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "    title = f\"Coverage distribution for {coverage_label} in {','.join(assay_list)} (no fc/pval)\"\n",
    "    fig.update_layout(title_text=f\"{title}\", height=1200)\n",
    "\n",
    "    # Update y-axis range\n",
    "    try:\n",
    "        fig.update_yaxes(range=[-0.001, max(assay_sub_df[coverage_label])])\n",
    "    except ValueError:\n",
    "        # Set a default y-axis range when no samples are available\n",
    "        fig.update_yaxes(range=[-0.001, 1])\n",
    "\n",
    "    fig.update_traces(marker=dict(size=1))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    title = get_valid_filename(title)\n",
    "    fig.write_html(unknown_logdir / f\"{title}.html\")\n",
    "    fig.write_image(unknown_logdir / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epiRR version (1 epiRR ~ 1 data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_predict_df = unknown_predict_df[\n",
    "    ~unknown_predict_df[ASSAY].str.contains(pat=\"wgb\", case=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = unknown_predict_df[\"Predicted class\"].unique()\n",
    "\n",
    "epirr_df = (\n",
    "    unknown_predict_df.groupby([\"EpiRR\", \"Predicted class\"])\n",
    "    .agg({\"Max pred\": [\"mean\", \"median\"], \"chrY\": [\"mean\", \"median\"], \"EpiRR\": [\"count\"]})\n",
    "    .reset_index()\n",
    "    .set_index(\"EpiRR\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unknown_predict_df.shape)\n",
    "print(epirr_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epirr_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir_unknown_epirr = (\n",
    "    general_local_logdir / \"chrY_coverage_results\" / \"unknown_per_epirr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_label = \"chrY\"\n",
    "thresholds = [0, 0.7, 0.9]\n",
    "\n",
    "for agg_metric in [\"mean\", \"median\"]:\n",
    "    subplot_titles = [f\"{agg_metric}(pred)>{threshold}\" for threshold in thresholds]\n",
    "    fig = make_subplots(\n",
    "        rows=3,\n",
    "        cols=1,\n",
    "        subplot_titles=subplot_titles,\n",
    "        vertical_spacing=0.075,\n",
    "        x_title=\"Predicted class (nb of predictions)\",\n",
    "        y_title=\"agg(Mean chrY coverage) \",\n",
    "    )\n",
    "\n",
    "    for row_idx, pred_threshold in enumerate(thresholds):\n",
    "        threshold_sub_df = epirr_df[\n",
    "            epirr_df[\"Max pred\"][f\"{agg_metric}\"] > pred_threshold\n",
    "        ]\n",
    "\n",
    "        for target in classes:\n",
    "            sub_df = threshold_sub_df[threshold_sub_df[\"Predicted class\"] == target]\n",
    "\n",
    "            # Add traces with checks for empty subsets\n",
    "            if sub_df.shape[0] == 0:\n",
    "                y_values = [\n",
    "                    threshold_sub_df[coverage_label][f\"{agg_metric}\"].mean()\n",
    "                ]  # Minimal synthetic data\n",
    "                sample_count = 0\n",
    "                sample_text = [\"PLACEHOLDER - NO DATA\"]\n",
    "            else:\n",
    "                y_values = sub_df[coverage_label][f\"{agg_metric}\"]\n",
    "                sample_count = sub_df.shape[0]\n",
    "                sample_text = [\n",
    "                    (f\"{value:.3f}\", epirr, f\"{agg_metric}={pred:.3f}(n={count})\")\n",
    "                    for value, (epirr, count), pred in zip(\n",
    "                        y_values, sub_df.index, sub_df[\"Max pred\"][f\"{agg_metric}\"]\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    y=y_values,\n",
    "                    name=f\"{target}: {agg_metric}({coverage_label}) ({sample_count})\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=\"all\",\n",
    "                    text=sample_text,\n",
    "                    hovertemplate=\"%{text}\",\n",
    "                    line_color=COLORS_DICT[target],\n",
    "                    legendgroup=target,\n",
    "                ),\n",
    "                row=row_idx + 1,\n",
    "                col=1,\n",
    "            )\n",
    "\n",
    "    title = f\"Coverage distribution of {agg_metric}({coverage_label}) for {label} per epiRR (no fc/pval/wgb)\"\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{title}\", height=1200  # Adjust the overall height of the figure\n",
    "    )\n",
    "\n",
    "    # Update y-axis range\n",
    "    try:\n",
    "        fig.update_yaxes(range=[-0.001, max(epirr_df[coverage_label][f\"{agg_metric}\"])])\n",
    "    except ValueError as e:\n",
    "        fig.update_yaxes(range=[-0.001, 1])\n",
    "\n",
    "    fig.update_traces(marker=dict(size=1))\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    title = get_valid_filename(title).replace(\"_br_\", \"_\")\n",
    "    fig.write_html(logdir_unknown_epirr / f\"{title}.html\")\n",
    "    fig.write_image(logdir_unknown_epirr / f\"{title}.png\", scale=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
