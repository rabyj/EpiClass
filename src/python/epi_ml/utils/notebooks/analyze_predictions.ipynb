{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"See markdown\"\"\"\n",
    "# pylint: disable=line-too-long, redefined-outer-name, import-error, pointless-statement, use-dict-literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze prediction values of correct vs false predictions. Can we find a good prediction score threshold that lets us eliminate important errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction distributions (per cell of confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from epi_ml.utils.general_utility import get_valid_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = Path.home() / \"downloads\" / \"temp\"\n",
    "\n",
    "path = logdir / \"sex3_oversample_full-10fold-validation_prediction_augmented-all.csv\"\n",
    "df = pd.read_csv(path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df[\"True class\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"harmonized_donor_sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in classes:\n",
    "#     df_label = df[df[\"True class\"] == label]\n",
    "#     fig = go.Figure()\n",
    "\n",
    "#     # Iterate classes each target and add a violin plot for it\n",
    "#     for target in classes:\n",
    "#         vals = df_label[df_label[\"Predicted class\"] == target][\"Max pred\"]\n",
    "#         print(df_label[\"assay_epiclass\"].value_counts())\n",
    "\n",
    "#         fig.add_trace(\n",
    "#             go.Violin(\n",
    "#                 y=vals,\n",
    "#                 name=f\"{target} ({len(vals)})\",\n",
    "#                 box_visible=True,\n",
    "#                 meanline_visible=True,\n",
    "#                 points=\"all\",\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     fig.update_layout(\n",
    "#         title_text=f\"Predicted value distribution for {label} ({df_label.shape[0]})\",\n",
    "#         yaxis_title=\"Prediction score\",\n",
    "#         xaxis_title=\"Target\",\n",
    "#     )\n",
    "#     fig.update_yaxes(range=[1 / len(classes), 1.01])\n",
    "\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine chrY coverage information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_path = logdir / \"coverage_combined.csv\"\n",
    "coverage_df = pd.read_csv(coverage_path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df.merge(coverage_df, left_index=True, right_index=True, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df, coverage_df, merged_df]:\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df[\"Max pred\"] > 0.9]\n",
    "merged_df = merged_df[\n",
    "    ~merged_df[\"assay_epiclass\"].str.contains(case=False, regex=True, pat=\"input|wgb\")\n",
    "]\n",
    "\n",
    "for label in classes:\n",
    "    df_label = merged_df[merged_df[\"Predicted class\"] == label]\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Iterate classes each target and add a violin plot for it\n",
    "    for target in classes:\n",
    "        for coverage_label in [\"chrY\", \"chrX\", \"chrY/chrX\"]:\n",
    "            sub_df = df_label[df_label[\"True class\"] == target]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    y=sub_df[coverage_label],\n",
    "                    name=f\"{target}: {coverage_label} ({sub_df.shape[0]})\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=\"all\",\n",
    "                    text=sub_df.index,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # title = f\"Coverage distribution for prediction {label}\"\n",
    "    title = f\"Coverage distribution for prediction {label}, max_pred > 0.9\"\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{title} ({df_label.shape[0]})\",\n",
    "        yaxis_title=\"Mean coverage\",\n",
    "        xaxis_title=\"True class\",\n",
    "    )\n",
    "    fig.update_yaxes(range=[-0.001, 2])\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    title = get_valid_filename(title)\n",
    "    fig.write_html(logdir / f\"{title}.html\")\n",
    "    fig.write_image(logdir / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in classes:\n",
    "    df_label = merged_df[merged_df[\"True class\"] == label]\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Iterate classes each target and add a violin plot for it\n",
    "    for target in classes:\n",
    "        for coverage_label in [\"chrY\", \"chrX\", \"chrY/chrX\"]:\n",
    "            sub_df = df_label[df_label[\"Predicted class\"] == target]\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Violin(\n",
    "                    y=sub_df[coverage_label],\n",
    "                    name=f\"{target}: {coverage_label} ({sub_df.shape[0]})\",\n",
    "                    box_visible=True,\n",
    "                    meanline_visible=True,\n",
    "                    points=\"all\",\n",
    "                    text=sub_df.index,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # title = f\"Coverage distribution for label {label}\"\n",
    "    title = f\"Coverage distribution for label {label}, max_pred > 0.9\"\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{title} ({df_label.shape[0]})\",\n",
    "        yaxis_title=\"Mean coverage\",\n",
    "        xaxis_title=\"Predicted class\",\n",
    "    )\n",
    "    fig.update_yaxes(range=[-0.001, 2])\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    title = get_valid_filename(title)\n",
    "    fig.write_html(logdir / f\"{title}.html\")\n",
    "    fig.write_image(logdir / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " unknown samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_predict_path = (\n",
    "    logdir\n",
    "    / \"sex3_complete_no_valid_oversample_test_prediction_100kb_all_none_dfreeze_v2.1_sex_mixed_unknown_augmented-all.csv\"\n",
    ")\n",
    "unknown_predict_df = pd.read_csv(unknown_predict_path, index_col=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"unknown\"\n",
    "unknown_predict_df = unknown_predict_df[unknown_predict_df[\"True class\"] == label]\n",
    "unknown_predict_df = unknown_predict_df.merge(\n",
    "    coverage_df, left_index=True, right_index=True, how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "classes = unknown_predict_df[\"Predicted class\"].unique()\n",
    "unknown_predict_df = unknown_predict_df[unknown_predict_df[\"Max pred\"] > 0.7]\n",
    "unknown_predict_df = unknown_predict_df[\n",
    "    ~unknown_predict_df[\"assay_epiclass\"].str.contains(\n",
    "        case=False, regex=True, pat=\"input|wgb\"\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "for target in classes:\n",
    "    sub_df = unknown_predict_df[unknown_predict_df[\"Predicted class\"] == target]\n",
    "    for coverage_label in [\"chrY\", \"chrX\", \"chrY/chrX\"]:\n",
    "        fig.add_trace(\n",
    "            go.Violin(\n",
    "                y=sub_df[coverage_label],\n",
    "                name=f\"{target}: {coverage_label} ({sub_df.shape[0]})\",\n",
    "                box_visible=True,\n",
    "                meanline_visible=True,\n",
    "                points=\"all\",\n",
    "                text=sub_df.index,\n",
    "            )\n",
    "        )\n",
    "\n",
    "# title = f\"Coverage distribution for label {label}\"\n",
    "title = f\"Coverage distribution for label {label}, max_pred > 0.9\"\n",
    "fig.update_layout(\n",
    "    title_text=f\"{title} ({unknown_predict_df.shape[0]})\",\n",
    "    yaxis_title=\"Mean coverage\",\n",
    "    xaxis_title=\"Predicted class\",\n",
    ")\n",
    "fig.update_yaxes(range=[-0.001, 2])\n",
    "fig.show()\n",
    "\n",
    "title = get_valid_filename(title)\n",
    "fig.write_html(logdir / f\"{title}.html\")\n",
    "fig.write_image(logdir / f\"{title}.png\", scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Miaw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence threshold impact on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(\n",
    "    df: pd.DataFrame,\n",
    "    threshold: float,\n",
    "    true_col: str,\n",
    "    pred_col: str,\n",
    "    pred_prob_cols: List[str],\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute the accuracy and subset size for a given probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the true labels, predicted labels, and predicted probabilities.\n",
    "    threshold (float): The probability threshold for filtering the DataFrame.\n",
    "    true_col (str): The column name containing the true labels.\n",
    "    pred_col (str): The column name containing the predicted labels.\n",
    "    pred_prob_cols (List[str]): List of column names containing the predicted probabilities.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[float, float, float]: A tuple containing the threshold, the calculated accuracy (%), and the subset size (%) respectively.\n",
    "    \"\"\"\n",
    "    # Filter rows where the max predicted probability is above the threshold\n",
    "    total_size = len(df)\n",
    "    subset_df = df[df[pred_prob_cols].max(axis=1) >= threshold]\n",
    "\n",
    "    if len(subset_df) == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    # Calculate the accuracy for this subset\n",
    "    correct_preds = np.sum(subset_df[true_col] == subset_df[pred_col])\n",
    "    accuracy = (correct_preds / len(subset_df)) * 100\n",
    "\n",
    "    # Calculate the size of this subset as a percentage of the total dataset\n",
    "    subset_size_percent = (len(subset_df) / total_size) * 100\n",
    "\n",
    "    return threshold, accuracy, subset_size_percent\n",
    "\n",
    "\n",
    "def evaluate_thresholds(\n",
    "    df: pd.DataFrame, thresholds: List[float]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy and subset size for different probability thresholds with improved automatic column detection.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataframe containing true labels and predicted probabilities.\n",
    "    thresholds (list): List of probability thresholds to evaluate.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A dataframe containing the accuracy and subset size for each threshold.\n",
    "    \"\"\"\n",
    "    # Automatic column detection\n",
    "    likely_true_class_cols = [col for col in df.columns if \"true\" in col.lower()]\n",
    "    likely_pred_class_cols = [col for col in df.columns if \"pred\" in col.lower()]\n",
    "\n",
    "    if not likely_true_class_cols or not likely_pred_class_cols:\n",
    "        raise ValueError(\n",
    "            \"Could not automatically detect 'True class' or 'Predicted class' columns.\"\n",
    "        )\n",
    "\n",
    "    true_col = likely_true_class_cols[0]\n",
    "    pred_col = likely_pred_class_cols[0]\n",
    "    if df[true_col].dtype != object or df[pred_col].dtype != object:\n",
    "        print(f\"{true_col} and {pred_col} are not string columns. Could cause issues.\")\n",
    "\n",
    "    classes = df[true_col].unique().tolist() + [\"all\"]\n",
    "    pred_prob_cols = classes[0:-1]\n",
    "    # Evaluate each threshold over each class\n",
    "    results_dfs = {}\n",
    "    for class_label in classes:\n",
    "        results = []\n",
    "        filtered_df = df if class_label == \"all\" else df[df[true_col] == class_label]\n",
    "\n",
    "        for thresh in thresholds:\n",
    "            result = compute_accuracy(\n",
    "                filtered_df, thresh, true_col, pred_col, pred_prob_cols\n",
    "            )\n",
    "            results.append(result)\n",
    "\n",
    "        # Convert to DataFrame for easier manipulation\n",
    "        short_class_label = class_label[0:10]\n",
    "        results_df = pd.DataFrame(\n",
    "            results,\n",
    "            columns=[\n",
    "                \"Threshold\",\n",
    "                f\"Accuracy_{short_class_label} (%)\",\n",
    "                f\"Subset_Size_{short_class_label} (%) ({filtered_df.shape[0]})\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        results_dfs[class_label] = results_df\n",
    "\n",
    "    return results_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thresholds_graph(threshold_dfs: Dict[str, pd.DataFrame], name: str):\n",
    "    \"\"\"\n",
    "    Return graph of the accuracy and subset size at different probability thresholds for all classes.\n",
    "\n",
    "    Parameters:\n",
    "    threshold_dfs (Dict[str, pd.DataFrame]): A dictionary containing dataframes for each class label and the general case.\n",
    "    name (str): Graph title suffix.\n",
    "\n",
    "    Returns:\n",
    "    plt.Figure: The figure object of the plotted graph.\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"k\"]\n",
    "    markers = [\"o\"]\n",
    "    marker_size = 3\n",
    "\n",
    "    for idx, (_, threshold_df) in enumerate(threshold_dfs.items()):\n",
    "        # Make the plot for \"Accuracy (%)\"\n",
    "        marker = markers[idx % len(markers)]\n",
    "        color = colors[idx % len(colors)]\n",
    "\n",
    "        acc_label = threshold_df.filter(like=\"Accuracy\").columns[0]\n",
    "        acc_subset = threshold_df.filter(like=\"Subset\").columns[0]\n",
    "\n",
    "        ax1.plot(\n",
    "            threshold_df[\"Threshold\"],\n",
    "            threshold_df[acc_label],\n",
    "            label=acc_label,\n",
    "            marker=marker,\n",
    "            color=color,\n",
    "            markersize=marker_size,\n",
    "        )\n",
    "\n",
    "        # Make the plot for \"Subset Size (%)\"\n",
    "        ax2.plot(\n",
    "            threshold_df[\"Threshold\"],\n",
    "            threshold_df[acc_subset],\n",
    "            label=acc_subset,\n",
    "            marker=marker,\n",
    "            color=color,\n",
    "            linestyle=\"--\",  # Use dashed line for better distinction\n",
    "            markersize=marker_size,\n",
    "        )\n",
    "\n",
    "    ax1.set_xlabel(\"Probability Threshold\")\n",
    "    ax1.set_ylabel(\"Accuracy (%)\")\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"b\")\n",
    "\n",
    "    ax2.set_ylabel(\"Subset Size (%)\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"r\")\n",
    "\n",
    "    # Set 11 ticks on the x-axis\n",
    "    ax1.set_xticks(np.linspace(0, 1, 11))\n",
    "\n",
    "    # Add grid, legend and title\n",
    "    ax1.grid(True)\n",
    "    _ = ax1.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 0.5), title=\"Accuracy\")\n",
    "    _ = ax2.legend(loc=\"lower right\", bbox_to_anchor=(1.3, 0.5), title=\"Subset Size\")\n",
    "    plt.title(\"Accuracy and Subset Size at Different Probability Thresholds\\n\" + name)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thresholds_graph_plotly(threshold_dfs: Dict[str, pd.DataFrame], name: str):\n",
    "    \"\"\"\n",
    "    Return graph of the accuracy and subset size at different probability thresholds for all classes.\n",
    "\n",
    "    Parameters:\n",
    "    threshold_dfs (Dict[str, pd.DataFrame]): A dictionary containing dataframes for each class label and the general case.\n",
    "    name (str): Graph title.\n",
    "\n",
    "    Returns:\n",
    "    go.Figure: Plotly figure object with the plotted graph.\n",
    "    \"\"\"\n",
    "\n",
    "    fig = go.Figure()\n",
    "    colors = px.colors.qualitative.Dark24\n",
    "    marker = \"circle\"\n",
    "\n",
    "    for idx, (_, threshold_df) in enumerate(threshold_dfs.items()):\n",
    "        color = colors[idx % len(colors)]\n",
    "\n",
    "        acc_label = threshold_df.filter(like=\"Accuracy\").columns[0]\n",
    "        acc_subset = threshold_df.filter(like=\"Subset\").columns[0]\n",
    "\n",
    "        # Plot accuracy\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=threshold_df[\"Threshold\"],\n",
    "                y=threshold_df[acc_label],\n",
    "                name=acc_label,\n",
    "                line=dict(color=color),\n",
    "                marker_symbol=marker,\n",
    "                mode=\"lines+markers\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Plot subset size on secondary Y-axis\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=threshold_df[\"Threshold\"],\n",
    "                y=threshold_df[acc_subset],\n",
    "                name=acc_subset,\n",
    "                line=dict(color=color, dash=\"dash\"),  # Dashed line for subset size\n",
    "                marker_symbol=marker,\n",
    "                yaxis=\"y2\",\n",
    "                mode=\"lines+markers\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Adjusting the layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Accuracy and Subset Size at Different Probability Thresholds<br>{name}\",\n",
    "        xaxis_title=\"Probability Threshold\",\n",
    "        xaxis=dict(\n",
    "            tickvals=np.linspace(0, 1, 11),\n",
    "            ticktext=[f\"{x:.1f}\" for x in np.linspace(0, 1, 11)],\n",
    "        ),\n",
    "        yaxis_title=\"Accuracy (%)\",\n",
    "        yaxis2=dict(title=\"Subset Size (%)\", overlaying=\"y\", side=\"right\"),\n",
    "        legend=dict(orientation=\"v\", x=1.05, y=1),\n",
    "        height=1000,\n",
    "        width=1600,\n",
    "    )\n",
    "    fig.update_xaxes(range=[-0.001, 1.001])\n",
    "    fig.update_traces(line={\"width\": 1})\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select files for analysis\n",
    "list_path = (\n",
    "    Path.home()\n",
    "    / \"projects\"\n",
    "    / \"epilap\"\n",
    "    / \"output\"\n",
    "    / \"dfreeze_results\"\n",
    "    / \"10fold_results.list\"\n",
    ")\n",
    "with open(list_path, \"r\", encoding=\"utf8\") as f:\n",
    "    files = [Path(line.strip()) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = list(np.arange(0, 1, 1 / 20)) + [0.99]\n",
    "for file in files:\n",
    "    print(file)\n",
    "    # compute\n",
    "    df = pd.read_csv(file, header=0, dtype={\"True class\": str, \"Predicted class\": str})\n",
    "    nb_samples = df.shape[0]\n",
    "    nb_classes = len(df.select_dtypes(include=[np.number]).columns.tolist())\n",
    "\n",
    "    threshold_df = evaluate_thresholds(df, thresholds)\n",
    "\n",
    "    # plot\n",
    "    name = f\"{file.parents[1].name} - {file.parents[0].name} - {nb_classes} classes\"\n",
    "    graph = create_thresholds_graph_plotly(threshold_df, f\"{name} - n={nb_samples}\")\n",
    "\n",
    "    # save\n",
    "    filename = f\"threshold_impact_graph_full_{get_valid_filename(name)}\".replace(\n",
    "        \"_-_\", \"-\"\n",
    "    )\n",
    "    print(filename)\n",
    "    graph.write_html(file.parent / (filename + \".html\"))\n",
    "    graph.write_image(file.parent / (filename + \".png\"), scale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds = list(np.arange(0, 1, 1 / 20)) + [0.99]\n",
    "# nb_samples = df.shape[0]\n",
    "# nb_classes = len(df.select_dtypes(include=[np.number]).columns.tolist())\n",
    "\n",
    "# results_dfs = evaluate_thresholds(df, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = create_thresholds_graph_plotly(results_dfs, f\"sex3 - n={nb_samples}\")\n",
    "# graph.show()\n",
    "# plt.savefig(file.parent / \"threshold_impact_graph.png\")\n",
    "# new_filename = (\n",
    "#     name.replace(\" \", \"_\").replace(\"\\n\", \"_\").replace(\"-\", \"\").replace(\"__\", \"_\")\n",
    "# )\n",
    "# graph.savefig(f\"threshold_impact_graph_{new_filename}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epiclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
