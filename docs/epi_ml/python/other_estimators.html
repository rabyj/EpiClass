<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>python.other_estimators API documentation</title>
<meta name="description" content="Main" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>python.other_estimators</code></h1>
</header>
<section id="section-intro">
<p>Main</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Main&#34;&#34;&#34;
from __future__ import annotations

import argparse
import glob
import json
import os
import sys
from pathlib import Path

import src.python.core.estimators as estimators
from src.python.argparseutils.DefaultHelpParser import DefaultHelpParser as ArgumentParser
from src.python.argparseutils.directorychecker import DirectoryChecker
from src.python.core import data, metadata
from src.python.core.data_source import EpiDataSource
from src.python.core.epiatlas_treatment import EpiAtlasFoldFactory
from src.python.core.lgbm import tune_lgbm
from src.python.utils.modify_metadata import filter_cell_types_by_pairs
from src.python.utils.time import time_now

if os.getenv(&#34;CONCURRENT_CV&#34;) is not None:
    CONCURRENT_CV = int(os.environ[&#34;CONCURRENT_CV&#34;])
else:
    CONCURRENT_CV = 1


def parse_arguments() -&gt; argparse.Namespace:
    &#34;&#34;&#34;Argument parser for command line.&#34;&#34;&#34;
    # fmt: off
    parser = ArgumentParser()
    group1 = parser.add_argument_group(&#34;General&#34;)
    group1.add_argument(
        &#34;category&#34;, type=str, help=&#34;The metatada category to analyse.&#34;
        )
    group1.add_argument(
        &#34;hdf5&#34;, type=Path, help=&#34;A file with hdf5 filenames. Use absolute path!&#34;
    )
    group1.add_argument(
        &#34;chromsize&#34;, type=Path, help=&#34;A file with chrom sizes.&#34;
        )
    group1.add_argument(
        &#34;metadata&#34;, type=Path, help=&#34;A metadata JSON file.&#34;
        )
    group1.add_argument(
        &#34;logdir&#34;, type=DirectoryChecker(), help=&#34;Directory for the output logs.&#34;
    )
    group1.add_argument(
        &#34;--models&#34;, nargs=&#34;+&#34;, type=str, help=&#34;Specify models to tune and/or predict.&#34;,
        choices=[&#34;all&#34;, &#34;LinearSVC&#34;, &#34;RF&#34;, &#34;LR&#34;, &#34;LGBM&#34;], default=[&#34;all&#34;]
        )

    mode = parser.add_argument_group(&#34;Mode&#34;)
    mode = mode.add_mutually_exclusive_group(required=True)
    mode.add_argument(
        &#34;--tune&#34;, action=&#34;store_true&#34;, help=&#34;Search best hyperparameters.&#34;
        )
    mode.add_argument(
        &#34;--predict&#34;, action=&#34;store_true&#34;, help=&#34;Fit and predict using hyperparameters.&#34;
        )
    mode.add_argument(
        &#34;--predict-new&#34;, action=&#34;store_true&#34;, help=&#34;Use saved models to predict labels of new samples.&#34;
        )
    mode.add_argument(
        &#34;--full-run&#34;, action=&#34;store_true&#34;, help=&#34;Tune then predict&#34;
        )

    tune = parser.add_argument_group(&#34;Tune&#34;)
    tune.add_argument(
        &#34;-n&#34;,
        type=int,
        default=30,
        help=&#34;Number of BayesSearchCV hyperparameters iterations.&#34;,
    )
    # fmt: on
    return parser.parse_args()


def main():
    &#34;&#34;&#34;Takes command line arguments.&#34;&#34;&#34;
    begin = time_now()
    print(f&#34;begin {begin}&#34;)

    # --- PARSE params and LOAD external files ---
    cli = parse_arguments()

    if cli.tune:
        mode_tune = True
        mode_predict = False
    elif cli.predict:
        mode_tune = False
        mode_predict = True
    elif cli.full_run:
        mode_tune = True
        mode_predict = True
    elif cli.predict_new:
        mode_tune = False
        mode_predict = False
    else:
        raise ValueError(&#34;Houston we have a problem.&#34;)

    if &#34;all&#34; in cli.models:
        models = estimators.model_mapping.keys()
    else:
        models = cli.models

    my_datasource = EpiDataSource(cli.hdf5, cli.chromsize, cli.metadata)

    # --- Prefilter metadata, must put in EpiAtlasDataset to actually use it ---
    my_metadata = metadata.Metadata(my_datasource.metadata_file)
    my_metadata.remove_category_subsets(
        label_category=&#34;track_type&#34;, labels=[&#34;Unique.raw&#34;]
    )

    label_list = metadata.env_filtering(my_metadata, cli.category)

    if os.getenv(&#34;MIN_CLASS_SIZE&#34;) is not None:
        min_class_size = int(os.environ[&#34;MIN_CLASS_SIZE&#34;])
    else:
        min_class_size = 10

    if cli.category == &#34;harm_sample_ontology_intermediate&#34;:
        my_metadata = filter_cell_types_by_pairs(my_metadata)

    # Tuning mode
    loading_begin = time_now()
    if mode_tune is True:  # type: ignore
        print(&#34;Entering tuning mode&#34;)
        ea_handler = EpiAtlasFoldFactory.from_datasource(
            my_datasource,
            cli.category,
            label_list,
            n_fold=estimators.NFOLD_TUNE,
            test_ratio=0.1,
            min_class_size=min_class_size,
            metadata=my_metadata,
        )
        loading_time = time_now() - loading_begin
        print(f&#34;Initial hdf5 loading time: {loading_time}&#34;)

        n_iter = cli.n

        for name in models:
            if name == &#34;LGBM&#34;:
                # optuna.logging.set_verbosity(optuna.logging.DEBUG)  # type: ignore
                tune_lgbm(ea_handler, cli.logdir)
            else:
                estimators.optimize_estimator(ea_handler, cli.logdir, n_iter, name)

    # Predict mode
    # TODO: Pre-check, with a separate init script for best_params.json existence
    if mode_predict is True:  # type: ignore
        print(&#34;Entering fit/prediction mode&#34;)

        pattern = f&#34;{cli.logdir / estimators.best_params_file_format.format(name=&#39;*&#39;)}&#34;
        hparam_files = glob.glob(pattern)
        if hparam_files:

            ea_handler = EpiAtlasFoldFactory.from_datasource(
                my_datasource,
                cli.category,
                label_list,
                n_fold=estimators.NFOLD_PREDICT,
                test_ratio=0,
                min_class_size=min_class_size,
                metadata=my_metadata,
            )
            loading_time = time_now() - loading_begin
            print(f&#34;Initial hdf5 loading time: {loading_time}&#34;)

            # Intersect available hparam files with chose models.
            available = set([estimators.get_model_name(path) for path in hparam_files])
            chosen = available &amp; set(models)
            hparam_files = [pattern.replace(&#34;*&#34;, name) for name in chosen]

            if not hparam_files:
                print(&#34;No parameters file found, finishing now.&#34;)
                sys.exit()

            for filepath in hparam_files:

                print(f&#34;Using {Path(filepath).resolve()}.&#34;)
                with open(filepath, &#34;r&#34;, encoding=&#34;utf-8&#34;) as file:
                    hparams = json.load(file)

                name = estimators.get_model_name(filepath)
                if name == &#34;LGBM&#34;:
                    hparams = {
                        k: v
                        for k, v in hparams.items()
                        if k in estimators.lgbm_allowed_params
                    }

                estimator = estimators.model_mapping[name]
                estimator.set_params(**hparams)

                estimators.run_predictions(ea_handler, estimator, name, cli.logdir)

        else:
            print(&#34;No parameters file found, finishing now.&#34;)
            sys.exit()

    # Giving predictions with chosen models, for all files in hdf5 list.
    if cli.predict_new:

        pattern = &#34;{log}/**{name}*.pickle&#34;
        to_load = []
        for model in models:
            save_name = estimators.save_mapping[model]
            to_load += glob.glob(pattern.format(log=cli.logdir, name=save_name))

        if to_load:

            my_data = data.DataSetFactory.from_epidata(
                my_datasource,
                my_metadata,
                cli.category,
                min_class_size=min_class_size,
                validation_ratio=0,
                test_ratio=1,
                onehot=False,
                oversample=False,
            )

            for model_path in to_load:
                my_model = estimators.EstimatorAnalyzer.restore_model_from_path(
                    full_path=model_path
                )
                my_analyzer = estimators.EstimatorAnalyzer(
                    classes=my_metadata.unique_classes(cli.category), estimator=my_model
                )

                predict_path = (
                    cli.logdir / f&#34;{Path(model_path).stem}_prediction_{cli.hdf5.stem}.csv&#34;
                )

                print(f&#34;Saving predictions to: {predict_path}&#34;)
                my_analyzer.predict_file(
                    ids=my_data.test.ids,
                    X=my_data.test.signals,
                    y=my_data.test.encoded_labels,
                    log=predict_path,
                )

        else:
            print(&#34;No saved model file found, finishing now.&#34;)
            sys.exit()


if __name__ == &#34;__main__&#34;:
    os.environ[&#34;TF_CPP_MIN_LOG_LEVEL&#34;] = &#34;3&#34;
    main()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="python.other_estimators.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Takes command line arguments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main():
    &#34;&#34;&#34;Takes command line arguments.&#34;&#34;&#34;
    begin = time_now()
    print(f&#34;begin {begin}&#34;)

    # --- PARSE params and LOAD external files ---
    cli = parse_arguments()

    if cli.tune:
        mode_tune = True
        mode_predict = False
    elif cli.predict:
        mode_tune = False
        mode_predict = True
    elif cli.full_run:
        mode_tune = True
        mode_predict = True
    elif cli.predict_new:
        mode_tune = False
        mode_predict = False
    else:
        raise ValueError(&#34;Houston we have a problem.&#34;)

    if &#34;all&#34; in cli.models:
        models = estimators.model_mapping.keys()
    else:
        models = cli.models

    my_datasource = EpiDataSource(cli.hdf5, cli.chromsize, cli.metadata)

    # --- Prefilter metadata, must put in EpiAtlasDataset to actually use it ---
    my_metadata = metadata.Metadata(my_datasource.metadata_file)
    my_metadata.remove_category_subsets(
        label_category=&#34;track_type&#34;, labels=[&#34;Unique.raw&#34;]
    )

    label_list = metadata.env_filtering(my_metadata, cli.category)

    if os.getenv(&#34;MIN_CLASS_SIZE&#34;) is not None:
        min_class_size = int(os.environ[&#34;MIN_CLASS_SIZE&#34;])
    else:
        min_class_size = 10

    if cli.category == &#34;harm_sample_ontology_intermediate&#34;:
        my_metadata = filter_cell_types_by_pairs(my_metadata)

    # Tuning mode
    loading_begin = time_now()
    if mode_tune is True:  # type: ignore
        print(&#34;Entering tuning mode&#34;)
        ea_handler = EpiAtlasFoldFactory.from_datasource(
            my_datasource,
            cli.category,
            label_list,
            n_fold=estimators.NFOLD_TUNE,
            test_ratio=0.1,
            min_class_size=min_class_size,
            metadata=my_metadata,
        )
        loading_time = time_now() - loading_begin
        print(f&#34;Initial hdf5 loading time: {loading_time}&#34;)

        n_iter = cli.n

        for name in models:
            if name == &#34;LGBM&#34;:
                # optuna.logging.set_verbosity(optuna.logging.DEBUG)  # type: ignore
                tune_lgbm(ea_handler, cli.logdir)
            else:
                estimators.optimize_estimator(ea_handler, cli.logdir, n_iter, name)

    # Predict mode
    # TODO: Pre-check, with a separate init script for best_params.json existence
    if mode_predict is True:  # type: ignore
        print(&#34;Entering fit/prediction mode&#34;)

        pattern = f&#34;{cli.logdir / estimators.best_params_file_format.format(name=&#39;*&#39;)}&#34;
        hparam_files = glob.glob(pattern)
        if hparam_files:

            ea_handler = EpiAtlasFoldFactory.from_datasource(
                my_datasource,
                cli.category,
                label_list,
                n_fold=estimators.NFOLD_PREDICT,
                test_ratio=0,
                min_class_size=min_class_size,
                metadata=my_metadata,
            )
            loading_time = time_now() - loading_begin
            print(f&#34;Initial hdf5 loading time: {loading_time}&#34;)

            # Intersect available hparam files with chose models.
            available = set([estimators.get_model_name(path) for path in hparam_files])
            chosen = available &amp; set(models)
            hparam_files = [pattern.replace(&#34;*&#34;, name) for name in chosen]

            if not hparam_files:
                print(&#34;No parameters file found, finishing now.&#34;)
                sys.exit()

            for filepath in hparam_files:

                print(f&#34;Using {Path(filepath).resolve()}.&#34;)
                with open(filepath, &#34;r&#34;, encoding=&#34;utf-8&#34;) as file:
                    hparams = json.load(file)

                name = estimators.get_model_name(filepath)
                if name == &#34;LGBM&#34;:
                    hparams = {
                        k: v
                        for k, v in hparams.items()
                        if k in estimators.lgbm_allowed_params
                    }

                estimator = estimators.model_mapping[name]
                estimator.set_params(**hparams)

                estimators.run_predictions(ea_handler, estimator, name, cli.logdir)

        else:
            print(&#34;No parameters file found, finishing now.&#34;)
            sys.exit()

    # Giving predictions with chosen models, for all files in hdf5 list.
    if cli.predict_new:

        pattern = &#34;{log}/**{name}*.pickle&#34;
        to_load = []
        for model in models:
            save_name = estimators.save_mapping[model]
            to_load += glob.glob(pattern.format(log=cli.logdir, name=save_name))

        if to_load:

            my_data = data.DataSetFactory.from_epidata(
                my_datasource,
                my_metadata,
                cli.category,
                min_class_size=min_class_size,
                validation_ratio=0,
                test_ratio=1,
                onehot=False,
                oversample=False,
            )

            for model_path in to_load:
                my_model = estimators.EstimatorAnalyzer.restore_model_from_path(
                    full_path=model_path
                )
                my_analyzer = estimators.EstimatorAnalyzer(
                    classes=my_metadata.unique_classes(cli.category), estimator=my_model
                )

                predict_path = (
                    cli.logdir / f&#34;{Path(model_path).stem}_prediction_{cli.hdf5.stem}.csv&#34;
                )

                print(f&#34;Saving predictions to: {predict_path}&#34;)
                my_analyzer.predict_file(
                    ids=my_data.test.ids,
                    X=my_data.test.signals,
                    y=my_data.test.encoded_labels,
                    log=predict_path,
                )

        else:
            print(&#34;No saved model file found, finishing now.&#34;)
            sys.exit()</code></pre>
</details>
</dd>
<dt id="python.other_estimators.parse_arguments"><code class="name flex">
<span>def <span class="ident">parse_arguments</span></span>(<span>) ‑> argparse.Namespace</span>
</code></dt>
<dd>
<div class="desc"><p>Argument parser for command line.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_arguments() -&gt; argparse.Namespace:
    &#34;&#34;&#34;Argument parser for command line.&#34;&#34;&#34;
    # fmt: off
    parser = ArgumentParser()
    group1 = parser.add_argument_group(&#34;General&#34;)
    group1.add_argument(
        &#34;category&#34;, type=str, help=&#34;The metatada category to analyse.&#34;
        )
    group1.add_argument(
        &#34;hdf5&#34;, type=Path, help=&#34;A file with hdf5 filenames. Use absolute path!&#34;
    )
    group1.add_argument(
        &#34;chromsize&#34;, type=Path, help=&#34;A file with chrom sizes.&#34;
        )
    group1.add_argument(
        &#34;metadata&#34;, type=Path, help=&#34;A metadata JSON file.&#34;
        )
    group1.add_argument(
        &#34;logdir&#34;, type=DirectoryChecker(), help=&#34;Directory for the output logs.&#34;
    )
    group1.add_argument(
        &#34;--models&#34;, nargs=&#34;+&#34;, type=str, help=&#34;Specify models to tune and/or predict.&#34;,
        choices=[&#34;all&#34;, &#34;LinearSVC&#34;, &#34;RF&#34;, &#34;LR&#34;, &#34;LGBM&#34;], default=[&#34;all&#34;]
        )

    mode = parser.add_argument_group(&#34;Mode&#34;)
    mode = mode.add_mutually_exclusive_group(required=True)
    mode.add_argument(
        &#34;--tune&#34;, action=&#34;store_true&#34;, help=&#34;Search best hyperparameters.&#34;
        )
    mode.add_argument(
        &#34;--predict&#34;, action=&#34;store_true&#34;, help=&#34;Fit and predict using hyperparameters.&#34;
        )
    mode.add_argument(
        &#34;--predict-new&#34;, action=&#34;store_true&#34;, help=&#34;Use saved models to predict labels of new samples.&#34;
        )
    mode.add_argument(
        &#34;--full-run&#34;, action=&#34;store_true&#34;, help=&#34;Tune then predict&#34;
        )

    tune = parser.add_argument_group(&#34;Tune&#34;)
    tune.add_argument(
        &#34;-n&#34;,
        type=int,
        default=30,
        help=&#34;Number of BayesSearchCV hyperparameters iterations.&#34;,
    )
    # fmt: on
    return parser.parse_args()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="python" href="index.html">python</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="python.other_estimators.main" href="#python.other_estimators.main">main</a></code></li>
<li><code><a title="python.other_estimators.parse_arguments" href="#python.other_estimators.parse_arguments">parse_arguments</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>