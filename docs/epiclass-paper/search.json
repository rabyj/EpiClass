[
  {
    "href": "about.html",
    "objectID": "about.html",
    "section": "",
    "text": "This website renders the figures from of the EpiClass paper originally created using Python and Plotly.\nMost figures are interactive, thanks to Plotly. The related code is present on Github, at rabyj/EpiClass.",
    "title": "About"
  },
  {
    "href": "figs/fig3.html",
    "objectID": "figs/fig3.html",
    "section": "",
    "text": "Results section 3 figures\n\n\n\n\n\n\nImportantTO BE RELEASED SOON\n\n\n\nTHIS IS A WORK IN PROGRESS.\nThis page will be updated with the figures and their corresponding code in the following weeks.\nMessage date: 2025-09-10\n\n\nFormatting of the figures may not be identical to the paper, but they contain the same data points.\nAll code is folded by default, click on \u201cCode\u201d to expand it.",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata"
  },
  {
    "href": "index.html",
    "objectID": "index.html",
    "section": "",
    "text": "This is a Quarto website presenting figures from the EpiClass paper.",
    "title": "EpiClass paper - Figures"
  },
  {
    "href": "index.html#figures",
    "objectID": "index.html#figures",
    "section": "Figures",
    "text": "Figures\nHere are the figure pages:\n\nSection 1: EpiClass accurately predicts EpiATLAS assay and biospecimen metadata\nSection 2: EpiClass reliably validates and augments other categories of EpiATLAS metadata\nSection 3: Application of EpiClass on public epigenomic data",
    "title": "EpiClass paper - Figures"
  },
  {
    "href": "index.html#quarto",
    "objectID": "index.html#quarto",
    "section": "Quarto",
    "text": "Quarto\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.",
    "title": "EpiClass paper - Figures"
  },
  {
    "href": "figs/fig1.html",
    "objectID": "figs/fig1.html",
    "section": "",
    "text": "Formatting of the figures may not be identical to the paper, but they contain the same data points.\nAll code is folded by default, click on \u201cCode\u201d to expand it.\n\n\n\n\n\n\nImportantIMPORTANT\n\n\n\nTHIS IS A WORK IN PROGRESS. Most figures are here, but some figure captions and code cell basic descriptions are still missing.\n\n\n\n\nThe harmonized EpiATLAS data and metadata used to develop EpiClass comprise 7,464 datasets (experiments) from 2,216 epigenomes (biological samples) generated by consortia such as ENCODE, Blueprint and CEEHRC. Each epigenome included data from up to nine different assays, which we hereafter refer to as our \u2018core assays\u2019: six ChIP-Seq histone modifications sharing a single control Input file, RNA-Seq and WGBS (Fig. 1A, Supplementary Table 1). The total training set included 20,922 signal files, comprising multiple normalization outputs per ChIP-Seq dataset (raw, fold change and p-value) and strand-specific files for RNA-Seq and WGBS assays. The rationale to include the three different track types and stranded tracks was to increase the robustness of the classifiers and increase the size of the training set.\nUsing five different machine learning approaches, we evaluated classification performance through stratified 10-fold cross-validation on 100 kb non-overlapping genome-wide bins (excluding the Y chromosome) (Methods). The Assay classifiers achieved ~99% accuracy, F1-score, and Area Under the Curve of the Receiver Operating Characteristic (AUROC), while the Biospecimen classifiers reached ~95% across the 16 most abundant classes comprising 84% of the epigenomes (the remaining being distributed in 46 smaller classes ignored) (Fig. 1B, Supplementary Fig. 1A-C).\nThe Multi-Layer Perceptron (MLP, or dense feedforward neural network) showed marginally superior performance on the more complex biospecimen classification task having important class-imbalance (certain classes being either over or under-represented) (Supplementary Table 2). As our primary goal was to establish a proof-of-concept for this approach, we selected the MLP and focused our subsequent efforts on assessing the model\u2019s performance across different genomic resolutions, rather than on exhaustive hyperparameter optimization. Further analysis with this approach revealed that larger genome-wide bins (1 Mb and 10 Mb) substantially decreased performance, while smaller bins (10 kb and 1 kb) offered minimal improvements despite greatly increasing computational demand (Fig. 1C, Supplementary Table 2). Additional data preprocessing steps, including blacklisted region removal and winsorization, showed no significant impact on performance (Supplementary Fig. 1D, Supplementary Table 3, Methods), leading us to adopt the 100 kb bins resolution without further filtering to simplify subsequent analyses.\nWe also evaluated alternative genomic features including protein coding genes (~68 kb on average), cis-regulatory elements showing high correlation between H3K27ac level and gene expression (avg. ~2.3 kb)26, and highly variable DNA methylation segments (200 bp)27. For both Assay and Biospecimen classifiers, none of these alternative feature sets improved the average accuracy by more than 1% compared to the 100 kb bins, with the notable exception of WGBS data. In this case accuracy improved substantially from 85% with 100 kb bins to 93% when using smaller bin sizes and more relevant features (Supplementary Fig. 2, Supplementary Table 2). These findings validated our choice of using the 100 kb approach as an effective compromise, providing comprehensive genome-wide coverage without introducing selection bias from predefined regions, while maintaining strong classification performance and simplifying data processing.\nInterestingly, the confusion matrix of the Assay classifier revealed that the very few prediction errors of some individual files occur mainly in specific scenarios: they arise between different protocol types of RNA-seq (mRNA vs total RNA) and WGBS (standard vs PBAT), they involve misclassifications with control Input datasets, or they occur between the activating histone marks (H3K27ac, H3K4me3, H3K4me1) that are typically localized around promoters/enhancers (Fig. 1D). These confusion patterns are all biologically understandable given the functional similarities. For the ChIP and RNA-seq assays, the vast majority of prediction scores exceeded 0.98 and are above 0.9 for biospecimen prediction, with much lower scores for Input and WGBS assays as expected at the chosen resolution (Supplementary Fig. 1E-F, Supplementary File 1). Importantly, the classifier performances are positively correlated with the prediction scores, allowing to use the score as a reliable confidence metric (Supplementary Fig. 1H-I). Increasing the prediction score threshold empirically increases performance, even though the scores should not be directly interpreted as true probabilities.\nEpiClass demonstrated practical utility during the development phase by identifying eleven datasets with potentially incorrect assay annotation. After reviewing our findings, data generators examined their original datasets and decided to correct one sample swap between two datasets, and excluded eight contaminated datasets from subsequent EpiATLAS versions (Fig. 1E, Supplementary Fig. 3, Supplementary Table 4). The Assay classifier also validated imputed ChIP datasets from EpiATLAS, achieving perfect predictions and very high prediction scores across all assays (Supplementary Fig. 1G, Supplementary File 2, Methods). Additionally, EpiClass contributed to the identification of 134 low-quality ChIP datasets that were also excluded by the EpiATLAS harmonization working group through notably low prediction scores (or high Input prediction score), indicating noisy signal (Supplementary Fig. 4, Supplementary Table 4).\n\n\n\nSetup imports.\n\n\nCode\nfrom __future__ import annotations\n\nimport copy\nimport logging\nimport re\nimport tempfile\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Dict, Optional, Tuple\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom IPython.core.display import Image\nfrom IPython.display import display\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import auc, confusion_matrix as sk_cm, roc_curve\nfrom sklearn.preprocessing import label_binarize\n\nfrom epiclass.core.confusion_matrix import ConfusionMatrixWriter\nfrom epiclass.utils.notebooks.paper.metrics_per_assay import MetricsPerAssay\nfrom epiclass.utils.notebooks.paper.paper_utilities import (\n    ASSAY,\n    ASSAY_MERGE_DICT,\n    ASSAY_ORDER,\n    CELL_TYPE,\n    SEX,\n    IHECColorMap,\n    MetadataHandler,\n    SplitResultsHandler,\n    extract_input_sizes_from_output_files,\n    merge_similar_assays,\n)\n\n\nSetup paths.\n\n\nCode\nbase_dir = Path.home() / \"Projects/epiclass/output/paper\"\npaper_dir = base_dir\nif not paper_dir.exists():\n    raise FileNotFoundError(f\"Directory {paper_dir} does not exist.\")\n\nbase_data_dir = base_dir / \"data\"\nbase_fig_dir = base_dir / \"figures\"\n\n\nSetup colors.\n\n\nCode\nIHECColorMap = IHECColorMap(base_fig_dir)\nassay_colors = IHECColorMap.assay_color_map\ncell_type_colors = IHECColorMap.cell_type_color_map\n\n\nSetup metadata and prediction files handlers.\n\n\nCode\nsplit_results_handler = SplitResultsHandler()\n\nmetadata_handler = MetadataHandler(paper_dir)\nmetadata_v2 = metadata_handler.load_metadata(\"v2\")\nmetadata_v2_df = metadata_v2.to_df()\n\n\nSetup data directories.\n\n\nCode\ngen_data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\nif not gen_data_dir.exists():\n    raise FileNotFoundError(f\"Directory {gen_data_dir} does not exist.\")\n\ndata_dir_100kb = gen_data_dir / \"hg38_100kb_all_none\"\nif not data_dir_100kb.exists():\n    raise FileNotFoundError(f\"Directory {data_dir_100kb} does not exist.\")\n\n\nSetup figures general settings.\n\n\nCode\nmain_title_settings = {\n    \"title\":dict(\n        automargin=True,\n        x=0.5,\n        xanchor=\"center\",\n        yanchor=\"top\",\n        y=0.98\n        ),\n    \"margin\":dict(t=50, l=10, r=10)\n}\n\n\n\n\n\nPerformance of EpiClass Assay and Biospecimen classifiers.\n\n\n\nFig. 1A: Overview of the EpiClass training process for various classifiers and their inference on external data. Each classifier is trained independently.\n\n\n\nPath setup.\n\n\nCode\nmixed_data_dir = gen_data_dir / \"mixed\"\nif not mixed_data_dir.exists():\n    raise FileNotFoundError(f\"Directory {mixed_data_dir} does not exist.\")\n\n\nFeature sets setup.\n\n\nCode\nfeature_sets_14 = [\n    \"hg38_10mb_all_none_1mb_coord\",\n    \"hg38_100kb_random_n316_none\",\n    \"hg38_1mb_all_none\",\n    \"hg38_100kb_random_n3044_none\",\n    \"hg38_100kb_all_none\",\n    \"hg38_gene_regions_100kb_coord_n19864\",\n    \"hg38_10kb_random_n30321_none\",\n    \"hg38_regulatory_regions_n30321\",\n    \"hg38_1kb_random_n30321_none\",\n    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n    \"hg38_10kb_all_none\",\n    \"hg38_regulatory_regions_n303114\",\n    \"hg38_1kb_random_n303114_none\",\n    \"hg38_cpg_topvar_200bp_10kb_coord_n300k\",\n]\nfig1_sets = [\n    \"hg38_10mb_all_none_1mb_coord\",\n    \"hg38_100kb_random_n316_none\",\n    \"hg38_1mb_all_none\",\n    \"hg38_100kb_random_n3044_none\",\n    \"hg38_100kb_all_none\",\n    \"hg38_10kb_random_n30321_none\",\n    \"hg38_1kb_random_n30321_none\",\n    \"hg38_10kb_all_none\",\n    \"hg38_1kb_random_n303114_none\",\n]\nflagship_selection_4cat = [\n    \"hg38_100kb_all_none\",\n    \"hg38_gene_regions_100kb_coord_n19864\",\n    \"hg38_regulatory_regions_n30321\",\n    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n]\ndifferent_nature_sets = [\n    \"hg38_regulatory_regions_n30321\",\n    \"hg38_regulatory_regions_n303114\",\n    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n    \"hg38_cpg_topvar_200bp_10kb_coord_n300k\",\n    \"hg38_cpg_topvar_2bp_10kb_coord_n30k\",\n    \"hg38_cpg_topvar_2bp_10kb_coord_n300k\",\n    \"hg38_gene_regions_100kb_coord_n19864\",\n    \"hg38_100kb_all_none\",\n    \"hg38_10kb_all_none\",\n    \"hg38_10kb_random_n30321_none\",\n    \"hg38_1kb_random_n30321_none\",\n    \"hg38_1kb_random_n303114_none\",\n]\n\nmetric_orders_map = {\n    \"flagship_selection_4cat\": flagship_selection_4cat,\n    \"fig1_sets\": fig1_sets,\n    \"feature_sets_14\": feature_sets_14,\n    \"different_nature_sets\": different_nature_sets,\n}\n\n\nCompute input sizes for each feature set.\n\n\nCode\ninput_sizes = extract_input_sizes_from_output_files(mixed_data_dir)  # type: ignore\ninput_sizes: Dict[str, int] = {k: v.pop() for k, v in input_sizes.items() if len(v) == 1}  # type: ignore\n\n\nSet selection.\n\n\nCode\nset_selection_name = \"feature_sets_14\"\n\nlogdir = (\n    base_fig_dir\n    / \"fig2_EpiAtlas_other\"\n    / \"fig2--reduced_feature_sets\"\n    / \"test\"\n    / set_selection_name\n)\nlogdir.mkdir(parents=True, exist_ok=True)\n\n\nCompute metrics.\n\n\nCode\nall_metrics = split_results_handler.obtain_all_feature_set_data(\n    parent_folder=mixed_data_dir,\n    merge_assays=True,\n    return_type=\"metrics\",\n    include_categories=[ASSAY, CELL_TYPE],\n    include_sets=metric_orders_map[set_selection_name],\n    exclude_names=[\"16ct\", \"27ct\", \"7c\", \"chip-seq-only\"],\n)\n\n# Order the metrics\nall_metrics = {\n    name: all_metrics[name]  # type: ignore\n    for name in metric_orders_map[set_selection_name]\n    if name in all_metrics\n}\n\n\nLabel correction.\n\n\nCode\n# correct a name\ntry:\n    all_metrics[\"hg38_100kb_all_none\"][ASSAY] = all_metrics[\"hg38_100kb_all_none\"][  # type: ignore\n        f\"{ASSAY}_11c\"\n    ]\n    del all_metrics[\"hg38_100kb_all_none\"][f\"{ASSAY}_11c\"]\nexcept KeyError:\n    pass\n\n\nResolution/feature set \u2013&gt; color mapping.\n\n\nCode\nresolution_colors = {\n    \"100kb\": px.colors.qualitative.Safe[0],\n    \"10kb\": px.colors.qualitative.Safe[1],\n    \"1kb\": px.colors.qualitative.Safe[2],\n    \"regulatory\": px.colors.qualitative.Safe[3],\n    \"gene\": px.colors.qualitative.Safe[4],\n    \"cpg\": px.colors.qualitative.Safe[5],\n    \"1mb\": px.colors.qualitative.Safe[6],\n    \"5mb\": px.colors.qualitative.Safe[7],\n    \"10mb\": px.colors.qualitative.Safe[8],\n}\n\n\nDefine function graph_feature_set_metrics.\n\n\nCode\ndef graph_feature_set_metrics(\n    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n    input_sizes: Dict[str, int],\n    logdir: Path | None = None,\n    sort_by_input_size: bool = False,\n    name: str | None = None,\n    y_range: Tuple[float, float] | None = None,\n    boxpoints: str = \"all\",\n    width: int = 1200,\n    height: int = 1200,\n) -&gt; None:\n    \"\"\"Graph the metrics for all feature sets.\n\n    Args:\n        all_metrics (Dict[str, Dict[str, Dict[str, Dict[str, float]]]): A dictionary containing all metrics for all feature sets.\n            Format: {feature_set: {task_name: {split_name: metric_dict}}}\n        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n        logdir (Path): The directory where the figure will be saved. If None, the figure will only be displayed.\n        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n        name (str|None): The name of the figure.\n        y_range (Tuple[float, float]|None): The y-axis range for the figure.\n        boxpoints (str): The type of boxpoints to display. Can be \"all\" or \"outliers\". Defaults to \"all\".\n    \"\"\"\n    if boxpoints not in [\"all\", \"outliers\"]:\n        raise ValueError(\"Invalid boxpoints value.\")\n\n    reference_hdf5_type = \"hg38_100kb_all_none\"\n    metadata_categories = list(all_metrics[reference_hdf5_type].keys())\n\n    non_standard_names = {ASSAY: f\"{ASSAY}_11c\", SEX: f\"{SEX}_w-mixed\"}\n    non_standard_assay_task_names = [\"hg38_100kb_all_none\"]\n    non_standard_sex_task_name = [\n        \"hg38_100kb_all_none\",\n        \"hg38_regulatory_regions_n30321\",\n        \"hg38_regulatory_regions_n303114\",\n    ]\n    used_resolutions = set()\n    for i in range(len(metadata_categories)):\n        category_idx = i\n        category_fig = make_subplots(\n            rows=1,\n            cols=2,\n            shared_yaxes=True,\n            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n            horizontal_spacing=0.01,\n        )\n\n        trace_names = []\n        order = list(all_metrics.keys())\n        if sort_by_input_size:\n            order = sorted(\n                all_metrics.keys(),\n                key=lambda x: input_sizes[x],\n            )\n        for feature_set_name in order:\n            # print(feature_set_name)\n            tasks_dicts = all_metrics[feature_set_name]\n            meta_categories = copy.deepcopy(metadata_categories)\n\n            if feature_set_name not in input_sizes:\n                print(f\"Skipping {feature_set_name}, no input size found.\")\n                continue\n\n            task_name = meta_categories[category_idx]\n            if \"split\" in task_name:\n                raise ValueError(\"Split in task name. Wrong metrics dict.\")\n\n            try:\n                task_dict = tasks_dicts[task_name]\n            except KeyError as err:\n                if SEX in str(err) and feature_set_name in non_standard_sex_task_name:\n                    task_dict = tasks_dicts[non_standard_names[SEX]]\n                elif (\n                    ASSAY in str(err)\n                    and feature_set_name in non_standard_assay_task_names\n                ):\n                    task_dict = tasks_dicts[non_standard_names[ASSAY]]\n                else:\n                    print(\"Skipping\", feature_set_name, task_name)\n                    continue\n\n            input_size = input_sizes[feature_set_name]\n\n            feature_set_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n            feature_set_name = re.sub(r\"\\_[\\dmkb]+\\_coord\", \"\", feature_set_name)\n\n            resolution = feature_set_name.split(\"_\")[0]\n            used_resolutions.add(resolution)\n\n            trace_name = f\"{input_size}|{feature_set_name}\"\n            trace_names.append(trace_name)\n\n            # Accuracy\n            metric = \"Accuracy\"\n            y_vals = [task_dict[split][metric] for split in task_dict]\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"\n                for split, metrics_dict in task_dict.items()\n            ]\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=boxpoints,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    fillcolor=resolution_colors[resolution],\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                    legendgroup=resolution,\n                    showlegend=False,\n                ),\n                row=1,\n                col=1,\n            )\n\n            metric = \"F1_macro\"\n            y_vals = [task_dict[split][metric] for split in task_dict]\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"\n                for split, metrics_dict in task_dict.items()\n            ]\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=boxpoints,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    fillcolor=resolution_colors[resolution],\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                    legendgroup=resolution,\n                    showlegend=False,\n                ),\n                row=1,\n                col=2,\n            )\n\n        title = f\"{metadata_categories[category_idx]} classification\"\n        title = title.replace(CELL_TYPE, \"biospecimen\")\n        if name is not None:\n            title += f\" - {name}\"\n        category_fig.update_layout(\n            width=width,\n            height=height,\n            title_text=title,\n            **main_title_settings\n        )\n\n        # dummy scatters for resolution colors\n        for resolution, color in resolution_colors.items():\n            if resolution not in used_resolutions:\n                continue\n            category_fig.add_trace(\n                go.Scatter(\n                    x=[None],\n                    y=[None],\n                    mode=\"markers\",\n                    name=resolution,\n                    marker=dict(color=color, size=5),\n                    showlegend=True,\n                    legendgroup=resolution,\n                )\n            )\n\n        category_fig.update_layout(legend=dict(itemsizing=\"constant\"))\n\n        # y-axis\n        if y_range:\n            category_fig.update_yaxes(range=y_range)\n        else:\n            if ASSAY in task_name:\n                category_fig.update_yaxes(range=[0.96, 1.001])\n            if CELL_TYPE in task_name:\n                category_fig.update_yaxes(range=[0.75, 1])\n\n        category_fig.update_layout(**main_title_settings)\n\n        # Save figure\n        if logdir:\n            base_name = f\"feature_set_metrics_{metadata_categories[category_idx]}\"\n            if name is not None:\n                base_name = base_name + f\"_{name}\"\n            category_fig.write_html(logdir / f\"{base_name}.html\")\n            category_fig.write_image(logdir / f\"{base_name}.svg\")\n            category_fig.write_image(logdir / f\"{base_name}.png\")\n\n        category_fig.show()\n\n\n\n\n\nGraph 100kb resolution MLP metrics.\n\nCode\nmetrics_fig1b = {name: all_metrics[name] for name in [\"hg38_100kb_all_none\"]}\n\nmetrics_fig1b_1 = {\n    \"hg38_100kb_all_none\": {ASSAY: metrics_fig1b[\"hg38_100kb_all_none\"][ASSAY]}\n}\ngraph_feature_set_metrics(\n    all_metrics=metrics_fig1b_1,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=425,\n    height=400,\n    y_range=(0.98, 1.001),\n)\n\nmetrics_fig1b_2 = {\n    \"hg38_100kb_all_none\": {CELL_TYPE: metrics_fig1b[\"hg38_100kb_all_none\"][CELL_TYPE]}\n}\ngraph_feature_set_metrics(\n    all_metrics=metrics_fig1b_2,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=425,\n    height=400,\n    y_range=(0.93, 1.001),\n)\n\n\n\n\n                                                \n\n\n                                                \n\n\n\nFig. 1B: Distribution of accuracy and F1-score for each of the ten training folds (dots) for the Assay and Biospecimen MLP classifiers.\n\n\n\nGraph.\n\nCode\nmetrics_fig1c = {name: all_metrics[name] for name in fig1_sets}\n\ngraph_feature_set_metrics(\n    all_metrics=metrics_fig1c,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=900,\n    height=600,\n)\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\nFig. 1C-alt: Distribution of accuracy per training fold for different bin resolutions for the Assay and Biospecimen classifiers.\n\n\nDefine function parse_bin_size to extract a numerical bin size in base pairs.\n\n\nCode\ndef parse_bin_size(feature_set_name: str) -&gt; Optional[float]:\n    \"\"\"\n    Parses the feature set name to extract a numerical bin size in base pairs.\n    Handles formats like '100kb', '5mb', 'regulatory', 'gene', 'cpg'.\n\n    Returns numerical size (float) or None if unparseable or non-numeric.\n    Assigns placeholder values for non-genomic-range types if needed,\n    but for a continuous axis, it's better to return None or filter later.\n    \"\"\"\n    name_parts = feature_set_name.replace(\"hg38_\", \"\").split(\"_\")\n    if not name_parts:\n        return None\n\n    resolution_str = name_parts[0].lower()\n\n    # Handle standard genomic ranges\n    match_kb = re.match(r\"(\\d+)kb\", resolution_str)\n    if match_kb:\n        return float(match_kb.group(1)) * 1_000\n    match_mb = re.match(r\"(\\d+)mb\", resolution_str)\n    if match_mb:\n        return float(match_mb.group(1)) * 1_000_000\n\n    # Handle non-range types - decide how to represent them.\n    # Option 1: Return None (they won't be plotted on the numeric axis)\n    # Option 2: Assign arbitrary numbers (might distort scale)\n    # Option 3: Could use different marker symbols later if needed\n    if resolution_str in [\"regulatory\", \"gene\", \"cpg\"]:\n        # For now, let's return None so they are filtered out from the numeric plot\n        # Or assign a placeholder if you want to handle them differently:\n        # if resolution_str == 'regulatory': return 1e1 # Example placeholder\n        # if resolution_str == 'gene': return 1e2 # Example placeholder\n        # if resolution_str == 'cpg': return 1e0 # Example placeholder\n        return None  # Returning None is cleaner for a pure numeric axis\n\n    # Fallback for unrecognised formats\n    try:\n        # Maybe it's just a number (e.g., representing window size)?\n        return float(resolution_str)\n    except ValueError:\n        return None\n\n\nDefine function graph_feature_set_scatter to graph performance metrics as a scatter plot instead of bar plot.\n\n\nCode\ndef graph_feature_set_scatter(\n    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n    input_sizes: Dict[str, int],\n    logdir: Optional[Path] = None,\n    metric_to_plot: str = \"Accuracy\",\n    name: Optional[str] = None,\n    metric_range: Optional[Tuple[float, float]] = None,\n    assay_task_key: str = ASSAY,\n    sex_task_key: str = SEX,\n    cell_type_task_key: str = CELL_TYPE,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Graphs performance metrics as a scatter plot with modifications.\n\n    X-axis: Number of Features (log scale).\n    Y-axis: Average performance metric (e.g., Accuracy, F1_macro) across folds.\n            Vertical lines indicate the min/max range across folds.\n    Color: Bin Size (bp, log scale).\n\n    Args:\n        all_metrics: Nested dict {feature_set: {task_name: {split_name: metric_dict}}}.\n        input_sizes: Dict {feature_set: num_features}.\n        logdir: Directory to save figures. If None, display only.\n        metric_to_plot: The metric key to use for the Y-axis ('Accuracy', 'F1_macro').\n        name: Optional suffix for figure titles and filenames.\n        metric_range: Optional tuple (min, max) to set the Y-axis range.\n        assay_task_key: Key used for the assay prediction task.\n        sex_task_key: Key used for the sex prediction task.\n        cell_type_task_key: Key used for the cell type prediction task.\n    \"\"\"\n    if metric_to_plot not in [\"Accuracy\", \"F1_macro\"]:\n        raise ValueError(\"metric_to_plot must be 'Accuracy' or 'F1_macro'\")\n\n    # --- Standard Name Handling (simplified from original) ---\n    non_standard_names = {ASSAY: f\"{ASSAY}_11c\", SEX: f\"{SEX}_w-mixed\"}\n    # These lists are no longer strictly needed by the simplified lookup, but kept for context\n    # non_standard_assay_task_names = [\"hg38_100kb_all_none\"]\n    # non_standard_sex_task_name = [\n    #     \"hg38_100kb_all_none\",\n    #     \"hg38_regulatory_regions_n30321\",\n    #     \"hg38_regulatory_regions_n303114\",\n    # ]\n\n    # --- Find reference and task names ----\n    reference_hdf5_type = next(iter(all_metrics), None)\n    if reference_hdf5_type is None or not all_metrics.get(reference_hdf5_type):\n        print(\n            \"Warning: Could not determine tasks from all_metrics. Trying default tasks.\"\n        )\n        cleaned_metadata_categories = {assay_task_key, sex_task_key, cell_type_task_key}\n    else:\n        metadata_categories = list(all_metrics[reference_hdf5_type].keys())\n        cleaned_metadata_categories = set()\n        for cat in metadata_categories:\n            original_name = cat\n            for standard, non_standard in non_standard_names.items():\n                if cat == non_standard:\n                    original_name = standard\n                    break\n            cleaned_metadata_categories.add(original_name)\n\n    # --- Define Bin size categories and Colors ---\n    bin_category_names = [\"1Kb\", \"10Kb\", \"100Kb\", \"1Mb\", \"10Mb\"]\n    bin_category_values = [1000, 10000, 100 * 1000, 1000 * 1000, 10000 * 1000]\n    discrete_colors = px.colors.sequential.Viridis_r\n    color_map = {\n        name: discrete_colors[i * 2] for i, name in enumerate(bin_category_names)\n    }\n\n    if verbose:\n        print(f\"Plotting for tasks: {list(cleaned_metadata_categories)}\")\n\n    for category_name in cleaned_metadata_categories:\n        plot_data_points = []\n\n        for feature_set_name_orig in all_metrics.keys():\n            try:\n                num_features = input_sizes[feature_set_name_orig]\n            except KeyError as e:\n                raise ValueError(\n                    f\"Feature set '{feature_set_name_orig}' not found in input_sizes\"\n                ) from e\n\n            # Parse Bin Size\n            bin_size = parse_bin_size(feature_set_name_orig)\n            if bin_size is None:\n                print(\n                    f\"Skipping {feature_set_name_orig}, could not parse numeric bin size.\"\n                )\n                continue\n\n            # 3. Get Metric Values (Average, Min, Max)\n            tasks_dicts = all_metrics[feature_set_name_orig]\n\n            # --- Task Name Lookup ---\n            # 1. Try the standard category name first\n            # 2. If standard name not found, use non-standard name\n            task_dict = None\n            task_name = category_name\n            if category_name in tasks_dicts:\n                task_dict = tasks_dicts[category_name]\n            else:\n                non_standard_task_name = non_standard_names.get(category_name)\n                if non_standard_task_name and non_standard_task_name in tasks_dicts:\n                    task_name = non_standard_task_name\n                    task_dict = tasks_dicts[non_standard_task_name]\n\n                if task_dict is None:\n                    raise ValueError(\n                        f\"Task '{category_name}' not found in feature set '{feature_set_name_orig}'\"\n                    )\n            # --- End Task Name Lookup ---\n\n            # Calculate average, min, max metric value across splits\n            try:\n                metric_values = []\n                for split, split_data in task_dict.items():\n                    if metric_to_plot in split_data:\n                        metric_values.append(split_data[metric_to_plot])\n                    else:\n                        print(\n                            f\"Warning: Metric '{metric_to_plot}' not found in split '{split}' for {feature_set_name_orig} / {task_name}\"\n                        )\n\n                if not metric_values:\n                    print(\n                        f\"Warning: No metric values found for {feature_set_name_orig} / {task_name} / {metric_to_plot}\"\n                    )\n                    continue\n\n                avg_metric = np.mean(metric_values)\n                min_metric = np.min(metric_values)\n                max_metric = np.max(metric_values)\n\n            except Exception as e:  # pylint: disable=broad-except\n                raise ValueError(\n                    f\"Error calculating metrics for {feature_set_name_orig} / {task_name}: {e}\"\n                ) from e\n\n            # Clean feature set name for hover text\n            clean_name = feature_set_name_orig.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n            clean_name = re.sub(r\"\\_[\\dmkb]+\\_coord\", \"\", clean_name)\n\n            # Store data for this point\n            plot_data_points.append(\n                {\n                    \"bin_size\": bin_size,\n                    \"num_features\": num_features,\n                    \"metric_value\": avg_metric,\n                    \"min_metric\": min_metric,  # For error bar low\n                    \"max_metric\": max_metric,  # For error bar high\n                    \"name\": clean_name,\n                    \"raw_name\": feature_set_name_orig,\n                }\n            )\n\n        if not plot_data_points:\n            raise ValueError(\n                f\"No suitable data points found to plot for task: {category_name}\"\n            )\n\n        # --- Determine Marker Symbols ---\n        marker_symbols = []\n        default_symbol = \"circle\"\n        random_symbol = \"cross\"\n        for p in plot_data_points:\n            if \"random\" in p[\"raw_name\"]:\n                marker_symbols.append(random_symbol)\n            else:\n                marker_symbols.append(default_symbol)\n\n        # --- Group Data by Category ---\n        points_by_category = {name: [] for name in bin_category_names}\n        for i, point_data in enumerate(plot_data_points):\n            bin_size = point_data[\"bin_size\"]\n            assigned_category = None\n            for cat_name, cat_value in zip(bin_category_names, bin_category_values):\n                if bin_size == cat_value:\n                    assigned_category = cat_name\n                    break\n            else:\n                raise ValueError(f\"Could not find category for bin size: {bin_size}\")\n\n            points_by_category[assigned_category].append(\n                {\n                    \"x\": point_data[\"num_features\"],  # X is Num Features\n                    \"y\": point_data[\"metric_value\"],\n                    \"error_up\": point_data[\"max_metric\"] - point_data[\"metric_value\"],\n                    \"error_down\": point_data[\"metric_value\"] - point_data[\"min_metric\"],\n                    \"text\": point_data[\"name\"],\n                    \"customdata\": [\n                        point_data[\"min_metric\"],\n                        point_data[\"max_metric\"],\n                        point_data[\"bin_size\"],\n                    ],  # Keep bin size for hover\n                    \"symbol\": marker_symbols[i],  # Assign symbol determined earlier\n                }\n            )\n\n        # --- Create Figure and Add Traces PER CATEGORY ---\n        fig = go.Figure()\n        traces = []\n\n        for cat_name in bin_category_names:  # Iterate in defined order for legend\n            points_in_cat = points_by_category[cat_name]\n            if not points_in_cat:\n                continue\n\n            category_color = color_map[cat_name]\n\n            # Extract data for all points in this category\n            x_vals = [p[\"x\"] for p in points_in_cat]\n            y_vals = [p[\"y\"] for p in points_in_cat]\n            error_up_vals = [p[\"error_up\"] for p in points_in_cat]\n            error_down_vals = [p[\"error_down\"] for p in points_in_cat]\n            text_vals = [p[\"text\"] for p in points_in_cat]\n            customdata_vals = [p[\"customdata\"] for p in points_in_cat]\n            symbols_vals = [p[\"symbol\"] for p in points_in_cat]\n\n            trace = go.Scatter(\n                x=x_vals,\n                y=y_vals,\n                mode=\"markers\",\n                name=cat_name,\n                showlegend=False,\n                legendgroup=cat_name,  # Group legend entries\n                marker=dict(\n                    color=category_color,\n                    size=15,\n                    symbol=symbols_vals,\n                    line=dict(width=1, color=\"DarkSlateGrey\"),\n                ),\n                error_y=dict(\n                    type=\"data\",\n                    symmetric=False,\n                    array=error_up_vals,\n                    arrayminus=error_down_vals,\n                    visible=True,\n                    thickness=1.5,\n                    width=15,\n                    color=category_color,\n                ),\n                text=text_vals,\n                customdata=customdata_vals,\n                hovertemplate=(\n                    f\"&lt;b&gt;%{{text}}&lt;/b&gt;&lt;br&gt;&lt;br&gt;\"\n                    f\"Num Features: %{{x:,.0f}}&lt;br&gt;\"\n                    f\"{metric_to_plot}: %{{y:.4f}}&lt;br&gt;\"\n                    f\"Bin Size: %{{customdata:,.0f}} bp&lt;br&gt;\"\n                    f\"{metric_to_plot} Range (10-fold): %{{customdata:.4f}} - %{{customdata:.4f}}\"\n                    \"&lt;extra&gt;&lt;/extra&gt;\"\n                ),\n            )\n            traces.append(trace)\n\n        fig.add_traces(traces)\n\n        # --- Add Legend ---\n        # Add a hidden scatter trace with square markers for legend\n        for cat_name in bin_category_names:\n            category_color = color_map[cat_name]\n            legend_trace = go.Scatter(\n                x=[None],\n                y=[None],\n                mode=\"markers\",\n                name=cat_name,\n                marker=dict(\n                    color=category_color,\n                    size=15,\n                    symbol=\"square\",\n                    line=dict(width=1, color=\"DarkSlateGrey\"),\n                ),\n                legendgroup=cat_name,\n                showlegend=True,\n            )\n            fig.add_trace(legend_trace)\n\n        # --- Update layout ---\n        title_name = category_name.replace(CELL_TYPE, \"biospecimen\")\n\n        plot_title = f\"{metric_to_plot} vs Number of Features - {title_name}\"\n        if name:\n            plot_title += f\" - {name}\"\n        xaxis_title = \"Number of Features (log scale)\"\n        xaxis_type = \"log\"\n\n        yaxis_title = metric_to_plot.replace(\"_\", \" \").title()\n        yaxis_type = \"linear\"\n\n        fig.update_layout(\n            xaxis_title=xaxis_title,\n            yaxis_title=yaxis_title,\n            xaxis_type=xaxis_type,\n            yaxis_type=yaxis_type,\n            yaxis_range=metric_range,\n            width=500,\n            height=500,\n            hovermode=\"closest\",\n            legend_title_text=\"Bin Size\",\n            title_text=plot_title,\n            **main_title_settings\n        )\n\n        if category_name == CELL_TYPE:\n            fig.update_yaxes(range=[0.75, 1.005])\n        elif category_name == ASSAY:\n            fig.update_yaxes(range=[0.96, 1.001])\n\n        # --- Save or show figure ---\n        if logdir:\n            logdir.mkdir(parents=True, exist_ok=True)\n            # Include \"modified\" or similar in filename to distinguish\n            base_name = f\"feature_scatter_MODIFIED_v2_{category_name}_{metric_to_plot}\"\n            if name:\n                base_name += f\"_{name}\"\n            html_path = logdir / f\"{base_name}.html\"\n            svg_path = logdir / f\"{base_name}.svg\"\n            png_path = logdir / f\"{base_name}.png\"\n\n            print(f\"Saving modified plot for {category_name} to {html_path}\")\n            fig.write_html(html_path)\n            fig.write_image(svg_path)\n            fig.write_image(png_path)\n\n        fig.show()\n\n\nGraph\n\nCode\nfor metric in [\"Accuracy\", \"F1_macro\"]:\n    graph_feature_set_scatter(\n        all_metrics=metrics_fig1c,  # type: ignore\n        input_sizes=input_sizes,\n        metric_to_plot=metric,\n        verbose=False,\n    )\n\n\n\n\n                                                \n\n\n                                                \n\n\n\n\n                                                \n\n\n                                                \n\n\n\nFig. 1C: Distribution of accuracy per training fold for different bin resolutions for the Assay and Biospecimen classifiers. The circles represent the means and the whiskers the min and max values of the ten training folds.\n\n\n\nDefine create_confusion_matrix to create and show a confusion matrix.\n\n\nCode\ndef create_confusion_matrix(\n    df: pd.DataFrame,\n    name: str = \"confusion_matrix\",\n    logdir: Path | None = None,\n    min_pred_score: float = 0,\n    majority: bool = False,\n    verbose:bool=False\n) -&gt; None:\n    \"\"\"Create a confusion matrix for the given DataFrame and save it to the logdir.\n\n    Args:\n        df (pd.DataFrame): The DataFrame containing the results.\n        logdir (Path): The directory path for saving the figures.\n        name (str): The name for the saved figures.\n        min_pred_score (float): The minimum prediction score to consider.\n        majority (bool): Whether to use majority vote (uuid-wise) for the predicted class.\n    \"\"\"\n    # Compute confusion matrix\n    classes = sorted(df[\"True class\"].unique())\n    if \"Max pred\" not in df.columns:\n        df[\"Max pred\"] = df[classes].max(axis=1)  # type: ignore\n    filtered_df = df[df[\"Max pred\"] &gt; min_pred_score]\n\n    if majority:\n        # Majority vote for predicted class\n        groupby_uuid = filtered_df.groupby([\"uuid\", \"True class\", \"Predicted class\"])[\n            \"Max pred\"\n        ].aggregate([\"size\", \"mean\"])\n\n        if groupby_uuid[\"size\"].max() &gt; 3:\n            raise ValueError(\"More than three predictions for the same uuid.\")\n\n        groupby_uuid = groupby_uuid.reset_index().sort_values(\n            [\"uuid\", \"True class\", \"size\"], ascending=[True, True, False]\n        )\n        groupby_uuid = groupby_uuid.drop_duplicates(\n            subset=[\"uuid\", \"True class\"], keep=\"first\"\n        )\n        filtered_df = groupby_uuid\n\n    confusion_mat = sk_cm(\n        filtered_df[\"True class\"], filtered_df[\"Predicted class\"], labels=classes\n    )\n\n    mat_writer = ConfusionMatrixWriter(labels=classes, confusion_matrix=confusion_mat)\n\n    if logdir is None:\n        logdir = Path(tempfile.gettempdir())\n\n    files = mat_writer.to_all_formats(logdir, name=f\"{name}_n{len(filtered_df)}\")\n\n    if verbose:\n        print(f\"Saved confusion matrix to {logdir}:\")\n        for file in files:\n            print(Path(file).name)\n\n    for file in files:\n        if \"png\" in file.name:\n            scale = 0.6\n            display(Image(filename=file, width=1250*scale, height=1000*scale))\n\n\nPrepare prediction data for confusion matrix.\n\n\nCode\nassay_split_dfs = split_results_handler.gather_split_results_across_methods(\n    results_dir=data_dir_100kb, label_category=ASSAY, only_NN=True\n)\nconcat_assay_df = split_results_handler.concatenate_split_results(assay_split_dfs)[\"NN\"]\n\ndf_with_meta = metadata_handler.join_metadata(concat_assay_df, metadata_v2)  # type: ignore\nif \"Predicted class\" not in df_with_meta.columns:\n    raise ValueError(\"`Predicted class` not in DataFrame\")\n\nclassifier_name = \"MLP\"\nmin_pred_score = 0\nmajority = False\n\nname = f\"{classifier_name}_pred&gt;{min_pred_score}\"\n\nlogdir = base_fig_dir / \"fig1_EpiAtlas_assay\" / \"fig1_supp_D-assay_c11_confusion_matrices\"\nif majority:\n    logdir = logdir / \"per_uuid\"\nelse:\n    logdir = logdir / \"per_file\"\nlogdir.mkdir(parents=True, exist_ok=True)\n\n\nGraph.\n\n\nCode\ncreate_confusion_matrix(\n    df=df_with_meta,\n    min_pred_score=min_pred_score,\n    majority=majority,\n)\n\n\n\n\n\n\n\n\n\nFig. 1D: Confusion matrix aggregating the cross-validation folds (therefore showing all files) without applying a prediction score threshold. RNA-seq and WGBS data were both separated according to two protocols during initial training (but combined thereafter to nine assays).\n\n\n\n\nFig. 1E: Genome browser representation showing in black the datasets swap between H3K4me3 and H3K27ac for IHECRE00001897 in the metadata freeze v1.0, along with typical correct datasets over a representative region.\n\n\n\n\nMore detailled performance of EpiClass Assay and Biospecimen classifiers.\n\n\nFig. 1A,B data points are included in these two graphs (MLP data points).\n\n\n\nDefine graphing function plot_multiple_models_split_metrics.\n\n\nCode\ndef plot_multiple_models_split_metrics(\n    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n    label_category: str,\n    logdir: Path | None = None,\n    filename: str = \"fig1_all_classifiers_metrics\",\n) -&gt; None:\n    \"\"\"Render to box plots the metrics per classifier/models and split, each in its own subplot.\n\n    Args:\n        split_metrics: A dictionary containing metric scores for each classifier and split.\n        label_category: The label category for the classification task.\n        name: The name of the figure.\n        logdir: The directory to save the figure to. If None, the figure is only displayed.\n\n    Returns:\n        None: Displays the figure and saves it to the logdir if provided.\n    \"\"\"\n    metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n    classifier_names = list(next(iter(split_metrics.values())).keys())\n    classifier_names = [\"NN\", \"LR\", \"LGBM\", \"LinearSVC\", \"RF\"]\n\n    # Create subplots, one row for each metric\n    fig = make_subplots(\n        rows=1,\n        cols=len(metrics),\n        subplot_titles=metrics,\n        horizontal_spacing=0.075,\n    )\n\n    for i, metric in enumerate(metrics):\n        for classifier in classifier_names:\n            values = [split_metrics[split][classifier][metric] for split in split_metrics]\n            if classifier == \"NN\":\n                classifier = \"MLP\"\n            fig.add_trace(\n                go.Box(\n                    y=values,\n                    name=classifier,\n                    line=dict(color=\"black\", width=1.5),\n                    marker=dict(size=3, color=\"black\"),\n                    boxmean=True,\n                    boxpoints=\"all\",  # or \"outliers\" to show only outliers\n                    pointpos=-1.4,\n                    showlegend=False,\n                    width=0.5,\n                    hovertemplate=\"%{text}\",\n                    text=[\n                        f\"{split}: {value:.4f}\"\n                        for split, value in zip(split_metrics, values)\n                    ],\n                ),\n                row=1,\n                col=i + 1,\n            )\n\n    fig.update_layout(\n        title_text=f\"{label_category} classification\",\n        boxmode=\"group\",\n        **main_title_settings,\n    )\n\n    # Adjust y-axis\n    if label_category == ASSAY:\n        range_acc = [0.95, 1.001]\n        range_AUC = [0.992, 1.0001]\n    elif label_category == CELL_TYPE:\n        range_acc = [0.81, 1]\n        range_AUC = [0.96, 1]\n    else:\n        range_acc = [0.6, 1.001]\n        range_AUC = [0.9, 1.0001]\n\n    fig.update_layout(\n        yaxis=dict(range=range_acc),\n        yaxis2=dict(range=range_acc),\n        yaxis3=dict(range=range_AUC),\n        yaxis4=dict(range=range_AUC),\n        height=450,\n    )\n\n    fig.update_layout(margin=dict(l=20, r=20))\n\n    # Save figure\n    if logdir:\n        fig.write_image(logdir / f\"{filename}.svg\")\n        fig.write_image(logdir / f\"{filename}.png\")\n        fig.write_html(logdir / f\"{filename}.html\")\n\n    fig.show()\n\n\nGraph.\n\nCode\nmerge_assays = True\n\nfor label_category in [ASSAY, CELL_TYPE]:\n    all_split_dfs = split_results_handler.gather_split_results_across_methods(\n        results_dir=data_dir_100kb,\n        label_category=label_category,\n        only_NN=False,\n    )\n\n    if merge_assays and label_category == ASSAY:\n        for split_name, split_dfs in all_split_dfs.items():\n            for classifier_type, df in split_dfs.items():\n                split_dfs[classifier_type] = merge_similar_assays(df)\n\n    split_metrics = split_results_handler.compute_split_metrics(all_split_dfs)\n\n    plot_multiple_models_split_metrics(\n        split_metrics,\n        label_category=label_category,\n    )\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\nSupplementary Figure 1A,B: Distribution of performance scores (accuracy, F1 as well as micro and macro AUROC) per training fold (dots) for each machine learning approach used for training on the Assay (A) and Biospecimen (B) metadata. Micro-averaging aggregates contributions from all classes (global true positive rate and false positive rate); macro-averaging averages the true positive rate from each class. Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5\u00d7 the interquartile range.\n\n\nGoing forward, all results are for MLP classifiers.\n\n\n\nDefine graphing function plot_roc_curves. Computes macro-average ROC curves manually.\n\n\nCode\ndef plot_roc_curves(\n    results_df: pd.DataFrame,\n    label_category: str,\n    logdir: Path | None = None,\n    name: str = \"roc_curve\",\n    title: str | None = None,\n    colors_dict: Dict | None = None,  # Optional specific colors\n    verbose: bool = False,\n) -&gt; None:\n    \"\"\"\n    Generates and plots ROC curves for multi-class classification results using Plotly.\n\n    Calculates and plots individual class ROC curves, micro-average, and macro-average ROC curves.\n\n    Args:\n        results_df (pd.DataFrame): DataFrame with true labels and prediction probabilities for each class.\n                                   Must contain the `label_category` column (e.g., 'True class')\n                                   and probability columns named after each class.\n        label_category (str): The column name containing the true labels (e.g., 'True class', ASSAY, CELL_TYPE).\n        logdir (Path | None): Directory to save the figure. If None, only displays the figure.\n        name (str): Base name for saved files (e.g., \"supp_fig1e\").\n        title (str | None): Title suffix for the plot. If None, a default title based on label_category is used.\n        colors_dict (Dict | None): Optional dictionary mapping class names to colors. If None or a class\n                                   is missing, default Plotly colors are used.\n    \"\"\"\n    df = results_df.copy()\n    true_label_col = \"True class\"  # Assuming 'True class' holds the ground truth labels\n\n    if true_label_col not in df.columns:\n        raise ValueError(f\"True label column '{true_label_col}' not found in DataFrame.\")\n\n    classes = sorted(df[true_label_col].unique())\n    if verbose:\n        print(f\"Using classes: {classes}\")\n\n    n_classes = len(classes)\n    if n_classes &lt; 2:\n        print(\n            f\"Warning: Only {n_classes} class found after processing. Cannot generate ROC curve.\"\n        )\n        return\n\n    # Check if probability columns exist for all determined classes\n    missing_cols = [c for c in classes if c not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing probability columns for classes: {missing_cols}\")\n\n    # Binarize the true labels against the final set of classes\n    try:\n        y_true = label_binarize(df[true_label_col], classes=classes)\n    except ValueError as e:\n        raise ValueError(\n            f\"Error binarizing labels for classes {classes}. Check if all labels in '{true_label_col}' are included in 'classes'.\"\n        ) from e\n\n    if n_classes == 2 and y_true.shape[1] == 1:\n        # Adjust for binary case where label_binarize might return one column\n        y_true = np.hstack((1 - y_true, y_true))  # type: ignore\n    elif y_true.shape[1] != n_classes:\n        raise ValueError(\n            f\"Binarized labels shape {y_true.shape} does not match number of classes {n_classes}\"\n        )\n\n    # Get the predicted probabilities for each class\n    # Ensure columns are in the same order as 'classes'\n    y_score = df[classes].values\n\n    # --- Compute ROC curve and ROC area for each class ---\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i, class_name in enumerate(classes):\n        try:\n            fpr[class_name], tpr[class_name], _ = roc_curve(\n                y_true=y_true[:, i], y_score=y_score[:, i]  # type: ignore\n            )\n            roc_auc[class_name] = auc(fpr[class_name], tpr[class_name])\n        except ValueError as e:\n            raise ValueError(\"Could not compute ROC for class {class_name}.\") from e\n\n    # --- Compute micro-average ROC curve and ROC area ---\n    try:\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_score.ravel())  # type: ignore\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    except ValueError as e:\n        raise ValueError(\"Could not compute micro-average ROC.\") from e\n\n    # --- Compute macro-average ROC curve and ROC area ---\n    try:\n        # Aggregate all false positive rates\n        all_fpr = np.unique(\n            np.concatenate(\n                [fpr[class_name] for class_name in classes if class_name in fpr]\n            )\n        )\n        # Interpolate all ROC curves at these points\n        mean_tpr = np.zeros_like(all_fpr)\n        valid_classes_count = 0\n        for class_name in classes:\n            if class_name in fpr and class_name in tpr:\n                mean_tpr += np.interp(all_fpr, fpr[class_name], tpr[class_name])\n                valid_classes_count += 1\n\n        # Average it and compute AUC\n        if valid_classes_count &gt; 0:\n            mean_tpr /= valid_classes_count\n            fpr[\"macro\"] = all_fpr\n            tpr[\"macro\"] = mean_tpr\n            roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n        else:\n            raise ValueError(\"No valid classes found for macro averaging.\")\n\n    except ValueError as e:\n        raise ValueError(\"Could not compute macro-average ROC.\") from e\n\n    # --- Plot all ROC curves ---\n    fig = go.Figure()\n\n    # Plot diagonal line for reference\n    fig.add_shape(\n        type=\"line\", line=dict(dash=\"dash\", color=\"grey\", width=1), x0=0, x1=1, y0=0, y1=1\n    )\n\n    # Define colors for plotting\n    color_cycle = px.colors.qualitative.Plotly  # Default cycle\n    plot_colors = {}\n    for i, cls_name in enumerate(classes):\n        if colors_dict and cls_name in colors_dict:\n            plot_colors[cls_name] = colors_dict[cls_name]\n        else:\n            plot_colors[cls_name] = color_cycle[i % len(color_cycle)]\n\n    # Plot Micro-average ROC curve first (often plotted thicker/dashed)\n    fig.add_trace(\n        go.Scatter(\n            x=fpr[\"micro\"],\n            y=tpr[\"micro\"],\n            mode=\"lines\",\n            name=f'Micro-average ROC (AUC = {roc_auc[\"micro\"]:.5f})',\n            line=dict(color=\"deeppink\", width=3, dash=\"dash\"),\n            hoverinfo=\"skip\",  # Less important for hover usually\n        )\n    )\n\n    # Plot Macro-average ROC curve\n    fig.add_trace(\n        go.Scatter(\n            x=fpr[\"macro\"],\n            y=tpr[\"macro\"],\n            mode=\"lines\",\n            name=f'Macro-average ROC (AUC = {roc_auc[\"macro\"]:.5f})',\n            line=dict(color=\"navy\", width=3, dash=\"dash\"),\n            hoverinfo=\"skip\",\n        )\n    )\n\n    # Plot individual class ROC curves\n    for class_name in classes:\n        if class_name not in fpr or class_name not in tpr or class_name not in roc_auc:\n            continue  # Skip if calculation failed\n        fig.add_trace(\n            go.Scatter(\n                x=fpr[class_name],\n                y=tpr[class_name],\n                mode=\"lines\",\n                name=f\"{class_name} (AUC = {roc_auc[class_name]:.5f})\",\n                line=dict(width=1.5, color=plot_colors.get(class_name)),\n                hovertemplate=f\"&lt;b&gt;{class_name}&lt;/b&gt;&lt;br&gt;FPR=%{{x:.5f}}&lt;br&gt;TPR=%{{y:.5f}}&lt;extra&gt;&lt;/extra&gt;\",  # Show class name and values on hover\n            )\n        )\n\n    # --- Update layout ---\n    base_title = f\"ROC Curves&lt;br&gt;{label_category}\"\n    plot_title = f\"{base_title} - {title}\" if title else base_title\n\n    title_settings=dict(\n        yanchor=\"top\",\n        yref=\"paper\",\n        y=0.97,\n        xanchor=\"center\",\n        xref=\"paper\",\n        x=0.5,\n    )\n\n    fig.update_layout(\n        title=title_settings,\n        title_text=plot_title,\n        xaxis_title=\"False Positive Rate (1 - Specificity)\",\n        yaxis_title=\"True Positive Rate (Sensitivity)\",\n        xaxis=dict(range=[0.0, 1.0], constrain=\"domain\"),  # Ensure axes range 0-1\n        yaxis=dict(\n            range=[0.0, 1.01], scaleanchor=\"x\", scaleratio=1, constrain=\"domain\"\n        ),  # Make it square-ish, slight top margin\n        width=800,\n        height=650,\n        hovermode=\"closest\",\n        legend=dict(\n            traceorder=\"reversed\",  # Show averages first in legend\n            title=\"Classes & Averages\",\n            font=dict(size=9),\n            itemsizing=\"constant\",\n            y=0.8,\n            yref=\"paper\",\n        ),\n        margin=dict(l=60, r=30, t=0, b=0),\n    )\n\n    # --- Save figure if logdir is provided ---\n    if logdir:\n        logdir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n        filename_base = f\"{name}_{label_category}_roc\"\n        filepath_base = logdir / filename_base\n\n        fig.write_html(f\"{filepath_base}.html\")\n        fig.write_image(f\"{filepath_base}.svg\", width=800, height=750)\n        fig.write_image(f\"{filepath_base}.png\", width=800, height=750, scale=2)\n\n        print(f\"Saved ROC curve plots for {label_category} to {logdir}\")\n        print(f\" -&gt; {filename_base}.html / .svg / .png\")\n\n    fig.show()\n\n\nPrepare assay data for plotting.\n\n\nCode\ndata_dir = (\n    mixed_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{ASSAY}_1l_3000n\"\n    / \"11c\"\n    / \"10fold-oversampling\"\n)\nif not data_dir.exists():\n    raise FileNotFoundError(f\"Directory {data_dir} does not exist.\")\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\n\ndf = merge_similar_assays(concat_df_w_meta.copy())\n\n\nGraph assay results.\n\n\nCode\nplot_roc_curves(\n    results_df=df.copy(),\n    label_category=ASSAY,\n    title=\"Aggregated 10fold\",  # Title suffix\n    colors_dict=assay_colors,\n    verbose=False,\n)\n\n\n                                                \n\n\nPrepare biospecimen data for plotting.\n\n\nCode\ndata_dir = (\n    mixed_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{CELL_TYPE}_1l_3000n\"\n    / \"10fold-oversampling\"\n)\nif not data_dir.exists():\n    raise FileNotFoundError(f\"Directory {data_dir} does not exist.\")\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\n\n\nGraph biospecimen results.\n\n\nCode\nplot_roc_curves(\n    results_df=concat_df_w_meta,\n    label_category=CELL_TYPE,\n    title=\"Aggregated 10fold\",  # Title suffix\n    colors_dict=cell_type_colors,\n    verbose=False,\n)\n\n\n                                                \n\n\nSupplementary Figure 1C: ROC curves from aggregated cross-validation results for the Assay and Biospecimen classifiers. Curves for each class are computed in a one-vs-rest scheme.\n\n\n\nDefine graphing function create_blklst_graphs.\n\n\nCode\ndef create_blklst_graphs(\n    feature_set_metrics_dict: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n    logdir: Path | None = None,\n) -&gt; List[go.Figure]:\n    \"\"\"Create boxplots for blacklisted related feature sets.\n\n    Args:\n        feature_set_metrics_dict (Dict[str, Dict[str, Dict[str, Dict[str, float]]]]): The dictionary containing all metrics for all blklst related feature sets.\n            format: {feature_set: {task_name: {split_name: metric_dict}}}\n        logdir (Path, Optional): The directory to save the figure to. If None, the figure is only displayed.\n    \"\"\"\n    figs = []\n\n    # Assume names exist in all feature sets\n    task_names = list(feature_set_metrics_dict.values())[0].keys()\n\n    traces_names_dict = {\n        \"hg38_100kb_all_none\": \"observed\",\n        \"hg38_100kb_all_none_0blklst\": \"0blklst\",\n        \"hg38_100kb_all_none_0blklst_winsorized\": \"0blklst_winsorized\",\n    }\n\n    for task_name in task_names:\n        category_fig = make_subplots(\n            rows=1,\n            cols=2,\n            shared_yaxes=False,\n            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n            horizontal_spacing=0.1,\n        )\n        for feature_set_name, tasks_dicts in feature_set_metrics_dict.items():\n            task_dict = tasks_dicts[task_name]\n            trace_name = traces_names_dict[feature_set_name]\n\n            # Accuracy\n            metric = \"Accuracy\"\n            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n                for split, metrics_dict in task_dict.items()\n            ]\n\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=\"all\",\n                    showlegend=False,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                ),\n                row=1,\n                col=1,\n            )\n\n            metric = \"F1_macro\"\n            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n                for split, metrics_dict in task_dict.items()\n            ]\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=\"all\",\n                    showlegend=False,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                ),\n                row=1,\n                col=2,\n            )\n\n        category_fig.update_xaxes(\n            categoryorder=\"array\",\n            categoryarray=list(traces_names_dict.values()),\n        )\n        category_fig.update_yaxes(range=[0.9, 1.001])\n\n        category_fig.update_layout(\n            title_text=task_name,\n            height=600,\n            width=500,\n            **main_title_settings\n        )\n\n        # Save figure\n        if logdir:\n            task_name = task_name.replace(\"_1l_3000n-10fold\", \"\")\n            base_name = f\"metrics_{task_name}\"\n\n            category_fig.write_html(logdir / f\"{base_name}.html\")\n            category_fig.write_image(logdir / f\"{base_name}.svg\")\n            category_fig.write_image(logdir / f\"{base_name}.png\")\n\n        figs.append(category_fig)\n\n    return figs\n\n\nPrepare paths.\n\n\nCode\ninclude_sets = [\n    \"hg38_100kb_all_none\",\n    \"hg38_100kb_all_none_0blklst\",\n    \"hg38_100kb_all_none_0blklst_winsorized\",\n]\n\nresults_folder_blklst = base_data_dir / \"training_results\" / \"2023-01-epiatlas-freeze\"\nif not results_folder_blklst.exists():\n    raise FileNotFoundError(f\"Folder '{results_folder_blklst}' not found\")\n\n\nCompute metrics.\n\n\nCode\n# Select 10-fold oversampling runs\n# expected result shape: {feature_set: {task_name: {split_name: metrics_dict}}}\nall_metrics_blklst: Dict[\n    str, Dict[str, Dict[str, Dict[str, float]]]\n] = split_results_handler.obtain_all_feature_set_data(\n    return_type=\"metrics\",\n    parent_folder=results_folder_blklst,\n    merge_assays=True,\n    include_categories=[ASSAY, CELL_TYPE],\n    include_sets=include_sets,\n    oversampled_only=False,\n    verbose=False,\n)  # type: ignore\n\n\nGraph.\n\nCode\nfigs = create_blklst_graphs(all_metrics_blklst)\n\nfigs[0].show()\nfigs[1].show()\n\n\n\n\n                                                \n\n\n                                                \n\n\n\nSupplementary Figure 1D: Distribution of accuracy and F1-score per training fold (dots) for the Assay and Biospecimen classifiers after removing signal from blacklisted regions and applying winsorization of 0.1%. Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5\u00d7 the interquartile range.\n\n\n\n\nE: Assay training 10-fold cross-validation\nF: Assay complete training (mixed tracks), predictions on imputed data (all pval)\nG: Biospecimen 10-fold cross-validation\n\n\nDefine graphing function plot_prediction_scores_distribution.\nSupplementary Figure 1E-G: Distribution of average prediction score per file (dots) for the majority-vote class (up to three track type files) (E, F) or individual file (G), from the MLP approach for the Assay (E, G) and Biospecimen classifiers (F), using aggregated cross-validation results from observed data (E, F) or results from the classifier trained on all observed data and applied to imputed data from EpiATLAS (G). Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5\u00d7 the interquartile range, with a violin representation on top.\n\n\nGather prediction scores.\n\n\nCode\ndata_dir = (\n    mixed_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{ASSAY}_1l_3000n\"\n    / \"11c\"\n    / \"10fold-oversampling\"\n)\nif not data_dir.exists():\n    raise FileNotFoundError(f\"Directory {data_dir} does not exist.\")\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\n\n\nGraph.\n\n\nCode\nplot_prediction_scores_distribution(\n    results_df=concat_df_w_meta,\n    group_by_column=ASSAY,\n    merge_assay_pairs=True,\n    min_y=0.7,\n    title=\"11 classes assay training&lt;br&gt;Prediction scores for 10-fold cross-validation\",\n)\n\n\n                                                \n\n\n\n\n\nGather prediction scores.\n\n\nCode\ndata_dir = data_dir_100kb / f\"{CELL_TYPE}_1l_3000n\" / \"10fold-oversampling\"\nif not data_dir.exists():\n    raise FileNotFoundError(f\"Directory {data_dir} does not exist.\")\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\nconcat_df_w_meta.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n\n\nGraph.\n\n\nCode\nplot_prediction_scores_distribution(\n    results_df=concat_df_w_meta,\n    group_by_column=ASSAY,\n    min_y=0,\n    title=\"Biospecimen training&lt;br&gt;Prediction scores for 10-fold cross-validation\",\n)\n\n\nSkipping assay merging: Wrong results dataframe, rna or wgbs columns missing.\n\n\n                                                \n\n\n\n\n\nGather imputed signal metadata.\n\n\nCode\nmetadata_path = (\n    paper_dir\n    / \"data\"\n    / \"metadata\"\n    / \"epiatlas\"\n    / \"imputed\"\n    / \"hg38_epiatlas_imputed_pval_chip_2024-02.json\"\n)\nmetadata_imputed: pd.DataFrame = metadata_handler.load_any_metadata(metadata_path, as_dataframe=True)  # type: ignore\n\n\nGather prediction scores.\n\n\nCode\ndata_dir = (\n    gen_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{ASSAY}_1l_3000n\"\n    / \"11c\"\n    / \"complete_no_valid_oversample\"\n    / \"predictions\"\n    / \"epiatlas_imputed\"\n    / \"ChIP\"\n)\nif not data_dir.exists():\n    raise FileNotFoundError(f\"Directory {data_dir} does not exist.\")\n\ndf_pred = pd.read_csv(\n    data_dir / \"complete_no_valid_oversample_prediction.csv\",\n    index_col=0,\n)\n\n\nPrepare dataframe for graphing.\n\n\nCode\nassay_classes = list(metadata_v2_df[ASSAY].unique())\ndf_pred = split_results_handler.add_max_pred(df_pred, expected_classes=assay_classes)\n\naugmented_df = pd.merge(df_pred, metadata_imputed, left_index=True, right_on=\"md5sum\")\naugmented_df[\"True class\"] = augmented_df[ASSAY]\nprint(\"Number of files per assay:\")\nprint(augmented_df[\"True class\"].value_counts(dropna=False).to_string())\n\n\nNumber of files per assay:\nh3k36me3    1703\nh3k27me3    1703\nh3k9me3     1700\nh3k4me1     1688\nh3k4me3     1688\nh3k27ac     1088\n\n\n\n\nGraph.\n\n\nCode\nplot_prediction_scores_distribution(\n    results_df=augmented_df,\n    group_by_column=ASSAY,\n    merge_assay_pairs=True,\n    min_y=0.79,\n    use_aggregate_vote=False,\n    title=\"Complete 11c assay classifier&lt;br&gt;inference on imputed data\",\n)\n\n\n                                                \n\n\n\n\n\n\nFor the code that produced the figures, see src/python/epiclass/utils/notebooks/paper/confidence_threshold.ipynb.\n\n\n\n\nSupplementary Figure 1H,I: Distribution of aggregated accuracy, F1-score and corresponding file subset size across varying prediction score thresholds, based on pooled predictions from all cross-validation folds for the Assay (H) and Biospecimen (I) classifiers.\n\n\n\n\n\n\n\nCode\nmetrics_supp2 = {name: all_metrics[name] for name in feature_sets_14}\n\ngraph_feature_set_metrics(\n    all_metrics=metrics_supp2,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=900,\n    height=600,\n)\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n\n\nCode\ndef prepare_metric_sets_per_assay(\n    all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]], verbose: bool = False\n) -&gt; Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]:\n    \"\"\"Prepare metric sets per assay.\n\n    Args:\n        all_results (Dict[str, Dict[str, Dict[str, pd.DataFrame]]]): A dictionary containing all results for all feature sets.\n\n    Returns:\n        Dict[str, Dict[str, Dict[str, Dict[str, float]]]]: A dictionary containing all metrics per assay for all feature sets.\n            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n    \"\"\"\n    if verbose:\n        print(\"Loading metadata.\")\n    metadata = metadata_handler.load_metadata(\"v2\")\n    metadata.convert_classes(ASSAY, ASSAY_MERGE_DICT)\n    md5_per_assay = metadata.md5_per_class(ASSAY)\n    md5_per_assay = {k: set(v) for k, v in md5_per_assay.items()}\n\n    if verbose:\n        print(\"Getting results per assay.\")\n    results_per_assay = {}\n    for assay_label in ASSAY_ORDER:\n        if verbose:\n            print(assay_label)\n        results_per_assay[assay_label] = {}\n        for feature_set, task_dict in all_results.items():\n            if verbose:\n                print(feature_set)\n            results_per_assay[assay_label][feature_set] = {}\n            for task_name, split_dict in task_dict.items():\n                if verbose:\n                    print(task_name)\n                results_per_assay[assay_label][feature_set][task_name] = {}\n\n                # Only keep the relevant assay\n                for split_name, split_df in split_dict.items():\n                    if verbose:\n                        print(split_name)\n                    assay_df = split_df[split_df.index.isin(md5_per_assay[assay_label])]\n                    results_per_assay[assay_label][feature_set][task_name][\n                        split_name\n                    ] = assay_df\n\n    if verbose:\n        print(\"Finished getting results per assay. Now computing metrics.\")\n    metrics_per_assay = {}\n    for assay_label in ASSAY_ORDER:\n        if verbose:\n            print(assay_label)\n        metrics_per_assay[assay_label] = {}\n        for feature_set, task_dict in results_per_assay[assay_label].items():\n            if verbose:\n                print(feature_set)\n            assay_metrics = split_results_handler.compute_split_metrics(\n                task_dict, concat_first_level=True\n            )\n            inverted_dict = split_results_handler.invert_metrics_dict(assay_metrics)\n            metrics_per_assay[assay_label][feature_set] = inverted_dict\n\n    return metrics_per_assay\n\n\n\n\nCode\ndef graph_feature_set_metrics_per_assay(\n    all_metrics_per_assay: Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]],\n    input_sizes: Dict[str, int],\n    logdir: Path | None = None,\n    sort_by_input_size: bool = False,\n    name: str | None = None,\n    y_range: Tuple[float, float] | None = None,\n    boxpoints: str = \"outliers\",\n) -&gt; None:\n    \"\"\"Graph the metrics for all feature sets, per assay, with separate plots for accuracy and F1-score.\n\n    Args:\n        all_metrics_per_assay (Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]): A dictionary containing all metrics per assay for all feature sets.\n            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n        logdir (Path): The directory where the figures will be saved. If None, the figures will only be displayed.\n        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n        name (str|None): The name of the figure.\n        y_range (Tuple[float, float]|None): The y-axis range for the plots.\n        boxpoints (str): The type of points to display in the box plots. Defaults to \"outliers\".\n    \"\"\"\n    valid_boxpoints = [\"all\", \"outliers\"]\n    if boxpoints not in valid_boxpoints:\n        raise ValueError(f\"Invalid boxpoints value. Choose from {valid_boxpoints}.\")\n\n    fig_assay_order = [\n        \"rna_seq\",\n        \"h3k27ac\",\n        \"h3k4me1\",\n        \"h3k4me3\",\n        \"h3k36me3\",\n        \"h3k27me3\",\n        \"h3k9me3\",\n        \"input\",\n        \"wgbs\",\n    ]\n\n    reference_assay = next(iter(all_metrics_per_assay))\n    reference_feature_set = next(iter(all_metrics_per_assay[reference_assay]))\n    metadata_categories = list(\n        all_metrics_per_assay[reference_assay][reference_feature_set].keys()\n    )\n\n    for _, category in enumerate(metadata_categories):\n        for metric, metric_name in [\n            (\"Accuracy\", \"Accuracy\"),\n            (\"F1_macro\", \"F1-score (macro)\"),\n        ]:\n            fig = go.Figure()\n\n            feature_sets = list(all_metrics_per_assay[reference_assay].keys())\n            unique_feature_sets = set(feature_sets)\n            for assay in fig_assay_order:\n                if set(all_metrics_per_assay[assay].keys()) != unique_feature_sets:\n                    raise ValueError(\"Different feature sets through assays.\")\n\n            feature_set_order = feature_sets\n            if sort_by_input_size:\n                feature_set_order = sorted(\n                    feature_set_order, key=lambda x: input_sizes[x]\n                )\n\n            # Adjust spacing so each assay group has dedicated space based on the number of feature sets\n            spacing_multiplier = (\n                1.1  # Increase this multiplier if needed to add more spacing\n            )\n            x_positions = {\n                assay: i * len(feature_set_order) * spacing_multiplier\n                for i, assay in enumerate(fig_assay_order)\n            }\n\n            for i, feature_set_name in enumerate(feature_set_order):\n                resolution = (\n                    feature_set_name.replace(\"_none\", \"\")\n                    .replace(\"hg38_\", \"\")\n                    .split(\"_\")[0]\n                )\n                color = resolution_colors[resolution]\n                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n\n                for assay in fig_assay_order:\n                    if feature_set_name not in all_metrics_per_assay[assay]:\n                        continue\n\n                    tasks_dicts = all_metrics_per_assay[assay][feature_set_name]\n\n                    if feature_set_name not in input_sizes:\n                        print(f\"Skipping {feature_set_name}, no input size found.\")\n                        continue\n\n                    task_name = category\n                    if \"split\" in task_name:\n                        raise ValueError(\"Split in task name. Wrong metrics dict.\")\n\n                    try:\n                        task_dict = tasks_dicts[task_name]\n                    except KeyError:\n                        print(\n                            f\"Skipping {feature_set_name}, {task_name} for assay {assay}\"\n                        )\n                        continue\n\n                    y_vals = [task_dict[split][metric] for split in task_dict]\n                    hovertext = [\n                        f\"{assay} - {display_name} - {split}: {metrics_dict[metric]:.4f}\"\n                        for split, metrics_dict in task_dict.items()\n                    ]\n\n                    x_position = x_positions[assay] + i\n                    fig.add_trace(\n                        go.Box(\n                            x=[x_position] * len(y_vals),\n                            y=y_vals,\n                            name=f\"{assay}|{display_name}\",\n                            boxmean=True,\n                            boxpoints=boxpoints,\n                            marker=dict(size=3, color=\"black\"),\n                            line=dict(width=1, color=\"black\"),\n                            fillcolor=color,\n                            hovertemplate=\"%{text}\",\n                            text=hovertext,\n                            showlegend=False,\n                            legendgroup=display_name,\n                        )\n                    )\n\n                    # separate box groups\n                    fig.add_vline(\n                        x=x_positions[assay] - 1, line_width=1, line_color=\"black\"\n                    )\n\n            # Add dummy traces for the legend\n            for feature_set_name in feature_set_order:\n                resolution = (\n                    feature_set_name.replace(\"_none\", \"\")\n                    .replace(\"hg38_\", \"\")\n                    .split(\"_\")[0]\n                )\n                color = resolution_colors[resolution]\n                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n                display_name = re.sub(r\"\\_[\\dmkb]+\\_coord\", \"\", display_name)\n\n                fig.add_trace(\n                    go.Scatter(\n                        name=display_name,\n                        x=[None],\n                        y=[None],\n                        mode=\"markers\",\n                        marker=dict(size=10, color=color),\n                        showlegend=True,\n                        legendgroup=display_name,\n                    )\n                )\n\n            title = f\"{category} - {metric_name} (per assay)\"\n            if name is not None:\n                title += f\" - {name}\"\n\n            fig.update_layout(\n                width=1250,\n                height=900,\n                title_text=title,\n                xaxis_title=\"Assay\",\n                yaxis_title=metric_name,\n                **main_title_settings\n            )\n\n            # Create x-axis labels\n            fig.update_xaxes(\n                tickmode=\"array\",\n                tickvals=[\n                    x_positions[assay] + len(feature_set_order) / 2\n                    for assay in fig_assay_order\n                ],\n                ticktext=list(x_positions.keys()),\n                title=\"Assay\",\n            )\n\n            fig.update_layout(\n                legend=dict(\n                    title=\"Feature Sets\", itemsizing=\"constant\", traceorder=\"normal\"\n                )\n            )\n            if y_range:\n                fig.update_yaxes(range=y_range)\n\n            if logdir:\n                base_name = f\"feature_set_metrics_{category}_{metric}_per_assay\"\n                if name is not None:\n                    base_name = base_name + f\"_{name}\"\n                fig.write_html(logdir / f\"{base_name}.html\")\n                fig.write_image(logdir / f\"{base_name}.svg\")\n                fig.write_image(logdir / f\"{base_name}.png\")\n\n            fig.show()\n\n\n\n\nCode\nset_selection_name = \"feature_sets_14\"\nall_results = split_results_handler.obtain_all_feature_set_data(\n    parent_folder=mixed_data_dir,\n    merge_assays=True,\n    return_type=\"split_results\",\n    include_categories=[CELL_TYPE],\n    include_sets=metric_orders_map[set_selection_name],\n    exclude_names=[\"16ct\", \"27ct\", \"7c\", \"chip-seq-only\"],\n)\n\n\n\n\nCode\nmetrics_per_assay = prepare_metric_sets_per_assay(all_results)  # type: ignore\n\n\n\n\nCode\n# Reorder feature sets\nfeature_set_order = metric_orders_map[set_selection_name]\nfor assay, feature_sets in list(metrics_per_assay.items()):\n    metrics_per_assay[assay] = {\n        feature_set_name: metrics_per_assay[assay][feature_set_name]\n        for feature_set_name in feature_set_order\n    }\n\n\n\nCode\ngraph_feature_set_metrics_per_assay(\n    all_metrics_per_assay=metrics_per_assay,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    sort_by_input_size=False,\n    y_range=(0.1, 1.01)\n)",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata"
  },
  {
    "href": "figs/fig1.html#article-text",
    "objectID": "figs/fig1.html#article-text",
    "section": "",
    "text": "The harmonized EpiATLAS data and metadata used to develop EpiClass comprise 7,464 datasets (experiments) from 2,216 epigenomes (biological samples) generated by consortia such as ENCODE, Blueprint and CEEHRC. Each epigenome included data from up to nine different assays, which we hereafter refer to as our \u2018core assays\u2019: six ChIP-Seq histone modifications sharing a single control Input file, RNA-Seq and WGBS (Fig. 1A, Supplementary Table 1). The total training set included 20,922 signal files, comprising multiple normalization outputs per ChIP-Seq dataset (raw, fold change and p-value) and strand-specific files for RNA-Seq and WGBS assays. The rationale to include the three different track types and stranded tracks was to increase the robustness of the classifiers and increase the size of the training set.\nUsing five different machine learning approaches, we evaluated classification performance through stratified 10-fold cross-validation on 100 kb non-overlapping genome-wide bins (excluding the Y chromosome) (Methods). The Assay classifiers achieved ~99% accuracy, F1-score, and Area Under the Curve of the Receiver Operating Characteristic (AUROC), while the Biospecimen classifiers reached ~95% across the 16 most abundant classes comprising 84% of the epigenomes (the remaining being distributed in 46 smaller classes ignored) (Fig. 1B, Supplementary Fig. 1A-C).\nThe Multi-Layer Perceptron (MLP, or dense feedforward neural network) showed marginally superior performance on the more complex biospecimen classification task having important class-imbalance (certain classes being either over or under-represented) (Supplementary Table 2). As our primary goal was to establish a proof-of-concept for this approach, we selected the MLP and focused our subsequent efforts on assessing the model\u2019s performance across different genomic resolutions, rather than on exhaustive hyperparameter optimization. Further analysis with this approach revealed that larger genome-wide bins (1 Mb and 10 Mb) substantially decreased performance, while smaller bins (10 kb and 1 kb) offered minimal improvements despite greatly increasing computational demand (Fig. 1C, Supplementary Table 2). Additional data preprocessing steps, including blacklisted region removal and winsorization, showed no significant impact on performance (Supplementary Fig. 1D, Supplementary Table 3, Methods), leading us to adopt the 100 kb bins resolution without further filtering to simplify subsequent analyses.\nWe also evaluated alternative genomic features including protein coding genes (~68 kb on average), cis-regulatory elements showing high correlation between H3K27ac level and gene expression (avg. ~2.3 kb)26, and highly variable DNA methylation segments (200 bp)27. For both Assay and Biospecimen classifiers, none of these alternative feature sets improved the average accuracy by more than 1% compared to the 100 kb bins, with the notable exception of WGBS data. In this case accuracy improved substantially from 85% with 100 kb bins to 93% when using smaller bin sizes and more relevant features (Supplementary Fig. 2, Supplementary Table 2). These findings validated our choice of using the 100 kb approach as an effective compromise, providing comprehensive genome-wide coverage without introducing selection bias from predefined regions, while maintaining strong classification performance and simplifying data processing.\nInterestingly, the confusion matrix of the Assay classifier revealed that the very few prediction errors of some individual files occur mainly in specific scenarios: they arise between different protocol types of RNA-seq (mRNA vs total RNA) and WGBS (standard vs PBAT), they involve misclassifications with control Input datasets, or they occur between the activating histone marks (H3K27ac, H3K4me3, H3K4me1) that are typically localized around promoters/enhancers (Fig. 1D). These confusion patterns are all biologically understandable given the functional similarities. For the ChIP and RNA-seq assays, the vast majority of prediction scores exceeded 0.98 and are above 0.9 for biospecimen prediction, with much lower scores for Input and WGBS assays as expected at the chosen resolution (Supplementary Fig. 1E-F, Supplementary File 1). Importantly, the classifier performances are positively correlated with the prediction scores, allowing to use the score as a reliable confidence metric (Supplementary Fig. 1H-I). Increasing the prediction score threshold empirically increases performance, even though the scores should not be directly interpreted as true probabilities.\nEpiClass demonstrated practical utility during the development phase by identifying eleven datasets with potentially incorrect assay annotation. After reviewing our findings, data generators examined their original datasets and decided to correct one sample swap between two datasets, and excluded eight contaminated datasets from subsequent EpiATLAS versions (Fig. 1E, Supplementary Fig. 3, Supplementary Table 4). The Assay classifier also validated imputed ChIP datasets from EpiATLAS, achieving perfect predictions and very high prediction scores across all assays (Supplementary Fig. 1G, Supplementary File 2, Methods). Additionally, EpiClass contributed to the identification of 134 low-quality ChIP datasets that were also excluded by the EpiATLAS harmonization working group through notably low prediction scores (or high Input prediction score), indicating noisy signal (Supplementary Fig. 4, Supplementary Table 4).",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata"
  },
  {
    "href": "figs/fig1.html#setup-code---imports-and-co.",
    "objectID": "figs/fig1.html#setup-code---imports-and-co.",
    "section": "",
    "text": "Setup imports.\n\n\nCode\nfrom __future__ import annotations\n\nimport copy\nimport logging\nimport re\nimport tempfile\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Dict, Optional, Tuple\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom IPython.core.display import Image\nfrom IPython.display import display\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import auc, confusion_matrix as sk_cm, roc_curve\nfrom sklearn.preprocessing import label_binarize\n\nfrom epiclass.core.confusion_matrix import ConfusionMatrixWriter\nfrom epiclass.utils.notebooks.paper.metrics_per_assay import MetricsPerAssay\nfrom epiclass.utils.notebooks.paper.paper_utilities import (\n    ASSAY,\n    ASSAY_MERGE_DICT,\n    ASSAY_ORDER,\n    CELL_TYPE,\n    SEX,\n    IHECColorMap,\n    MetadataHandler,\n    SplitResultsHandler,\n    extract_input_sizes_from_output_files,\n    merge_similar_assays,\n)\n\n\nSetup paths.\n\n\nCode\nbase_dir = Path.home() / \"Projects/epiclass/output/paper\"\npaper_dir = base_dir\nif not paper_dir.exists():\n    raise FileNotFoundError(f\"Directory {paper_dir} does not exist.\")\n\nbase_data_dir = base_dir / \"data\"\nbase_fig_dir = base_dir / \"figures\"\n\n\nSetup colors.\n\n\nCode\nIHECColorMap = IHECColorMap(base_fig_dir)\nassay_colors = IHECColorMap.assay_color_map\ncell_type_colors = IHECColorMap.cell_type_color_map\n\n\nSetup metadata and prediction files handlers.\n\n\nCode\nsplit_results_handler = SplitResultsHandler()\n\nmetadata_handler = MetadataHandler(paper_dir)\nmetadata_v2 = metadata_handler.load_metadata(\"v2\")\nmetadata_v2_df = metadata_v2.to_df()\n\n\nSetup data directories.\n\n\nCode\ngen_data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\nif not gen_data_dir.exists():\n    raise FileNotFoundError(f\"Directory {gen_data_dir} does not exist.\")\n\ndata_dir_100kb = gen_data_dir / \"hg38_100kb_all_none\"\nif not data_dir_100kb.exists():\n    raise FileNotFoundError(f\"Directory {data_dir_100kb} does not exist.\")\n\n\nSetup figures general settings.\n\n\nCode\nmain_title_settings = {\n    \"title\":dict(\n        automargin=True,\n        x=0.5,\n        xanchor=\"center\",\n        yanchor=\"top\",\n        y=0.98\n        ),\n    \"margin\":dict(t=50, l=10, r=10)\n}",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata"
  },
  {
    "href": "figs/fig1.html#figure-1",
    "objectID": "figs/fig1.html#figure-1",
    "section": "",
    "text": "Performance of EpiClass Assay and Biospecimen classifiers.\n\n\n\nFig. 1A: Overview of the EpiClass training process for various classifiers and their inference on external data. Each classifier is trained independently.\n\n\n\nPath setup.\n\n\nCode\nmixed_data_dir = gen_data_dir / \"mixed\"\nif not mixed_data_dir.exists():\n    raise FileNotFoundError(f\"Directory {mixed_data_dir} does not exist.\")\n\n\nFeature sets setup.\n\n\nCode\nfeature_sets_14 = [\n    \"hg38_10mb_all_none_1mb_coord\",\n    \"hg38_100kb_random_n316_none\",\n    \"hg38_1mb_all_none\",\n    \"hg38_100kb_random_n3044_none\",\n    \"hg38_100kb_all_none\",\n    \"hg38_gene_regions_100kb_coord_n19864\",\n    \"hg38_10kb_random_n30321_none\",\n    \"hg38_regulatory_regions_n30321\",\n    \"hg38_1kb_random_n30321_none\",\n    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n    \"hg38_10kb_all_none\",\n    \"hg38_regulatory_regions_n303114\",\n    \"hg38_1kb_random_n303114_none\",\n    \"hg38_cpg_topvar_200bp_10kb_coord_n300k\",\n]\nfig1_sets = [\n    \"hg38_10mb_all_none_1mb_coord\",\n    \"hg38_100kb_random_n316_none\",\n    \"hg38_1mb_all_none\",\n    \"hg38_100kb_random_n3044_none\",\n    \"hg38_100kb_all_none\",\n    \"hg38_10kb_random_n30321_none\",\n    \"hg38_1kb_random_n30321_none\",\n    \"hg38_10kb_all_none\",\n    \"hg38_1kb_random_n303114_none\",\n]\nflagship_selection_4cat = [\n    \"hg38_100kb_all_none\",\n    \"hg38_gene_regions_100kb_coord_n19864\",\n    \"hg38_regulatory_regions_n30321\",\n    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n]\ndifferent_nature_sets = [\n    \"hg38_regulatory_regions_n30321\",\n    \"hg38_regulatory_regions_n303114\",\n    \"hg38_cpg_topvar_200bp_10kb_coord_n30k\",\n    \"hg38_cpg_topvar_200bp_10kb_coord_n300k\",\n    \"hg38_cpg_topvar_2bp_10kb_coord_n30k\",\n    \"hg38_cpg_topvar_2bp_10kb_coord_n300k\",\n    \"hg38_gene_regions_100kb_coord_n19864\",\n    \"hg38_100kb_all_none\",\n    \"hg38_10kb_all_none\",\n    \"hg38_10kb_random_n30321_none\",\n    \"hg38_1kb_random_n30321_none\",\n    \"hg38_1kb_random_n303114_none\",\n]\n\nmetric_orders_map = {\n    \"flagship_selection_4cat\": flagship_selection_4cat,\n    \"fig1_sets\": fig1_sets,\n    \"feature_sets_14\": feature_sets_14,\n    \"different_nature_sets\": different_nature_sets,\n}\n\n\nCompute input sizes for each feature set.\n\n\nCode\ninput_sizes = extract_input_sizes_from_output_files(mixed_data_dir)  # type: ignore\ninput_sizes: Dict[str, int] = {k: v.pop() for k, v in input_sizes.items() if len(v) == 1}  # type: ignore\n\n\nSet selection.\n\n\nCode\nset_selection_name = \"feature_sets_14\"\n\nlogdir = (\n    base_fig_dir\n    / \"fig2_EpiAtlas_other\"\n    / \"fig2--reduced_feature_sets\"\n    / \"test\"\n    / set_selection_name\n)\nlogdir.mkdir(parents=True, exist_ok=True)\n\n\nCompute metrics.\n\n\nCode\nall_metrics = split_results_handler.obtain_all_feature_set_data(\n    parent_folder=mixed_data_dir,\n    merge_assays=True,\n    return_type=\"metrics\",\n    include_categories=[ASSAY, CELL_TYPE],\n    include_sets=metric_orders_map[set_selection_name],\n    exclude_names=[\"16ct\", \"27ct\", \"7c\", \"chip-seq-only\"],\n)\n\n# Order the metrics\nall_metrics = {\n    name: all_metrics[name]  # type: ignore\n    for name in metric_orders_map[set_selection_name]\n    if name in all_metrics\n}\n\n\nLabel correction.\n\n\nCode\n# correct a name\ntry:\n    all_metrics[\"hg38_100kb_all_none\"][ASSAY] = all_metrics[\"hg38_100kb_all_none\"][  # type: ignore\n        f\"{ASSAY}_11c\"\n    ]\n    del all_metrics[\"hg38_100kb_all_none\"][f\"{ASSAY}_11c\"]\nexcept KeyError:\n    pass\n\n\nResolution/feature set \u2013&gt; color mapping.\n\n\nCode\nresolution_colors = {\n    \"100kb\": px.colors.qualitative.Safe[0],\n    \"10kb\": px.colors.qualitative.Safe[1],\n    \"1kb\": px.colors.qualitative.Safe[2],\n    \"regulatory\": px.colors.qualitative.Safe[3],\n    \"gene\": px.colors.qualitative.Safe[4],\n    \"cpg\": px.colors.qualitative.Safe[5],\n    \"1mb\": px.colors.qualitative.Safe[6],\n    \"5mb\": px.colors.qualitative.Safe[7],\n    \"10mb\": px.colors.qualitative.Safe[8],\n}\n\n\nDefine function graph_feature_set_metrics.\n\n\nCode\ndef graph_feature_set_metrics(\n    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n    input_sizes: Dict[str, int],\n    logdir: Path | None = None,\n    sort_by_input_size: bool = False,\n    name: str | None = None,\n    y_range: Tuple[float, float] | None = None,\n    boxpoints: str = \"all\",\n    width: int = 1200,\n    height: int = 1200,\n) -&gt; None:\n    \"\"\"Graph the metrics for all feature sets.\n\n    Args:\n        all_metrics (Dict[str, Dict[str, Dict[str, Dict[str, float]]]): A dictionary containing all metrics for all feature sets.\n            Format: {feature_set: {task_name: {split_name: metric_dict}}}\n        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n        logdir (Path): The directory where the figure will be saved. If None, the figure will only be displayed.\n        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n        name (str|None): The name of the figure.\n        y_range (Tuple[float, float]|None): The y-axis range for the figure.\n        boxpoints (str): The type of boxpoints to display. Can be \"all\" or \"outliers\". Defaults to \"all\".\n    \"\"\"\n    if boxpoints not in [\"all\", \"outliers\"]:\n        raise ValueError(\"Invalid boxpoints value.\")\n\n    reference_hdf5_type = \"hg38_100kb_all_none\"\n    metadata_categories = list(all_metrics[reference_hdf5_type].keys())\n\n    non_standard_names = {ASSAY: f\"{ASSAY}_11c\", SEX: f\"{SEX}_w-mixed\"}\n    non_standard_assay_task_names = [\"hg38_100kb_all_none\"]\n    non_standard_sex_task_name = [\n        \"hg38_100kb_all_none\",\n        \"hg38_regulatory_regions_n30321\",\n        \"hg38_regulatory_regions_n303114\",\n    ]\n    used_resolutions = set()\n    for i in range(len(metadata_categories)):\n        category_idx = i\n        category_fig = make_subplots(\n            rows=1,\n            cols=2,\n            shared_yaxes=True,\n            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n            horizontal_spacing=0.01,\n        )\n\n        trace_names = []\n        order = list(all_metrics.keys())\n        if sort_by_input_size:\n            order = sorted(\n                all_metrics.keys(),\n                key=lambda x: input_sizes[x],\n            )\n        for feature_set_name in order:\n            # print(feature_set_name)\n            tasks_dicts = all_metrics[feature_set_name]\n            meta_categories = copy.deepcopy(metadata_categories)\n\n            if feature_set_name not in input_sizes:\n                print(f\"Skipping {feature_set_name}, no input size found.\")\n                continue\n\n            task_name = meta_categories[category_idx]\n            if \"split\" in task_name:\n                raise ValueError(\"Split in task name. Wrong metrics dict.\")\n\n            try:\n                task_dict = tasks_dicts[task_name]\n            except KeyError as err:\n                if SEX in str(err) and feature_set_name in non_standard_sex_task_name:\n                    task_dict = tasks_dicts[non_standard_names[SEX]]\n                elif (\n                    ASSAY in str(err)\n                    and feature_set_name in non_standard_assay_task_names\n                ):\n                    task_dict = tasks_dicts[non_standard_names[ASSAY]]\n                else:\n                    print(\"Skipping\", feature_set_name, task_name)\n                    continue\n\n            input_size = input_sizes[feature_set_name]\n\n            feature_set_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n            feature_set_name = re.sub(r\"\\_[\\dmkb]+\\_coord\", \"\", feature_set_name)\n\n            resolution = feature_set_name.split(\"_\")[0]\n            used_resolutions.add(resolution)\n\n            trace_name = f\"{input_size}|{feature_set_name}\"\n            trace_names.append(trace_name)\n\n            # Accuracy\n            metric = \"Accuracy\"\n            y_vals = [task_dict[split][metric] for split in task_dict]\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"\n                for split, metrics_dict in task_dict.items()\n            ]\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=boxpoints,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    fillcolor=resolution_colors[resolution],\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                    legendgroup=resolution,\n                    showlegend=False,\n                ),\n                row=1,\n                col=1,\n            )\n\n            metric = \"F1_macro\"\n            y_vals = [task_dict[split][metric] for split in task_dict]\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"\n                for split, metrics_dict in task_dict.items()\n            ]\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=boxpoints,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    fillcolor=resolution_colors[resolution],\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                    legendgroup=resolution,\n                    showlegend=False,\n                ),\n                row=1,\n                col=2,\n            )\n\n        title = f\"{metadata_categories[category_idx]} classification\"\n        title = title.replace(CELL_TYPE, \"biospecimen\")\n        if name is not None:\n            title += f\" - {name}\"\n        category_fig.update_layout(\n            width=width,\n            height=height,\n            title_text=title,\n            **main_title_settings\n        )\n\n        # dummy scatters for resolution colors\n        for resolution, color in resolution_colors.items():\n            if resolution not in used_resolutions:\n                continue\n            category_fig.add_trace(\n                go.Scatter(\n                    x=[None],\n                    y=[None],\n                    mode=\"markers\",\n                    name=resolution,\n                    marker=dict(color=color, size=5),\n                    showlegend=True,\n                    legendgroup=resolution,\n                )\n            )\n\n        category_fig.update_layout(legend=dict(itemsizing=\"constant\"))\n\n        # y-axis\n        if y_range:\n            category_fig.update_yaxes(range=y_range)\n        else:\n            if ASSAY in task_name:\n                category_fig.update_yaxes(range=[0.96, 1.001])\n            if CELL_TYPE in task_name:\n                category_fig.update_yaxes(range=[0.75, 1])\n\n        category_fig.update_layout(**main_title_settings)\n\n        # Save figure\n        if logdir:\n            base_name = f\"feature_set_metrics_{metadata_categories[category_idx]}\"\n            if name is not None:\n                base_name = base_name + f\"_{name}\"\n            category_fig.write_html(logdir / f\"{base_name}.html\")\n            category_fig.write_image(logdir / f\"{base_name}.svg\")\n            category_fig.write_image(logdir / f\"{base_name}.png\")\n\n        category_fig.show()\n\n\n\n\n\nGraph 100kb resolution MLP metrics.\n\nCode\nmetrics_fig1b = {name: all_metrics[name] for name in [\"hg38_100kb_all_none\"]}\n\nmetrics_fig1b_1 = {\n    \"hg38_100kb_all_none\": {ASSAY: metrics_fig1b[\"hg38_100kb_all_none\"][ASSAY]}\n}\ngraph_feature_set_metrics(\n    all_metrics=metrics_fig1b_1,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=425,\n    height=400,\n    y_range=(0.98, 1.001),\n)\n\nmetrics_fig1b_2 = {\n    \"hg38_100kb_all_none\": {CELL_TYPE: metrics_fig1b[\"hg38_100kb_all_none\"][CELL_TYPE]}\n}\ngraph_feature_set_metrics(\n    all_metrics=metrics_fig1b_2,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=425,\n    height=400,\n    y_range=(0.93, 1.001),\n)\n\n\n\n\n                                                \n\n\n                                                \n\n\n\nFig. 1B: Distribution of accuracy and F1-score for each of the ten training folds (dots) for the Assay and Biospecimen MLP classifiers.\n\n\n\nGraph.\n\nCode\nmetrics_fig1c = {name: all_metrics[name] for name in fig1_sets}\n\ngraph_feature_set_metrics(\n    all_metrics=metrics_fig1c,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=900,\n    height=600,\n)\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\nFig. 1C-alt: Distribution of accuracy per training fold for different bin resolutions for the Assay and Biospecimen classifiers.\n\n\nDefine function parse_bin_size to extract a numerical bin size in base pairs.\n\n\nCode\ndef parse_bin_size(feature_set_name: str) -&gt; Optional[float]:\n    \"\"\"\n    Parses the feature set name to extract a numerical bin size in base pairs.\n    Handles formats like '100kb', '5mb', 'regulatory', 'gene', 'cpg'.\n\n    Returns numerical size (float) or None if unparseable or non-numeric.\n    Assigns placeholder values for non-genomic-range types if needed,\n    but for a continuous axis, it's better to return None or filter later.\n    \"\"\"\n    name_parts = feature_set_name.replace(\"hg38_\", \"\").split(\"_\")\n    if not name_parts:\n        return None\n\n    resolution_str = name_parts[0].lower()\n\n    # Handle standard genomic ranges\n    match_kb = re.match(r\"(\\d+)kb\", resolution_str)\n    if match_kb:\n        return float(match_kb.group(1)) * 1_000\n    match_mb = re.match(r\"(\\d+)mb\", resolution_str)\n    if match_mb:\n        return float(match_mb.group(1)) * 1_000_000\n\n    # Handle non-range types - decide how to represent them.\n    # Option 1: Return None (they won't be plotted on the numeric axis)\n    # Option 2: Assign arbitrary numbers (might distort scale)\n    # Option 3: Could use different marker symbols later if needed\n    if resolution_str in [\"regulatory\", \"gene\", \"cpg\"]:\n        # For now, let's return None so they are filtered out from the numeric plot\n        # Or assign a placeholder if you want to handle them differently:\n        # if resolution_str == 'regulatory': return 1e1 # Example placeholder\n        # if resolution_str == 'gene': return 1e2 # Example placeholder\n        # if resolution_str == 'cpg': return 1e0 # Example placeholder\n        return None  # Returning None is cleaner for a pure numeric axis\n\n    # Fallback for unrecognised formats\n    try:\n        # Maybe it's just a number (e.g., representing window size)?\n        return float(resolution_str)\n    except ValueError:\n        return None\n\n\nDefine function graph_feature_set_scatter to graph performance metrics as a scatter plot instead of bar plot.\n\n\nCode\ndef graph_feature_set_scatter(\n    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n    input_sizes: Dict[str, int],\n    logdir: Optional[Path] = None,\n    metric_to_plot: str = \"Accuracy\",\n    name: Optional[str] = None,\n    metric_range: Optional[Tuple[float, float]] = None,\n    assay_task_key: str = ASSAY,\n    sex_task_key: str = SEX,\n    cell_type_task_key: str = CELL_TYPE,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Graphs performance metrics as a scatter plot with modifications.\n\n    X-axis: Number of Features (log scale).\n    Y-axis: Average performance metric (e.g., Accuracy, F1_macro) across folds.\n            Vertical lines indicate the min/max range across folds.\n    Color: Bin Size (bp, log scale).\n\n    Args:\n        all_metrics: Nested dict {feature_set: {task_name: {split_name: metric_dict}}}.\n        input_sizes: Dict {feature_set: num_features}.\n        logdir: Directory to save figures. If None, display only.\n        metric_to_plot: The metric key to use for the Y-axis ('Accuracy', 'F1_macro').\n        name: Optional suffix for figure titles and filenames.\n        metric_range: Optional tuple (min, max) to set the Y-axis range.\n        assay_task_key: Key used for the assay prediction task.\n        sex_task_key: Key used for the sex prediction task.\n        cell_type_task_key: Key used for the cell type prediction task.\n    \"\"\"\n    if metric_to_plot not in [\"Accuracy\", \"F1_macro\"]:\n        raise ValueError(\"metric_to_plot must be 'Accuracy' or 'F1_macro'\")\n\n    # --- Standard Name Handling (simplified from original) ---\n    non_standard_names = {ASSAY: f\"{ASSAY}_11c\", SEX: f\"{SEX}_w-mixed\"}\n    # These lists are no longer strictly needed by the simplified lookup, but kept for context\n    # non_standard_assay_task_names = [\"hg38_100kb_all_none\"]\n    # non_standard_sex_task_name = [\n    #     \"hg38_100kb_all_none\",\n    #     \"hg38_regulatory_regions_n30321\",\n    #     \"hg38_regulatory_regions_n303114\",\n    # ]\n\n    # --- Find reference and task names ----\n    reference_hdf5_type = next(iter(all_metrics), None)\n    if reference_hdf5_type is None or not all_metrics.get(reference_hdf5_type):\n        print(\n            \"Warning: Could not determine tasks from all_metrics. Trying default tasks.\"\n        )\n        cleaned_metadata_categories = {assay_task_key, sex_task_key, cell_type_task_key}\n    else:\n        metadata_categories = list(all_metrics[reference_hdf5_type].keys())\n        cleaned_metadata_categories = set()\n        for cat in metadata_categories:\n            original_name = cat\n            for standard, non_standard in non_standard_names.items():\n                if cat == non_standard:\n                    original_name = standard\n                    break\n            cleaned_metadata_categories.add(original_name)\n\n    # --- Define Bin size categories and Colors ---\n    bin_category_names = [\"1Kb\", \"10Kb\", \"100Kb\", \"1Mb\", \"10Mb\"]\n    bin_category_values = [1000, 10000, 100 * 1000, 1000 * 1000, 10000 * 1000]\n    discrete_colors = px.colors.sequential.Viridis_r\n    color_map = {\n        name: discrete_colors[i * 2] for i, name in enumerate(bin_category_names)\n    }\n\n    if verbose:\n        print(f\"Plotting for tasks: {list(cleaned_metadata_categories)}\")\n\n    for category_name in cleaned_metadata_categories:\n        plot_data_points = []\n\n        for feature_set_name_orig in all_metrics.keys():\n            try:\n                num_features = input_sizes[feature_set_name_orig]\n            except KeyError as e:\n                raise ValueError(\n                    f\"Feature set '{feature_set_name_orig}' not found in input_sizes\"\n                ) from e\n\n            # Parse Bin Size\n            bin_size = parse_bin_size(feature_set_name_orig)\n            if bin_size is None:\n                print(\n                    f\"Skipping {feature_set_name_orig}, could not parse numeric bin size.\"\n                )\n                continue\n\n            # 3. Get Metric Values (Average, Min, Max)\n            tasks_dicts = all_metrics[feature_set_name_orig]\n\n            # --- Task Name Lookup ---\n            # 1. Try the standard category name first\n            # 2. If standard name not found, use non-standard name\n            task_dict = None\n            task_name = category_name\n            if category_name in tasks_dicts:\n                task_dict = tasks_dicts[category_name]\n            else:\n                non_standard_task_name = non_standard_names.get(category_name)\n                if non_standard_task_name and non_standard_task_name in tasks_dicts:\n                    task_name = non_standard_task_name\n                    task_dict = tasks_dicts[non_standard_task_name]\n\n                if task_dict is None:\n                    raise ValueError(\n                        f\"Task '{category_name}' not found in feature set '{feature_set_name_orig}'\"\n                    )\n            # --- End Task Name Lookup ---\n\n            # Calculate average, min, max metric value across splits\n            try:\n                metric_values = []\n                for split, split_data in task_dict.items():\n                    if metric_to_plot in split_data:\n                        metric_values.append(split_data[metric_to_plot])\n                    else:\n                        print(\n                            f\"Warning: Metric '{metric_to_plot}' not found in split '{split}' for {feature_set_name_orig} / {task_name}\"\n                        )\n\n                if not metric_values:\n                    print(\n                        f\"Warning: No metric values found for {feature_set_name_orig} / {task_name} / {metric_to_plot}\"\n                    )\n                    continue\n\n                avg_metric = np.mean(metric_values)\n                min_metric = np.min(metric_values)\n                max_metric = np.max(metric_values)\n\n            except Exception as e:  # pylint: disable=broad-except\n                raise ValueError(\n                    f\"Error calculating metrics for {feature_set_name_orig} / {task_name}: {e}\"\n                ) from e\n\n            # Clean feature set name for hover text\n            clean_name = feature_set_name_orig.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n            clean_name = re.sub(r\"\\_[\\dmkb]+\\_coord\", \"\", clean_name)\n\n            # Store data for this point\n            plot_data_points.append(\n                {\n                    \"bin_size\": bin_size,\n                    \"num_features\": num_features,\n                    \"metric_value\": avg_metric,\n                    \"min_metric\": min_metric,  # For error bar low\n                    \"max_metric\": max_metric,  # For error bar high\n                    \"name\": clean_name,\n                    \"raw_name\": feature_set_name_orig,\n                }\n            )\n\n        if not plot_data_points:\n            raise ValueError(\n                f\"No suitable data points found to plot for task: {category_name}\"\n            )\n\n        # --- Determine Marker Symbols ---\n        marker_symbols = []\n        default_symbol = \"circle\"\n        random_symbol = \"cross\"\n        for p in plot_data_points:\n            if \"random\" in p[\"raw_name\"]:\n                marker_symbols.append(random_symbol)\n            else:\n                marker_symbols.append(default_symbol)\n\n        # --- Group Data by Category ---\n        points_by_category = {name: [] for name in bin_category_names}\n        for i, point_data in enumerate(plot_data_points):\n            bin_size = point_data[\"bin_size\"]\n            assigned_category = None\n            for cat_name, cat_value in zip(bin_category_names, bin_category_values):\n                if bin_size == cat_value:\n                    assigned_category = cat_name\n                    break\n            else:\n                raise ValueError(f\"Could not find category for bin size: {bin_size}\")\n\n            points_by_category[assigned_category].append(\n                {\n                    \"x\": point_data[\"num_features\"],  # X is Num Features\n                    \"y\": point_data[\"metric_value\"],\n                    \"error_up\": point_data[\"max_metric\"] - point_data[\"metric_value\"],\n                    \"error_down\": point_data[\"metric_value\"] - point_data[\"min_metric\"],\n                    \"text\": point_data[\"name\"],\n                    \"customdata\": [\n                        point_data[\"min_metric\"],\n                        point_data[\"max_metric\"],\n                        point_data[\"bin_size\"],\n                    ],  # Keep bin size for hover\n                    \"symbol\": marker_symbols[i],  # Assign symbol determined earlier\n                }\n            )\n\n        # --- Create Figure and Add Traces PER CATEGORY ---\n        fig = go.Figure()\n        traces = []\n\n        for cat_name in bin_category_names:  # Iterate in defined order for legend\n            points_in_cat = points_by_category[cat_name]\n            if not points_in_cat:\n                continue\n\n            category_color = color_map[cat_name]\n\n            # Extract data for all points in this category\n            x_vals = [p[\"x\"] for p in points_in_cat]\n            y_vals = [p[\"y\"] for p in points_in_cat]\n            error_up_vals = [p[\"error_up\"] for p in points_in_cat]\n            error_down_vals = [p[\"error_down\"] for p in points_in_cat]\n            text_vals = [p[\"text\"] for p in points_in_cat]\n            customdata_vals = [p[\"customdata\"] for p in points_in_cat]\n            symbols_vals = [p[\"symbol\"] for p in points_in_cat]\n\n            trace = go.Scatter(\n                x=x_vals,\n                y=y_vals,\n                mode=\"markers\",\n                name=cat_name,\n                showlegend=False,\n                legendgroup=cat_name,  # Group legend entries\n                marker=dict(\n                    color=category_color,\n                    size=15,\n                    symbol=symbols_vals,\n                    line=dict(width=1, color=\"DarkSlateGrey\"),\n                ),\n                error_y=dict(\n                    type=\"data\",\n                    symmetric=False,\n                    array=error_up_vals,\n                    arrayminus=error_down_vals,\n                    visible=True,\n                    thickness=1.5,\n                    width=15,\n                    color=category_color,\n                ),\n                text=text_vals,\n                customdata=customdata_vals,\n                hovertemplate=(\n                    f\"&lt;b&gt;%{{text}}&lt;/b&gt;&lt;br&gt;&lt;br&gt;\"\n                    f\"Num Features: %{{x:,.0f}}&lt;br&gt;\"\n                    f\"{metric_to_plot}: %{{y:.4f}}&lt;br&gt;\"\n                    f\"Bin Size: %{{customdata:,.0f}} bp&lt;br&gt;\"\n                    f\"{metric_to_plot} Range (10-fold): %{{customdata:.4f}} - %{{customdata:.4f}}\"\n                    \"&lt;extra&gt;&lt;/extra&gt;\"\n                ),\n            )\n            traces.append(trace)\n\n        fig.add_traces(traces)\n\n        # --- Add Legend ---\n        # Add a hidden scatter trace with square markers for legend\n        for cat_name in bin_category_names:\n            category_color = color_map[cat_name]\n            legend_trace = go.Scatter(\n                x=[None],\n                y=[None],\n                mode=\"markers\",\n                name=cat_name,\n                marker=dict(\n                    color=category_color,\n                    size=15,\n                    symbol=\"square\",\n                    line=dict(width=1, color=\"DarkSlateGrey\"),\n                ),\n                legendgroup=cat_name,\n                showlegend=True,\n            )\n            fig.add_trace(legend_trace)\n\n        # --- Update layout ---\n        title_name = category_name.replace(CELL_TYPE, \"biospecimen\")\n\n        plot_title = f\"{metric_to_plot} vs Number of Features - {title_name}\"\n        if name:\n            plot_title += f\" - {name}\"\n        xaxis_title = \"Number of Features (log scale)\"\n        xaxis_type = \"log\"\n\n        yaxis_title = metric_to_plot.replace(\"_\", \" \").title()\n        yaxis_type = \"linear\"\n\n        fig.update_layout(\n            xaxis_title=xaxis_title,\n            yaxis_title=yaxis_title,\n            xaxis_type=xaxis_type,\n            yaxis_type=yaxis_type,\n            yaxis_range=metric_range,\n            width=500,\n            height=500,\n            hovermode=\"closest\",\n            legend_title_text=\"Bin Size\",\n            title_text=plot_title,\n            **main_title_settings\n        )\n\n        if category_name == CELL_TYPE:\n            fig.update_yaxes(range=[0.75, 1.005])\n        elif category_name == ASSAY:\n            fig.update_yaxes(range=[0.96, 1.001])\n\n        # --- Save or show figure ---\n        if logdir:\n            logdir.mkdir(parents=True, exist_ok=True)\n            # Include \"modified\" or similar in filename to distinguish\n            base_name = f\"feature_scatter_MODIFIED_v2_{category_name}_{metric_to_plot}\"\n            if name:\n                base_name += f\"_{name}\"\n            html_path = logdir / f\"{base_name}.html\"\n            svg_path = logdir / f\"{base_name}.svg\"\n            png_path = logdir / f\"{base_name}.png\"\n\n            print(f\"Saving modified plot for {category_name} to {html_path}\")\n            fig.write_html(html_path)\n            fig.write_image(svg_path)\n            fig.write_image(png_path)\n\n        fig.show()\n\n\nGraph\n\nCode\nfor metric in [\"Accuracy\", \"F1_macro\"]:\n    graph_feature_set_scatter(\n        all_metrics=metrics_fig1c,  # type: ignore\n        input_sizes=input_sizes,\n        metric_to_plot=metric,\n        verbose=False,\n    )\n\n\n\n\n                                                \n\n\n                                                \n\n\n\n\n                                                \n\n\n                                                \n\n\n\nFig. 1C: Distribution of accuracy per training fold for different bin resolutions for the Assay and Biospecimen classifiers. The circles represent the means and the whiskers the min and max values of the ten training folds.\n\n\n\nDefine create_confusion_matrix to create and show a confusion matrix.\n\n\nCode\ndef create_confusion_matrix(\n    df: pd.DataFrame,\n    name: str = \"confusion_matrix\",\n    logdir: Path | None = None,\n    min_pred_score: float = 0,\n    majority: bool = False,\n    verbose:bool=False\n) -&gt; None:\n    \"\"\"Create a confusion matrix for the given DataFrame and save it to the logdir.\n\n    Args:\n        df (pd.DataFrame): The DataFrame containing the results.\n        logdir (Path): The directory path for saving the figures.\n        name (str): The name for the saved figures.\n        min_pred_score (float): The minimum prediction score to consider.\n        majority (bool): Whether to use majority vote (uuid-wise) for the predicted class.\n    \"\"\"\n    # Compute confusion matrix\n    classes = sorted(df[\"True class\"].unique())\n    if \"Max pred\" not in df.columns:\n        df[\"Max pred\"] = df[classes].max(axis=1)  # type: ignore\n    filtered_df = df[df[\"Max pred\"] &gt; min_pred_score]\n\n    if majority:\n        # Majority vote for predicted class\n        groupby_uuid = filtered_df.groupby([\"uuid\", \"True class\", \"Predicted class\"])[\n            \"Max pred\"\n        ].aggregate([\"size\", \"mean\"])\n\n        if groupby_uuid[\"size\"].max() &gt; 3:\n            raise ValueError(\"More than three predictions for the same uuid.\")\n\n        groupby_uuid = groupby_uuid.reset_index().sort_values(\n            [\"uuid\", \"True class\", \"size\"], ascending=[True, True, False]\n        )\n        groupby_uuid = groupby_uuid.drop_duplicates(\n            subset=[\"uuid\", \"True class\"], keep=\"first\"\n        )\n        filtered_df = groupby_uuid\n\n    confusion_mat = sk_cm(\n        filtered_df[\"True class\"], filtered_df[\"Predicted class\"], labels=classes\n    )\n\n    mat_writer = ConfusionMatrixWriter(labels=classes, confusion_matrix=confusion_mat)\n\n    if logdir is None:\n        logdir = Path(tempfile.gettempdir())\n\n    files = mat_writer.to_all_formats(logdir, name=f\"{name}_n{len(filtered_df)}\")\n\n    if verbose:\n        print(f\"Saved confusion matrix to {logdir}:\")\n        for file in files:\n            print(Path(file).name)\n\n    for file in files:\n        if \"png\" in file.name:\n            scale = 0.6\n            display(Image(filename=file, width=1250*scale, height=1000*scale))\n\n\nPrepare prediction data for confusion matrix.\n\n\nCode\nassay_split_dfs = split_results_handler.gather_split_results_across_methods(\n    results_dir=data_dir_100kb, label_category=ASSAY, only_NN=True\n)\nconcat_assay_df = split_results_handler.concatenate_split_results(assay_split_dfs)[\"NN\"]\n\ndf_with_meta = metadata_handler.join_metadata(concat_assay_df, metadata_v2)  # type: ignore\nif \"Predicted class\" not in df_with_meta.columns:\n    raise ValueError(\"`Predicted class` not in DataFrame\")\n\nclassifier_name = \"MLP\"\nmin_pred_score = 0\nmajority = False\n\nname = f\"{classifier_name}_pred&gt;{min_pred_score}\"\n\nlogdir = base_fig_dir / \"fig1_EpiAtlas_assay\" / \"fig1_supp_D-assay_c11_confusion_matrices\"\nif majority:\n    logdir = logdir / \"per_uuid\"\nelse:\n    logdir = logdir / \"per_file\"\nlogdir.mkdir(parents=True, exist_ok=True)\n\n\nGraph.\n\n\nCode\ncreate_confusion_matrix(\n    df=df_with_meta,\n    min_pred_score=min_pred_score,\n    majority=majority,\n)\n\n\n\n\n\n\n\n\n\nFig. 1D: Confusion matrix aggregating the cross-validation folds (therefore showing all files) without applying a prediction score threshold. RNA-seq and WGBS data were both separated according to two protocols during initial training (but combined thereafter to nine assays).\n\n\n\n\nFig. 1E: Genome browser representation showing in black the datasets swap between H3K4me3 and H3K27ac for IHECRE00001897 in the metadata freeze v1.0, along with typical correct datasets over a representative region.",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata"
  },
  {
    "href": "figs/fig1.html#supplementary-figure-1",
    "objectID": "figs/fig1.html#supplementary-figure-1",
    "section": "",
    "text": "More detailled performance of EpiClass Assay and Biospecimen classifiers.\n\n\nFig. 1A,B data points are included in these two graphs (MLP data points).\n\n\n\nDefine graphing function plot_multiple_models_split_metrics.\n\n\nCode\ndef plot_multiple_models_split_metrics(\n    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n    label_category: str,\n    logdir: Path | None = None,\n    filename: str = \"fig1_all_classifiers_metrics\",\n) -&gt; None:\n    \"\"\"Render to box plots the metrics per classifier/models and split, each in its own subplot.\n\n    Args:\n        split_metrics: A dictionary containing metric scores for each classifier and split.\n        label_category: The label category for the classification task.\n        name: The name of the figure.\n        logdir: The directory to save the figure to. If None, the figure is only displayed.\n\n    Returns:\n        None: Displays the figure and saves it to the logdir if provided.\n    \"\"\"\n    metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n    classifier_names = list(next(iter(split_metrics.values())).keys())\n    classifier_names = [\"NN\", \"LR\", \"LGBM\", \"LinearSVC\", \"RF\"]\n\n    # Create subplots, one row for each metric\n    fig = make_subplots(\n        rows=1,\n        cols=len(metrics),\n        subplot_titles=metrics,\n        horizontal_spacing=0.075,\n    )\n\n    for i, metric in enumerate(metrics):\n        for classifier in classifier_names:\n            values = [split_metrics[split][classifier][metric] for split in split_metrics]\n            if classifier == \"NN\":\n                classifier = \"MLP\"\n            fig.add_trace(\n                go.Box(\n                    y=values,\n                    name=classifier,\n                    line=dict(color=\"black\", width=1.5),\n                    marker=dict(size=3, color=\"black\"),\n                    boxmean=True,\n                    boxpoints=\"all\",  # or \"outliers\" to show only outliers\n                    pointpos=-1.4,\n                    showlegend=False,\n                    width=0.5,\n                    hovertemplate=\"%{text}\",\n                    text=[\n                        f\"{split}: {value:.4f}\"\n                        for split, value in zip(split_metrics, values)\n                    ],\n                ),\n                row=1,\n                col=i + 1,\n            )\n\n    fig.update_layout(\n        title_text=f\"{label_category} classification\",\n        boxmode=\"group\",\n        **main_title_settings,\n    )\n\n    # Adjust y-axis\n    if label_category == ASSAY:\n        range_acc = [0.95, 1.001]\n        range_AUC = [0.992, 1.0001]\n    elif label_category == CELL_TYPE:\n        range_acc = [0.81, 1]\n        range_AUC = [0.96, 1]\n    else:\n        range_acc = [0.6, 1.001]\n        range_AUC = [0.9, 1.0001]\n\n    fig.update_layout(\n        yaxis=dict(range=range_acc),\n        yaxis2=dict(range=range_acc),\n        yaxis3=dict(range=range_AUC),\n        yaxis4=dict(range=range_AUC),\n        height=450,\n    )\n\n    fig.update_layout(margin=dict(l=20, r=20))\n\n    # Save figure\n    if logdir:\n        fig.write_image(logdir / f\"{filename}.svg\")\n        fig.write_image(logdir / f\"{filename}.png\")\n        fig.write_html(logdir / f\"{filename}.html\")\n\n    fig.show()\n\n\nGraph.\n\nCode\nmerge_assays = True\n\nfor label_category in [ASSAY, CELL_TYPE]:\n    all_split_dfs = split_results_handler.gather_split_results_across_methods(\n        results_dir=data_dir_100kb,\n        label_category=label_category,\n        only_NN=False,\n    )\n\n    if merge_assays and label_category == ASSAY:\n        for split_name, split_dfs in all_split_dfs.items():\n            for classifier_type, df in split_dfs.items():\n                split_dfs[classifier_type] = merge_similar_assays(df)\n\n    split_metrics = split_results_handler.compute_split_metrics(all_split_dfs)\n\n    plot_multiple_models_split_metrics(\n        split_metrics,\n        label_category=label_category,\n    )\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\nSupplementary Figure 1A,B: Distribution of performance scores (accuracy, F1 as well as micro and macro AUROC) per training fold (dots) for each machine learning approach used for training on the Assay (A) and Biospecimen (B) metadata. Micro-averaging aggregates contributions from all classes (global true positive rate and false positive rate); macro-averaging averages the true positive rate from each class. Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5\u00d7 the interquartile range.\n\n\nGoing forward, all results are for MLP classifiers.\n\n\n\nDefine graphing function plot_roc_curves. Computes macro-average ROC curves manually.\n\n\nCode\ndef plot_roc_curves(\n    results_df: pd.DataFrame,\n    label_category: str,\n    logdir: Path | None = None,\n    name: str = \"roc_curve\",\n    title: str | None = None,\n    colors_dict: Dict | None = None,  # Optional specific colors\n    verbose: bool = False,\n) -&gt; None:\n    \"\"\"\n    Generates and plots ROC curves for multi-class classification results using Plotly.\n\n    Calculates and plots individual class ROC curves, micro-average, and macro-average ROC curves.\n\n    Args:\n        results_df (pd.DataFrame): DataFrame with true labels and prediction probabilities for each class.\n                                   Must contain the `label_category` column (e.g., 'True class')\n                                   and probability columns named after each class.\n        label_category (str): The column name containing the true labels (e.g., 'True class', ASSAY, CELL_TYPE).\n        logdir (Path | None): Directory to save the figure. If None, only displays the figure.\n        name (str): Base name for saved files (e.g., \"supp_fig1e\").\n        title (str | None): Title suffix for the plot. If None, a default title based on label_category is used.\n        colors_dict (Dict | None): Optional dictionary mapping class names to colors. If None or a class\n                                   is missing, default Plotly colors are used.\n    \"\"\"\n    df = results_df.copy()\n    true_label_col = \"True class\"  # Assuming 'True class' holds the ground truth labels\n\n    if true_label_col not in df.columns:\n        raise ValueError(f\"True label column '{true_label_col}' not found in DataFrame.\")\n\n    classes = sorted(df[true_label_col].unique())\n    if verbose:\n        print(f\"Using classes: {classes}\")\n\n    n_classes = len(classes)\n    if n_classes &lt; 2:\n        print(\n            f\"Warning: Only {n_classes} class found after processing. Cannot generate ROC curve.\"\n        )\n        return\n\n    # Check if probability columns exist for all determined classes\n    missing_cols = [c for c in classes if c not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing probability columns for classes: {missing_cols}\")\n\n    # Binarize the true labels against the final set of classes\n    try:\n        y_true = label_binarize(df[true_label_col], classes=classes)\n    except ValueError as e:\n        raise ValueError(\n            f\"Error binarizing labels for classes {classes}. Check if all labels in '{true_label_col}' are included in 'classes'.\"\n        ) from e\n\n    if n_classes == 2 and y_true.shape[1] == 1:\n        # Adjust for binary case where label_binarize might return one column\n        y_true = np.hstack((1 - y_true, y_true))  # type: ignore\n    elif y_true.shape[1] != n_classes:\n        raise ValueError(\n            f\"Binarized labels shape {y_true.shape} does not match number of classes {n_classes}\"\n        )\n\n    # Get the predicted probabilities for each class\n    # Ensure columns are in the same order as 'classes'\n    y_score = df[classes].values\n\n    # --- Compute ROC curve and ROC area for each class ---\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i, class_name in enumerate(classes):\n        try:\n            fpr[class_name], tpr[class_name], _ = roc_curve(\n                y_true=y_true[:, i], y_score=y_score[:, i]  # type: ignore\n            )\n            roc_auc[class_name] = auc(fpr[class_name], tpr[class_name])\n        except ValueError as e:\n            raise ValueError(\"Could not compute ROC for class {class_name}.\") from e\n\n    # --- Compute micro-average ROC curve and ROC area ---\n    try:\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_score.ravel())  # type: ignore\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    except ValueError as e:\n        raise ValueError(\"Could not compute micro-average ROC.\") from e\n\n    # --- Compute macro-average ROC curve and ROC area ---\n    try:\n        # Aggregate all false positive rates\n        all_fpr = np.unique(\n            np.concatenate(\n                [fpr[class_name] for class_name in classes if class_name in fpr]\n            )\n        )\n        # Interpolate all ROC curves at these points\n        mean_tpr = np.zeros_like(all_fpr)\n        valid_classes_count = 0\n        for class_name in classes:\n            if class_name in fpr and class_name in tpr:\n                mean_tpr += np.interp(all_fpr, fpr[class_name], tpr[class_name])\n                valid_classes_count += 1\n\n        # Average it and compute AUC\n        if valid_classes_count &gt; 0:\n            mean_tpr /= valid_classes_count\n            fpr[\"macro\"] = all_fpr\n            tpr[\"macro\"] = mean_tpr\n            roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n        else:\n            raise ValueError(\"No valid classes found for macro averaging.\")\n\n    except ValueError as e:\n        raise ValueError(\"Could not compute macro-average ROC.\") from e\n\n    # --- Plot all ROC curves ---\n    fig = go.Figure()\n\n    # Plot diagonal line for reference\n    fig.add_shape(\n        type=\"line\", line=dict(dash=\"dash\", color=\"grey\", width=1), x0=0, x1=1, y0=0, y1=1\n    )\n\n    # Define colors for plotting\n    color_cycle = px.colors.qualitative.Plotly  # Default cycle\n    plot_colors = {}\n    for i, cls_name in enumerate(classes):\n        if colors_dict and cls_name in colors_dict:\n            plot_colors[cls_name] = colors_dict[cls_name]\n        else:\n            plot_colors[cls_name] = color_cycle[i % len(color_cycle)]\n\n    # Plot Micro-average ROC curve first (often plotted thicker/dashed)\n    fig.add_trace(\n        go.Scatter(\n            x=fpr[\"micro\"],\n            y=tpr[\"micro\"],\n            mode=\"lines\",\n            name=f'Micro-average ROC (AUC = {roc_auc[\"micro\"]:.5f})',\n            line=dict(color=\"deeppink\", width=3, dash=\"dash\"),\n            hoverinfo=\"skip\",  # Less important for hover usually\n        )\n    )\n\n    # Plot Macro-average ROC curve\n    fig.add_trace(\n        go.Scatter(\n            x=fpr[\"macro\"],\n            y=tpr[\"macro\"],\n            mode=\"lines\",\n            name=f'Macro-average ROC (AUC = {roc_auc[\"macro\"]:.5f})',\n            line=dict(color=\"navy\", width=3, dash=\"dash\"),\n            hoverinfo=\"skip\",\n        )\n    )\n\n    # Plot individual class ROC curves\n    for class_name in classes:\n        if class_name not in fpr or class_name not in tpr or class_name not in roc_auc:\n            continue  # Skip if calculation failed\n        fig.add_trace(\n            go.Scatter(\n                x=fpr[class_name],\n                y=tpr[class_name],\n                mode=\"lines\",\n                name=f\"{class_name} (AUC = {roc_auc[class_name]:.5f})\",\n                line=dict(width=1.5, color=plot_colors.get(class_name)),\n                hovertemplate=f\"&lt;b&gt;{class_name}&lt;/b&gt;&lt;br&gt;FPR=%{{x:.5f}}&lt;br&gt;TPR=%{{y:.5f}}&lt;extra&gt;&lt;/extra&gt;\",  # Show class name and values on hover\n            )\n        )\n\n    # --- Update layout ---\n    base_title = f\"ROC Curves&lt;br&gt;{label_category}\"\n    plot_title = f\"{base_title} - {title}\" if title else base_title\n\n    title_settings=dict(\n        yanchor=\"top\",\n        yref=\"paper\",\n        y=0.97,\n        xanchor=\"center\",\n        xref=\"paper\",\n        x=0.5,\n    )\n\n    fig.update_layout(\n        title=title_settings,\n        title_text=plot_title,\n        xaxis_title=\"False Positive Rate (1 - Specificity)\",\n        yaxis_title=\"True Positive Rate (Sensitivity)\",\n        xaxis=dict(range=[0.0, 1.0], constrain=\"domain\"),  # Ensure axes range 0-1\n        yaxis=dict(\n            range=[0.0, 1.01], scaleanchor=\"x\", scaleratio=1, constrain=\"domain\"\n        ),  # Make it square-ish, slight top margin\n        width=800,\n        height=650,\n        hovermode=\"closest\",\n        legend=dict(\n            traceorder=\"reversed\",  # Show averages first in legend\n            title=\"Classes & Averages\",\n            font=dict(size=9),\n            itemsizing=\"constant\",\n            y=0.8,\n            yref=\"paper\",\n        ),\n        margin=dict(l=60, r=30, t=0, b=0),\n    )\n\n    # --- Save figure if logdir is provided ---\n    if logdir:\n        logdir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n        filename_base = f\"{name}_{label_category}_roc\"\n        filepath_base = logdir / filename_base\n\n        fig.write_html(f\"{filepath_base}.html\")\n        fig.write_image(f\"{filepath_base}.svg\", width=800, height=750)\n        fig.write_image(f\"{filepath_base}.png\", width=800, height=750, scale=2)\n\n        print(f\"Saved ROC curve plots for {label_category} to {logdir}\")\n        print(f\" -&gt; {filename_base}.html / .svg / .png\")\n\n    fig.show()\n\n\nPrepare assay data for plotting.\n\n\nCode\ndata_dir = (\n    mixed_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{ASSAY}_1l_3000n\"\n    / \"11c\"\n    / \"10fold-oversampling\"\n)\nif not data_dir.exists():\n    raise FileNotFoundError(f\"Directory {data_dir} does not exist.\")\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\n\ndf = merge_similar_assays(concat_df_w_meta.copy())\n\n\nGraph assay results.\n\n\nCode\nplot_roc_curves(\n    results_df=df.copy(),\n    label_category=ASSAY,\n    title=\"Aggregated 10fold\",  # Title suffix\n    colors_dict=assay_colors,\n    verbose=False,\n)\n\n\n                                                \n\n\nPrepare biospecimen data for plotting.\n\n\nCode\ndata_dir = (\n    mixed_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{CELL_TYPE}_1l_3000n\"\n    / \"10fold-oversampling\"\n)\nif not data_dir.exists():\n    raise FileNotFoundError(f\"Directory {data_dir} does not exist.\")\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\n\n\nGraph biospecimen results.\n\n\nCode\nplot_roc_curves(\n    results_df=concat_df_w_meta,\n    label_category=CELL_TYPE,\n    title=\"Aggregated 10fold\",  # Title suffix\n    colors_dict=cell_type_colors,\n    verbose=False,\n)\n\n\n                                                \n\n\nSupplementary Figure 1C: ROC curves from aggregated cross-validation results for the Assay and Biospecimen classifiers. Curves for each class are computed in a one-vs-rest scheme.\n\n\n\nDefine graphing function create_blklst_graphs.\n\n\nCode\ndef create_blklst_graphs(\n    feature_set_metrics_dict: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n    logdir: Path | None = None,\n) -&gt; List[go.Figure]:\n    \"\"\"Create boxplots for blacklisted related feature sets.\n\n    Args:\n        feature_set_metrics_dict (Dict[str, Dict[str, Dict[str, Dict[str, float]]]]): The dictionary containing all metrics for all blklst related feature sets.\n            format: {feature_set: {task_name: {split_name: metric_dict}}}\n        logdir (Path, Optional): The directory to save the figure to. If None, the figure is only displayed.\n    \"\"\"\n    figs = []\n\n    # Assume names exist in all feature sets\n    task_names = list(feature_set_metrics_dict.values())[0].keys()\n\n    traces_names_dict = {\n        \"hg38_100kb_all_none\": \"observed\",\n        \"hg38_100kb_all_none_0blklst\": \"0blklst\",\n        \"hg38_100kb_all_none_0blklst_winsorized\": \"0blklst_winsorized\",\n    }\n\n    for task_name in task_names:\n        category_fig = make_subplots(\n            rows=1,\n            cols=2,\n            shared_yaxes=False,\n            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n            horizontal_spacing=0.1,\n        )\n        for feature_set_name, tasks_dicts in feature_set_metrics_dict.items():\n            task_dict = tasks_dicts[task_name]\n            trace_name = traces_names_dict[feature_set_name]\n\n            # Accuracy\n            metric = \"Accuracy\"\n            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n                for split, metrics_dict in task_dict.items()\n            ]\n\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=\"all\",\n                    showlegend=False,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                ),\n                row=1,\n                col=1,\n            )\n\n            metric = \"F1_macro\"\n            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n                for split, metrics_dict in task_dict.items()\n            ]\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=\"all\",\n                    showlegend=False,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                ),\n                row=1,\n                col=2,\n            )\n\n        category_fig.update_xaxes(\n            categoryorder=\"array\",\n            categoryarray=list(traces_names_dict.values()),\n        )\n        category_fig.update_yaxes(range=[0.9, 1.001])\n\n        category_fig.update_layout(\n            title_text=task_name,\n            height=600,\n            width=500,\n            **main_title_settings\n        )\n\n        # Save figure\n        if logdir:\n            task_name = task_name.replace(\"_1l_3000n-10fold\", \"\")\n            base_name = f\"metrics_{task_name}\"\n\n            category_fig.write_html(logdir / f\"{base_name}.html\")\n            category_fig.write_image(logdir / f\"{base_name}.svg\")\n            category_fig.write_image(logdir / f\"{base_name}.png\")\n\n        figs.append(category_fig)\n\n    return figs\n\n\nPrepare paths.\n\n\nCode\ninclude_sets = [\n    \"hg38_100kb_all_none\",\n    \"hg38_100kb_all_none_0blklst\",\n    \"hg38_100kb_all_none_0blklst_winsorized\",\n]\n\nresults_folder_blklst = base_data_dir / \"training_results\" / \"2023-01-epiatlas-freeze\"\nif not results_folder_blklst.exists():\n    raise FileNotFoundError(f\"Folder '{results_folder_blklst}' not found\")\n\n\nCompute metrics.\n\n\nCode\n# Select 10-fold oversampling runs\n# expected result shape: {feature_set: {task_name: {split_name: metrics_dict}}}\nall_metrics_blklst: Dict[\n    str, Dict[str, Dict[str, Dict[str, float]]]\n] = split_results_handler.obtain_all_feature_set_data(\n    return_type=\"metrics\",\n    parent_folder=results_folder_blklst,\n    merge_assays=True,\n    include_categories=[ASSAY, CELL_TYPE],\n    include_sets=include_sets,\n    oversampled_only=False,\n    verbose=False,\n)  # type: ignore\n\n\nGraph.\n\nCode\nfigs = create_blklst_graphs(all_metrics_blklst)\n\nfigs[0].show()\nfigs[1].show()\n\n\n\n\n                                                \n\n\n                                                \n\n\n\nSupplementary Figure 1D: Distribution of accuracy and F1-score per training fold (dots) for the Assay and Biospecimen classifiers after removing signal from blacklisted regions and applying winsorization of 0.1%. Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5\u00d7 the interquartile range.\n\n\n\n\nE: Assay training 10-fold cross-validation\nF: Assay complete training (mixed tracks), predictions on imputed data (all pval)\nG: Biospecimen 10-fold cross-validation\n\n\nDefine graphing function plot_prediction_scores_distribution.\nSupplementary Figure 1E-G: Distribution of average prediction score per file (dots) for the majority-vote class (up to three track type files) (E, F) or individual file (G), from the MLP approach for the Assay (E, G) and Biospecimen classifiers (F), using aggregated cross-validation results from observed data (E, F) or results from the classifier trained on all observed data and applied to imputed data from EpiATLAS (G). Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5\u00d7 the interquartile range, with a violin representation on top.\n\n\nGather prediction scores.\n\n\nCode\ndata_dir = (\n    mixed_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{ASSAY}_1l_3000n\"\n    / \"11c\"\n    / \"10fold-oversampling\"\n)\nif not data_dir.exists():\n    raise FileNotFoundError(f\"Directory {data_dir} does not exist.\")\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\n\n\nGraph.\n\n\nCode\nplot_prediction_scores_distribution(\n    results_df=concat_df_w_meta,\n    group_by_column=ASSAY,\n    merge_assay_pairs=True,\n    min_y=0.7,\n    title=\"11 classes assay training&lt;br&gt;Prediction scores for 10-fold cross-validation\",\n)\n\n\n                                                \n\n\n\n\n\nGather prediction scores.\n\n\nCode\ndata_dir = data_dir_100kb / f\"{CELL_TYPE}_1l_3000n\" / \"10fold-oversampling\"\nif not data_dir.exists():\n    raise FileNotFoundError(f\"Directory {data_dir} does not exist.\")\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\nconcat_df_w_meta.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n\n\nGraph.\n\n\nCode\nplot_prediction_scores_distribution(\n    results_df=concat_df_w_meta,\n    group_by_column=ASSAY,\n    min_y=0,\n    title=\"Biospecimen training&lt;br&gt;Prediction scores for 10-fold cross-validation\",\n)\n\n\nSkipping assay merging: Wrong results dataframe, rna or wgbs columns missing.\n\n\n                                                \n\n\n\n\n\nGather imputed signal metadata.\n\n\nCode\nmetadata_path = (\n    paper_dir\n    / \"data\"\n    / \"metadata\"\n    / \"epiatlas\"\n    / \"imputed\"\n    / \"hg38_epiatlas_imputed_pval_chip_2024-02.json\"\n)\nmetadata_imputed: pd.DataFrame = metadata_handler.load_any_metadata(metadata_path, as_dataframe=True)  # type: ignore\n\n\nGather prediction scores.\n\n\nCode\ndata_dir = (\n    gen_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{ASSAY}_1l_3000n\"\n    / \"11c\"\n    / \"complete_no_valid_oversample\"\n    / \"predictions\"\n    / \"epiatlas_imputed\"\n    / \"ChIP\"\n)\nif not data_dir.exists():\n    raise FileNotFoundError(f\"Directory {data_dir} does not exist.\")\n\ndf_pred = pd.read_csv(\n    data_dir / \"complete_no_valid_oversample_prediction.csv\",\n    index_col=0,\n)\n\n\nPrepare dataframe for graphing.\n\n\nCode\nassay_classes = list(metadata_v2_df[ASSAY].unique())\ndf_pred = split_results_handler.add_max_pred(df_pred, expected_classes=assay_classes)\n\naugmented_df = pd.merge(df_pred, metadata_imputed, left_index=True, right_on=\"md5sum\")\naugmented_df[\"True class\"] = augmented_df[ASSAY]\nprint(\"Number of files per assay:\")\nprint(augmented_df[\"True class\"].value_counts(dropna=False).to_string())\n\n\nNumber of files per assay:\nh3k36me3    1703\nh3k27me3    1703\nh3k9me3     1700\nh3k4me1     1688\nh3k4me3     1688\nh3k27ac     1088\n\n\n\n\nGraph.\n\n\nCode\nplot_prediction_scores_distribution(\n    results_df=augmented_df,\n    group_by_column=ASSAY,\n    merge_assay_pairs=True,\n    min_y=0.79,\n    use_aggregate_vote=False,\n    title=\"Complete 11c assay classifier&lt;br&gt;inference on imputed data\",\n)\n\n\n                                                \n\n\n\n\n\n\nFor the code that produced the figures, see src/python/epiclass/utils/notebooks/paper/confidence_threshold.ipynb.\n\n\n\n\nSupplementary Figure 1H,I: Distribution of aggregated accuracy, F1-score and corresponding file subset size across varying prediction score thresholds, based on pooled predictions from all cross-validation folds for the Assay (H) and Biospecimen (I) classifiers.",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata"
  },
  {
    "href": "figs/fig1.html#supplementary-figure-2---biospecimen-performance-on-various-feature-sets.",
    "objectID": "figs/fig1.html#supplementary-figure-2---biospecimen-performance-on-various-feature-sets.",
    "section": "",
    "text": "Code\nmetrics_supp2 = {name: all_metrics[name] for name in feature_sets_14}\n\ngraph_feature_set_metrics(\n    all_metrics=metrics_supp2,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=900,\n    height=600,\n)\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n\n\nCode\ndef prepare_metric_sets_per_assay(\n    all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]], verbose: bool = False\n) -&gt; Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]:\n    \"\"\"Prepare metric sets per assay.\n\n    Args:\n        all_results (Dict[str, Dict[str, Dict[str, pd.DataFrame]]]): A dictionary containing all results for all feature sets.\n\n    Returns:\n        Dict[str, Dict[str, Dict[str, Dict[str, float]]]]: A dictionary containing all metrics per assay for all feature sets.\n            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n    \"\"\"\n    if verbose:\n        print(\"Loading metadata.\")\n    metadata = metadata_handler.load_metadata(\"v2\")\n    metadata.convert_classes(ASSAY, ASSAY_MERGE_DICT)\n    md5_per_assay = metadata.md5_per_class(ASSAY)\n    md5_per_assay = {k: set(v) for k, v in md5_per_assay.items()}\n\n    if verbose:\n        print(\"Getting results per assay.\")\n    results_per_assay = {}\n    for assay_label in ASSAY_ORDER:\n        if verbose:\n            print(assay_label)\n        results_per_assay[assay_label] = {}\n        for feature_set, task_dict in all_results.items():\n            if verbose:\n                print(feature_set)\n            results_per_assay[assay_label][feature_set] = {}\n            for task_name, split_dict in task_dict.items():\n                if verbose:\n                    print(task_name)\n                results_per_assay[assay_label][feature_set][task_name] = {}\n\n                # Only keep the relevant assay\n                for split_name, split_df in split_dict.items():\n                    if verbose:\n                        print(split_name)\n                    assay_df = split_df[split_df.index.isin(md5_per_assay[assay_label])]\n                    results_per_assay[assay_label][feature_set][task_name][\n                        split_name\n                    ] = assay_df\n\n    if verbose:\n        print(\"Finished getting results per assay. Now computing metrics.\")\n    metrics_per_assay = {}\n    for assay_label in ASSAY_ORDER:\n        if verbose:\n            print(assay_label)\n        metrics_per_assay[assay_label] = {}\n        for feature_set, task_dict in results_per_assay[assay_label].items():\n            if verbose:\n                print(feature_set)\n            assay_metrics = split_results_handler.compute_split_metrics(\n                task_dict, concat_first_level=True\n            )\n            inverted_dict = split_results_handler.invert_metrics_dict(assay_metrics)\n            metrics_per_assay[assay_label][feature_set] = inverted_dict\n\n    return metrics_per_assay\n\n\n\n\nCode\ndef graph_feature_set_metrics_per_assay(\n    all_metrics_per_assay: Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]],\n    input_sizes: Dict[str, int],\n    logdir: Path | None = None,\n    sort_by_input_size: bool = False,\n    name: str | None = None,\n    y_range: Tuple[float, float] | None = None,\n    boxpoints: str = \"outliers\",\n) -&gt; None:\n    \"\"\"Graph the metrics for all feature sets, per assay, with separate plots for accuracy and F1-score.\n\n    Args:\n        all_metrics_per_assay (Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]): A dictionary containing all metrics per assay for all feature sets.\n            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n        logdir (Path): The directory where the figures will be saved. If None, the figures will only be displayed.\n        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n        name (str|None): The name of the figure.\n        y_range (Tuple[float, float]|None): The y-axis range for the plots.\n        boxpoints (str): The type of points to display in the box plots. Defaults to \"outliers\".\n    \"\"\"\n    valid_boxpoints = [\"all\", \"outliers\"]\n    if boxpoints not in valid_boxpoints:\n        raise ValueError(f\"Invalid boxpoints value. Choose from {valid_boxpoints}.\")\n\n    fig_assay_order = [\n        \"rna_seq\",\n        \"h3k27ac\",\n        \"h3k4me1\",\n        \"h3k4me3\",\n        \"h3k36me3\",\n        \"h3k27me3\",\n        \"h3k9me3\",\n        \"input\",\n        \"wgbs\",\n    ]\n\n    reference_assay = next(iter(all_metrics_per_assay))\n    reference_feature_set = next(iter(all_metrics_per_assay[reference_assay]))\n    metadata_categories = list(\n        all_metrics_per_assay[reference_assay][reference_feature_set].keys()\n    )\n\n    for _, category in enumerate(metadata_categories):\n        for metric, metric_name in [\n            (\"Accuracy\", \"Accuracy\"),\n            (\"F1_macro\", \"F1-score (macro)\"),\n        ]:\n            fig = go.Figure()\n\n            feature_sets = list(all_metrics_per_assay[reference_assay].keys())\n            unique_feature_sets = set(feature_sets)\n            for assay in fig_assay_order:\n                if set(all_metrics_per_assay[assay].keys()) != unique_feature_sets:\n                    raise ValueError(\"Different feature sets through assays.\")\n\n            feature_set_order = feature_sets\n            if sort_by_input_size:\n                feature_set_order = sorted(\n                    feature_set_order, key=lambda x: input_sizes[x]\n                )\n\n            # Adjust spacing so each assay group has dedicated space based on the number of feature sets\n            spacing_multiplier = (\n                1.1  # Increase this multiplier if needed to add more spacing\n            )\n            x_positions = {\n                assay: i * len(feature_set_order) * spacing_multiplier\n                for i, assay in enumerate(fig_assay_order)\n            }\n\n            for i, feature_set_name in enumerate(feature_set_order):\n                resolution = (\n                    feature_set_name.replace(\"_none\", \"\")\n                    .replace(\"hg38_\", \"\")\n                    .split(\"_\")[0]\n                )\n                color = resolution_colors[resolution]\n                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n\n                for assay in fig_assay_order:\n                    if feature_set_name not in all_metrics_per_assay[assay]:\n                        continue\n\n                    tasks_dicts = all_metrics_per_assay[assay][feature_set_name]\n\n                    if feature_set_name not in input_sizes:\n                        print(f\"Skipping {feature_set_name}, no input size found.\")\n                        continue\n\n                    task_name = category\n                    if \"split\" in task_name:\n                        raise ValueError(\"Split in task name. Wrong metrics dict.\")\n\n                    try:\n                        task_dict = tasks_dicts[task_name]\n                    except KeyError:\n                        print(\n                            f\"Skipping {feature_set_name}, {task_name} for assay {assay}\"\n                        )\n                        continue\n\n                    y_vals = [task_dict[split][metric] for split in task_dict]\n                    hovertext = [\n                        f\"{assay} - {display_name} - {split}: {metrics_dict[metric]:.4f}\"\n                        for split, metrics_dict in task_dict.items()\n                    ]\n\n                    x_position = x_positions[assay] + i\n                    fig.add_trace(\n                        go.Box(\n                            x=[x_position] * len(y_vals),\n                            y=y_vals,\n                            name=f\"{assay}|{display_name}\",\n                            boxmean=True,\n                            boxpoints=boxpoints,\n                            marker=dict(size=3, color=\"black\"),\n                            line=dict(width=1, color=\"black\"),\n                            fillcolor=color,\n                            hovertemplate=\"%{text}\",\n                            text=hovertext,\n                            showlegend=False,\n                            legendgroup=display_name,\n                        )\n                    )\n\n                    # separate box groups\n                    fig.add_vline(\n                        x=x_positions[assay] - 1, line_width=1, line_color=\"black\"\n                    )\n\n            # Add dummy traces for the legend\n            for feature_set_name in feature_set_order:\n                resolution = (\n                    feature_set_name.replace(\"_none\", \"\")\n                    .replace(\"hg38_\", \"\")\n                    .split(\"_\")[0]\n                )\n                color = resolution_colors[resolution]\n                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n                display_name = re.sub(r\"\\_[\\dmkb]+\\_coord\", \"\", display_name)\n\n                fig.add_trace(\n                    go.Scatter(\n                        name=display_name,\n                        x=[None],\n                        y=[None],\n                        mode=\"markers\",\n                        marker=dict(size=10, color=color),\n                        showlegend=True,\n                        legendgroup=display_name,\n                    )\n                )\n\n            title = f\"{category} - {metric_name} (per assay)\"\n            if name is not None:\n                title += f\" - {name}\"\n\n            fig.update_layout(\n                width=1250,\n                height=900,\n                title_text=title,\n                xaxis_title=\"Assay\",\n                yaxis_title=metric_name,\n                **main_title_settings\n            )\n\n            # Create x-axis labels\n            fig.update_xaxes(\n                tickmode=\"array\",\n                tickvals=[\n                    x_positions[assay] + len(feature_set_order) / 2\n                    for assay in fig_assay_order\n                ],\n                ticktext=list(x_positions.keys()),\n                title=\"Assay\",\n            )\n\n            fig.update_layout(\n                legend=dict(\n                    title=\"Feature Sets\", itemsizing=\"constant\", traceorder=\"normal\"\n                )\n            )\n            if y_range:\n                fig.update_yaxes(range=y_range)\n\n            if logdir:\n                base_name = f\"feature_set_metrics_{category}_{metric}_per_assay\"\n                if name is not None:\n                    base_name = base_name + f\"_{name}\"\n                fig.write_html(logdir / f\"{base_name}.html\")\n                fig.write_image(logdir / f\"{base_name}.svg\")\n                fig.write_image(logdir / f\"{base_name}.png\")\n\n            fig.show()\n\n\n\n\nCode\nset_selection_name = \"feature_sets_14\"\nall_results = split_results_handler.obtain_all_feature_set_data(\n    parent_folder=mixed_data_dir,\n    merge_assays=True,\n    return_type=\"split_results\",\n    include_categories=[CELL_TYPE],\n    include_sets=metric_orders_map[set_selection_name],\n    exclude_names=[\"16ct\", \"27ct\", \"7c\", \"chip-seq-only\"],\n)\n\n\n\n\nCode\nmetrics_per_assay = prepare_metric_sets_per_assay(all_results)  # type: ignore\n\n\n\n\nCode\n# Reorder feature sets\nfeature_set_order = metric_orders_map[set_selection_name]\nfor assay, feature_sets in list(metrics_per_assay.items()):\n    metrics_per_assay[assay] = {\n        feature_set_name: metrics_per_assay[assay][feature_set_name]\n        for feature_set_name in feature_set_order\n    }\n\n\n\nCode\ngraph_feature_set_metrics_per_assay(\n    all_metrics_per_assay=metrics_per_assay,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    sort_by_input_size=False,\n    y_range=(0.1, 1.01)\n)",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata"
  },
  {
    "href": "figs/fig2.html",
    "objectID": "figs/fig2.html",
    "section": "",
    "text": "Results section 2 figures\n\n\n\n\n\n\nImportantTO BE RELEASED SOON\n\n\n\nTHIS IS A WORK IN PROGRESS.\nThis page will be updated with the figures and their corresponding code in the following weeks.\nMessage date: 2025-09-10\n\n\nFormatting of the figures may not be identical to the paper, but they contain the same data points.\nAll code is folded by default, click on \u201cCode\u201d to expand it.",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata"
  }
]
