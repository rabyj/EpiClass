[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website renders the figures from of the EpiClass paper originally created using Python and Plotly.\nMost figures are interactive, thanks to Plotly. The related code is present on Github, at rabyj/EpiClass."
  },
  {
    "objectID": "figs/fig2.html",
    "href": "figs/fig2.html",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata",
    "section": "",
    "text": "The formatting of the figures may differ slightly from those in the paper, but they display the same data points.\nAll code cells are folded by default. To view any cell, click “Code” to expand it, or use the code options near the main title above to unfold all at once.\nSome code may be repeated, as the original Python notebook was designed for figures to be generated semi-independently.\n\n\nSetup imports.\n\n\nCode\nfrom __future__ import annotations\n\nimport itertools\nimport re\nimport tarfile\nfrom pathlib import Path\nfrom typing import Dict, List, Sequence\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom IPython.display import display\nfrom plotly.subplots import make_subplots\nfrom scipy.stats import zscore\n\nfrom epiclass.utils.notebooks.paper.paper_utilities import (\n    ASSAY,\n    ASSAY_MERGE_DICT,\n    ASSAY_ORDER,\n    BIOMATERIAL_TYPE,\n    CELL_TYPE,\n    LIFE_STAGE,\n    SEX,\n    IHECColorMap,\n    MetadataHandler,\n    SplitResultsHandler,\n    create_mislabel_corrector,\n    PathChecker\n)\n\nCORE7_ASSAYS = ASSAY_ORDER[0:7]\n\n\nSetup paths.\n\n\nCode\n# Root path\nbase_dir = Path.home() / \"Projects/epiclass/output/paper\"\nPathChecker.check_directory(base_dir)\n\n# More precise\nbase_data_dir = base_dir / \"data\"\nbase_fig_dir = base_dir / \"figures\"\ntable_dir = base_dir / \"tables\"\nmetadata_dir = base_data_dir / \"metadata\"\n\nofficial_metadata_dir = metadata_dir / \"epiatlas\" / \"official\"\nPathChecker.check_directory(official_metadata_dir)\n\n# alias\npaper_dir = base_dir\n\n\nSetup colors.\n\n\nCode\nIHECColorMap = IHECColorMap(base_fig_dir)\nassay_colors = IHECColorMap.assay_color_map\ncell_type_colors = IHECColorMap.cell_type_color_map\nsex_colors = IHECColorMap.sex_color_map\n\n\nSetup metadata and prediction files handlers.\n\n\nCode\nsplit_results_handler = SplitResultsHandler()\n\nmetadata_handler = MetadataHandler(paper_dir)\nmetadata_v2 = metadata_handler.load_metadata(\"v2\")\nmetadata_v2_df = metadata_v2.to_df()\n\n\nSetup figures general settings.\n\n\nCode\nmain_title_settings = {\n    \"title\":dict(\n        automargin=True,\n        x=0.5,\n        xanchor=\"center\",\n        yanchor=\"top\",\n        y=0.98\n        ),\n    \"margin\":dict(t=50, l=10, r=10)\n}\n\n\n\n\n\n\n\n\n2A: Accuracy\n2B: F1-Score\nSupp 5A: AUC scores\n\nDefine function that graphs performance accross classification tasks.\n\n\nCode\ndef NN_performance_across_classification_tasks(\n    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n    name: str | None = None,\n    logdir: Path | None = None,\n    exclude_categories: List[str] | None = None,\n    y_range: List[float] | None = None,\n    sort_by_acc: bool = False,\n    metric_names: Sequence[str] = (\"Accuracy\", \"F1_macro\"),\n    title: str | None = None,\n) -&gt; List[str]:\n    \"\"\"Render box plots of metrics per classifier and split, each in its own subplot.\n\n    This function generates a figure with subplots, each representing a different\n    metric. Each subplot contains box plots for each classifier, ordered by accuracy.\n\n    Args:\n        split_metrics: A nested dictionary with structure {split: {classifier: {metric: score}}}.\n        logdir: The directory path to save the output plots. If None, only display the plot.\n        name: The base name for the output plot files.\n        exclude_categories: Task categories to exclude from the plot.\n        y_range: The y-axis range for the plots.\n        sort_by_acc: Whether to sort the classifiers by accuracy.\n        metric_names: The metrics to include in the plot.\n\n    Returns:\n        The list of classifier names in the order they appear in the plot.\n    \"\"\"\n    # Exclude some categories\n    classifier_names = list(split_metrics[\"split0\"].keys())\n    if exclude_categories is not None:\n        for category in exclude_categories:\n            classifier_names = [c for c in classifier_names if category not in c]\n\n    available_metrics = list(split_metrics[\"split0\"][classifier_names[0]].keys())\n    try:\n        available_metrics.remove(\"count\")\n    except ValueError:\n        pass\n    if any(metric not in available_metrics for metric in metric_names):\n        raise ValueError(f\"Invalid metric. Metrics need to be in {available_metrics}\")\n\n    # Get classifier counts\n    classifiers_N = split_results_handler.extract_count_from_metrics(split_metrics)\n\n    # Sort classifiers by accuracy\n    if sort_by_acc:\n        mean_acc = {}\n        for classifier in classifier_names:\n            mean_acc[classifier] = np.mean(\n                [split_metrics[split][classifier][\"Accuracy\"] for split in split_metrics]\n            )\n        classifier_names = sorted(\n            classifier_names, key=lambda x: mean_acc[x], reverse=True\n        )\n\n    # Create subplots, one column for each metric\n    fig = make_subplots(\n        rows=1,\n        cols=len(metric_names),\n        subplot_titles=metric_names,\n        horizontal_spacing=0.03,\n    )\n\n    color_group = px.colors.qualitative.Plotly\n    colors = {\n        classifier: color_group[i % len(color_group)]\n        for i, classifier in enumerate(classifier_names)\n    }\n\n    point_pos = 0\n    for i, metric in enumerate(metric_names):\n        for classifier_name in classifier_names:\n            values = [\n                split_metrics[split][classifier_name][metric] for split in split_metrics\n            ]\n\n            fig.add_trace(\n                go.Box(\n                    y=values,\n                    name=f\"{classifier_name} (N={classifiers_N[classifier_name]})\",\n                    fillcolor=colors[classifier_name],\n                    line=dict(color=\"black\", width=1.5),\n                    marker=dict(size=3, color=\"white\", line_width=1),\n                    boxmean=True,\n                    boxpoints=\"all\",\n                    pointpos=point_pos,\n                    showlegend=i == 0,  # Only show legend in the first subplot\n                    hovertemplate=\"%{text}\",\n                    text=[\n                        f\"{split}: {value:.4f}\"\n                        for split, value in zip(split_metrics, values)\n                    ],\n                    legendgroup=classifier_name,\n                    width=0.5,\n                ),\n                row=1,\n                col=i + 1,\n            )\n\n    # Title\n    title_text = (\n        \"Neural network classification - Metric distribution for 10-fold cross-validation\"\n    )\n    if title:\n        title_text = title\n    fig.update_layout(title_text=title_text, **main_title_settings)\n\n    # Layout\n    fig.update_layout(\n        yaxis_title=\"Value\",\n        boxmode=\"group\",\n        height=1200 * 0.8,\n        width=1750 * 0.8,\n    )\n\n    # Acc, F1\n    fig.update_layout(yaxis=dict(range=[0.88, 1.001]))\n    fig.update_layout(yaxis2=dict(range=[0.80, 1.001]))\n\n    # AUC\n    range_auc = [0.986, 1.0001]\n    fig.update_layout(yaxis3=dict(range=range_auc))\n    fig.update_layout(yaxis4=dict(range=range_auc))\n\n    if y_range is not None:\n        fig.update_yaxes(range=y_range)\n\n    # Save figure\n    if logdir:\n        if name is None:\n            name = \"MLP_metrics_various_tasks\"\n        fig.write_image(logdir / f\"{name}.svg\")\n        fig.write_image(logdir / f\"{name}.png\")\n        fig.write_html(logdir / f\"{name}.html\")\n\n    fig.show()\n\n    return classifier_names\n\n\nCompute metrics.\n\n\nCode\nexclude_categories = [\"track_type\", \"group\", \"disease\", \"PE\", \"martin\"]\n# exclude_categories = [\"track_type\", \"group\", \"disease\"]\nexclude_names = [\"chip-seq\", \"7c\", \"16ct\", \"no-mixed\"]\n\nhdf5_type = \"hg38_100kb_all_none\"\nresults_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\nPathChecker.check_directory(results_dir)\n\nmislabel_correction = True\nif mislabel_correction:\n    mislabel_corrector = create_mislabel_corrector(paper_dir)\nelse:\n    mislabel_corrector = None\n\nsplit_results_metrics, all_split_results = split_results_handler.general_split_metrics(\n    results_dir,\n    merge_assays=True,\n    exclude_categories=exclude_categories,\n    exclude_names=exclude_names,\n    return_type=\"both\",\n    oversampled_only=True,\n    mislabel_corrections=mislabel_corrector,\n    verbose=False,\n)\n\n\nGraph metrics.\n\n\nCode\nmetrics_full = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\nfig_name = f\"{hdf5_type}_perf_across_categories_full\"\nsorted_task_names = NN_performance_across_classification_tasks(\n    split_results_metrics,  # type: ignore\n    sort_by_acc=True,\n    metric_names=metrics_full,\n)\n\n\n                                                \n\n\nFig. 2A,B: Distribution of accuracy and F1-score evaluated per training fold (dots) for each metadata classifier. Performance metrics are reported without applying a prediction score threshold. Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5× the interquartile range.\n\n\n\nDefine function compute_class_imbalance.\n\n\nCode\ndef compute_class_imbalance(\n    all_split_results: Dict[str, Dict[str, pd.DataFrame]],\n) -&gt; pd.DataFrame:\n    \"\"\"Compute class imbalance for each task and split.\n\n    Args:\n        all_split_results: A dictionary with structure {task_name: {split_name: split_results_df}}.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the following columns:\n            - avg(balance_ratio): The average balance ratio for each task.\n            - n: The number of classes for each task (used for the average).\n    \"\"\"\n    # combine md5 lists\n    task_md5s = {\n        classifier_task: [split_df.index for split_df in split_results.values()]\n        for classifier_task, split_results in all_split_results.items()\n    }\n    task_md5s = {\n        classifier_task: [list(split_md5s) for split_md5s in md5s]\n        for classifier_task, md5s in task_md5s.items()\n    }\n    task_md5s = {\n        classifier_task: list(itertools.chain(*md5s))\n        for classifier_task, md5s in task_md5s.items()\n    }\n\n    # get metadata\n    metadata_df = metadata_handler.load_metadata_df(\"v2-encode\")\n\n    label_counts = {}\n    for classifier_task, md5s in task_md5s.items():\n        try:\n            label_counts[classifier_task] = metadata_df.loc[md5s][\n                classifier_task\n            ].value_counts()\n        except KeyError as e:\n            category_name = classifier_task.rsplit(\"_\", maxsplit=1)[0]\n            try:\n                label_counts[classifier_task] = metadata_df.loc[md5s][\n                    category_name\n                ].value_counts()\n            except KeyError as e:\n                raise e\n\n    # Compute Shannon Entropy\n    class_balance = {}\n    for classifier_task, counts in label_counts.items():\n        total_count = counts.sum()\n        k = len(counts)\n        p_x = counts / total_count  # class proportions\n        p_x = p_x.values\n        shannon_entropy = -np.sum(p_x * np.log2(p_x))\n        balance = shannon_entropy / np.log2(k)\n        class_balance[classifier_task] = (balance, k)\n\n    df_class_balance = pd.DataFrame.from_dict(\n        class_balance, orient=\"index\", columns=[\"Normalized Shannon Entropy\", \"k\"]\n    ).sort_index()\n\n    return df_class_balance\n\n\nCompute class imbalance (Shannon entropy).\n\n\nCode\nsubset = {\n    k: v\n    for k, v in all_split_results.items()  # type: ignore\n    if not any(label in k for label in [\"martin\", \"PE\"])\n}\ndf_class_balance = compute_class_imbalance(subset)  # type: ignore\n\n\nDefine graphing function plot_shannon_entropy.\n\n\nCode\ndef plot_shannon_entropy(class_balance_df: pd.DataFrame, ordered_task_names: List[str]|None) -&gt; None:\n    \"\"\"Graph Shannon entropy values, in the order given by task_names\"\"\"\n    df = class_balance_df.copy()\n\n    # Reorder df\n    task_names = df.index\n    if ordered_task_names:\n        task_names = [\n            task_name for task_name in task_names if task_name in ordered_task_names\n    ]\n    df = df.loc[sorted_task_names]\n\n    # plot\n    fig = px.scatter(\n        df,\n        x=df.index,\n        y=\"Normalized Shannon Entropy\",\n        labels={\n            \"k\": \"Number of classes\",\n            \"Normalized Shannon Entropy\": \"Normalized Shannon Entropy\",\n        },\n        title=\"Class imbalance across tasks (higher is more balanced)\",\n    )\n    fig.update_layout(\n        yaxis=dict(range=[0, 1]),\n        xaxis_title=None,\n    )\n\n    fig.show()\n\n\nGraph.\n\n\nCode\nplot_shannon_entropy(\n    class_balance_df=df_class_balance,\n    ordered_task_names=sorted_task_names,\n    )\n\n\n                                                \n\n\nFig. 2C: Shannon entropy scores for each metadata category.\n\n\n\nIHEC_sample_metadata_harmonization.v1.1.extended.csv contains 314 EpiRRs with unknown sex. We applied a fully trained sex classifier on those.\nTo properly create sex classification related graphs (D,E) we need\n- Classifier predictions on samples with unknown sex label\n- Metadata pre/post correction\n- Average ChrY signal for each file\nLoad v1.1 and v1.2 official metadata.\n\n\nCode\nmetadata_v1_1_path = (\n    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.1.extended.csv\"\n)\nmetadata_v1_1 = pd.read_csv(metadata_v1_1_path, index_col=0)\n\nmetadata_v1_2_path = (\n    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.2.extended.csv\"\n)\nmetadata_v1_2 = pd.read_csv(metadata_v1_2_path, index_col=0)\n\n\nSanity check: make sure that the number of unknown labels is the same in our metadata VS official v1.1\n\n\nCode\nfull_metadata_df = metadata_v2_df\nfull_metadata_df[\"md5sum\"] = full_metadata_df.index\nassert (\n    metadata_v2_df[metadata_v2_df[SEX].isin([\"unknown\"])][\"EpiRR\"].nunique()\n    == metadata_v1_1[metadata_v1_1[SEX] == \"unknown\"].index.nunique()\n    == 314\n)\n\n\nLoad predictions for unknown sex samples.\n\n\nCode\nsex_results_dir = (\n    base_data_dir\n    / \"training_results\"\n    / \"dfreeze_v2\"\n    / \"hg38_100kb_all_none\"\n    / f\"{SEX}_1l_3000n\"\n)\nsex_full_model_dir = sex_results_dir / \"complete_no_valid_oversample\"\nPathChecker.check_directory(sex_full_model_dir)\n\npred_unknown_file_path = (\n    sex_full_model_dir\n    / \"predictions\"\n    / \"complete_no_valid_oversample_test_prediction_100kb_all_none_dfreeze_v2.1_sex_mixed_unknown.csv\"\n)\npred_unknown_df = pd.read_csv(pred_unknown_file_path, index_col=0, header=0)\n\n\nJoin metadata to predictions.\n\n\nCode\npred_unknown_df = pred_unknown_df[pred_unknown_df[\"True class\"] == \"unknown\"]\npred_unknown_df = split_results_handler.add_max_pred(pred_unknown_df)  # type: ignore\npred_unknown_df = metadata_handler.join_metadata(pred_unknown_df, metadata_v2)\npred_unknown_df[\"md5sum\"] = pred_unknown_df.index\n\n\nLoad 10-fold cross validation results.\n\n\nCode\nsex_10fold_dir = sex_results_dir / \"10fold-oversampling\"\nPathChecker.check_directory(sex_10fold_dir)\n\nsplit_results: Dict[str, pd.DataFrame] = split_results_handler.read_split_results(\n    sex_10fold_dir\n)\nconcat_results_10fold: pd.DataFrame = split_results_handler.concatenate_split_results(split_results, depth=1)  # type: ignore\nconcat_results_10fold = split_results_handler.add_max_pred(concat_results_10fold)\nconcat_results_10fold = metadata_handler.join_metadata(concat_results_10fold, metadata_v2)\n\n\n\n\nFig. 2D: Proportion of donor sex metadata originally annotated (inner circle, metadata v1.1) and predicted with high-confidence (outer circle, metadata v2.0) for female (red) and male (blue) (mixed sex not shown).\nCompute values for the inner portion of the pie chart.\n\n\nCode\n# Proportion of unknown, excluding mixed. same as v1.1 ihec metadata\nno_mixed = full_metadata_df[full_metadata_df[SEX] != \"mixed\"]\n\nwith pd.option_context(\"display.float_format\", \"{:.2%}\".format):\n    print(\"file-wise:\")\n    print(no_mixed[SEX].value_counts(dropna=False) / no_mixed.shape[0])\n\n    print(\"\\nEpiRR-wise:\")\n    epirr_no_mixed = no_mixed.drop_duplicates(subset=[\"EpiRR\"])\n    print(epirr_no_mixed[SEX].value_counts(dropna=False) / epirr_no_mixed.shape[0])\n\n\nfile-wise:\nfemale    44.88%\nmale      42.41%\nunknown   12.71%\nName: harmonized_donor_sex, dtype: float64\n\nEpiRR-wise:\nmale      45.14%\nfemale    40.60%\nunknown   14.26%\nName: harmonized_donor_sex, dtype: float64\n\n\n\n\n\nOuter ring represents SEX metadata labnels v1.2 (without mixed labels), which had those modifications:\n\nSome unknown SEX files were labelled, using (assay,track type) z-score in conjunction with fully trained model predictions.\nCorrection of some mislabels, using 10fold cross-validation results\n\n\n\nCode\nmeta_v1_2_no_mixed = metadata_v1_2[metadata_v1_2[SEX] != \"mixed\"]\nwith pd.option_context(\"display.float_format\", \"{:.2%}\".format):\n    print(\"EpiRR-wise:\")\n    print(meta_v1_2_no_mixed.value_counts(SEX) / meta_v1_2_no_mixed.shape[0])\n\n\nEpiRR-wise:\nharmonized_donor_sex\nmale      51.42%\nfemale    46.54%\nunknown    2.04%\ndtype: float64\n\n\n\n\n\nWork done in another script:\n\nFor each bigwig file, the chrY average value is computed with the pyBigWig module, in chrY_bigwig_mean.py (permalink).\nFor each assay, the z-score distribution (of the mean chrY value) of the file group is computed.\n\nOutputs chrXY_all.csv\nOutputs chrY_zscores.csv\n\n\nFig. 2E is made by averaging for each EpiRR the z-score value in each assay distribution.\nThis data is needed for the full sex mislabel context table.\n\nDefine function compute_chrY_zscores.\n\n\nCode\ndef compute_chrY_zscores(\n    chrY_dir: Path, version: str, save: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"Compute z-scores for chrY signal data.\n\n    Computes two distributions of z-scores:\n    1) Per assay group, excluding raw, pval, and Unique_raw tracks.\n    2) Per assay+track group.\n\n    In both cases, rna-seq/mrna-seq and wgbs-standard/wgbs-pbat are put as one assay.\n\n    Args:\n        chrY_dir: The directory containing the chrY signal data.\n        version: The metadata version to use.\n        save: Whether to save the results.\n\n    Returns:\n        pd.DataFrame: The chrY signal data with z-scores appended.\n    \"\"\"\n    output_dir: Path = chrY_dir\n    if save:\n        output_dir = chrY_dir / f\"dfreeze_{version}_stats\"\n        output_dir.mkdir(parents=False, exist_ok=True)\n\n    # Get chrY signal data\n    chrY_df = pd.read_csv(chrY_dir / \"chrXY_all.csv\", header=0)\n\n    # Filter out md5s not in metadata version\n    metadata = MetadataHandler(paper_dir).load_metadata(version)\n    md5s = set(metadata.md5s)\n    chrY_df = chrY_df[chrY_df[\"filename\"].isin(md5s)]\n\n    # Make sure all values are non-zero\n    if not (chrY_df[\"chrY\"] != 0).all():\n        raise ValueError(\"Some chrY values are zero.\")\n\n    # Merge metadata\n    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n    metadata_df.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n    chrY_df = chrY_df.merge(\n        metadata_df, left_on=\"filename\", right_on=\"md5sum\"\n    )\n\n    # Compute stats for distributions\n    metric_name_1 = \"chrY_zscore_vs_assay_w_track_exclusion\"\n    metric_name_2 = \"chrY_zscore_vs_assay_track\"\n    files1 = chrY_df[\n        ~chrY_df[\"track_type\"].isin([\"raw\", \"pval\", \"Unique_raw\"])\n    ]\n    files2 = chrY_df\n    dist1 = files1.groupby(ASSAY).agg({\"chrY\": [\"mean\", \"std\", \"count\"]})\n    dist2 = files2.groupby([ASSAY, \"track_type\"]).agg({\"chrY\": [\"mean\", \"std\", \"count\"]})\n    if save:\n        dist1.to_csv(output_dir / \"chrY_stats_assay_w_track_exclusion.csv\")\n        dist2.to_csv(output_dir / \"chrY_stats_assay_and_track.csv\")\n\n    # Compute full z-score distributions\n    for groups in files1.groupby(ASSAY):\n        _, group_df = groups\n        group_df[\"zscore\"] = zscore(group_df[\"chrY\"])\n        chrY_df.loc[group_df.index, metric_name_1] = group_df[\"zscore\"]\n        chrY_df.loc[group_df.index, f\"N_{metric_name_1}\"] = groups[1].shape[0]\n    for groups in files2.groupby([ASSAY, \"track_type\"]):\n        _, group_df = groups\n        group_df[\"zscore\"] = zscore(group_df[\"chrY\"])\n        chrY_df.loc[group_df.index, metric_name_2] = group_df[\"zscore\"]\n        chrY_df.loc[group_df.index, f\"N_{metric_name_2}\"] = groups[1].shape[0]\n\n    # Fill in missing values\n    for N_name in [f\"N_{metric_name_1}\", f\"N_{metric_name_2}\"]:\n        chrY_df[N_name] = chrY_df[N_name].fillna(0).astype(int)\n    chrY_df.fillna(pd.NA, inplace=True)\n\n    if save:\n        output_cols = [\n            \"filename\",\n            ASSAY,\n            \"track_type\",\n            \"chrY\",\n            metric_name_1,\n            f\"N_{metric_name_1}\",\n            metric_name_2,\n            f\"N_{metric_name_2}\",\n        ]\n        chrY_df[output_cols].to_csv(\n            output_dir / \"chrY_zscores.csv\", index=False, na_rep=\"NA\"  # type: ignore\n        )\n    return chrY_df\n\n\nCompute chrY zscores.\n\n\nCode\nchrY_dir = base_data_dir / \"chrY\"\nPathChecker.check_directory(chrY_dir)\n\nchrY_df = compute_chrY_zscores(chrY_dir, \"v2\", save=False)\n\n\n\n\n\nThe following folded code generates a similar table to what was used to determine which unknown sex sample to label. It was not used to produce any graph.\n\n\nCode\ndef create_sex_pred_pivot_table(pred_unknown: pd.DataFrame, chrY_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Generate a pivot table containing group metrics per predicted label, for each EpiRR.\"\"\"\n    index_cols = [\n        \"EpiRR\",\n        \"project\",\n        \"harmonized_donor_type\",\n        CELL_TYPE,\n        SEX,\n        \"Predicted class\",\n    ]\n    pred_plus_chrY_df = pd.merge(\n        pred_unknown,\n        chrY_df,\n        on=\"md5sum\",\n        suffixes=(\"\", \"_DROP\")\n    )\n    pred_plus_chrY_df.drop(\n        columns=[c for c in pred_plus_chrY_df.columns if c.endswith(\"_DROP\")],\n        inplace=True,\n    )\n\n    val_cols = [\"Max pred\", \"chrY_zscore_vs_assay_track\"]\n    pivot_table = pred_plus_chrY_df.pivot_table(\n        index=index_cols,\n        values=val_cols,\n        aggfunc=[\"mean\", \"median\", \"std\", \"count\"],\n    )\n\n    return pivot_table\n\n\ncreate_sex_pred_pivot_table(\n    pred_unknown=pred_unknown_df,\n    chrY_df=chrY_df,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nmedian\nstd\ncount\n\n\n\n\n\n\n\n\nMax pred\nchrY_zscore_vs_assay_track\nMax pred\nchrY_zscore_vs_assay_track\nMax pred\nchrY_zscore_vs_assay_track\nMax pred\nchrY_zscore_vs_assay_track\n\n\nEpiRR\nproject\nharmonized_donor_type\nharmonized_sample_ontology_intermediate\nharmonized_donor_sex\nPredicted class\n\n\n\n\n\n\n\n\n\n\n\n\nIHECRE00000036.3\nBLUEPRINT\nSingle donor\nhematopoietic cell\nunknown\nmale\n0.781766\n-0.081182\n0.781766\n-0.081182\nNaN\nNaN\n1\n1\n\n\nIHECRE00000042.3\nBLUEPRINT\nSingle donor\nhematopoietic cell\nunknown\nmale\n0.556080\n-0.880438\n0.556080\n-0.880438\nNaN\nNaN\n1\n1\n\n\nIHECRE00000047.3\nBLUEPRINT\nSingle donor\nhematopoietic cell\nunknown\nfemale\n0.769686\n-0.876301\n0.769686\n-0.876301\nNaN\nNaN\n1\n1\n\n\nIHECRE00000067.3\nBLUEPRINT\nSingle donor\nhematopoietic cell\nunknown\nmale\n0.964230\n0.620305\n0.964230\n0.620305\nNaN\nNaN\n1\n1\n\n\nIHECRE00000069.3\nBLUEPRINT\nSingle donor\nhematopoietic cell\nunknown\nmale\n0.978939\n0.398611\n0.978939\n0.398611\nNaN\nNaN\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nIHECRE00004904.1\nNIH Roadmap Epigenomics\nSingle donor\nneural progenitor cell\nunknown\nfemale\n0.679335\n-0.536291\n0.679335\n-0.536291\n0.013960\n0.113480\n2\n2\n\n\nIHECRE00004908.1\nNIH Roadmap Epigenomics\nSingle donor\nplacenta\nunknown\nfemale\n0.590977\n-0.458150\n0.590977\n-0.458150\nNaN\nNaN\n1\n1\n\n\nmixed\n0.893086\n-0.682532\n0.893086\n-0.682532\nNaN\nNaN\n1\n1\n\n\nIHECRE00004910.1\nNIH Roadmap Epigenomics\nSingle donor\nneural progenitor cell\nunknown\nfemale\n0.753894\n-0.288177\n0.753894\n-0.288177\nNaN\nNaN\n1\n1\n\n\nmale\n0.956916\n0.402645\n0.983646\n-0.383976\n0.065293\n1.434576\n5\n5\n\n\n\n\n409 rows × 8 columns\n\n\n\n\n\n\n\n\nThis section generates a similar table to what was used to determine which EpiRR sex labels might be mistaken. It aggregates results from the 10 cross-validation classifiers.\n\n\nCode\ncross_val_analysis = concat_results_10fold.merge(chrY_df, left_index=True, right_on=\"md5sum\", suffixes=(\"\", \"_DROP\"))  # type: ignore\ncross_val_analysis.drop(\n    columns=[c for c in cross_val_analysis.columns if c.endswith(\"_DROP\")], inplace=True\n)\n\n\n\n\nCode\nindex_cols = [\n    \"EpiRR\",\n    \"project\",\n    \"harmonized_donor_type\",\n    CELL_TYPE,\n    SEX,\n    \"Predicted class\",\n]\nval_cols = [\"Max pred\", \"chrY_zscore_vs_assay_track\"]\n\n# not directly used in full mislabel analysis\nto_drop = [\n    \"N_chrY_zscore_vs_assay_w_track_exclusion\",\n    \"chrY_zscore_vs_assay_w_track_exclusion\",\n]\ncross_val_analysis_track = cross_val_analysis.drop(to_drop, axis=1)\n\npivot_table = cross_val_analysis_track.pivot_table(\n    index=index_cols,\n    values=val_cols,\n    aggfunc=[\"mean\", \"median\", \"std\", \"count\"],\n)\n\ndisplay(pivot_table.head())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nmedian\nstd\ncount\n\n\n\n\n\n\n\n\nMax pred\nchrY_zscore_vs_assay_track\nMax pred\nchrY_zscore_vs_assay_track\nMax pred\nchrY_zscore_vs_assay_track\nMax pred\nchrY_zscore_vs_assay_track\n\n\nEpiRR\nproject\nharmonized_donor_type\nharmonized_sample_ontology_intermediate\nharmonized_donor_sex\nPredicted class\n\n\n\n\n\n\n\n\n\n\n\n\nIHECRE00000001.4\nCEEHRC\nSingle donor\nepithelial cell derived cell line\nfemale\nfemale\n0.964847\n-0.649520\n0.988930\n-0.677161\n0.063663\n0.405152\n20\n20\n\n\nmixed\n0.610971\n-0.378015\n0.570102\n-0.568708\n0.137748\n0.653722\n3\n3\n\n\nIHECRE00000002.3\nBLUEPRINT\nSingle donor\nmyeloid cell\nfemale\nfemale\n0.963702\n-0.127269\n0.996254\n-0.554242\n0.096154\n1.082720\n20\n20\n\n\nIHECRE00000004.3\nBLUEPRINT\nSingle donor\nneutrophil\nfemale\nfemale\n0.833395\n-0.040080\n0.900267\n-0.542614\n0.177544\n1.395165\n18\n18\n\n\nmale\n0.651440\n0.404992\n0.651231\n-0.627901\n0.115929\n2.277509\n5\n5\n\n\n\n\n\n\n\n\n\n\n\nDefine function zscore_merged_assays that computes the chrY average signal metric.\n\n\nCode\ndef zscore_merged_assays(\n    zscore_df: pd.DataFrame,\n    sex_mislabels: Dict[str, str],\n    name: str | None = None,\n    logdir: Path | None = None,\n    min_pred: float | None = None,\n    no_rna: bool = False,\n) -&gt; None:\n    \"\"\"Male vs Female z-score distribution for merged assays, excluding wgbs.\n\n    Does not include pval and raw tracks.\n\n    Highlights mislabels in the plot.\n\n    Args:\n        zscore_df (pd.DataFrame): The dataframe with z-score data.\n        sex_mislabels (Dict[str, str]): {EpiRR_no-v: corrected_sex_label}\n        logdir (Path): The directory path to save the output plots. If None, only display the plot.\n        name (str): The base name for the output plot files.\n        min_pred (float|None): Minimum prediction value to include in the plot. Used on average EpiRR 'Max pred' values.\n        no_rna (bool): Whether to exclude rna_seq from the plot.\n    \"\"\"\n    zscore_df = zscore_df.copy(deep=True)\n\n    # Remove pval/raw tracks + rna unstranded\n    zscore_df = zscore_df[~zscore_df[\"track_type\"].isin([\"pval\", \"raw\", \"Unique_raw\"])]\n\n    # Merge rna protocols\n    zscore_df.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n\n    # wgbs reverses male/female chrY tendency, so removed here\n    zscore_df = zscore_df[~zscore_df[ASSAY].str.contains(\"wgb\")]\n\n    if no_rna:\n        zscore_df = zscore_df[~zscore_df[ASSAY].str.contains(\"rna\")]\n\n    N_assays = len(zscore_df[ASSAY].unique())\n    print(\n        f\"Average chrY z-score values computed from:\\n{zscore_df[ASSAY].value_counts(dropna=False)}\"\n    )\n\n    # Average chrY z-score values\n    metric_label = \"chrY_zscore_vs_assay_w_track_exclusion\"\n    zscore_df = zscore_df[zscore_df[metric_label] != \"NA\"]\n    mean_chrY_values_df = zscore_df.groupby([\"EpiRR\", SEX]).agg(\n        {metric_label: \"mean\", \"Max pred\": \"mean\"}\n    )\n    mean_chrY_values_df.reset_index(inplace=True)\n    if not mean_chrY_values_df[\"EpiRR\"].is_unique:\n        raise ValueError(\"EpiRR is not unique.\")\n\n    # Filter out low prediction values\n    if min_pred is not None:\n        mean_chrY_values_df = mean_chrY_values_df[\n            mean_chrY_values_df[\"Max pred\"] &gt; min_pred\n        ]\n\n    mean_chrY_values_df.reset_index(drop=True, inplace=True)\n    mean_chrY_values_df[\"EpiRR_no_v\"] = mean_chrY_values_df[\"EpiRR\"].str.extract(\n        pat=r\"(\\w+\\d+).\\d+\"\n    )[0]\n\n    chrY_values = mean_chrY_values_df[metric_label]\n    female_idx = np.argwhere((mean_chrY_values_df[SEX] == \"female\").values).flatten()  # type: ignore\n    male_idx = np.argwhere((mean_chrY_values_df[SEX] == \"male\").values).flatten()  # type: ignore\n\n    # Mislabels\n    predicted_as_female = set(\n        epirr_no_v for epirr_no_v, label in sex_mislabels.items() if label == \"female\"\n    )\n    predicted_as_male = set(\n        epirr_no_v for epirr_no_v, label in sex_mislabels.items() if label == \"male\"\n    )\n    predicted_as_female_idx = np.argwhere(mean_chrY_values_df[\"EpiRR_no_v\"].isin(predicted_as_female).values).flatten()  # type: ignore\n    predicted_as_male_idx = np.argwhere(mean_chrY_values_df[\"EpiRR_no_v\"].isin(predicted_as_male).values).flatten()  # type: ignore\n\n    print(\n        f\"Adding mislabels to graph: {len(predicted_as_female_idx)} male-&gt;female, {len(predicted_as_male_idx)} female-&gt;male\"\n    )\n\n    # Hovertext\n    hovertext = [\n        f\"{epirr}: &lt;z-score&gt;={z_score:.3f}\"\n        for epirr, z_score in zip(\n            mean_chrY_values_df[\"EpiRR\"],\n            mean_chrY_values_df[metric_label],\n        )\n    ]\n    hovertext = np.array(hovertext)\n\n    # sanity check\n    if mean_chrY_values_df[\"EpiRR\"].nunique() != mean_chrY_values_df.shape[0]:\n        raise ValueError(\"EpiRR is not unique.\")\n\n    # Create figure\n    fig = go.Figure()\n    fig.add_trace(\n        go.Box(\n            name=\"Female\",\n            x=[0] * len(female_idx),\n            y=chrY_values[female_idx],  # type: ignore\n            boxmean=True,\n            boxpoints=\"all\",\n            pointpos=-2,\n            hovertemplate=\"%{text}\",\n            text=hovertext[female_idx],\n            marker=dict(size=2),\n            line=dict(width=1, color=\"black\"),\n            fillcolor=sex_colors[\"female\"],\n            showlegend=True,\n        ),\n    )\n\n    fig.add_trace(\n        go.Box(\n            name=\"Male\",\n            x=[1] * len(female_idx),\n            y=chrY_values[male_idx],  # type: ignore\n            boxmean=True,\n            boxpoints=\"all\",\n            pointpos=-2,\n            hovertemplate=\"%{text}\",\n            text=hovertext[male_idx],\n            marker=dict(size=2),\n            line=dict(width=1, color=\"black\"),\n            fillcolor=sex_colors[\"male\"],\n            showlegend=True,\n        ),\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            name=\"Male\",\n            x=[-0.5] * len(predicted_as_male_idx),\n            y=chrY_values[predicted_as_male_idx],  # type: ignore\n            mode=\"markers\",\n            marker=dict(\n                size=10, color=sex_colors[\"male\"], line=dict(width=1, color=\"black\")\n            ),\n            hovertemplate=\"%{text}\",\n            text=hovertext[predicted_as_male_idx],\n            showlegend=False,\n        ),\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            name=\"Female\",\n            x=[0.5] * len(predicted_as_female_idx),\n            y=chrY_values[predicted_as_female_idx],  # type: ignore\n            mode=\"markers\",\n            marker=dict(\n                size=10, color=sex_colors[\"female\"], line=dict(width=1, color=\"black\")\n            ),\n            hovertemplate=\"%{text}\",\n            text=hovertext[predicted_as_female_idx],\n            showlegend=False,\n        ),\n    )\n\n    fig.update_xaxes(showticklabels=False)\n\n    fig.update_yaxes(range=[-1.5, 3])\n    title = f\"z-score(mean chrY signal per file) distribution - z-scores averaged over {N_assays} assays\"\n    if min_pred is not None:\n        title += f\"&lt;br&gt;avg_maxPred&gt;{min_pred}\"\n\n    fig.update_layout(\n        title=dict(text=title, x=0.5),\n        xaxis_title=SEX,\n        yaxis_title=\"Average z-score\",\n        width=750,\n        height=750,\n    )\n\n    # Save figure\n    if logdir:\n        this_name = f\"{metric_label}_n{mean_chrY_values_df.shape[0]}\"\n        if name:\n            this_name = f\"{name}_{this_name}\"\n        fig.write_image(logdir / f\"{this_name}.svg\")\n        fig.write_image(logdir / f\"{this_name}.png\")\n        fig.write_html(logdir / f\"{this_name}.html\")\n\n    fig.show()\n\n\nGraph zscores.\n\n\nCode\nzscore_merged_assays(\n    zscore_df=cross_val_analysis,\n    sex_mislabels=create_mislabel_corrector(paper_dir)[1][SEX],\n    name=\"no_RNA\",\n    no_rna=True,\n)\n\n\nAverage chrY z-score values computed from:\nh3k27ac     1447\nh3k4me1      850\nh3k4me3      678\ninput        657\nh3k36me3     595\nh3k27me3     575\nh3k9me3      537\nName: assay_epiclass, dtype: int64\nAdding mislabels to graph: 8 male-&gt;female, 8 female-&gt;male\n\n\n                                                \n\n\nFig 2E: Distribution of the average z-score signal of epigenomes (dots) over chrY, computed on the ChIP-Seq datasets (up to 7 assays per epigenome) using the fold change track type files for female (red) and male (blue). Originally mislabeled epigenomes are shown as big dots. Boxplot elements are as in Fig. 2A.\n\n\n\nLoad official IHEC sample metadata.\n\n\nCode\nmeta_v1_2_df = pd.read_csv(\n    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.2.extended.csv\"\n)\nmeta_v1_1_df = pd.read_csv(\n    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.1.extended.csv\"\n)\n\n\nSanity check, both metadata versions have the same ‘cell line’ samples.\n\n\nCode\ncell_lines_v11 = meta_v1_1_df[meta_v1_1_df[BIOMATERIAL_TYPE] == \"cell line\"][\n    \"epirr_id_without_version\"\n].unique()\ncell_lines_v12 = meta_v1_2_df[meta_v1_2_df[BIOMATERIAL_TYPE] == \"cell line\"][\n    \"epirr_id_without_version\"\n].unique()\nassert set(cell_lines_v11) == set(cell_lines_v12)\n\n\nLoad GP-age results.\n\n\nCode\ngp_age_dir = base_data_dir / \"GP_age\"\nPathChecker.check_directory(gp_age_dir)\n\ndf_gp_age = pd.read_csv(gp_age_dir / \"life_stage_prediction.GPage.20250513.tsv\", sep=\"\\t\")\ndf_gp_age[\"graph_age\"] = df_gp_age[\"model30\"]\n\nepiclass_pred_col = \"epiclass_predicted_lifestage\"\n\ndisplay(df_gp_age.head())\n\n\n\n\n\n\n\n\n\nepirr_noV\nversion\nmodel30\nmodel71\nmodel30_imputed_data\nmodel71_imputed_data\nnbCpG_model30\nnbCpG_model71\nharmonized_donor_life_stage\nharmonized_donor_age\nharmonized_donor_age_unit\nepiclass_predicted_lifestage\ntissue\ntissue_subtype\ngraph_age\n\n\n\n\n0\nIHECRE00000001\n4\n39.356\n28.973\n71.782\n62.627\n30\n71\nunknown\nunknown\nunknown\nadult_pred\nREPR\nbreast\n39.356\n\n\n1\nIHECRE00000004\n3\n49.972\n46.109\n53.737\n49.893\n30\n70\nadult\n60-65\nyear\nadult\nIMMU\nblood-venous\n49.972\n\n\n2\nIHECRE00000008\n3\n55.681\n51.238\n40.128\n36.052\n28\n67\nadult\n50-55\nyear\nadult\nIMMU\nblood-venous\n55.681\n\n\n3\nIHECRE00000013\n3\n61.915\n64.748\n35.262\n32.777\n29\n67\nchild\n0-5\nyear\nadult_pred\nIMMU\nblood-venous\n61.915\n\n\n4\nIHECRE00000014\n2\n48.480\n49.848\n71.979\n64.951\n29\n67\nadult\n70-75\nyear\nadult\nIMMU\nbone-marrow\n48.480\n\n\n\n\n\n\n\nThe epiclass_predicted_lifestage column is manually curated, from analyzing all predictions for all samples. When an expected label is known, and epiclass predictions are inconclusive (low average max pred and/or no majority consensus), the expected label is kept.\nColumns tissue and tissue_subtype are formatting of harmonized_sample_organ_system_order_AnetaMikulasova and harmonized_sample_organ_order_AnetaMikulasova.\nFollowing code remaps 5 life stage classes to 3 (perinatal, pediatric, adult).\n\n\nCode\ndf_gp_age.loc[:, \"graph_age_categories\"] = df_gp_age[epiclass_pred_col].str.removesuffix(\n    \"_pred\"\n)\n\ngp_age_categories = {\n    \"adult\": \"adult\",\n    \"child\": \"pediatric\",\n    \"embryonic\": \"perinatal\",\n    \"fetal\": \"perinatal\",\n    \"newborn\": \"perinatal\",\n    \"unknown\": \"unknown\",\n}\ndf_gp_age.loc[:, \"graph_age_categories\"] = df_gp_age[\"graph_age_categories\"].map(\n    gp_age_categories\n)\ndisplay(df_gp_age[\"graph_age_categories\"].value_counts(dropna=False))\n\n\nadult        479\npediatric     54\nperinatal     53\nunknown       48\nName: graph_age_categories, dtype: int64\n\n\nHere we merge GP-age data with additional EpiAtlas metadata.\n\n\nCode\nepirr_col = \"epirr_id_without_version\"\n\nmerged_dp_age = pd.merge(\n    df_gp_age,\n    meta_v1_2_df[[epirr_col, CELL_TYPE, BIOMATERIAL_TYPE]],\n    left_on=\"epirr_noV\",\n    right_on=epirr_col,\n    how=\"left\",\n)\nmerged_dp_age.drop_duplicates(subset=[epirr_col], inplace=True)\nmerged_dp_age.drop(columns=[\"epirr_noV\"], inplace=True)\n\n\nExcluding ‘cell line’ samples from considered data.\n\n\nCode\n# Removing cell lines: life stage makes less sense\n# type: ignore\nN_before = merged_dp_age.shape[0]\nmerged_dp_age: pd.DataFrame = merged_dp_age[\n    merged_dp_age[BIOMATERIAL_TYPE] != \"cell line\"\n]\nprint(f\"Removed {N_before - merged_dp_age.shape[0]} cell lines.\")\n\n\nRemoved 29 cell lines.\n\n\nCreating the graph categories.\nWe need to categorize separately whole blood since from other tissues since GP-Age training is only made of whole blood. We keep ‘immune system’ tissues, specifically venous and umbilical blood since they match the most closely to the training data. unsure about blood marrow.\n\n\nCode\nlayer1_vals = [\"IMMU\"]\nlayer2_vals = [\"blood-umbilical-cord\", \"blood-venous\"]\n\nmerged_dp_age.loc[:, \"tissue_group\"] = [\n    \"blood\" if (val1 in layer1_vals and val2 in layer2_vals) else \"other\"\n    for val1, val2 in merged_dp_age.loc[:, [\"tissue\", \"tissue_subtype\"]].values\n]\n\n\nSanity check, no NaN present.\n\n\nCode\nimportant_cols = [\n    \"epirr_id_without_version\",\n    \"tissue_group\",\n    epiclass_pred_col,\n    \"graph_age\",\n]\nmissing_N = merged_dp_age.loc[:, important_cols].isna().sum().sum()\nif missing_N &gt; 0:\n    raise ValueError(f\"Missing values in merged_dp_age: {missing_N}\")\n\n\nDefine graphing function graph_gp_age.\n\n\nCode\ndef graph_gp_age(\n    df_gp_age: pd.DataFrame,\n    logdir: Path | None = None,\n    name: str | None = None,\n    title: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Plot the GP age predictions.\n\n    Args:\n        df_gp_age: The dataframe with GP age data.\n    \"\"\"\n    df = df_gp_age.copy(deep=True)\n\n    tissue_colors = {\"blood\": \"red\", \"other\": \"gray\"}\n\n    age_cat_label = \"graph_age_categories\"\n\n    fig = go.Figure()\n    for tissue_group in sorted(df[\"tissue_group\"].unique()):\n        sub_df = df[df[\"tissue_group\"] == tissue_group]\n        fig.add_trace(\n            go.Box(\n                name=f\"{tissue_group} (n={len(sub_df)})\",\n                x=sub_df[age_cat_label],\n                y=sub_df[\"graph_age\"],\n                boxmean=True,\n                boxpoints=\"all\",\n                hovertemplate=\"%{text}\",\n                text=[\n                    f\"{ct}: {age:.3f}\"\n                    for ct, age in zip(sub_df[CELL_TYPE], sub_df[\"graph_age\"])\n                ],\n                marker=dict(size=2, color=tissue_colors[tissue_group]),\n                showlegend=True,\n            ),\n        )\n\n    fig.update_layout(\n        title=f\"GP age predictions - Using MLP predicted labels ({title})\",\n        xaxis_title=\"Life stage\",\n        yaxis_title=\"GP-Age : Predicted age\",\n        width=750,\n        height=750,\n        boxmode=\"group\",\n    )\n\n    # Order x-axis\n    label_order = [\"perinatal\", \"pediatric\", \"adult\"]\n    axis_labels = [\n        f\"{age_cat} (n={(df[age_cat_label] == age_cat).sum()})\" for age_cat in label_order\n    ]\n\n    fig.update_xaxes(categoryorder=\"array\", categoryarray=label_order)\n    fig.update_xaxes(tickvals=[0, 1, 2], ticktext=axis_labels)\n\n    # Save figure\n    if logdir:\n        if name is None:\n            name = \"GP_age_predictions\"\n        fig.write_image(logdir / f\"{name}.svg\")\n        fig.write_image(logdir / f\"{name}.png\")\n        fig.write_html(logdir / f\"{name}.html\")\n\n    fig.show()\n\n\nDefine dataframe content function display_tissue_age_df.\n\n\nCode\ndef display_tissue_age_df(df: pd.DataFrame) -&gt; None:\n    \"\"\"Display tissue group count breakdown.\"\"\"\n    count = df.groupby([\"tissue_group\", \"graph_age_categories\"]).size()\n    print(\"Tissue group summary\")\n    print(f\"Blood: {count.loc['blood'].sum()}\")  # type: ignore\n    print(f\"Other: {count.loc['other'].sum()}\")  # type: ignore\n    print(f\"Total: {count.sum()}\")\n    print(\"Detail:\")\n    print(count)\n\n\nFor initially unknown labels, only predictions that were manually confirmed as mislabels.\n\n\nCode\n# Remove inconclusive predictions\ngp_graph_df = merged_dp_age[merged_dp_age[epiclass_pred_col] != \"unknown\"]\n\nonly_unknown_df: pd.DataFrame = gp_graph_df[gp_graph_df[LIFE_STAGE] == \"unknown\"]\ndisplay_tissue_age_df(only_unknown_df)\n\ngraph_gp_age(\n    df_gp_age=only_unknown_df,\n    name=\"GP_age_predictions_unknown_only_MLP_predicted_labels\",\n    title=\"Samples with unknown life stage\",\n)\n\n\nTissue group summary\nBlood: 82\nOther: 63\nTotal: 145\nDetail:\ntissue_group  graph_age_categories\nblood         adult                   76\n              pediatric                3\n              perinatal                3\nother         adult                   52\n              pediatric                3\n              perinatal                8\ndtype: int64\n\n\n                                                \n\n\nFig 2F: Distribution of the age prediction from GP-age for the WGBS datasets with an originally unknown life stage predicted by EpiClass (datasets related to blood biospecimen are in red, others in grey). Boxplot elements are as in Fig. 2A.\n\n\nConsidering all samples that had a conclusive epiclass prediction (or existing label and no conclusion)\n\n\nCode\ndisplay_tissue_age_df(gp_graph_df)\n\ngraph_gp_age(\n    df_gp_age=gp_graph_df,\n    name=\"GP_age_predictions_all_samples_MLP_predicted_labels\",\n    title=\"All samples (has a life stage pred + gp-age)\",\n)\n\n\nTissue group summary\nBlood: 239\nOther: 326\nTotal: 565\nDetail:\ntissue_group  graph_age_categories\nblood         adult                   201\n              pediatric                26\n              perinatal                12\nother         adult                   272\n              pediatric                25\n              perinatal                29\ndtype: int64\n\n\n                                                \n\n\nSupp. Fig. 5D: Distribution of the age prediction from GP-age for the 565 epigenomes (dots) with conclusive consensus predictions (epigenomes related to blood biospecimen are in red, others in grey). The perinatal category encompasses the original embryonic, fetal and newborn categories of metadata as they individually contain too few samples. Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5× the interquartile range.\n\n\n\n\nFor full code, since the processing is more complex, see src/python/epiclass/utils/notebooks/analyze_hdf5_vals.ipynb (permalink), particularly section “ChromScore hdf5 values”.\n\n\n\n\nFig 2G: Distribution of the average ChromScore values over the important Biospecimen classifier regions according to SHAP (blue) compared to the global distribution (pink). Statistical significance was assessed using a two-sided Welch’s t-test. Boxplot elements are as in Fig. 2A, with a violin representation on top.\n\n\n\nSee src/python/epiclass/utils/notebooks/profile_bed.ipynb (permalink) for creation of gene ontology files. The code compares important SHAP values regions of different cell types with gene gff (using the gProfiler module)\nDefine GO terms.\n\n\nCode\nselected_cell_types = [\n    \"T_cell\",\n    \"neutrophil\",\n    \"lymphocyte_of_B_lineage\",\n    \"brain\",\n    \"hepatocyte\",\n]\ngo_terms_table = [\n    \"T cell receptor complex\",\n    \"plasma membrane signaling receptor complex\",\n    \"adaptive immune response\",\n    \"receptor complex\",\n    \"secretory granule\",\n    \"secretory vesicle\",\n    \"secretory granule membrane\",\n    \"intracellular vesicle\",\n    \"immunoglobulin complex\",\n    \"immune response\",\n    \"immune system process\",\n    \"homophilic cell adhesion via plasma membrane adhesion molecules\",\n    \"DNA binding\",\n    \"cell-cell adhesion via plasma-membrane adhesion molecules\",\n    \"RNA polymerase II cis-regulatory region sequence-specific DNA binding\",\n    \"blood microparticle\",\n    \"platelet alpha granule lumen\",\n    \"fibrinogen complex\",\n    \"endoplasmic reticulum lumen\",\n]\n\n# Add some line breaks for readability\ngo_terms_graph = [\n    \"T cell receptor complex\",\n    \"plasma membrane&lt;br&gt;signaling receptor complex\",\n    \"adaptive immune response\",\n    \"receptor complex\",\n    \"secretory granule\",\n    \"secretory vesicle\",\n    \"secretory granule membrane\",\n    \"intracellular vesicle\",\n    \"immunoglobulin complex\",\n    \"immune response\",\n    \"immune system process\",\n    \"homophilic cell adhesion via&lt;br&gt;plasma membrane adhesion molecules\",\n    \"DNA binding\",\n    \"cell-cell adhesion via&lt;br&gt;plasma-membrane adhesion molecules\",\n    \"RNA polymerase II cis-regulatory&lt;br&gt;region sequence-specific DNA binding\",\n    \"blood microparticle\",\n    \"platelet alpha granule lumen\",\n    \"fibrinogen complex\",\n    \"endoplasmic reticulum lumen\",\n]\n\n\nSetup paths.\n\n\nCode\nhdf5_type = \"hg38_100kb_all_none\"\n\nSHAP_dir = base_data_dir / \"SHAP\"\n\ncell_type_shap_dir = (\n    SHAP_dir / hdf5_type / f\"{CELL_TYPE}_1l_3000n\" / \"10fold-oversampling\"\n)\nbeds_file = cell_type_shap_dir / \"select_beds_top303.tar.gz\"\nPathChecker.check_file(beds_file)\n\n\nLoad top g:profiler results for each cell type.\n\n\nCode\nall_go_dfs: Dict[str, pd.DataFrame] = {}\nwith tarfile.open(beds_file, \"r:gz\") as tar:\n    for member in tar.getmembers():\n        filename = member.name\n        if filename.endswith(\"profiler.tsv\") and \"merge_samplings\" in filename:\n            with tar.extractfile(member) as f:  # type: ignore\n                go_df = pd.read_csv(f, sep=\"\\t\", index_col=0)\n                all_go_dfs[member.name] = go_df\n\nassert len(all_go_dfs) == 16\n\n\nMerge results for all cell types into one table.\n\n\nCode\nfor name, df in all_go_dfs.items():\n    sub_df = df.copy()\n    sub_df.loc[:, \"shap_source\"] = re.match(r\".*/merge_samplings_(.*)_features_intersect_gff_gprofiler.tsv\", name).group(1)  # type: ignore\n    sub_df.loc[:, \"table_val\"] = -np.log10(sub_df.loc[:, \"p_value\"])\n    all_go_dfs[name] = sub_df\n\nfull_concat_df = pd.concat(all_go_dfs.values())\nfull_concat_df = full_concat_df.drop([\"significant\", \"query\"], axis=1)\n\nassert all(go_term in full_concat_df[\"name\"].values for go_term in go_terms_table)\n\ntable = full_concat_df.pivot_table(\n    index=\"name\", columns=\"shap_source\", values=\"table_val\", aggfunc=\"mean\"\n)\n\n\nCollect subset of table for the five selected cell types.\n\n\nCode\ntable_5_ct = table.loc[go_terms_table, selected_cell_types].copy()\nassert table_5_ct.shape == (len(go_terms_graph), len(selected_cell_types))\n\n# Rename index\ntable_5_ct = pd.DataFrame(table_5_ct, index=go_terms_graph, columns=table_5_ct.columns)\n\n\nDefine graphing function plot_go_heatmap.\n\n\nCode\ndef plot_go_heatmap(table: pd.DataFrame, width: int, height: int, title: str|None=None):\n    \"\"\"Plot a GO heatmap.\"\"\"\n    # Define colorbar\n    sigma = \"\\u03c3\"\n\n    colorbar = dict(\n        title=\"-log&lt;sub&gt;10&lt;/sub&gt;(p-value)\",\n        tickvals=[0, 1.30, 2, 3, 5, 6.53, 10],\n        ticktext=[\n            \"0\",\n            f\"1.30: p=0.05~2{sigma}\",\n            \"2: p=0.01\",\n            f\"3: p=0.001~3{sigma}\",\n            \"5\",\n            f\"6.53: p=3x10&lt;sup&gt;-7&lt;/sup&gt; = 5{sigma}\",\n            \"10\",\n        ],\n    )\n\n    # keep NaNs in for labeling\n    z = table.values\n    text = np.where(np.isnan(z), \"NS\", np.char.mod(\"%.2f\", z))\n\n    fig = go.Figure(\n        data=go.Heatmap(\n            z=np.nan_to_num(z, nan=0),  # replace NaN with 0 for coloring\n            x=table.columns,\n            y=table.index,\n            colorscale=\"Blues\",\n            zmin=0,\n            zmax=10,\n            colorbar=colorbar,\n            text=text,\n            texttemplate=\"%{text}\",\n            hovertemplate=\"GO Term: %{y}&lt;br&gt;Class: %{x}&lt;br&gt;Value: %{z:.2f}&lt;extra&gt;&lt;/extra&gt;\",\n            showscale=True,\n            xgap=2,\n            ygap=2,\n        )\n    )\n\n    fig.update_layout(\n        title_text=title,\n        width=width,\n        height=height,\n        plot_bgcolor=\"black\",\n    )\n    fig.update_xaxes(showgrid=False)\n    fig.update_yaxes(showgrid=False)\n\n    fig.show()\n\n\nGraph GO for 5 cell types.\n\n\nCode\nplot_go_heatmap(\n    table=table_5_ct,\n    width=600,\n    height=1000,\n)\n\n\n                                                \n\n\nFig 2H: Top four gene ontology (GO) terms enriched by g:Profiler[35] for genes within important Biospecimen classifier regions according to SHAP values.\n\n\nCollect the 5 most significants terms for each the 16 cell types.\n\n\nCode\n# preserve order\ntop_5_terms = []\nfor name, df in all_go_dfs.items():\n    df.sort_values(\"p_value\", inplace=True)\n    top_5 = df[\"name\"].head(5).to_list()\n    top_5_terms.extend(top_5)\n\ntop_5_terms = list(dict.fromkeys(top_5_terms))\n\n\nGraph.\n\n\nCode\ntable_top_5_GO = table.loc[top_5_terms, :].copy()\n\nplot_go_heatmap(\n    table=table_top_5_GO,\n    width=1300,\n    height=2000,\n)\n\n\n                                                \n\n\n\n\n\n\nImages extracted from Epilogos viewer, using:\n\nRegion: chr11:118300000-118400000\nView mode: Single\nDataset: IHEC\nAll biosamples\nSaliency Metric: S1\n\n\nFig. 2I: Epilogos visualization of one of the important Biospecimen classifier regions enriched in the T cell receptor complex GO term from Fig. 2H.\nSee Annex A for a more detailled Epilogos color legend.\n\n\n\nNote: The code uses the acronym CNV (Copy Number Variant/Variation) instead of CNA.\nSee CNV_treatment.ipynb (permalink) for the creation of the CNA stats. The following figures represent the Z-scores of the signal within the top SHAP features (N=336) vs 200 random feature sets of same size, for signatures created using patients samples with cancer types similar to EpiAtlas content (see paper Methods).\n\nLoad z-scores.\n\n\nCode\ncnv_dir = base_data_dir / \"CNV\"\ncnv_intersection_results = (\n    cnv_dir\n    / \"signature_analysis\"\n    / \"epiatlas_cancer_types\"\n    / \"important_cancer_features_z_scores_vs_random200.tsv\"\n)\nPathChecker.check_file(cnv_intersection_results)\n\ncnv_df = pd.read_csv(cnv_intersection_results, sep=\"\\t\", index_col=0)\ncnv_df.name = cnv_intersection_results.stem\n\n\nDefine graphing function plot_cnv_zscores, which uses the CNA groups defined by Signatures of copy number alterations in human cancer. See Supplementary Table 8 for details.\n\n\nCode\ndef plot_cnv_zscores(cnv_df: pd.DataFrame, logdir: Path | None = None) -&gt; None:\n    \"\"\"Plot z-scores of top SHAP features vs random feature sets, grouped by CNV groups.\n\n    Args:\n        cnv_df: The DataFrame with z-scores.\n        logdir: The output directory to save the plot.\n    \"\"\"\n    n_beds = int(cnv_df.name.split(\"random\")[1])\n    signature_subset_name = \"EpiATLAS cancer types\"\n\n    CN_groups = [\n        [f\"CN{i}\" for i in range(1, 4)],\n        [f\"CN{i}\" for i in range(9, 13)],\n        [f\"CN{i}\" for i in range(13, 17)],\n        [f\"CN{i}\" for i in range(17, 18)],\n        [f\"CN{i}\" for i in range(18, 22)],\n        [f\"CN{i}\" for i in range(4, 9)],\n    ]\n    CN_names = [\n        \"CN1-CN3\",\n        \"CN9-CN12\",\n        \"CN13-CN16\",\n        \"CN17\",\n        \"CN18-CN21\",\n        \"CN4-CN8\",\n    ]\n\n    # Assign groups to the DataFrame\n    cnv_df[\"group\"] = \"Other\"\n    for i, group in enumerate(CN_groups):\n        cnv_df.loc[cnv_df.index.isin(group), \"group\"] = CN_names[i]\n\n    # Sort groups\n    group_medians = (\n        cnv_df.groupby(\"group\")[\"z_score\"].median().sort_values(ascending=False)\n    )\n    sorted_CN_names = group_medians.index.tolist()\n\n    # Create the figure\n    fig = go.Figure()\n\n    for group in sorted_CN_names:\n        group_data = cnv_df[cnv_df[\"group\"] == group]\n        marker_size = 4 if group != \"CN17\" else 6\n\n        # Add the box plot without points\n        fig.add_trace(\n            go.Box(\n                y=group_data[\"z_score\"],\n                name=group,\n                boxmean=True,\n                boxpoints=False,  # Don't show points in the box plot\n                line=dict(color=\"black\"),\n                fillcolor=\"rgba(255,255,255,0)\",\n                showlegend=False,\n            )\n        )\n\n        # Add scatter plot for individual points\n        fig.add_trace(\n            go.Scatter(\n                x=[group] * len(group_data),\n                y=group_data[\"z_score\"],\n                mode=\"markers\",\n                marker=dict(\n                    color=\"red\",\n                    size=marker_size,\n                ),\n                name=group,\n                showlegend=False,\n                text=group_data.index,  # Use CN names as hover text\n                hoverinfo=\"text+y\",  # Show CN name and y-value on hover\n            )\n        )\n    # Update layout\n    fig.update_layout(\n        xaxis_title=\"CNA Group\",\n        yaxis_title=\"Z-score\",\n        **main_title_settings\n    )\n\n    # Add a horizontal line at y=0 for reference\n    fig.add_hline(y=0, line_color=\"grey\", line_width=1)\n\n    # Show and save the figure\n    if logdir:\n        name = \"important_cancer_features_z_scores_boxplot\"\n        fig.write_image(logdir / f\"{name}.png\")\n        fig.write_image(logdir / f\"{name}.svg\")\n        fig.write_html(logdir / f\"{name}.html\")\n\n    fig.show()\n\n\nGraph.\n\n\nCode\nplot_cnv_zscores(cnv_df)\n\n\n                                                \n\n\nDefine the graphing function plot_cnv_zscores_alt, which groups CNA by focal vs chromosomal events.\n\n\nCode\ndef plot_cnv_zscores_alt(cnv_df: pd.DataFrame, logdir: Path | None = None) -&gt; None:\n    \"\"\"Plot z-scores of top SHAP features vs random feature sets, grouped by focal vs chromosomal events.\n\n    Args:\n        cnv_df: The DataFrame with z-scores.\n        logdir: The output directory to save the plot.\n    \"\"\"\n    n_beds = int(cnv_df.name.split(\"random\")[1])\n    signature_subset_name = \"EpiATLAS cancer types\"\n\n    # Assign groups\n    CN_groups = [\n        [f\"CN{i}\" for i in range(1, 9)] + [f\"CN{i}\" for i in range(13, 17)],\n        [f\"CN{i}\" for i in range(9, 13)] + [f\"CN{i}\" for i in range(17, 22)],\n    ]\n    CN_names = [\n        \"Chromosomal events (CN1-CN8, CN13-CN16)\",\n        \"Focal events (CN9-CN12, CN17-CN21)\",\n    ]\n\n    # Assign groups to the DataFrame\n    cnv_df[\"group\"] = \"Other\"\n    for i, group in enumerate(CN_groups):\n        cnv_df.loc[cnv_df.index.isin(group), \"group\"] = CN_names[i]\n\n    # Sort groups\n    group_medians = (\n        cnv_df.groupby(\"group\")[\"z_score\"].median().sort_values(ascending=False)\n    )\n    sorted_CN_names = group_medians.index.tolist()\n\n    # Create the figure\n    fig = go.Figure()\n\n    for group in sorted_CN_names:\n        group_data = cnv_df[cnv_df[\"group\"] == group]\n        hover_text = [\n            f\"{CN_name}: Z={val:.3f}\"\n            for CN_name, val in zip(group_data.index, group_data[\"z_score\"])\n        ]\n\n        # Add the box plot without points\n        fig.add_trace(\n            go.Box(\n                y=group_data[\"z_score\"],\n                name=group,\n                boxmean=True,\n                boxpoints=\"all\",\n                line=dict(color=\"black\"),\n                fillcolor=\"rgba(255,255,255,0)\",\n                showlegend=False,\n                hoverinfo=\"text\",  # Show CN name and y-value on hover\n                hovertext=hover_text,\n            )\n        )\n\n    # Update layout\n    fig.update_layout(\n        xaxis_title=\"Copy Number Alteration Event Type\",\n        yaxis_title=\"CNA Enrichment (Z-score)\",\n        **main_title_settings\n    )\n\n    # Add a horizontal line at y=0 for reference\n    fig.add_hline(y=0, line_color=\"grey\", line_width=1)\n\n    # Show and save the figure\n    if logdir:\n        name = \"important_cancer_features_z_scores_boxplot_V2\"\n        fig.write_image(logdir / f\"{name}.png\")\n        fig.write_image(logdir / f\"{name}.svg\")\n        fig.write_html(logdir / f\"{name}.html\")\n\n    fig.show()\n\n\nGraph.\n\n\nCode\nplot_cnv_zscores_alt(cnv_df)\n\n\n                                                \n\n\nFig. 2J: Distribution of the z-score enrichment for copy number (CN) signatures (dots) in genomic regions identified as important by the Cancer status classifier compared to random control regions. CN signatures are grouped as being mostly associated with focal changes (CN9-12,17-21) or chromosomal ones (CN1-8,13-16). Boxplot elements are as in Fig 2A.\n\n\n\n\nSee Supplementary Figures page (separate to limit maximum website page size and loading times).\n\n\n\n\n\nbiv = bivalent\nTx = Transcription\nRepr = Repressed\nPC = Polycomb"
  },
  {
    "objectID": "figs/fig2.html#setup-code---imports-and-co.",
    "href": "figs/fig2.html#setup-code---imports-and-co.",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata",
    "section": "",
    "text": "Setup imports.\n\n\nCode\nfrom __future__ import annotations\n\nimport itertools\nimport re\nimport tarfile\nfrom pathlib import Path\nfrom typing import Dict, List, Sequence\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom IPython.display import display\nfrom plotly.subplots import make_subplots\nfrom scipy.stats import zscore\n\nfrom epiclass.utils.notebooks.paper.paper_utilities import (\n    ASSAY,\n    ASSAY_MERGE_DICT,\n    ASSAY_ORDER,\n    BIOMATERIAL_TYPE,\n    CELL_TYPE,\n    LIFE_STAGE,\n    SEX,\n    IHECColorMap,\n    MetadataHandler,\n    SplitResultsHandler,\n    create_mislabel_corrector,\n    PathChecker\n)\n\nCORE7_ASSAYS = ASSAY_ORDER[0:7]\n\n\nSetup paths.\n\n\nCode\n# Root path\nbase_dir = Path.home() / \"Projects/epiclass/output/paper\"\nPathChecker.check_directory(base_dir)\n\n# More precise\nbase_data_dir = base_dir / \"data\"\nbase_fig_dir = base_dir / \"figures\"\ntable_dir = base_dir / \"tables\"\nmetadata_dir = base_data_dir / \"metadata\"\n\nofficial_metadata_dir = metadata_dir / \"epiatlas\" / \"official\"\nPathChecker.check_directory(official_metadata_dir)\n\n# alias\npaper_dir = base_dir\n\n\nSetup colors.\n\n\nCode\nIHECColorMap = IHECColorMap(base_fig_dir)\nassay_colors = IHECColorMap.assay_color_map\ncell_type_colors = IHECColorMap.cell_type_color_map\nsex_colors = IHECColorMap.sex_color_map\n\n\nSetup metadata and prediction files handlers.\n\n\nCode\nsplit_results_handler = SplitResultsHandler()\n\nmetadata_handler = MetadataHandler(paper_dir)\nmetadata_v2 = metadata_handler.load_metadata(\"v2\")\nmetadata_v2_df = metadata_v2.to_df()\n\n\nSetup figures general settings.\n\n\nCode\nmain_title_settings = {\n    \"title\":dict(\n        automargin=True,\n        x=0.5,\n        xanchor=\"center\",\n        yanchor=\"top\",\n        y=0.98\n        ),\n    \"margin\":dict(t=50, l=10, r=10)\n}"
  },
  {
    "objectID": "figs/fig2.html#figure-2",
    "href": "figs/fig2.html#figure-2",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata",
    "section": "",
    "text": "2A: Accuracy\n2B: F1-Score\nSupp 5A: AUC scores\n\nDefine function that graphs performance accross classification tasks.\n\n\nCode\ndef NN_performance_across_classification_tasks(\n    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n    name: str | None = None,\n    logdir: Path | None = None,\n    exclude_categories: List[str] | None = None,\n    y_range: List[float] | None = None,\n    sort_by_acc: bool = False,\n    metric_names: Sequence[str] = (\"Accuracy\", \"F1_macro\"),\n    title: str | None = None,\n) -&gt; List[str]:\n    \"\"\"Render box plots of metrics per classifier and split, each in its own subplot.\n\n    This function generates a figure with subplots, each representing a different\n    metric. Each subplot contains box plots for each classifier, ordered by accuracy.\n\n    Args:\n        split_metrics: A nested dictionary with structure {split: {classifier: {metric: score}}}.\n        logdir: The directory path to save the output plots. If None, only display the plot.\n        name: The base name for the output plot files.\n        exclude_categories: Task categories to exclude from the plot.\n        y_range: The y-axis range for the plots.\n        sort_by_acc: Whether to sort the classifiers by accuracy.\n        metric_names: The metrics to include in the plot.\n\n    Returns:\n        The list of classifier names in the order they appear in the plot.\n    \"\"\"\n    # Exclude some categories\n    classifier_names = list(split_metrics[\"split0\"].keys())\n    if exclude_categories is not None:\n        for category in exclude_categories:\n            classifier_names = [c for c in classifier_names if category not in c]\n\n    available_metrics = list(split_metrics[\"split0\"][classifier_names[0]].keys())\n    try:\n        available_metrics.remove(\"count\")\n    except ValueError:\n        pass\n    if any(metric not in available_metrics for metric in metric_names):\n        raise ValueError(f\"Invalid metric. Metrics need to be in {available_metrics}\")\n\n    # Get classifier counts\n    classifiers_N = split_results_handler.extract_count_from_metrics(split_metrics)\n\n    # Sort classifiers by accuracy\n    if sort_by_acc:\n        mean_acc = {}\n        for classifier in classifier_names:\n            mean_acc[classifier] = np.mean(\n                [split_metrics[split][classifier][\"Accuracy\"] for split in split_metrics]\n            )\n        classifier_names = sorted(\n            classifier_names, key=lambda x: mean_acc[x], reverse=True\n        )\n\n    # Create subplots, one column for each metric\n    fig = make_subplots(\n        rows=1,\n        cols=len(metric_names),\n        subplot_titles=metric_names,\n        horizontal_spacing=0.03,\n    )\n\n    color_group = px.colors.qualitative.Plotly\n    colors = {\n        classifier: color_group[i % len(color_group)]\n        for i, classifier in enumerate(classifier_names)\n    }\n\n    point_pos = 0\n    for i, metric in enumerate(metric_names):\n        for classifier_name in classifier_names:\n            values = [\n                split_metrics[split][classifier_name][metric] for split in split_metrics\n            ]\n\n            fig.add_trace(\n                go.Box(\n                    y=values,\n                    name=f\"{classifier_name} (N={classifiers_N[classifier_name]})\",\n                    fillcolor=colors[classifier_name],\n                    line=dict(color=\"black\", width=1.5),\n                    marker=dict(size=3, color=\"white\", line_width=1),\n                    boxmean=True,\n                    boxpoints=\"all\",\n                    pointpos=point_pos,\n                    showlegend=i == 0,  # Only show legend in the first subplot\n                    hovertemplate=\"%{text}\",\n                    text=[\n                        f\"{split}: {value:.4f}\"\n                        for split, value in zip(split_metrics, values)\n                    ],\n                    legendgroup=classifier_name,\n                    width=0.5,\n                ),\n                row=1,\n                col=i + 1,\n            )\n\n    # Title\n    title_text = (\n        \"Neural network classification - Metric distribution for 10-fold cross-validation\"\n    )\n    if title:\n        title_text = title\n    fig.update_layout(title_text=title_text, **main_title_settings)\n\n    # Layout\n    fig.update_layout(\n        yaxis_title=\"Value\",\n        boxmode=\"group\",\n        height=1200 * 0.8,\n        width=1750 * 0.8,\n    )\n\n    # Acc, F1\n    fig.update_layout(yaxis=dict(range=[0.88, 1.001]))\n    fig.update_layout(yaxis2=dict(range=[0.80, 1.001]))\n\n    # AUC\n    range_auc = [0.986, 1.0001]\n    fig.update_layout(yaxis3=dict(range=range_auc))\n    fig.update_layout(yaxis4=dict(range=range_auc))\n\n    if y_range is not None:\n        fig.update_yaxes(range=y_range)\n\n    # Save figure\n    if logdir:\n        if name is None:\n            name = \"MLP_metrics_various_tasks\"\n        fig.write_image(logdir / f\"{name}.svg\")\n        fig.write_image(logdir / f\"{name}.png\")\n        fig.write_html(logdir / f\"{name}.html\")\n\n    fig.show()\n\n    return classifier_names\n\n\nCompute metrics.\n\n\nCode\nexclude_categories = [\"track_type\", \"group\", \"disease\", \"PE\", \"martin\"]\n# exclude_categories = [\"track_type\", \"group\", \"disease\"]\nexclude_names = [\"chip-seq\", \"7c\", \"16ct\", \"no-mixed\"]\n\nhdf5_type = \"hg38_100kb_all_none\"\nresults_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / hdf5_type\nPathChecker.check_directory(results_dir)\n\nmislabel_correction = True\nif mislabel_correction:\n    mislabel_corrector = create_mislabel_corrector(paper_dir)\nelse:\n    mislabel_corrector = None\n\nsplit_results_metrics, all_split_results = split_results_handler.general_split_metrics(\n    results_dir,\n    merge_assays=True,\n    exclude_categories=exclude_categories,\n    exclude_names=exclude_names,\n    return_type=\"both\",\n    oversampled_only=True,\n    mislabel_corrections=mislabel_corrector,\n    verbose=False,\n)\n\n\nGraph metrics.\n\n\nCode\nmetrics_full = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\nfig_name = f\"{hdf5_type}_perf_across_categories_full\"\nsorted_task_names = NN_performance_across_classification_tasks(\n    split_results_metrics,  # type: ignore\n    sort_by_acc=True,\n    metric_names=metrics_full,\n)\n\n\n                                                \n\n\nFig. 2A,B: Distribution of accuracy and F1-score evaluated per training fold (dots) for each metadata classifier. Performance metrics are reported without applying a prediction score threshold. Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5× the interquartile range.\n\n\n\nDefine function compute_class_imbalance.\n\n\nCode\ndef compute_class_imbalance(\n    all_split_results: Dict[str, Dict[str, pd.DataFrame]],\n) -&gt; pd.DataFrame:\n    \"\"\"Compute class imbalance for each task and split.\n\n    Args:\n        all_split_results: A dictionary with structure {task_name: {split_name: split_results_df}}.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the following columns:\n            - avg(balance_ratio): The average balance ratio for each task.\n            - n: The number of classes for each task (used for the average).\n    \"\"\"\n    # combine md5 lists\n    task_md5s = {\n        classifier_task: [split_df.index for split_df in split_results.values()]\n        for classifier_task, split_results in all_split_results.items()\n    }\n    task_md5s = {\n        classifier_task: [list(split_md5s) for split_md5s in md5s]\n        for classifier_task, md5s in task_md5s.items()\n    }\n    task_md5s = {\n        classifier_task: list(itertools.chain(*md5s))\n        for classifier_task, md5s in task_md5s.items()\n    }\n\n    # get metadata\n    metadata_df = metadata_handler.load_metadata_df(\"v2-encode\")\n\n    label_counts = {}\n    for classifier_task, md5s in task_md5s.items():\n        try:\n            label_counts[classifier_task] = metadata_df.loc[md5s][\n                classifier_task\n            ].value_counts()\n        except KeyError as e:\n            category_name = classifier_task.rsplit(\"_\", maxsplit=1)[0]\n            try:\n                label_counts[classifier_task] = metadata_df.loc[md5s][\n                    category_name\n                ].value_counts()\n            except KeyError as e:\n                raise e\n\n    # Compute Shannon Entropy\n    class_balance = {}\n    for classifier_task, counts in label_counts.items():\n        total_count = counts.sum()\n        k = len(counts)\n        p_x = counts / total_count  # class proportions\n        p_x = p_x.values\n        shannon_entropy = -np.sum(p_x * np.log2(p_x))\n        balance = shannon_entropy / np.log2(k)\n        class_balance[classifier_task] = (balance, k)\n\n    df_class_balance = pd.DataFrame.from_dict(\n        class_balance, orient=\"index\", columns=[\"Normalized Shannon Entropy\", \"k\"]\n    ).sort_index()\n\n    return df_class_balance\n\n\nCompute class imbalance (Shannon entropy).\n\n\nCode\nsubset = {\n    k: v\n    for k, v in all_split_results.items()  # type: ignore\n    if not any(label in k for label in [\"martin\", \"PE\"])\n}\ndf_class_balance = compute_class_imbalance(subset)  # type: ignore\n\n\nDefine graphing function plot_shannon_entropy.\n\n\nCode\ndef plot_shannon_entropy(class_balance_df: pd.DataFrame, ordered_task_names: List[str]|None) -&gt; None:\n    \"\"\"Graph Shannon entropy values, in the order given by task_names\"\"\"\n    df = class_balance_df.copy()\n\n    # Reorder df\n    task_names = df.index\n    if ordered_task_names:\n        task_names = [\n            task_name for task_name in task_names if task_name in ordered_task_names\n    ]\n    df = df.loc[sorted_task_names]\n\n    # plot\n    fig = px.scatter(\n        df,\n        x=df.index,\n        y=\"Normalized Shannon Entropy\",\n        labels={\n            \"k\": \"Number of classes\",\n            \"Normalized Shannon Entropy\": \"Normalized Shannon Entropy\",\n        },\n        title=\"Class imbalance across tasks (higher is more balanced)\",\n    )\n    fig.update_layout(\n        yaxis=dict(range=[0, 1]),\n        xaxis_title=None,\n    )\n\n    fig.show()\n\n\nGraph.\n\n\nCode\nplot_shannon_entropy(\n    class_balance_df=df_class_balance,\n    ordered_task_names=sorted_task_names,\n    )\n\n\n                                                \n\n\nFig. 2C: Shannon entropy scores for each metadata category.\n\n\n\nIHEC_sample_metadata_harmonization.v1.1.extended.csv contains 314 EpiRRs with unknown sex. We applied a fully trained sex classifier on those.\nTo properly create sex classification related graphs (D,E) we need\n- Classifier predictions on samples with unknown sex label\n- Metadata pre/post correction\n- Average ChrY signal for each file\nLoad v1.1 and v1.2 official metadata.\n\n\nCode\nmetadata_v1_1_path = (\n    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.1.extended.csv\"\n)\nmetadata_v1_1 = pd.read_csv(metadata_v1_1_path, index_col=0)\n\nmetadata_v1_2_path = (\n    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.2.extended.csv\"\n)\nmetadata_v1_2 = pd.read_csv(metadata_v1_2_path, index_col=0)\n\n\nSanity check: make sure that the number of unknown labels is the same in our metadata VS official v1.1\n\n\nCode\nfull_metadata_df = metadata_v2_df\nfull_metadata_df[\"md5sum\"] = full_metadata_df.index\nassert (\n    metadata_v2_df[metadata_v2_df[SEX].isin([\"unknown\"])][\"EpiRR\"].nunique()\n    == metadata_v1_1[metadata_v1_1[SEX] == \"unknown\"].index.nunique()\n    == 314\n)\n\n\nLoad predictions for unknown sex samples.\n\n\nCode\nsex_results_dir = (\n    base_data_dir\n    / \"training_results\"\n    / \"dfreeze_v2\"\n    / \"hg38_100kb_all_none\"\n    / f\"{SEX}_1l_3000n\"\n)\nsex_full_model_dir = sex_results_dir / \"complete_no_valid_oversample\"\nPathChecker.check_directory(sex_full_model_dir)\n\npred_unknown_file_path = (\n    sex_full_model_dir\n    / \"predictions\"\n    / \"complete_no_valid_oversample_test_prediction_100kb_all_none_dfreeze_v2.1_sex_mixed_unknown.csv\"\n)\npred_unknown_df = pd.read_csv(pred_unknown_file_path, index_col=0, header=0)\n\n\nJoin metadata to predictions.\n\n\nCode\npred_unknown_df = pred_unknown_df[pred_unknown_df[\"True class\"] == \"unknown\"]\npred_unknown_df = split_results_handler.add_max_pred(pred_unknown_df)  # type: ignore\npred_unknown_df = metadata_handler.join_metadata(pred_unknown_df, metadata_v2)\npred_unknown_df[\"md5sum\"] = pred_unknown_df.index\n\n\nLoad 10-fold cross validation results.\n\n\nCode\nsex_10fold_dir = sex_results_dir / \"10fold-oversampling\"\nPathChecker.check_directory(sex_10fold_dir)\n\nsplit_results: Dict[str, pd.DataFrame] = split_results_handler.read_split_results(\n    sex_10fold_dir\n)\nconcat_results_10fold: pd.DataFrame = split_results_handler.concatenate_split_results(split_results, depth=1)  # type: ignore\nconcat_results_10fold = split_results_handler.add_max_pred(concat_results_10fold)\nconcat_results_10fold = metadata_handler.join_metadata(concat_results_10fold, metadata_v2)\n\n\n\n\nFig. 2D: Proportion of donor sex metadata originally annotated (inner circle, metadata v1.1) and predicted with high-confidence (outer circle, metadata v2.0) for female (red) and male (blue) (mixed sex not shown).\nCompute values for the inner portion of the pie chart.\n\n\nCode\n# Proportion of unknown, excluding mixed. same as v1.1 ihec metadata\nno_mixed = full_metadata_df[full_metadata_df[SEX] != \"mixed\"]\n\nwith pd.option_context(\"display.float_format\", \"{:.2%}\".format):\n    print(\"file-wise:\")\n    print(no_mixed[SEX].value_counts(dropna=False) / no_mixed.shape[0])\n\n    print(\"\\nEpiRR-wise:\")\n    epirr_no_mixed = no_mixed.drop_duplicates(subset=[\"EpiRR\"])\n    print(epirr_no_mixed[SEX].value_counts(dropna=False) / epirr_no_mixed.shape[0])\n\n\nfile-wise:\nfemale    44.88%\nmale      42.41%\nunknown   12.71%\nName: harmonized_donor_sex, dtype: float64\n\nEpiRR-wise:\nmale      45.14%\nfemale    40.60%\nunknown   14.26%\nName: harmonized_donor_sex, dtype: float64\n\n\n\n\n\nOuter ring represents SEX metadata labnels v1.2 (without mixed labels), which had those modifications:\n\nSome unknown SEX files were labelled, using (assay,track type) z-score in conjunction with fully trained model predictions.\nCorrection of some mislabels, using 10fold cross-validation results\n\n\n\nCode\nmeta_v1_2_no_mixed = metadata_v1_2[metadata_v1_2[SEX] != \"mixed\"]\nwith pd.option_context(\"display.float_format\", \"{:.2%}\".format):\n    print(\"EpiRR-wise:\")\n    print(meta_v1_2_no_mixed.value_counts(SEX) / meta_v1_2_no_mixed.shape[0])\n\n\nEpiRR-wise:\nharmonized_donor_sex\nmale      51.42%\nfemale    46.54%\nunknown    2.04%\ndtype: float64\n\n\n\n\n\nWork done in another script:\n\nFor each bigwig file, the chrY average value is computed with the pyBigWig module, in chrY_bigwig_mean.py (permalink).\nFor each assay, the z-score distribution (of the mean chrY value) of the file group is computed.\n\nOutputs chrXY_all.csv\nOutputs chrY_zscores.csv\n\n\nFig. 2E is made by averaging for each EpiRR the z-score value in each assay distribution.\nThis data is needed for the full sex mislabel context table.\n\nDefine function compute_chrY_zscores.\n\n\nCode\ndef compute_chrY_zscores(\n    chrY_dir: Path, version: str, save: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"Compute z-scores for chrY signal data.\n\n    Computes two distributions of z-scores:\n    1) Per assay group, excluding raw, pval, and Unique_raw tracks.\n    2) Per assay+track group.\n\n    In both cases, rna-seq/mrna-seq and wgbs-standard/wgbs-pbat are put as one assay.\n\n    Args:\n        chrY_dir: The directory containing the chrY signal data.\n        version: The metadata version to use.\n        save: Whether to save the results.\n\n    Returns:\n        pd.DataFrame: The chrY signal data with z-scores appended.\n    \"\"\"\n    output_dir: Path = chrY_dir\n    if save:\n        output_dir = chrY_dir / f\"dfreeze_{version}_stats\"\n        output_dir.mkdir(parents=False, exist_ok=True)\n\n    # Get chrY signal data\n    chrY_df = pd.read_csv(chrY_dir / \"chrXY_all.csv\", header=0)\n\n    # Filter out md5s not in metadata version\n    metadata = MetadataHandler(paper_dir).load_metadata(version)\n    md5s = set(metadata.md5s)\n    chrY_df = chrY_df[chrY_df[\"filename\"].isin(md5s)]\n\n    # Make sure all values are non-zero\n    if not (chrY_df[\"chrY\"] != 0).all():\n        raise ValueError(\"Some chrY values are zero.\")\n\n    # Merge metadata\n    metadata_df = pd.DataFrame.from_records(list(metadata.datasets))\n    metadata_df.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n    chrY_df = chrY_df.merge(\n        metadata_df, left_on=\"filename\", right_on=\"md5sum\"\n    )\n\n    # Compute stats for distributions\n    metric_name_1 = \"chrY_zscore_vs_assay_w_track_exclusion\"\n    metric_name_2 = \"chrY_zscore_vs_assay_track\"\n    files1 = chrY_df[\n        ~chrY_df[\"track_type\"].isin([\"raw\", \"pval\", \"Unique_raw\"])\n    ]\n    files2 = chrY_df\n    dist1 = files1.groupby(ASSAY).agg({\"chrY\": [\"mean\", \"std\", \"count\"]})\n    dist2 = files2.groupby([ASSAY, \"track_type\"]).agg({\"chrY\": [\"mean\", \"std\", \"count\"]})\n    if save:\n        dist1.to_csv(output_dir / \"chrY_stats_assay_w_track_exclusion.csv\")\n        dist2.to_csv(output_dir / \"chrY_stats_assay_and_track.csv\")\n\n    # Compute full z-score distributions\n    for groups in files1.groupby(ASSAY):\n        _, group_df = groups\n        group_df[\"zscore\"] = zscore(group_df[\"chrY\"])\n        chrY_df.loc[group_df.index, metric_name_1] = group_df[\"zscore\"]\n        chrY_df.loc[group_df.index, f\"N_{metric_name_1}\"] = groups[1].shape[0]\n    for groups in files2.groupby([ASSAY, \"track_type\"]):\n        _, group_df = groups\n        group_df[\"zscore\"] = zscore(group_df[\"chrY\"])\n        chrY_df.loc[group_df.index, metric_name_2] = group_df[\"zscore\"]\n        chrY_df.loc[group_df.index, f\"N_{metric_name_2}\"] = groups[1].shape[0]\n\n    # Fill in missing values\n    for N_name in [f\"N_{metric_name_1}\", f\"N_{metric_name_2}\"]:\n        chrY_df[N_name] = chrY_df[N_name].fillna(0).astype(int)\n    chrY_df.fillna(pd.NA, inplace=True)\n\n    if save:\n        output_cols = [\n            \"filename\",\n            ASSAY,\n            \"track_type\",\n            \"chrY\",\n            metric_name_1,\n            f\"N_{metric_name_1}\",\n            metric_name_2,\n            f\"N_{metric_name_2}\",\n        ]\n        chrY_df[output_cols].to_csv(\n            output_dir / \"chrY_zscores.csv\", index=False, na_rep=\"NA\"  # type: ignore\n        )\n    return chrY_df\n\n\nCompute chrY zscores.\n\n\nCode\nchrY_dir = base_data_dir / \"chrY\"\nPathChecker.check_directory(chrY_dir)\n\nchrY_df = compute_chrY_zscores(chrY_dir, \"v2\", save=False)\n\n\n\n\n\nThe following folded code generates a similar table to what was used to determine which unknown sex sample to label. It was not used to produce any graph.\n\n\nCode\ndef create_sex_pred_pivot_table(pred_unknown: pd.DataFrame, chrY_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Generate a pivot table containing group metrics per predicted label, for each EpiRR.\"\"\"\n    index_cols = [\n        \"EpiRR\",\n        \"project\",\n        \"harmonized_donor_type\",\n        CELL_TYPE,\n        SEX,\n        \"Predicted class\",\n    ]\n    pred_plus_chrY_df = pd.merge(\n        pred_unknown,\n        chrY_df,\n        on=\"md5sum\",\n        suffixes=(\"\", \"_DROP\")\n    )\n    pred_plus_chrY_df.drop(\n        columns=[c for c in pred_plus_chrY_df.columns if c.endswith(\"_DROP\")],\n        inplace=True,\n    )\n\n    val_cols = [\"Max pred\", \"chrY_zscore_vs_assay_track\"]\n    pivot_table = pred_plus_chrY_df.pivot_table(\n        index=index_cols,\n        values=val_cols,\n        aggfunc=[\"mean\", \"median\", \"std\", \"count\"],\n    )\n\n    return pivot_table\n\n\ncreate_sex_pred_pivot_table(\n    pred_unknown=pred_unknown_df,\n    chrY_df=chrY_df,\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nmedian\nstd\ncount\n\n\n\n\n\n\n\n\nMax pred\nchrY_zscore_vs_assay_track\nMax pred\nchrY_zscore_vs_assay_track\nMax pred\nchrY_zscore_vs_assay_track\nMax pred\nchrY_zscore_vs_assay_track\n\n\nEpiRR\nproject\nharmonized_donor_type\nharmonized_sample_ontology_intermediate\nharmonized_donor_sex\nPredicted class\n\n\n\n\n\n\n\n\n\n\n\n\nIHECRE00000036.3\nBLUEPRINT\nSingle donor\nhematopoietic cell\nunknown\nmale\n0.781766\n-0.081182\n0.781766\n-0.081182\nNaN\nNaN\n1\n1\n\n\nIHECRE00000042.3\nBLUEPRINT\nSingle donor\nhematopoietic cell\nunknown\nmale\n0.556080\n-0.880438\n0.556080\n-0.880438\nNaN\nNaN\n1\n1\n\n\nIHECRE00000047.3\nBLUEPRINT\nSingle donor\nhematopoietic cell\nunknown\nfemale\n0.769686\n-0.876301\n0.769686\n-0.876301\nNaN\nNaN\n1\n1\n\n\nIHECRE00000067.3\nBLUEPRINT\nSingle donor\nhematopoietic cell\nunknown\nmale\n0.964230\n0.620305\n0.964230\n0.620305\nNaN\nNaN\n1\n1\n\n\nIHECRE00000069.3\nBLUEPRINT\nSingle donor\nhematopoietic cell\nunknown\nmale\n0.978939\n0.398611\n0.978939\n0.398611\nNaN\nNaN\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nIHECRE00004904.1\nNIH Roadmap Epigenomics\nSingle donor\nneural progenitor cell\nunknown\nfemale\n0.679335\n-0.536291\n0.679335\n-0.536291\n0.013960\n0.113480\n2\n2\n\n\nIHECRE00004908.1\nNIH Roadmap Epigenomics\nSingle donor\nplacenta\nunknown\nfemale\n0.590977\n-0.458150\n0.590977\n-0.458150\nNaN\nNaN\n1\n1\n\n\nmixed\n0.893086\n-0.682532\n0.893086\n-0.682532\nNaN\nNaN\n1\n1\n\n\nIHECRE00004910.1\nNIH Roadmap Epigenomics\nSingle donor\nneural progenitor cell\nunknown\nfemale\n0.753894\n-0.288177\n0.753894\n-0.288177\nNaN\nNaN\n1\n1\n\n\nmale\n0.956916\n0.402645\n0.983646\n-0.383976\n0.065293\n1.434576\n5\n5\n\n\n\n\n409 rows × 8 columns\n\n\n\n\n\n\n\n\nThis section generates a similar table to what was used to determine which EpiRR sex labels might be mistaken. It aggregates results from the 10 cross-validation classifiers.\n\n\nCode\ncross_val_analysis = concat_results_10fold.merge(chrY_df, left_index=True, right_on=\"md5sum\", suffixes=(\"\", \"_DROP\"))  # type: ignore\ncross_val_analysis.drop(\n    columns=[c for c in cross_val_analysis.columns if c.endswith(\"_DROP\")], inplace=True\n)\n\n\n\n\nCode\nindex_cols = [\n    \"EpiRR\",\n    \"project\",\n    \"harmonized_donor_type\",\n    CELL_TYPE,\n    SEX,\n    \"Predicted class\",\n]\nval_cols = [\"Max pred\", \"chrY_zscore_vs_assay_track\"]\n\n# not directly used in full mislabel analysis\nto_drop = [\n    \"N_chrY_zscore_vs_assay_w_track_exclusion\",\n    \"chrY_zscore_vs_assay_w_track_exclusion\",\n]\ncross_val_analysis_track = cross_val_analysis.drop(to_drop, axis=1)\n\npivot_table = cross_val_analysis_track.pivot_table(\n    index=index_cols,\n    values=val_cols,\n    aggfunc=[\"mean\", \"median\", \"std\", \"count\"],\n)\n\ndisplay(pivot_table.head())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nmedian\nstd\ncount\n\n\n\n\n\n\n\n\nMax pred\nchrY_zscore_vs_assay_track\nMax pred\nchrY_zscore_vs_assay_track\nMax pred\nchrY_zscore_vs_assay_track\nMax pred\nchrY_zscore_vs_assay_track\n\n\nEpiRR\nproject\nharmonized_donor_type\nharmonized_sample_ontology_intermediate\nharmonized_donor_sex\nPredicted class\n\n\n\n\n\n\n\n\n\n\n\n\nIHECRE00000001.4\nCEEHRC\nSingle donor\nepithelial cell derived cell line\nfemale\nfemale\n0.964847\n-0.649520\n0.988930\n-0.677161\n0.063663\n0.405152\n20\n20\n\n\nmixed\n0.610971\n-0.378015\n0.570102\n-0.568708\n0.137748\n0.653722\n3\n3\n\n\nIHECRE00000002.3\nBLUEPRINT\nSingle donor\nmyeloid cell\nfemale\nfemale\n0.963702\n-0.127269\n0.996254\n-0.554242\n0.096154\n1.082720\n20\n20\n\n\nIHECRE00000004.3\nBLUEPRINT\nSingle donor\nneutrophil\nfemale\nfemale\n0.833395\n-0.040080\n0.900267\n-0.542614\n0.177544\n1.395165\n18\n18\n\n\nmale\n0.651440\n0.404992\n0.651231\n-0.627901\n0.115929\n2.277509\n5\n5\n\n\n\n\n\n\n\n\n\n\n\nDefine function zscore_merged_assays that computes the chrY average signal metric.\n\n\nCode\ndef zscore_merged_assays(\n    zscore_df: pd.DataFrame,\n    sex_mislabels: Dict[str, str],\n    name: str | None = None,\n    logdir: Path | None = None,\n    min_pred: float | None = None,\n    no_rna: bool = False,\n) -&gt; None:\n    \"\"\"Male vs Female z-score distribution for merged assays, excluding wgbs.\n\n    Does not include pval and raw tracks.\n\n    Highlights mislabels in the plot.\n\n    Args:\n        zscore_df (pd.DataFrame): The dataframe with z-score data.\n        sex_mislabels (Dict[str, str]): {EpiRR_no-v: corrected_sex_label}\n        logdir (Path): The directory path to save the output plots. If None, only display the plot.\n        name (str): The base name for the output plot files.\n        min_pred (float|None): Minimum prediction value to include in the plot. Used on average EpiRR 'Max pred' values.\n        no_rna (bool): Whether to exclude rna_seq from the plot.\n    \"\"\"\n    zscore_df = zscore_df.copy(deep=True)\n\n    # Remove pval/raw tracks + rna unstranded\n    zscore_df = zscore_df[~zscore_df[\"track_type\"].isin([\"pval\", \"raw\", \"Unique_raw\"])]\n\n    # Merge rna protocols\n    zscore_df.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n\n    # wgbs reverses male/female chrY tendency, so removed here\n    zscore_df = zscore_df[~zscore_df[ASSAY].str.contains(\"wgb\")]\n\n    if no_rna:\n        zscore_df = zscore_df[~zscore_df[ASSAY].str.contains(\"rna\")]\n\n    N_assays = len(zscore_df[ASSAY].unique())\n    print(\n        f\"Average chrY z-score values computed from:\\n{zscore_df[ASSAY].value_counts(dropna=False)}\"\n    )\n\n    # Average chrY z-score values\n    metric_label = \"chrY_zscore_vs_assay_w_track_exclusion\"\n    zscore_df = zscore_df[zscore_df[metric_label] != \"NA\"]\n    mean_chrY_values_df = zscore_df.groupby([\"EpiRR\", SEX]).agg(\n        {metric_label: \"mean\", \"Max pred\": \"mean\"}\n    )\n    mean_chrY_values_df.reset_index(inplace=True)\n    if not mean_chrY_values_df[\"EpiRR\"].is_unique:\n        raise ValueError(\"EpiRR is not unique.\")\n\n    # Filter out low prediction values\n    if min_pred is not None:\n        mean_chrY_values_df = mean_chrY_values_df[\n            mean_chrY_values_df[\"Max pred\"] &gt; min_pred\n        ]\n\n    mean_chrY_values_df.reset_index(drop=True, inplace=True)\n    mean_chrY_values_df[\"EpiRR_no_v\"] = mean_chrY_values_df[\"EpiRR\"].str.extract(\n        pat=r\"(\\w+\\d+).\\d+\"\n    )[0]\n\n    chrY_values = mean_chrY_values_df[metric_label]\n    female_idx = np.argwhere((mean_chrY_values_df[SEX] == \"female\").values).flatten()  # type: ignore\n    male_idx = np.argwhere((mean_chrY_values_df[SEX] == \"male\").values).flatten()  # type: ignore\n\n    # Mislabels\n    predicted_as_female = set(\n        epirr_no_v for epirr_no_v, label in sex_mislabels.items() if label == \"female\"\n    )\n    predicted_as_male = set(\n        epirr_no_v for epirr_no_v, label in sex_mislabels.items() if label == \"male\"\n    )\n    predicted_as_female_idx = np.argwhere(mean_chrY_values_df[\"EpiRR_no_v\"].isin(predicted_as_female).values).flatten()  # type: ignore\n    predicted_as_male_idx = np.argwhere(mean_chrY_values_df[\"EpiRR_no_v\"].isin(predicted_as_male).values).flatten()  # type: ignore\n\n    print(\n        f\"Adding mislabels to graph: {len(predicted_as_female_idx)} male-&gt;female, {len(predicted_as_male_idx)} female-&gt;male\"\n    )\n\n    # Hovertext\n    hovertext = [\n        f\"{epirr}: &lt;z-score&gt;={z_score:.3f}\"\n        for epirr, z_score in zip(\n            mean_chrY_values_df[\"EpiRR\"],\n            mean_chrY_values_df[metric_label],\n        )\n    ]\n    hovertext = np.array(hovertext)\n\n    # sanity check\n    if mean_chrY_values_df[\"EpiRR\"].nunique() != mean_chrY_values_df.shape[0]:\n        raise ValueError(\"EpiRR is not unique.\")\n\n    # Create figure\n    fig = go.Figure()\n    fig.add_trace(\n        go.Box(\n            name=\"Female\",\n            x=[0] * len(female_idx),\n            y=chrY_values[female_idx],  # type: ignore\n            boxmean=True,\n            boxpoints=\"all\",\n            pointpos=-2,\n            hovertemplate=\"%{text}\",\n            text=hovertext[female_idx],\n            marker=dict(size=2),\n            line=dict(width=1, color=\"black\"),\n            fillcolor=sex_colors[\"female\"],\n            showlegend=True,\n        ),\n    )\n\n    fig.add_trace(\n        go.Box(\n            name=\"Male\",\n            x=[1] * len(female_idx),\n            y=chrY_values[male_idx],  # type: ignore\n            boxmean=True,\n            boxpoints=\"all\",\n            pointpos=-2,\n            hovertemplate=\"%{text}\",\n            text=hovertext[male_idx],\n            marker=dict(size=2),\n            line=dict(width=1, color=\"black\"),\n            fillcolor=sex_colors[\"male\"],\n            showlegend=True,\n        ),\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            name=\"Male\",\n            x=[-0.5] * len(predicted_as_male_idx),\n            y=chrY_values[predicted_as_male_idx],  # type: ignore\n            mode=\"markers\",\n            marker=dict(\n                size=10, color=sex_colors[\"male\"], line=dict(width=1, color=\"black\")\n            ),\n            hovertemplate=\"%{text}\",\n            text=hovertext[predicted_as_male_idx],\n            showlegend=False,\n        ),\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            name=\"Female\",\n            x=[0.5] * len(predicted_as_female_idx),\n            y=chrY_values[predicted_as_female_idx],  # type: ignore\n            mode=\"markers\",\n            marker=dict(\n                size=10, color=sex_colors[\"female\"], line=dict(width=1, color=\"black\")\n            ),\n            hovertemplate=\"%{text}\",\n            text=hovertext[predicted_as_female_idx],\n            showlegend=False,\n        ),\n    )\n\n    fig.update_xaxes(showticklabels=False)\n\n    fig.update_yaxes(range=[-1.5, 3])\n    title = f\"z-score(mean chrY signal per file) distribution - z-scores averaged over {N_assays} assays\"\n    if min_pred is not None:\n        title += f\"&lt;br&gt;avg_maxPred&gt;{min_pred}\"\n\n    fig.update_layout(\n        title=dict(text=title, x=0.5),\n        xaxis_title=SEX,\n        yaxis_title=\"Average z-score\",\n        width=750,\n        height=750,\n    )\n\n    # Save figure\n    if logdir:\n        this_name = f\"{metric_label}_n{mean_chrY_values_df.shape[0]}\"\n        if name:\n            this_name = f\"{name}_{this_name}\"\n        fig.write_image(logdir / f\"{this_name}.svg\")\n        fig.write_image(logdir / f\"{this_name}.png\")\n        fig.write_html(logdir / f\"{this_name}.html\")\n\n    fig.show()\n\n\nGraph zscores.\n\n\nCode\nzscore_merged_assays(\n    zscore_df=cross_val_analysis,\n    sex_mislabels=create_mislabel_corrector(paper_dir)[1][SEX],\n    name=\"no_RNA\",\n    no_rna=True,\n)\n\n\nAverage chrY z-score values computed from:\nh3k27ac     1447\nh3k4me1      850\nh3k4me3      678\ninput        657\nh3k36me3     595\nh3k27me3     575\nh3k9me3      537\nName: assay_epiclass, dtype: int64\nAdding mislabels to graph: 8 male-&gt;female, 8 female-&gt;male\n\n\n                                                \n\n\nFig 2E: Distribution of the average z-score signal of epigenomes (dots) over chrY, computed on the ChIP-Seq datasets (up to 7 assays per epigenome) using the fold change track type files for female (red) and male (blue). Originally mislabeled epigenomes are shown as big dots. Boxplot elements are as in Fig. 2A.\n\n\n\nLoad official IHEC sample metadata.\n\n\nCode\nmeta_v1_2_df = pd.read_csv(\n    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.2.extended.csv\"\n)\nmeta_v1_1_df = pd.read_csv(\n    official_metadata_dir / \"IHEC_sample_metadata_harmonization.v1.1.extended.csv\"\n)\n\n\nSanity check, both metadata versions have the same ‘cell line’ samples.\n\n\nCode\ncell_lines_v11 = meta_v1_1_df[meta_v1_1_df[BIOMATERIAL_TYPE] == \"cell line\"][\n    \"epirr_id_without_version\"\n].unique()\ncell_lines_v12 = meta_v1_2_df[meta_v1_2_df[BIOMATERIAL_TYPE] == \"cell line\"][\n    \"epirr_id_without_version\"\n].unique()\nassert set(cell_lines_v11) == set(cell_lines_v12)\n\n\nLoad GP-age results.\n\n\nCode\ngp_age_dir = base_data_dir / \"GP_age\"\nPathChecker.check_directory(gp_age_dir)\n\ndf_gp_age = pd.read_csv(gp_age_dir / \"life_stage_prediction.GPage.20250513.tsv\", sep=\"\\t\")\ndf_gp_age[\"graph_age\"] = df_gp_age[\"model30\"]\n\nepiclass_pred_col = \"epiclass_predicted_lifestage\"\n\ndisplay(df_gp_age.head())\n\n\n\n\n\n\n\n\n\nepirr_noV\nversion\nmodel30\nmodel71\nmodel30_imputed_data\nmodel71_imputed_data\nnbCpG_model30\nnbCpG_model71\nharmonized_donor_life_stage\nharmonized_donor_age\nharmonized_donor_age_unit\nepiclass_predicted_lifestage\ntissue\ntissue_subtype\ngraph_age\n\n\n\n\n0\nIHECRE00000001\n4\n39.356\n28.973\n71.782\n62.627\n30\n71\nunknown\nunknown\nunknown\nadult_pred\nREPR\nbreast\n39.356\n\n\n1\nIHECRE00000004\n3\n49.972\n46.109\n53.737\n49.893\n30\n70\nadult\n60-65\nyear\nadult\nIMMU\nblood-venous\n49.972\n\n\n2\nIHECRE00000008\n3\n55.681\n51.238\n40.128\n36.052\n28\n67\nadult\n50-55\nyear\nadult\nIMMU\nblood-venous\n55.681\n\n\n3\nIHECRE00000013\n3\n61.915\n64.748\n35.262\n32.777\n29\n67\nchild\n0-5\nyear\nadult_pred\nIMMU\nblood-venous\n61.915\n\n\n4\nIHECRE00000014\n2\n48.480\n49.848\n71.979\n64.951\n29\n67\nadult\n70-75\nyear\nadult\nIMMU\nbone-marrow\n48.480\n\n\n\n\n\n\n\nThe epiclass_predicted_lifestage column is manually curated, from analyzing all predictions for all samples. When an expected label is known, and epiclass predictions are inconclusive (low average max pred and/or no majority consensus), the expected label is kept.\nColumns tissue and tissue_subtype are formatting of harmonized_sample_organ_system_order_AnetaMikulasova and harmonized_sample_organ_order_AnetaMikulasova.\nFollowing code remaps 5 life stage classes to 3 (perinatal, pediatric, adult).\n\n\nCode\ndf_gp_age.loc[:, \"graph_age_categories\"] = df_gp_age[epiclass_pred_col].str.removesuffix(\n    \"_pred\"\n)\n\ngp_age_categories = {\n    \"adult\": \"adult\",\n    \"child\": \"pediatric\",\n    \"embryonic\": \"perinatal\",\n    \"fetal\": \"perinatal\",\n    \"newborn\": \"perinatal\",\n    \"unknown\": \"unknown\",\n}\ndf_gp_age.loc[:, \"graph_age_categories\"] = df_gp_age[\"graph_age_categories\"].map(\n    gp_age_categories\n)\ndisplay(df_gp_age[\"graph_age_categories\"].value_counts(dropna=False))\n\n\nadult        479\npediatric     54\nperinatal     53\nunknown       48\nName: graph_age_categories, dtype: int64\n\n\nHere we merge GP-age data with additional EpiAtlas metadata.\n\n\nCode\nepirr_col = \"epirr_id_without_version\"\n\nmerged_dp_age = pd.merge(\n    df_gp_age,\n    meta_v1_2_df[[epirr_col, CELL_TYPE, BIOMATERIAL_TYPE]],\n    left_on=\"epirr_noV\",\n    right_on=epirr_col,\n    how=\"left\",\n)\nmerged_dp_age.drop_duplicates(subset=[epirr_col], inplace=True)\nmerged_dp_age.drop(columns=[\"epirr_noV\"], inplace=True)\n\n\nExcluding ‘cell line’ samples from considered data.\n\n\nCode\n# Removing cell lines: life stage makes less sense\n# type: ignore\nN_before = merged_dp_age.shape[0]\nmerged_dp_age: pd.DataFrame = merged_dp_age[\n    merged_dp_age[BIOMATERIAL_TYPE] != \"cell line\"\n]\nprint(f\"Removed {N_before - merged_dp_age.shape[0]} cell lines.\")\n\n\nRemoved 29 cell lines.\n\n\nCreating the graph categories.\nWe need to categorize separately whole blood since from other tissues since GP-Age training is only made of whole blood. We keep ‘immune system’ tissues, specifically venous and umbilical blood since they match the most closely to the training data. unsure about blood marrow.\n\n\nCode\nlayer1_vals = [\"IMMU\"]\nlayer2_vals = [\"blood-umbilical-cord\", \"blood-venous\"]\n\nmerged_dp_age.loc[:, \"tissue_group\"] = [\n    \"blood\" if (val1 in layer1_vals and val2 in layer2_vals) else \"other\"\n    for val1, val2 in merged_dp_age.loc[:, [\"tissue\", \"tissue_subtype\"]].values\n]\n\n\nSanity check, no NaN present.\n\n\nCode\nimportant_cols = [\n    \"epirr_id_without_version\",\n    \"tissue_group\",\n    epiclass_pred_col,\n    \"graph_age\",\n]\nmissing_N = merged_dp_age.loc[:, important_cols].isna().sum().sum()\nif missing_N &gt; 0:\n    raise ValueError(f\"Missing values in merged_dp_age: {missing_N}\")\n\n\nDefine graphing function graph_gp_age.\n\n\nCode\ndef graph_gp_age(\n    df_gp_age: pd.DataFrame,\n    logdir: Path | None = None,\n    name: str | None = None,\n    title: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Plot the GP age predictions.\n\n    Args:\n        df_gp_age: The dataframe with GP age data.\n    \"\"\"\n    df = df_gp_age.copy(deep=True)\n\n    tissue_colors = {\"blood\": \"red\", \"other\": \"gray\"}\n\n    age_cat_label = \"graph_age_categories\"\n\n    fig = go.Figure()\n    for tissue_group in sorted(df[\"tissue_group\"].unique()):\n        sub_df = df[df[\"tissue_group\"] == tissue_group]\n        fig.add_trace(\n            go.Box(\n                name=f\"{tissue_group} (n={len(sub_df)})\",\n                x=sub_df[age_cat_label],\n                y=sub_df[\"graph_age\"],\n                boxmean=True,\n                boxpoints=\"all\",\n                hovertemplate=\"%{text}\",\n                text=[\n                    f\"{ct}: {age:.3f}\"\n                    for ct, age in zip(sub_df[CELL_TYPE], sub_df[\"graph_age\"])\n                ],\n                marker=dict(size=2, color=tissue_colors[tissue_group]),\n                showlegend=True,\n            ),\n        )\n\n    fig.update_layout(\n        title=f\"GP age predictions - Using MLP predicted labels ({title})\",\n        xaxis_title=\"Life stage\",\n        yaxis_title=\"GP-Age : Predicted age\",\n        width=750,\n        height=750,\n        boxmode=\"group\",\n    )\n\n    # Order x-axis\n    label_order = [\"perinatal\", \"pediatric\", \"adult\"]\n    axis_labels = [\n        f\"{age_cat} (n={(df[age_cat_label] == age_cat).sum()})\" for age_cat in label_order\n    ]\n\n    fig.update_xaxes(categoryorder=\"array\", categoryarray=label_order)\n    fig.update_xaxes(tickvals=[0, 1, 2], ticktext=axis_labels)\n\n    # Save figure\n    if logdir:\n        if name is None:\n            name = \"GP_age_predictions\"\n        fig.write_image(logdir / f\"{name}.svg\")\n        fig.write_image(logdir / f\"{name}.png\")\n        fig.write_html(logdir / f\"{name}.html\")\n\n    fig.show()\n\n\nDefine dataframe content function display_tissue_age_df.\n\n\nCode\ndef display_tissue_age_df(df: pd.DataFrame) -&gt; None:\n    \"\"\"Display tissue group count breakdown.\"\"\"\n    count = df.groupby([\"tissue_group\", \"graph_age_categories\"]).size()\n    print(\"Tissue group summary\")\n    print(f\"Blood: {count.loc['blood'].sum()}\")  # type: ignore\n    print(f\"Other: {count.loc['other'].sum()}\")  # type: ignore\n    print(f\"Total: {count.sum()}\")\n    print(\"Detail:\")\n    print(count)\n\n\nFor initially unknown labels, only predictions that were manually confirmed as mislabels.\n\n\nCode\n# Remove inconclusive predictions\ngp_graph_df = merged_dp_age[merged_dp_age[epiclass_pred_col] != \"unknown\"]\n\nonly_unknown_df: pd.DataFrame = gp_graph_df[gp_graph_df[LIFE_STAGE] == \"unknown\"]\ndisplay_tissue_age_df(only_unknown_df)\n\ngraph_gp_age(\n    df_gp_age=only_unknown_df,\n    name=\"GP_age_predictions_unknown_only_MLP_predicted_labels\",\n    title=\"Samples with unknown life stage\",\n)\n\n\nTissue group summary\nBlood: 82\nOther: 63\nTotal: 145\nDetail:\ntissue_group  graph_age_categories\nblood         adult                   76\n              pediatric                3\n              perinatal                3\nother         adult                   52\n              pediatric                3\n              perinatal                8\ndtype: int64\n\n\n                                                \n\n\nFig 2F: Distribution of the age prediction from GP-age for the WGBS datasets with an originally unknown life stage predicted by EpiClass (datasets related to blood biospecimen are in red, others in grey). Boxplot elements are as in Fig. 2A.\n\n\nConsidering all samples that had a conclusive epiclass prediction (or existing label and no conclusion)\n\n\nCode\ndisplay_tissue_age_df(gp_graph_df)\n\ngraph_gp_age(\n    df_gp_age=gp_graph_df,\n    name=\"GP_age_predictions_all_samples_MLP_predicted_labels\",\n    title=\"All samples (has a life stage pred + gp-age)\",\n)\n\n\nTissue group summary\nBlood: 239\nOther: 326\nTotal: 565\nDetail:\ntissue_group  graph_age_categories\nblood         adult                   201\n              pediatric                26\n              perinatal                12\nother         adult                   272\n              pediatric                25\n              perinatal                29\ndtype: int64\n\n\n                                                \n\n\nSupp. Fig. 5D: Distribution of the age prediction from GP-age for the 565 epigenomes (dots) with conclusive consensus predictions (epigenomes related to blood biospecimen are in red, others in grey). The perinatal category encompasses the original embryonic, fetal and newborn categories of metadata as they individually contain too few samples. Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5× the interquartile range.\n\n\n\n\nFor full code, since the processing is more complex, see src/python/epiclass/utils/notebooks/analyze_hdf5_vals.ipynb (permalink), particularly section “ChromScore hdf5 values”.\n\n\n\n\nFig 2G: Distribution of the average ChromScore values over the important Biospecimen classifier regions according to SHAP (blue) compared to the global distribution (pink). Statistical significance was assessed using a two-sided Welch’s t-test. Boxplot elements are as in Fig. 2A, with a violin representation on top.\n\n\n\nSee src/python/epiclass/utils/notebooks/profile_bed.ipynb (permalink) for creation of gene ontology files. The code compares important SHAP values regions of different cell types with gene gff (using the gProfiler module)\nDefine GO terms.\n\n\nCode\nselected_cell_types = [\n    \"T_cell\",\n    \"neutrophil\",\n    \"lymphocyte_of_B_lineage\",\n    \"brain\",\n    \"hepatocyte\",\n]\ngo_terms_table = [\n    \"T cell receptor complex\",\n    \"plasma membrane signaling receptor complex\",\n    \"adaptive immune response\",\n    \"receptor complex\",\n    \"secretory granule\",\n    \"secretory vesicle\",\n    \"secretory granule membrane\",\n    \"intracellular vesicle\",\n    \"immunoglobulin complex\",\n    \"immune response\",\n    \"immune system process\",\n    \"homophilic cell adhesion via plasma membrane adhesion molecules\",\n    \"DNA binding\",\n    \"cell-cell adhesion via plasma-membrane adhesion molecules\",\n    \"RNA polymerase II cis-regulatory region sequence-specific DNA binding\",\n    \"blood microparticle\",\n    \"platelet alpha granule lumen\",\n    \"fibrinogen complex\",\n    \"endoplasmic reticulum lumen\",\n]\n\n# Add some line breaks for readability\ngo_terms_graph = [\n    \"T cell receptor complex\",\n    \"plasma membrane&lt;br&gt;signaling receptor complex\",\n    \"adaptive immune response\",\n    \"receptor complex\",\n    \"secretory granule\",\n    \"secretory vesicle\",\n    \"secretory granule membrane\",\n    \"intracellular vesicle\",\n    \"immunoglobulin complex\",\n    \"immune response\",\n    \"immune system process\",\n    \"homophilic cell adhesion via&lt;br&gt;plasma membrane adhesion molecules\",\n    \"DNA binding\",\n    \"cell-cell adhesion via&lt;br&gt;plasma-membrane adhesion molecules\",\n    \"RNA polymerase II cis-regulatory&lt;br&gt;region sequence-specific DNA binding\",\n    \"blood microparticle\",\n    \"platelet alpha granule lumen\",\n    \"fibrinogen complex\",\n    \"endoplasmic reticulum lumen\",\n]\n\n\nSetup paths.\n\n\nCode\nhdf5_type = \"hg38_100kb_all_none\"\n\nSHAP_dir = base_data_dir / \"SHAP\"\n\ncell_type_shap_dir = (\n    SHAP_dir / hdf5_type / f\"{CELL_TYPE}_1l_3000n\" / \"10fold-oversampling\"\n)\nbeds_file = cell_type_shap_dir / \"select_beds_top303.tar.gz\"\nPathChecker.check_file(beds_file)\n\n\nLoad top g:profiler results for each cell type.\n\n\nCode\nall_go_dfs: Dict[str, pd.DataFrame] = {}\nwith tarfile.open(beds_file, \"r:gz\") as tar:\n    for member in tar.getmembers():\n        filename = member.name\n        if filename.endswith(\"profiler.tsv\") and \"merge_samplings\" in filename:\n            with tar.extractfile(member) as f:  # type: ignore\n                go_df = pd.read_csv(f, sep=\"\\t\", index_col=0)\n                all_go_dfs[member.name] = go_df\n\nassert len(all_go_dfs) == 16\n\n\nMerge results for all cell types into one table.\n\n\nCode\nfor name, df in all_go_dfs.items():\n    sub_df = df.copy()\n    sub_df.loc[:, \"shap_source\"] = re.match(r\".*/merge_samplings_(.*)_features_intersect_gff_gprofiler.tsv\", name).group(1)  # type: ignore\n    sub_df.loc[:, \"table_val\"] = -np.log10(sub_df.loc[:, \"p_value\"])\n    all_go_dfs[name] = sub_df\n\nfull_concat_df = pd.concat(all_go_dfs.values())\nfull_concat_df = full_concat_df.drop([\"significant\", \"query\"], axis=1)\n\nassert all(go_term in full_concat_df[\"name\"].values for go_term in go_terms_table)\n\ntable = full_concat_df.pivot_table(\n    index=\"name\", columns=\"shap_source\", values=\"table_val\", aggfunc=\"mean\"\n)\n\n\nCollect subset of table for the five selected cell types.\n\n\nCode\ntable_5_ct = table.loc[go_terms_table, selected_cell_types].copy()\nassert table_5_ct.shape == (len(go_terms_graph), len(selected_cell_types))\n\n# Rename index\ntable_5_ct = pd.DataFrame(table_5_ct, index=go_terms_graph, columns=table_5_ct.columns)\n\n\nDefine graphing function plot_go_heatmap.\n\n\nCode\ndef plot_go_heatmap(table: pd.DataFrame, width: int, height: int, title: str|None=None):\n    \"\"\"Plot a GO heatmap.\"\"\"\n    # Define colorbar\n    sigma = \"\\u03c3\"\n\n    colorbar = dict(\n        title=\"-log&lt;sub&gt;10&lt;/sub&gt;(p-value)\",\n        tickvals=[0, 1.30, 2, 3, 5, 6.53, 10],\n        ticktext=[\n            \"0\",\n            f\"1.30: p=0.05~2{sigma}\",\n            \"2: p=0.01\",\n            f\"3: p=0.001~3{sigma}\",\n            \"5\",\n            f\"6.53: p=3x10&lt;sup&gt;-7&lt;/sup&gt; = 5{sigma}\",\n            \"10\",\n        ],\n    )\n\n    # keep NaNs in for labeling\n    z = table.values\n    text = np.where(np.isnan(z), \"NS\", np.char.mod(\"%.2f\", z))\n\n    fig = go.Figure(\n        data=go.Heatmap(\n            z=np.nan_to_num(z, nan=0),  # replace NaN with 0 for coloring\n            x=table.columns,\n            y=table.index,\n            colorscale=\"Blues\",\n            zmin=0,\n            zmax=10,\n            colorbar=colorbar,\n            text=text,\n            texttemplate=\"%{text}\",\n            hovertemplate=\"GO Term: %{y}&lt;br&gt;Class: %{x}&lt;br&gt;Value: %{z:.2f}&lt;extra&gt;&lt;/extra&gt;\",\n            showscale=True,\n            xgap=2,\n            ygap=2,\n        )\n    )\n\n    fig.update_layout(\n        title_text=title,\n        width=width,\n        height=height,\n        plot_bgcolor=\"black\",\n    )\n    fig.update_xaxes(showgrid=False)\n    fig.update_yaxes(showgrid=False)\n\n    fig.show()\n\n\nGraph GO for 5 cell types.\n\n\nCode\nplot_go_heatmap(\n    table=table_5_ct,\n    width=600,\n    height=1000,\n)\n\n\n                                                \n\n\nFig 2H: Top four gene ontology (GO) terms enriched by g:Profiler[35] for genes within important Biospecimen classifier regions according to SHAP values.\n\n\nCollect the 5 most significants terms for each the 16 cell types.\n\n\nCode\n# preserve order\ntop_5_terms = []\nfor name, df in all_go_dfs.items():\n    df.sort_values(\"p_value\", inplace=True)\n    top_5 = df[\"name\"].head(5).to_list()\n    top_5_terms.extend(top_5)\n\ntop_5_terms = list(dict.fromkeys(top_5_terms))\n\n\nGraph.\n\n\nCode\ntable_top_5_GO = table.loc[top_5_terms, :].copy()\n\nplot_go_heatmap(\n    table=table_top_5_GO,\n    width=1300,\n    height=2000,\n)\n\n\n                                                \n\n\n\n\n\n\nImages extracted from Epilogos viewer, using:\n\nRegion: chr11:118300000-118400000\nView mode: Single\nDataset: IHEC\nAll biosamples\nSaliency Metric: S1\n\n\nFig. 2I: Epilogos visualization of one of the important Biospecimen classifier regions enriched in the T cell receptor complex GO term from Fig. 2H.\nSee Annex A for a more detailled Epilogos color legend.\n\n\n\nNote: The code uses the acronym CNV (Copy Number Variant/Variation) instead of CNA.\nSee CNV_treatment.ipynb (permalink) for the creation of the CNA stats. The following figures represent the Z-scores of the signal within the top SHAP features (N=336) vs 200 random feature sets of same size, for signatures created using patients samples with cancer types similar to EpiAtlas content (see paper Methods).\n\nLoad z-scores.\n\n\nCode\ncnv_dir = base_data_dir / \"CNV\"\ncnv_intersection_results = (\n    cnv_dir\n    / \"signature_analysis\"\n    / \"epiatlas_cancer_types\"\n    / \"important_cancer_features_z_scores_vs_random200.tsv\"\n)\nPathChecker.check_file(cnv_intersection_results)\n\ncnv_df = pd.read_csv(cnv_intersection_results, sep=\"\\t\", index_col=0)\ncnv_df.name = cnv_intersection_results.stem\n\n\nDefine graphing function plot_cnv_zscores, which uses the CNA groups defined by Signatures of copy number alterations in human cancer. See Supplementary Table 8 for details.\n\n\nCode\ndef plot_cnv_zscores(cnv_df: pd.DataFrame, logdir: Path | None = None) -&gt; None:\n    \"\"\"Plot z-scores of top SHAP features vs random feature sets, grouped by CNV groups.\n\n    Args:\n        cnv_df: The DataFrame with z-scores.\n        logdir: The output directory to save the plot.\n    \"\"\"\n    n_beds = int(cnv_df.name.split(\"random\")[1])\n    signature_subset_name = \"EpiATLAS cancer types\"\n\n    CN_groups = [\n        [f\"CN{i}\" for i in range(1, 4)],\n        [f\"CN{i}\" for i in range(9, 13)],\n        [f\"CN{i}\" for i in range(13, 17)],\n        [f\"CN{i}\" for i in range(17, 18)],\n        [f\"CN{i}\" for i in range(18, 22)],\n        [f\"CN{i}\" for i in range(4, 9)],\n    ]\n    CN_names = [\n        \"CN1-CN3\",\n        \"CN9-CN12\",\n        \"CN13-CN16\",\n        \"CN17\",\n        \"CN18-CN21\",\n        \"CN4-CN8\",\n    ]\n\n    # Assign groups to the DataFrame\n    cnv_df[\"group\"] = \"Other\"\n    for i, group in enumerate(CN_groups):\n        cnv_df.loc[cnv_df.index.isin(group), \"group\"] = CN_names[i]\n\n    # Sort groups\n    group_medians = (\n        cnv_df.groupby(\"group\")[\"z_score\"].median().sort_values(ascending=False)\n    )\n    sorted_CN_names = group_medians.index.tolist()\n\n    # Create the figure\n    fig = go.Figure()\n\n    for group in sorted_CN_names:\n        group_data = cnv_df[cnv_df[\"group\"] == group]\n        marker_size = 4 if group != \"CN17\" else 6\n\n        # Add the box plot without points\n        fig.add_trace(\n            go.Box(\n                y=group_data[\"z_score\"],\n                name=group,\n                boxmean=True,\n                boxpoints=False,  # Don't show points in the box plot\n                line=dict(color=\"black\"),\n                fillcolor=\"rgba(255,255,255,0)\",\n                showlegend=False,\n            )\n        )\n\n        # Add scatter plot for individual points\n        fig.add_trace(\n            go.Scatter(\n                x=[group] * len(group_data),\n                y=group_data[\"z_score\"],\n                mode=\"markers\",\n                marker=dict(\n                    color=\"red\",\n                    size=marker_size,\n                ),\n                name=group,\n                showlegend=False,\n                text=group_data.index,  # Use CN names as hover text\n                hoverinfo=\"text+y\",  # Show CN name and y-value on hover\n            )\n        )\n    # Update layout\n    fig.update_layout(\n        xaxis_title=\"CNA Group\",\n        yaxis_title=\"Z-score\",\n        **main_title_settings\n    )\n\n    # Add a horizontal line at y=0 for reference\n    fig.add_hline(y=0, line_color=\"grey\", line_width=1)\n\n    # Show and save the figure\n    if logdir:\n        name = \"important_cancer_features_z_scores_boxplot\"\n        fig.write_image(logdir / f\"{name}.png\")\n        fig.write_image(logdir / f\"{name}.svg\")\n        fig.write_html(logdir / f\"{name}.html\")\n\n    fig.show()\n\n\nGraph.\n\n\nCode\nplot_cnv_zscores(cnv_df)\n\n\n                                                \n\n\nDefine the graphing function plot_cnv_zscores_alt, which groups CNA by focal vs chromosomal events.\n\n\nCode\ndef plot_cnv_zscores_alt(cnv_df: pd.DataFrame, logdir: Path | None = None) -&gt; None:\n    \"\"\"Plot z-scores of top SHAP features vs random feature sets, grouped by focal vs chromosomal events.\n\n    Args:\n        cnv_df: The DataFrame with z-scores.\n        logdir: The output directory to save the plot.\n    \"\"\"\n    n_beds = int(cnv_df.name.split(\"random\")[1])\n    signature_subset_name = \"EpiATLAS cancer types\"\n\n    # Assign groups\n    CN_groups = [\n        [f\"CN{i}\" for i in range(1, 9)] + [f\"CN{i}\" for i in range(13, 17)],\n        [f\"CN{i}\" for i in range(9, 13)] + [f\"CN{i}\" for i in range(17, 22)],\n    ]\n    CN_names = [\n        \"Chromosomal events (CN1-CN8, CN13-CN16)\",\n        \"Focal events (CN9-CN12, CN17-CN21)\",\n    ]\n\n    # Assign groups to the DataFrame\n    cnv_df[\"group\"] = \"Other\"\n    for i, group in enumerate(CN_groups):\n        cnv_df.loc[cnv_df.index.isin(group), \"group\"] = CN_names[i]\n\n    # Sort groups\n    group_medians = (\n        cnv_df.groupby(\"group\")[\"z_score\"].median().sort_values(ascending=False)\n    )\n    sorted_CN_names = group_medians.index.tolist()\n\n    # Create the figure\n    fig = go.Figure()\n\n    for group in sorted_CN_names:\n        group_data = cnv_df[cnv_df[\"group\"] == group]\n        hover_text = [\n            f\"{CN_name}: Z={val:.3f}\"\n            for CN_name, val in zip(group_data.index, group_data[\"z_score\"])\n        ]\n\n        # Add the box plot without points\n        fig.add_trace(\n            go.Box(\n                y=group_data[\"z_score\"],\n                name=group,\n                boxmean=True,\n                boxpoints=\"all\",\n                line=dict(color=\"black\"),\n                fillcolor=\"rgba(255,255,255,0)\",\n                showlegend=False,\n                hoverinfo=\"text\",  # Show CN name and y-value on hover\n                hovertext=hover_text,\n            )\n        )\n\n    # Update layout\n    fig.update_layout(\n        xaxis_title=\"Copy Number Alteration Event Type\",\n        yaxis_title=\"CNA Enrichment (Z-score)\",\n        **main_title_settings\n    )\n\n    # Add a horizontal line at y=0 for reference\n    fig.add_hline(y=0, line_color=\"grey\", line_width=1)\n\n    # Show and save the figure\n    if logdir:\n        name = \"important_cancer_features_z_scores_boxplot_V2\"\n        fig.write_image(logdir / f\"{name}.png\")\n        fig.write_image(logdir / f\"{name}.svg\")\n        fig.write_html(logdir / f\"{name}.html\")\n\n    fig.show()\n\n\nGraph.\n\n\nCode\nplot_cnv_zscores_alt(cnv_df)\n\n\n                                                \n\n\nFig. 2J: Distribution of the z-score enrichment for copy number (CN) signatures (dots) in genomic regions identified as important by the Cancer status classifier compared to random control regions. CN signatures are grouped as being mostly associated with focal changes (CN9-12,17-21) or chromosomal ones (CN1-8,13-16). Boxplot elements are as in Fig 2A."
  },
  {
    "objectID": "figs/fig2.html#supplementary-figures-5-to-7",
    "href": "figs/fig2.html#supplementary-figures-5-to-7",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata",
    "section": "",
    "text": "See Supplementary Figures page (separate to limit maximum website page size and loading times)."
  },
  {
    "objectID": "figs/fig2.html#sec-annex-a",
    "href": "figs/fig2.html#sec-annex-a",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata",
    "section": "",
    "text": "biv = bivalent\nTx = Transcription\nRepr = Repressed\nPC = Polycomb"
  },
  {
    "objectID": "figs/fig1.html",
    "href": "figs/fig1.html",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata",
    "section": "",
    "text": "The formatting of the figures may differ slightly from those in the paper, but they display the same data points.\nAll code cells are folded by default. To view any cell, click “Code” to expand it, or use the code options near the main title above to unfold all at once.\nSome code may be repeated, as the original Python notebook was designed for figures to be generated semi-independently.\n\n\nSetup imports.\n\n\nCode\nfrom __future__ import annotations\n\nimport copy\nimport logging\nimport re\nimport tempfile\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Dict, Optional, Tuple\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom IPython.core.display import Image\nfrom IPython.display import display\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import auc, confusion_matrix as sk_cm, roc_curve\nfrom sklearn.preprocessing import label_binarize\n\nfrom epiclass.core.confusion_matrix import ConfusionMatrixWriter\nfrom epiclass.utils.notebooks.paper.metrics_per_assay import MetricsPerAssay\nfrom epiclass.utils.notebooks.paper.paper_utilities import (\n    ASSAY,\n    ASSAY_MERGE_DICT,\n    ASSAY_ORDER,\n    CELL_TYPE,\n    SEX,\n    IHECColorMap,\n    MetadataHandler,\n    SplitResultsHandler,\n    extract_input_sizes_from_output_files,\n    merge_similar_assays,\n    PathChecker\n)\n\n\nSetup paths.\n\n\nCode\n# Root path\nbase_dir = Path.home() / \"Projects/epiclass/output/paper\"\nPathChecker.check_directory(base_dir)\n\n# More precise\nbase_data_dir = base_dir / \"data\"\nbase_fig_dir = base_dir / \"figures\"\n\n# alias\npaper_dir = base_dir\n\n\nSetup colors.\n\n\nCode\nIHECColorMap = IHECColorMap(base_fig_dir)\nassay_colors = IHECColorMap.assay_color_map\ncell_type_colors = IHECColorMap.cell_type_color_map\n\n\nSetup metadata and prediction files handlers.\n\n\nCode\nsplit_results_handler = SplitResultsHandler()\n\nmetadata_handler = MetadataHandler(paper_dir)\nmetadata_v2 = metadata_handler.load_metadata(\"v2\")\nmetadata_v2_df = metadata_v2.to_df()\n\n\nSetup data directories.\n\n\nCode\ngen_data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\nPathChecker.check_directory(gen_data_dir)\n\ndata_dir_100kb = gen_data_dir / \"hg38_100kb_all_none\"\nPathChecker.check_directory(data_dir_100kb)\n\n\nSetup figures general settings.\n\n\nCode\nmain_title_settings = {\n    \"title\":dict(\n        automargin=True,\n        x=0.5,\n        xanchor=\"center\",\n        yanchor=\"top\",\n        y=0.98\n        ),\n    \"margin\":dict(t=50, l=10, r=10)\n}\n\n\n\n\n\nPerformance of EpiClass Assay and Biospecimen classifiers.\n\n\n\nFig. 1A: Overview of the EpiClass training process for various classifiers and their inference on external data. Each classifier is trained independently.\n\n\n\nPath setup.\n\n\nCode\nmixed_data_dir = gen_data_dir / \"mixed\"\nPathChecker.check_directory(data_dir_100kb)\n\n\nFeature sets setup.\n\n\nCode\nfeature_sets_14 = [\n    \"hg38_10mb_all_none_1mb_coord\",\n    \"hg38_100kb_random_n316_none\",\n    \"hg38_1mb_all_none\",\n    \"hg38_100kb_random_n3044_none\",\n    \"hg38_100kb_all_none\",\n    \"hg38_gene_regions_100kb_coord_n19864\",\n    \"hg38_10kb_random_n30321_none\",\n    \"hg38_regulReg_allCorr_n30k\",\n    \"hg38_1kb_random_n30321_none\",\n    \"hg38_cpg_topvar_200bp_n30k_wrong_coordinates\",\n    \"hg38_10kb_all_none\",\n    \"hg38_regulReg_allCorr_n303k\",\n    \"hg38_1kb_random_n303114_none\",\n    \"hg38_cpg_topvar_200bp_n303k_wrong_coordinates\",\n]\nfig1_sets = [\n    \"hg38_10mb_all_none_1mb_coord\",\n    \"hg38_100kb_random_n316_none\",\n    \"hg38_1mb_all_none\",\n    \"hg38_100kb_random_n3044_none\",\n    \"hg38_100kb_all_none\",\n    \"hg38_10kb_random_n30321_none\",\n    \"hg38_1kb_random_n30321_none\",\n    \"hg38_10kb_all_none\",\n    \"hg38_1kb_random_n303114_none\",\n]\n\nmetric_orders_map = {\n    \"fig1_sets\": fig1_sets,\n    \"feature_sets_14\": feature_sets_14,\n}\n\n\nCompute input sizes for each feature set.\n\n\nCode\ninput_sizes = extract_input_sizes_from_output_files(mixed_data_dir)  # type: ignore\ninput_sizes: Dict[str, int] = {k: v.pop() for k, v in input_sizes.items() if len(v) == 1}  # type: ignore\n\n\nSet selection.\n\n\nCode\nset_selection_name = \"feature_sets_14\"\n\nlogdir = (\n    base_fig_dir\n    / \"fig2_EpiAtlas_other\"\n    / \"fig2--reduced_feature_sets\"\n    / \"test\"\n    / set_selection_name\n)\nlogdir.mkdir(parents=True, exist_ok=True)\n\n\nCompute metrics.\n\n\nCode\nall_metrics = split_results_handler.obtain_all_feature_set_data(\n    parent_folder=mixed_data_dir,\n    merge_assays=True,\n    return_type=\"metrics\",\n    include_categories=[ASSAY, CELL_TYPE],\n    include_sets=metric_orders_map[set_selection_name],\n    exclude_names=[\"16ct\", \"27ct\", \"7c\", \"chip-seq-only\"],\n)\n\n# Order the metrics\nall_metrics = {\n    name: all_metrics[name]  # type: ignore\n    for name in metric_orders_map[set_selection_name]\n    if name in all_metrics\n}\n\n\nLabel correction.\n\n\nCode\n# correct a name\ntry:\n    all_metrics[\"hg38_100kb_all_none\"][ASSAY] = all_metrics[\"hg38_100kb_all_none\"][  # type: ignore\n        f\"{ASSAY}_11c\"\n    ]\n    del all_metrics[\"hg38_100kb_all_none\"][f\"{ASSAY}_11c\"]\nexcept KeyError:\n    pass\n\n\nResolution/feature set –&gt; color mapping.\n\n\nCode\nresolution_colors = {\n    \"100kb\": px.colors.qualitative.Safe[0],\n    \"10kb\": px.colors.qualitative.Safe[1],\n    \"1kb\": px.colors.qualitative.Safe[2],\n    \"regulReg\": px.colors.qualitative.Safe[3],\n    \"gene\": px.colors.qualitative.Safe[4],\n    \"cpg\": px.colors.qualitative.Safe[5],\n    \"1mb\": px.colors.qualitative.Safe[6],\n    \"5mb\": px.colors.qualitative.Safe[7],\n    \"10mb\": px.colors.qualitative.Safe[8],\n}\n\n\nDefine graphing function graph_feature_set_metrics.\n\n\nCode\ndef graph_feature_set_metrics(\n    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n    input_sizes: Dict[str, int],\n    logdir: Path | None = None,\n    sort_by_input_size: bool = False,\n    name: str | None = None,\n    y_range: Tuple[float, float] | None = None,\n    boxpoints: str = \"all\",\n    width: int = 1200,\n    height: int = 1200,\n) -&gt; None:\n    \"\"\"Graph the metrics for all feature sets.\n\n    Args:\n        all_metrics (Dict[str, Dict[str, Dict[str, Dict[str, float]]]): A dictionary containing all metrics for all feature sets.\n            Format: {feature_set: {task_name: {split_name: metric_dict}}}\n        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n        logdir (Path): The directory where the figure will be saved. If None, the figure will only be displayed.\n        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n        name (str|None): The name of the figure.\n        y_range (Tuple[float, float]|None): The y-axis range for the figure.\n        boxpoints (str): The type of boxpoints to display. Can be \"all\" or \"outliers\". Defaults to \"all\".\n    \"\"\"\n    if boxpoints not in [\"all\", \"outliers\"]:\n        raise ValueError(\"Invalid boxpoints value.\")\n\n    reference_hdf5_type = \"hg38_100kb_all_none\"\n    metadata_categories = list(all_metrics[reference_hdf5_type].keys())\n\n    non_standard_names = {ASSAY: f\"{ASSAY}_11c\", SEX: f\"{SEX}_w-mixed\"}\n    non_standard_assay_task_names = [\"hg38_100kb_all_none\"]\n\n    used_resolutions = set()\n    for i in range(len(metadata_categories)):\n        category_idx = i\n        category_fig = make_subplots(\n            rows=1,\n            cols=2,\n            shared_yaxes=True,\n            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n            horizontal_spacing=0.01,\n        )\n\n        trace_names = []\n        order = list(all_metrics.keys())\n        if sort_by_input_size:\n            order = sorted(\n                all_metrics.keys(),\n                key=lambda x: input_sizes[x],\n            )\n        for feature_set_name in order:\n            # print(feature_set_name)\n            tasks_dicts = all_metrics[feature_set_name]\n            meta_categories = copy.deepcopy(metadata_categories)\n\n            if feature_set_name not in input_sizes:\n                print(f\"Skipping {feature_set_name}, no input size found.\")\n                continue\n\n            task_name = meta_categories[category_idx]\n            if \"split\" in task_name:\n                raise ValueError(\"Split in task name. Wrong metrics dict.\")\n\n            try:\n                task_dict = tasks_dicts[task_name]\n            except KeyError as err:\n                if SEX in str(err) and feature_set_name in non_standard_sex_task_name:\n                    task_dict = tasks_dicts[non_standard_names[SEX]]\n                elif (\n                    ASSAY in str(err)\n                    and feature_set_name in non_standard_assay_task_names\n                ):\n                    task_dict = tasks_dicts[non_standard_names[ASSAY]]\n                else:\n                    print(\"Skipping\", feature_set_name, task_name)\n                    continue\n\n            input_size = input_sizes[feature_set_name]\n\n            feature_set_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n            feature_set_name = re.sub(r\"\\_[\\dmkb]+\\_coord\", \"\", feature_set_name)\n\n            resolution = feature_set_name.split(\"_\")[0]\n            used_resolutions.add(resolution)\n\n            trace_name = f\"{input_size}|{feature_set_name}\"\n            trace_names.append(trace_name)\n\n            # Accuracy\n            metric = \"Accuracy\"\n            y_vals = [task_dict[split][metric] for split in task_dict]\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"\n                for split, metrics_dict in task_dict.items()\n            ]\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=boxpoints,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    fillcolor=resolution_colors[resolution],\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                    legendgroup=resolution,\n                    showlegend=False,\n                ),\n                row=1,\n                col=1,\n            )\n\n            metric = \"F1_macro\"\n            y_vals = [task_dict[split][metric] for split in task_dict]\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"\n                for split, metrics_dict in task_dict.items()\n            ]\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=boxpoints,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    fillcolor=resolution_colors[resolution],\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                    legendgroup=resolution,\n                    showlegend=False,\n                ),\n                row=1,\n                col=2,\n            )\n\n        title = f\"{metadata_categories[category_idx]} classification\"\n        title = title.replace(CELL_TYPE, \"biospecimen\")\n        if name is not None:\n            title += f\" - {name}\"\n        category_fig.update_layout(\n            width=width,\n            height=height,\n            title_text=title,\n            **main_title_settings\n        )\n\n        # dummy scatters for resolution colors\n        for resolution, color in resolution_colors.items():\n            if resolution not in used_resolutions:\n                continue\n            category_fig.add_trace(\n                go.Scatter(\n                    x=[None],\n                    y=[None],\n                    mode=\"markers\",\n                    name=resolution,\n                    marker=dict(color=color, size=5),\n                    showlegend=True,\n                    legendgroup=resolution,\n                )\n            )\n\n        category_fig.update_layout(legend=dict(itemsizing=\"constant\"))\n\n        # y-axis\n        if y_range:\n            category_fig.update_yaxes(range=y_range)\n        else:\n            if ASSAY in task_name:\n                category_fig.update_yaxes(range=[0.96, 1.001])\n            if CELL_TYPE in task_name:\n                category_fig.update_yaxes(range=[0.75, 1])\n\n        category_fig.update_layout(**main_title_settings)\n\n        # Save figure\n        if logdir:\n            base_name = f\"feature_set_metrics_{metadata_categories[category_idx]}\"\n            if name is not None:\n                base_name = base_name + f\"_{name}\"\n            category_fig.write_html(logdir / f\"{base_name}.html\")\n            category_fig.write_image(logdir / f\"{base_name}.svg\")\n            category_fig.write_image(logdir / f\"{base_name}.png\")\n\n        category_fig.show()\n\n\n\n\n\nGraph 100kb resolution MLP metrics.\n\nCode\nmetrics_fig1b = {name: all_metrics[name] for name in [\"hg38_100kb_all_none\"]}\n\nmetrics_fig1b_1 = {\n    \"hg38_100kb_all_none\": {ASSAY: metrics_fig1b[\"hg38_100kb_all_none\"][ASSAY]}\n}\ngraph_feature_set_metrics(\n    all_metrics=metrics_fig1b_1,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=425,\n    height=400,\n    y_range=(0.98, 1.001),\n)\n\nmetrics_fig1b_2 = {\n    \"hg38_100kb_all_none\": {CELL_TYPE: metrics_fig1b[\"hg38_100kb_all_none\"][CELL_TYPE]}\n}\ngraph_feature_set_metrics(\n    all_metrics=metrics_fig1b_2,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=425,\n    height=400,\n    y_range=(0.93, 1.001),\n)\n\n\n\n\n                                                \n\n\n                                                \n\n\n\nFig. 1B: Distribution of accuracy and F1-score for each of the ten training folds (dots) for the Assay and Biospecimen MLP classifiers.\n\n\n\nGraph.\n\nCode\nmetrics_fig1c = {name: all_metrics[name] for name in fig1_sets}\n\ngraph_feature_set_metrics(\n    all_metrics=metrics_fig1c,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=900,\n    height=600,\n)\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\nFig. 1C-alt: Distribution of accuracy per training fold for different bin resolutions for the Assay and Biospecimen classifiers.\n\n\nDefine function parse_bin_size to extract a numerical bin size in base pairs.\n\n\nCode\ndef parse_bin_size(feature_set_name: str) -&gt; Optional[float]:\n    \"\"\"\n    Parses the feature set name to extract a numerical bin size in base pairs.\n    Handles formats like '100kb', '5mb', 'regulReg', 'gene', 'cpg'.\n\n    Returns numerical size (float) or None if unparseable or non-numeric.\n    Assigns placeholder values for non-genomic-range types if needed,\n    but for a continuous axis, it's better to return None or filter later.\n    \"\"\"\n    name_parts = feature_set_name.replace(\"hg38_\", \"\").split(\"_\")\n    if not name_parts:\n        return None\n\n    resolution_str = name_parts[0].lower()\n\n    # Handle standard genomic ranges\n    match_kb = re.match(r\"(\\d+)kb\", resolution_str)\n    if match_kb:\n        return float(match_kb.group(1)) * 1_000\n    match_mb = re.match(r\"(\\d+)mb\", resolution_str)\n    if match_mb:\n        return float(match_mb.group(1)) * 1_000_000\n\n    # Handle non-range types - decide how to represent them.\n    # Option 1: Return None (they won't be plotted on the numeric axis)\n    # Option 2: Assign arbitrary numbers (might distort scale)\n    # Option 3: Could use different marker symbols later if needed\n    if resolution_str in [\"regulatory\", \"gene\", \"cpg\"]:\n        return None  # Returning None is cleaner for a pure numeric axis\n\n    # Fallback for unrecognised formats\n    try:\n        # Maybe it's just a number (e.g., representing window size)?\n        return float(resolution_str)\n    except ValueError:\n        return None\n\n\nDefine graphing function graph_feature_set_scatter to plot performance metrics as a scatter plot instead of bar plot.\n\n\nCode\ndef graph_feature_set_scatter(\n    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n    input_sizes: Dict[str, int],\n    logdir: Optional[Path] = None,\n    metric_to_plot: str = \"Accuracy\",\n    name: Optional[str] = None,\n    metric_range: Optional[Tuple[float, float]] = None,\n    assay_task_key: str = ASSAY,\n    sex_task_key: str = SEX,\n    cell_type_task_key: str = CELL_TYPE,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Graphs performance metrics as a scatter plot with modifications.\n\n    X-axis: Number of Features (log scale).\n    Y-axis: Average performance metric (e.g., Accuracy, F1_macro) across folds.\n            Vertical lines indicate the min/max range across folds.\n    Color: Bin Size (bp, log scale).\n\n    Args:\n        all_metrics: Nested dict {feature_set: {task_name: {split_name: metric_dict}}}.\n        input_sizes: Dict {feature_set: num_features}.\n        logdir: Directory to save figures. If None, display only.\n        metric_to_plot: The metric key to use for the Y-axis ('Accuracy', 'F1_macro').\n        name: Optional suffix for figure titles and filenames.\n        metric_range: Optional tuple (min, max) to set the Y-axis range.\n        assay_task_key: Key used for the assay prediction task.\n        sex_task_key: Key used for the sex prediction task.\n        cell_type_task_key: Key used for the cell type prediction task.\n    \"\"\"\n    if metric_to_plot not in [\"Accuracy\", \"F1_macro\"]:\n        raise ValueError(\"metric_to_plot must be 'Accuracy' or 'F1_macro'\")\n\n    # --- Standard Name Handling (simplified from original) ---\n    non_standard_names = {ASSAY: f\"{ASSAY}_11c\", SEX: f\"{SEX}_w-mixed\"}\n\n    # --- Find reference and task names ----\n    reference_hdf5_type = next(iter(all_metrics), None)\n    if reference_hdf5_type is None or not all_metrics.get(reference_hdf5_type):\n        print(\n            \"Warning: Could not determine tasks from all_metrics. Trying default tasks.\"\n        )\n        cleaned_metadata_categories = {assay_task_key, sex_task_key, cell_type_task_key}\n    else:\n        metadata_categories = list(all_metrics[reference_hdf5_type].keys())\n        cleaned_metadata_categories = set()\n        for cat in metadata_categories:\n            original_name = cat\n            for standard, non_standard in non_standard_names.items():\n                if cat == non_standard:\n                    original_name = standard\n                    break\n            cleaned_metadata_categories.add(original_name)\n\n    # --- Define Bin size categories and Colors ---\n    bin_category_names = [\"1Kb\", \"10Kb\", \"100Kb\", \"1Mb\", \"10Mb\"]\n    bin_category_values = [1000, 10000, 100 * 1000, 1000 * 1000, 10000 * 1000]\n    discrete_colors = px.colors.sequential.Viridis_r\n    color_map = {\n        name: discrete_colors[i * 2] for i, name in enumerate(bin_category_names)\n    }\n\n    if verbose:\n        print(f\"Plotting for tasks: {list(cleaned_metadata_categories)}\")\n\n    for category_name in cleaned_metadata_categories:\n        plot_data_points = []\n\n        for feature_set_name_orig in all_metrics.keys():\n            try:\n                num_features = input_sizes[feature_set_name_orig]\n            except KeyError as e:\n                raise ValueError(\n                    f\"Feature set '{feature_set_name_orig}' not found in input_sizes\"\n                ) from e\n\n            # Parse Bin Size\n            bin_size = parse_bin_size(feature_set_name_orig)\n            if bin_size is None:\n                print(\n                    f\"Skipping {feature_set_name_orig}, could not parse numeric bin size.\"\n                )\n                continue\n\n            # 3. Get Metric Values (Average, Min, Max)\n            tasks_dicts = all_metrics[feature_set_name_orig]\n\n            # --- Task Name Lookup ---\n            # 1. Try the standard category name first\n            # 2. If standard name not found, use non-standard name\n            task_dict = None\n            task_name = category_name\n            if category_name in tasks_dicts:\n                task_dict = tasks_dicts[category_name]\n            else:\n                non_standard_task_name = non_standard_names.get(category_name)\n                if non_standard_task_name and non_standard_task_name in tasks_dicts:\n                    task_name = non_standard_task_name\n                    task_dict = tasks_dicts[non_standard_task_name]\n\n                if task_dict is None:\n                    raise ValueError(\n                        f\"Task '{category_name}' not found in feature set '{feature_set_name_orig}'\"\n                    )\n            # --- End Task Name Lookup ---\n\n            # Calculate average, min, max metric value across splits\n            try:\n                metric_values = []\n                for split, split_data in task_dict.items():\n                    if metric_to_plot in split_data:\n                        metric_values.append(split_data[metric_to_plot])\n                    else:\n                        print(\n                            f\"Warning: Metric '{metric_to_plot}' not found in split '{split}' for {feature_set_name_orig} / {task_name}\"\n                        )\n\n                if not metric_values:\n                    print(\n                        f\"Warning: No metric values found for {feature_set_name_orig} / {task_name} / {metric_to_plot}\"\n                    )\n                    continue\n\n                avg_metric = np.mean(metric_values)\n                min_metric = np.min(metric_values)\n                max_metric = np.max(metric_values)\n\n            except Exception as e:  # pylint: disable=broad-except\n                raise ValueError(\n                    f\"Error calculating metrics for {feature_set_name_orig} / {task_name}: {e}\"\n                ) from e\n\n            # Clean feature set name for hover text\n            clean_name = feature_set_name_orig.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n            clean_name = re.sub(r\"\\_[\\dmkb]+\\_coord\", \"\", clean_name)\n\n            # Store data for this point\n            plot_data_points.append(\n                {\n                    \"bin_size\": bin_size,\n                    \"num_features\": num_features,\n                    \"metric_value\": avg_metric,\n                    \"min_metric\": min_metric,  # For error bar low\n                    \"max_metric\": max_metric,  # For error bar high\n                    \"name\": clean_name,\n                    \"raw_name\": feature_set_name_orig,\n                }\n            )\n\n        if not plot_data_points:\n            raise ValueError(\n                f\"No suitable data points found to plot for task: {category_name}\"\n            )\n\n        # --- Determine Marker Symbols ---\n        marker_symbols = []\n        default_symbol = \"circle\"\n        random_symbol = \"cross\"\n        for p in plot_data_points:\n            if \"random\" in p[\"raw_name\"]:\n                marker_symbols.append(random_symbol)\n            else:\n                marker_symbols.append(default_symbol)\n\n        # --- Group Data by Category ---\n        points_by_category = {name: [] for name in bin_category_names}\n        for i, point_data in enumerate(plot_data_points):\n            bin_size = point_data[\"bin_size\"]\n            assigned_category = None\n            for cat_name, cat_value in zip(bin_category_names, bin_category_values):\n                if bin_size == cat_value:\n                    assigned_category = cat_name\n                    break\n            else:\n                raise ValueError(f\"Could not find category for bin size: {bin_size}\")\n\n            points_by_category[assigned_category].append(\n                {\n                    \"x\": point_data[\"num_features\"],  # X is Num Features\n                    \"y\": point_data[\"metric_value\"],\n                    \"error_up\": point_data[\"max_metric\"] - point_data[\"metric_value\"],\n                    \"error_down\": point_data[\"metric_value\"] - point_data[\"min_metric\"],\n                    \"text\": point_data[\"name\"],\n                    \"customdata\": [\n                        point_data[\"min_metric\"],\n                        point_data[\"max_metric\"],\n                        point_data[\"bin_size\"],\n                    ],  # Keep bin size for hover\n                    \"symbol\": marker_symbols[i],  # Assign symbol determined earlier\n                }\n            )\n\n        # --- Create Figure and Add Traces PER CATEGORY ---\n        fig = go.Figure()\n        traces = []\n\n        for cat_name in bin_category_names:  # Iterate in defined order for legend\n            points_in_cat = points_by_category[cat_name]\n            if not points_in_cat:\n                continue\n\n            category_color = color_map[cat_name]\n\n            # Extract data for all points in this category\n            x_vals = [p[\"x\"] for p in points_in_cat]\n            y_vals = [p[\"y\"] for p in points_in_cat]\n            error_up_vals = [p[\"error_up\"] for p in points_in_cat]\n            error_down_vals = [p[\"error_down\"] for p in points_in_cat]\n            text_vals = [p[\"text\"] for p in points_in_cat]\n            customdata_vals = [p[\"customdata\"] for p in points_in_cat]\n            symbols_vals = [p[\"symbol\"] for p in points_in_cat]\n\n            trace = go.Scatter(\n                x=x_vals,\n                y=y_vals,\n                mode=\"markers\",\n                name=cat_name,\n                showlegend=False,\n                legendgroup=cat_name,  # Group legend entries\n                marker=dict(\n                    color=category_color,\n                    size=15,\n                    symbol=symbols_vals,\n                    line=dict(width=1, color=\"DarkSlateGrey\"),\n                ),\n                error_y=dict(\n                    type=\"data\",\n                    symmetric=False,\n                    array=error_up_vals,\n                    arrayminus=error_down_vals,\n                    visible=True,\n                    thickness=1.5,\n                    width=15,\n                    color=category_color,\n                ),\n                text=text_vals,\n                customdata=customdata_vals,\n                hovertemplate=(\n                    f\"&lt;b&gt;%{{text}}&lt;/b&gt;&lt;br&gt;&lt;br&gt;\"\n                    f\"Num Features: %{{x:,.0f}}&lt;br&gt;\"\n                    f\"{metric_to_plot}: %{{y:.4f}}&lt;br&gt;\"\n                    f\"Bin Size: %{{customdata:,.0f}} bp&lt;br&gt;\"\n                    f\"{metric_to_plot} Range (10-fold): %{{customdata:.4f}} - %{{customdata:.4f}}\"\n                    \"&lt;extra&gt;&lt;/extra&gt;\"\n                ),\n            )\n            traces.append(trace)\n\n        fig.add_traces(traces)\n\n        # --- Add Legend ---\n        # Add a hidden scatter trace with square markers for legend\n        for cat_name in bin_category_names:\n            category_color = color_map[cat_name]\n            legend_trace = go.Scatter(\n                x=[None],\n                y=[None],\n                mode=\"markers\",\n                name=cat_name,\n                marker=dict(\n                    color=category_color,\n                    size=15,\n                    symbol=\"square\",\n                    line=dict(width=1, color=\"DarkSlateGrey\"),\n                ),\n                legendgroup=cat_name,\n                showlegend=True,\n            )\n            fig.add_trace(legend_trace)\n\n        # --- Update layout ---\n        title_name = category_name.replace(CELL_TYPE, \"biospecimen\")\n\n        plot_title = f\"{metric_to_plot} vs Number of Features - {title_name}\"\n        if name:\n            plot_title += f\" - {name}\"\n        xaxis_title = \"Number of Features (log scale)\"\n        xaxis_type = \"log\"\n\n        yaxis_title = metric_to_plot.replace(\"_\", \" \").title()\n        yaxis_type = \"linear\"\n\n        fig.update_layout(\n            xaxis_title=xaxis_title,\n            yaxis_title=yaxis_title,\n            xaxis_type=xaxis_type,\n            yaxis_type=yaxis_type,\n            yaxis_range=metric_range,\n            width=500,\n            height=500,\n            hovermode=\"closest\",\n            legend_title_text=\"Bin Size\",\n            title_text=plot_title,\n            **main_title_settings\n        )\n\n        if category_name == CELL_TYPE:\n            fig.update_yaxes(range=[0.75, 1.005])\n        elif category_name == ASSAY:\n            fig.update_yaxes(range=[0.96, 1.001])\n\n        # --- Save or show figure ---\n        if logdir:\n            logdir.mkdir(parents=True, exist_ok=True)\n            # Include \"modified\" or similar in filename to distinguish\n            base_name = f\"feature_scatter_MODIFIED_v2_{category_name}_{metric_to_plot}\"\n            if name:\n                base_name += f\"_{name}\"\n            html_path = logdir / f\"{base_name}.html\"\n            svg_path = logdir / f\"{base_name}.svg\"\n            png_path = logdir / f\"{base_name}.png\"\n\n            print(f\"Saving modified plot for {category_name} to {html_path}\")\n            fig.write_html(html_path)\n            fig.write_image(svg_path)\n            fig.write_image(png_path)\n\n        fig.show()\n\n\nGraph\n\nCode\nfor metric in [\"Accuracy\", \"F1_macro\"]:\n    graph_feature_set_scatter(\n        all_metrics=metrics_fig1c,  # type: ignore\n        input_sizes=input_sizes,\n        metric_to_plot=metric,\n        verbose=False,\n    )\n\n\n\n\n                                                \n\n\n                                                \n\n\n\n\n                                                \n\n\n                                                \n\n\n\nFig. 1C: Distribution of accuracy per training fold for different bin resolutions for the Assay and Biospecimen classifiers. The circles represent the means and the whiskers the min and max values of the ten training folds.\n\n\n\nDefine function create_confusion_matrix to create and show a confusion matrix.\n\n\nCode\ndef create_confusion_matrix(\n    df: pd.DataFrame,\n    name: str = \"confusion_matrix\",\n    logdir: Path | None = None,\n    min_pred_score: float = 0,\n    majority: bool = False,\n    verbose:bool=False\n) -&gt; None:\n    \"\"\"Create a confusion matrix for the given DataFrame and save it to the logdir.\n\n    Args:\n        df (pd.DataFrame): The DataFrame containing the results.\n        logdir (Path): The directory path for saving the figures.\n        name (str): The name for the saved figures.\n        min_pred_score (float): The minimum prediction score to consider.\n        majority (bool): Whether to use majority vote (uuid-wise) for the predicted class.\n    \"\"\"\n    # Compute confusion matrix\n    classes = sorted(df[\"True class\"].unique())\n    if \"Max pred\" not in df.columns:\n        df[\"Max pred\"] = df[classes].max(axis=1)  # type: ignore\n    filtered_df = df[df[\"Max pred\"] &gt; min_pred_score]\n\n    if majority:\n        # Majority vote for predicted class\n        groupby_uuid = filtered_df.groupby([\"uuid\", \"True class\", \"Predicted class\"])[\n            \"Max pred\"\n        ].aggregate([\"size\", \"mean\"])\n\n        if groupby_uuid[\"size\"].max() &gt; 3:\n            raise ValueError(\"More than three predictions for the same uuid.\")\n\n        groupby_uuid = groupby_uuid.reset_index().sort_values(\n            [\"uuid\", \"True class\", \"size\"], ascending=[True, True, False]\n        )\n        groupby_uuid = groupby_uuid.drop_duplicates(\n            subset=[\"uuid\", \"True class\"], keep=\"first\"\n        )\n        filtered_df = groupby_uuid\n\n    confusion_mat = sk_cm(\n        filtered_df[\"True class\"], filtered_df[\"Predicted class\"], labels=classes\n    )\n\n    mat_writer = ConfusionMatrixWriter(labels=classes, confusion_matrix=confusion_mat)\n\n    if logdir is None:\n        logdir = Path(tempfile.gettempdir())\n\n    files = mat_writer.to_all_formats(logdir, name=f\"{name}_n{len(filtered_df)}\")\n\n    if verbose:\n        print(f\"Saved confusion matrix to {logdir}:\")\n        for file in files:\n            print(Path(file).name)\n\n    for file in files:\n        if \"png\" in file.name:\n            scale = 0.6\n            display(Image(filename=file, width=1250*scale, height=1000*scale))\n\n\nPrepare prediction data for confusion matrix.\n\n\nCode\nassay_split_dfs = split_results_handler.gather_split_results_across_methods(\n    results_dir=data_dir_100kb, label_category=ASSAY, only_NN=True\n)\nconcat_assay_df = split_results_handler.concatenate_split_results(assay_split_dfs)[\"NN\"]\n\ndf_with_meta = metadata_handler.join_metadata(concat_assay_df, metadata_v2)  # type: ignore\nif \"Predicted class\" not in df_with_meta.columns:\n    raise ValueError(\"`Predicted class` not in DataFrame\")\n\nclassifier_name = \"MLP\"\nmin_pred_score = 0\nmajority = False\n\nname = f\"{classifier_name}_pred&gt;{min_pred_score}\"\n\nlogdir = base_fig_dir / \"fig1_EpiAtlas_assay\" / \"fig1_supp_D-assay_c11_confusion_matrices\"\nif majority:\n    logdir = logdir / \"per_uuid\"\nelse:\n    logdir = logdir / \"per_file\"\nlogdir.mkdir(parents=True, exist_ok=True)\n\n\nGraph.\n\n\nCode\ncreate_confusion_matrix(\n    df=df_with_meta,\n    min_pred_score=min_pred_score,\n    majority=majority,\n)\n\n\n\n\n\n\n\n\n\nFig. 1D: Confusion matrix aggregating the cross-validation folds (therefore showing all files) without applying a prediction score threshold. RNA-seq and WGBS data were both separated according to two protocols during initial training (but combined thereafter to nine assays).\n\n\n\n\nFig. 1E: Genome browser representation showing in black the datasets swap between H3K4me3 and H3K27ac for IHECRE00001897 in the metadata freeze v1.0, along with typical correct datasets over a representative region.\n\n\n\n\nMore detailled performance of EpiClass Assay and Biospecimen classifiers.\n\n\nFig. 1A,B data points are included in these two graphs (MLP data points).\n\n\n\nDefine graphing function plot_multiple_models_split_metrics.\n\n\nCode\ndef plot_multiple_models_split_metrics(\n    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n    label_category: str,\n    logdir: Path | None = None,\n    filename: str = \"fig1_all_classifiers_metrics\",\n) -&gt; None:\n    \"\"\"Render to box plots the metrics per classifier/models and split, each in its own subplot.\n\n    Args:\n        split_metrics: A dictionary containing metric scores for each classifier and split.\n        label_category: The label category for the classification task.\n        name: The name of the figure.\n        logdir: The directory to save the figure to. If None, the figure is only displayed.\n\n    Returns:\n        None: Displays the figure and saves it to the logdir if provided.\n    \"\"\"\n    metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n    classifier_names = list(next(iter(split_metrics.values())).keys())\n    classifier_names = [\"NN\", \"LR\", \"LGBM\", \"LinearSVC\", \"RF\"]\n\n    # Create subplots, one row for each metric\n    fig = make_subplots(\n        rows=1,\n        cols=len(metrics),\n        subplot_titles=metrics,\n        horizontal_spacing=0.075,\n    )\n\n    for i, metric in enumerate(metrics):\n        for classifier in classifier_names:\n            values = [split_metrics[split][classifier][metric] for split in split_metrics]\n            if classifier == \"NN\":\n                classifier = \"MLP\"\n            fig.add_trace(\n                go.Box(\n                    y=values,\n                    name=classifier,\n                    line=dict(color=\"black\", width=1.5),\n                    marker=dict(size=3, color=\"black\"),\n                    boxmean=True,\n                    boxpoints=\"all\",  # or \"outliers\" to show only outliers\n                    pointpos=-1.4,\n                    showlegend=False,\n                    width=0.5,\n                    hovertemplate=\"%{text}\",\n                    text=[\n                        f\"{split}: {value:.4f}\"\n                        for split, value in zip(split_metrics, values)\n                    ],\n                ),\n                row=1,\n                col=i + 1,\n            )\n\n    fig.update_layout(\n        title_text=f\"{label_category} classification\",\n        boxmode=\"group\",\n        **main_title_settings,\n    )\n\n    # Adjust y-axis\n    if label_category == ASSAY:\n        range_acc = [0.95, 1.001]\n        range_AUC = [0.992, 1.0001]\n    elif label_category == CELL_TYPE:\n        range_acc = [0.81, 1]\n        range_AUC = [0.96, 1]\n    else:\n        range_acc = [0.6, 1.001]\n        range_AUC = [0.9, 1.0001]\n\n    fig.update_layout(\n        yaxis=dict(range=range_acc),\n        yaxis2=dict(range=range_acc),\n        yaxis3=dict(range=range_AUC),\n        yaxis4=dict(range=range_AUC),\n        height=450,\n    )\n\n    fig.update_layout(margin=dict(l=20, r=20))\n\n    # Save figure\n    if logdir:\n        fig.write_image(logdir / f\"{filename}.svg\")\n        fig.write_image(logdir / f\"{filename}.png\")\n        fig.write_html(logdir / f\"{filename}.html\")\n\n    fig.show()\n\n\nGraph.\n\nCode\nmerge_assays = True\n\nfor label_category in [ASSAY, CELL_TYPE]:\n    all_split_dfs = split_results_handler.gather_split_results_across_methods(\n        results_dir=data_dir_100kb,\n        label_category=label_category,\n        only_NN=False,\n    )\n\n    if merge_assays and label_category == ASSAY:\n        for split_name, split_dfs in all_split_dfs.items():\n            for classifier_type, df in split_dfs.items():\n                split_dfs[classifier_type] = merge_similar_assays(df)\n\n    split_metrics = split_results_handler.compute_split_metrics(all_split_dfs)\n\n    plot_multiple_models_split_metrics(\n        split_metrics,\n        label_category=label_category,\n    )\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\nSupplementary Figure 1A,B: Distribution of performance scores (accuracy, F1 as well as micro and macro AUROC) per training fold (dots) for each machine learning approach used for training on the Assay (A) and Biospecimen (B) metadata. Micro-averaging aggregates contributions from all classes (global true positive rate and false positive rate); macro-averaging averages the true positive rate from each class. Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5× the interquartile range.\n\n\nGoing forward, all results are for MLP classifiers.\n\n\n\nDefine graphing function plot_roc_curves. Computes macro-average ROC curves manually.\n\n\nCode\ndef plot_roc_curves(\n    results_df: pd.DataFrame,\n    label_category: str,\n    logdir: Path | None = None,\n    name: str = \"roc_curve\",\n    title: str | None = None,\n    colors_dict: Dict | None = None,  # Optional specific colors\n    verbose: bool = False,\n) -&gt; None:\n    \"\"\"\n    Generates and plots ROC curves for multi-class classification results using Plotly.\n\n    Calculates and plots individual class ROC curves, micro-average, and macro-average ROC curves.\n\n    Args:\n        results_df (pd.DataFrame): DataFrame with true labels and prediction probabilities for each class.\n                                   Must contain the `label_category` column (e.g., 'True class')\n                                   and probability columns named after each class.\n        label_category (str): The column name containing the true labels (e.g., 'True class', ASSAY, CELL_TYPE).\n        logdir (Path | None): Directory to save the figure. If None, only displays the figure.\n        name (str): Base name for saved files (e.g., \"supp_fig1e\").\n        title (str | None): Title suffix for the plot. If None, a default title based on label_category is used.\n        colors_dict (Dict | None): Optional dictionary mapping class names to colors. If None or a class\n                                   is missing, default Plotly colors are used.\n    \"\"\"\n    df = results_df.copy()\n    true_label_col = \"True class\"  # Assuming 'True class' holds the ground truth labels\n\n    if true_label_col not in df.columns:\n        raise ValueError(f\"True label column '{true_label_col}' not found in DataFrame.\")\n\n    classes = sorted(df[true_label_col].unique())\n    if verbose:\n        print(f\"Using classes: {classes}\")\n\n    n_classes = len(classes)\n    if n_classes &lt; 2:\n        print(\n            f\"Warning: Only {n_classes} class found after processing. Cannot generate ROC curve.\"\n        )\n        return\n\n    # Check if probability columns exist for all determined classes\n    missing_cols = [c for c in classes if c not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing probability columns for classes: {missing_cols}\")\n\n    # Binarize the true labels against the final set of classes\n    try:\n        y_true = label_binarize(df[true_label_col], classes=classes)\n    except ValueError as e:\n        raise ValueError(\n            f\"Error binarizing labels for classes {classes}. Check if all labels in '{true_label_col}' are included in 'classes'.\"\n        ) from e\n\n    if n_classes == 2 and y_true.shape[1] == 1:\n        # Adjust for binary case where label_binarize might return one column\n        y_true = np.hstack((1 - y_true, y_true))  # type: ignore\n    elif y_true.shape[1] != n_classes:\n        raise ValueError(\n            f\"Binarized labels shape {y_true.shape} does not match number of classes {n_classes}\"\n        )\n\n    # Get the predicted probabilities for each class\n    # Ensure columns are in the same order as 'classes'\n    y_score = df[classes].values\n\n    # --- Compute ROC curve and ROC area for each class ---\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i, class_name in enumerate(classes):\n        try:\n            fpr[class_name], tpr[class_name], _ = roc_curve(\n                y_true=y_true[:, i], y_score=y_score[:, i]  # type: ignore\n            )\n            roc_auc[class_name] = auc(fpr[class_name], tpr[class_name])\n        except ValueError as e:\n            raise ValueError(\"Could not compute ROC for class {class_name}.\") from e\n\n    # --- Compute micro-average ROC curve and ROC area ---\n    try:\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_score.ravel())  # type: ignore\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    except ValueError as e:\n        raise ValueError(\"Could not compute micro-average ROC.\") from e\n\n    # --- Compute macro-average ROC curve and ROC area ---\n    try:\n        # Aggregate all false positive rates\n        all_fpr = np.unique(\n            np.concatenate(\n                [fpr[class_name] for class_name in classes if class_name in fpr]\n            )\n        )\n        # Interpolate all ROC curves at these points\n        mean_tpr = np.zeros_like(all_fpr)\n        valid_classes_count = 0\n        for class_name in classes:\n            if class_name in fpr and class_name in tpr:\n                mean_tpr += np.interp(all_fpr, fpr[class_name], tpr[class_name])\n                valid_classes_count += 1\n\n        # Average it and compute AUC\n        if valid_classes_count &gt; 0:\n            mean_tpr /= valid_classes_count\n            fpr[\"macro\"] = all_fpr\n            tpr[\"macro\"] = mean_tpr\n            roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n        else:\n            raise ValueError(\"No valid classes found for macro averaging.\")\n\n    except ValueError as e:\n        raise ValueError(\"Could not compute macro-average ROC.\") from e\n\n    # --- Plot all ROC curves ---\n    fig = go.Figure()\n\n    # Plot diagonal line for reference\n    fig.add_shape(\n        type=\"line\", line=dict(dash=\"dash\", color=\"grey\", width=1), x0=0, x1=1, y0=0, y1=1\n    )\n\n    # Define colors for plotting\n    color_cycle = px.colors.qualitative.Plotly  # Default cycle\n    plot_colors = {}\n    for i, cls_name in enumerate(classes):\n        if colors_dict and cls_name in colors_dict:\n            plot_colors[cls_name] = colors_dict[cls_name]\n        else:\n            plot_colors[cls_name] = color_cycle[i % len(color_cycle)]\n\n    # Plot Micro-average ROC curve first (often plotted thicker/dashed)\n    fig.add_trace(\n        go.Scatter(\n            x=fpr[\"micro\"],\n            y=tpr[\"micro\"],\n            mode=\"lines\",\n            name=f'Micro-average ROC (AUC = {roc_auc[\"micro\"]:.5f})',\n            line=dict(color=\"deeppink\", width=3, dash=\"dash\"),\n            hoverinfo=\"skip\",  # Less important for hover usually\n        )\n    )\n\n    # Plot Macro-average ROC curve\n    fig.add_trace(\n        go.Scatter(\n            x=fpr[\"macro\"],\n            y=tpr[\"macro\"],\n            mode=\"lines\",\n            name=f'Macro-average ROC (AUC = {roc_auc[\"macro\"]:.5f})',\n            line=dict(color=\"navy\", width=3, dash=\"dash\"),\n            hoverinfo=\"skip\",\n        )\n    )\n\n    # Plot individual class ROC curves\n    for class_name in classes:\n        if class_name not in fpr or class_name not in tpr or class_name not in roc_auc:\n            continue  # Skip if calculation failed\n        fig.add_trace(\n            go.Scatter(\n                x=fpr[class_name],\n                y=tpr[class_name],\n                mode=\"lines\",\n                name=f\"{class_name} (AUC = {roc_auc[class_name]:.5f})\",\n                line=dict(width=1.5, color=plot_colors.get(class_name)),\n                hovertemplate=f\"&lt;b&gt;{class_name}&lt;/b&gt;&lt;br&gt;FPR=%{{x:.5f}}&lt;br&gt;TPR=%{{y:.5f}}&lt;extra&gt;&lt;/extra&gt;\",  # Show class name and values on hover\n            )\n        )\n\n    # --- Update layout ---\n    base_title = f\"ROC Curves&lt;br&gt;{label_category}\"\n    plot_title = f\"{base_title} - {title}\" if title else base_title\n\n    title_settings=dict(\n        yanchor=\"top\",\n        yref=\"paper\",\n        y=0.97,\n        xanchor=\"center\",\n        xref=\"paper\",\n        x=0.5,\n    )\n\n    fig.update_layout(\n        title=title_settings,\n        title_text=plot_title,\n        xaxis_title=\"False Positive Rate (1 - Specificity)\",\n        yaxis_title=\"True Positive Rate (Sensitivity)\",\n        xaxis=dict(range=[0.0, 1.0], constrain=\"domain\"),  # Ensure axes range 0-1\n        yaxis=dict(\n            range=[0.0, 1.01], scaleanchor=\"x\", scaleratio=1, constrain=\"domain\"\n        ),  # Make it square-ish, slight top margin\n        width=800,\n        height=650,\n        hovermode=\"closest\",\n        legend=dict(\n            traceorder=\"reversed\",  # Show averages first in legend\n            title=\"Classes & Averages\",\n            font=dict(size=9),\n            itemsizing=\"constant\",\n            y=0.8,\n            yref=\"paper\",\n        ),\n        margin=dict(l=60, r=30, t=0, b=0),\n    )\n\n    # --- Save figure if logdir is provided ---\n    if logdir:\n        logdir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n        filename_base = f\"{name}_{label_category}_roc\"\n        filepath_base = logdir / filename_base\n\n        fig.write_html(f\"{filepath_base}.html\")\n        fig.write_image(f\"{filepath_base}.svg\", width=800, height=750)\n        fig.write_image(f\"{filepath_base}.png\", width=800, height=750, scale=2)\n\n        print(f\"Saved ROC curve plots for {label_category} to {logdir}\")\n        print(f\" -&gt; {filename_base}.html / .svg / .png\")\n\n    fig.show()\n\n\nPrepare assay data for plotting.\n\n\nCode\ndata_dir = (\n    mixed_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{ASSAY}_1l_3000n\"\n    / \"11c\"\n    / \"10fold-oversampling\"\n)\nPathChecker.check_directory(data_dir)\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\n\ndf = merge_similar_assays(concat_df_w_meta.copy())\n\n\nGraph assay results.\n\n\nCode\nplot_roc_curves(\n    results_df=df.copy(),\n    label_category=ASSAY,\n    title=\"Aggregated 10fold\",  # Title suffix\n    colors_dict=assay_colors,\n    verbose=False,\n)\n\n\n                                                \n\n\nPrepare biospecimen data for plotting.\n\n\nCode\ndata_dir = (\n    mixed_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{CELL_TYPE}_1l_3000n\"\n    / \"10fold-oversampling\"\n)\nPathChecker.check_directory(data_dir)\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\n\n\nGraph biospecimen results.\n\n\nCode\nplot_roc_curves(\n    results_df=concat_df_w_meta,\n    label_category=CELL_TYPE,\n    title=\"Aggregated 10fold\",  # Title suffix\n    colors_dict=cell_type_colors,\n    verbose=False,\n)\n\n\n                                                \n\n\nSupplementary Figure 1C: ROC curves from aggregated cross-validation results for the Assay and Biospecimen classifiers. Curves for each class are computed in a one-vs-rest scheme.\n\n\n\nDefine graphing function create_blklst_graphs.\n\n\nCode\ndef create_blklst_graphs(\n    feature_set_metrics_dict: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n    logdir: Path | None = None,\n) -&gt; List[go.Figure]:\n    \"\"\"Create boxplots for blacklisted related feature sets.\n\n    Args:\n        feature_set_metrics_dict (Dict[str, Dict[str, Dict[str, Dict[str, float]]]]): The dictionary containing all metrics for all blklst related feature sets.\n            format: {feature_set: {task_name: {split_name: metric_dict}}}\n        logdir (Path, Optional): The directory to save the figure to. If None, the figure is only displayed.\n    \"\"\"\n    figs = []\n\n    # Assume names exist in all feature sets\n    task_names = list(feature_set_metrics_dict.values())[0].keys()\n\n    traces_names_dict = {\n        \"hg38_100kb_all_none\": \"observed\",\n        \"hg38_100kb_all_none_0blklst\": \"0blklst\",\n        \"hg38_100kb_all_none_0blklst_winsorized\": \"0blklst_winsorized\",\n    }\n\n    for task_name in task_names:\n        category_fig = make_subplots(\n            rows=1,\n            cols=2,\n            shared_yaxes=False,\n            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n            horizontal_spacing=0.1,\n        )\n        for feature_set_name, tasks_dicts in feature_set_metrics_dict.items():\n            task_dict = tasks_dicts[task_name]\n            trace_name = traces_names_dict[feature_set_name]\n\n            # Accuracy\n            metric = \"Accuracy\"\n            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n                for split, metrics_dict in task_dict.items()\n            ]\n\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=\"all\",\n                    showlegend=False,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                ),\n                row=1,\n                col=1,\n            )\n\n            metric = \"F1_macro\"\n            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n                for split, metrics_dict in task_dict.items()\n            ]\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=\"all\",\n                    showlegend=False,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                ),\n                row=1,\n                col=2,\n            )\n\n        category_fig.update_xaxes(\n            categoryorder=\"array\",\n            categoryarray=list(traces_names_dict.values()),\n        )\n        category_fig.update_yaxes(range=[0.9, 1.001])\n\n        category_fig.update_layout(\n            title_text=task_name,\n            height=600,\n            width=500,\n            **main_title_settings\n        )\n\n        # Save figure\n        if logdir:\n            task_name = task_name.replace(\"_1l_3000n-10fold\", \"\")\n            base_name = f\"metrics_{task_name}\"\n\n            category_fig.write_html(logdir / f\"{base_name}.html\")\n            category_fig.write_image(logdir / f\"{base_name}.svg\")\n            category_fig.write_image(logdir / f\"{base_name}.png\")\n\n        figs.append(category_fig)\n\n    return figs\n\n\nPrepare paths.\n\n\nCode\ninclude_sets = [\n    \"hg38_100kb_all_none\",\n    \"hg38_100kb_all_none_0blklst\",\n    \"hg38_100kb_all_none_0blklst_winsorized\",\n]\n\nresults_folder_blklst = base_data_dir / \"training_results\" / \"2023-01-epiatlas-freeze\"\nPathChecker.check_directory(results_folder_blklst)\n\n\nCompute metrics.\n\n\nCode\n# Select 10-fold oversampling runs\n# expected result shape: {feature_set: {task_name: {split_name: metrics_dict}}}\nall_metrics_blklst: Dict[\n    str, Dict[str, Dict[str, Dict[str, float]]]\n] = split_results_handler.obtain_all_feature_set_data(\n    return_type=\"metrics\",\n    parent_folder=results_folder_blklst,\n    merge_assays=True,\n    include_categories=[ASSAY, CELL_TYPE],\n    include_sets=include_sets,\n    oversampled_only=False,\n    verbose=False,\n)  # type: ignore\n\n\nGraph.\n\nCode\nfigs = create_blklst_graphs(all_metrics_blklst)\n\nfigs[0].show()\nfigs[1].show()\n\n\n\n\n                                                \n\n\n                                                \n\n\n\nSupplementary Figure 1D: Distribution of accuracy and F1-score per training fold (dots) for the Assay and Biospecimen classifiers after removing signal from blacklisted regions and applying winsorization of 0.1%. Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5× the interquartile range.\n\n\n\n\nE: Assay training 10-fold cross-validation\nF: Assay complete training (mixed tracks), predictions on imputed data (all pval)\nG: Biospecimen 10-fold cross-validation\n\n\nDefine graphing function plot_prediction_scores_distribution.\nSupplementary Figure 1E-G: Distribution of average prediction score per file (dots) for the majority-vote class (up to three track type files) (E, F) or individual file (G), from the MLP approach for the Assay (E, G) and Biospecimen classifiers (F), using aggregated cross-validation results from observed data (E, F) or results from the classifier trained on all observed data and applied to imputed data from EpiATLAS (G). Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5× the interquartile range, with a violin representation on top.\n\n\nGather prediction scores.\n\n\nCode\ndata_dir = (\n    mixed_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{ASSAY}_1l_3000n\"\n    / \"11c\"\n    / \"10fold-oversampling\"\n)\nPathChecker.check_directory(data_dir)\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\n\n\nGraph.\n\n\nCode\nplot_prediction_scores_distribution(\n    results_df=concat_df_w_meta,\n    group_by_column=ASSAY,\n    merge_assay_pairs=True,\n    min_y=0.7,\n    title=\"11 classes assay training&lt;br&gt;Prediction scores for 10-fold cross-validation\",\n)\n\n\n                                                \n\n\n\n\n\nGather prediction scores.\n\n\nCode\ndata_dir = data_dir_100kb / f\"{CELL_TYPE}_1l_3000n\" / \"10fold-oversampling\"\nPathChecker.check_directory(data_dir)\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\nconcat_df_w_meta.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n\n\nGraph.\n\n\nCode\nplot_prediction_scores_distribution(\n    results_df=concat_df_w_meta,\n    group_by_column=ASSAY,\n    min_y=0,\n    title=\"Biospecimen training&lt;br&gt;Prediction scores for 10-fold cross-validation\",\n)\n\n\nSkipping assay merging: Wrong results dataframe, rna or wgbs columns missing.\n\n\n                                                \n\n\n\n\n\nGather imputed signal metadata.\n\n\nCode\nmetadata_path = (\n    paper_dir\n    / \"data\"\n    / \"metadata\"\n    / \"epiatlas\"\n    / \"imputed\"\n    / \"hg38_epiatlas_imputed_pval_chip_2024-02.json\"\n)\nmetadata_imputed: pd.DataFrame = metadata_handler.load_any_metadata(metadata_path, as_dataframe=True)  # type: ignore\n\n\nGather prediction scores.\n\n\nCode\ndata_dir = (\n    gen_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{ASSAY}_1l_3000n\"\n    / \"11c\"\n    / \"complete_no_valid_oversample\"\n    / \"predictions\"\n    / \"epiatlas_imputed\"\n    / \"ChIP\"\n)\nPathChecker.check_directory(data_dir)\n\ndf_pred = pd.read_csv(\n    data_dir / \"complete_no_valid_oversample_prediction.csv\",\n    index_col=0,\n)\n\n\nPrepare dataframe for graphing.\n\n\nCode\nassay_classes = list(metadata_v2_df[ASSAY].unique())\ndf_pred = split_results_handler.add_max_pred(df_pred, expected_classes=assay_classes)\n\naugmented_df = pd.merge(df_pred, metadata_imputed, left_index=True, right_on=\"md5sum\")\naugmented_df[\"True class\"] = augmented_df[ASSAY]\nprint(\"Number of files per assay:\")\nprint(augmented_df[\"True class\"].value_counts(dropna=False).to_string())\n\n\nNumber of files per assay:\nh3k36me3    1703\nh3k27me3    1703\nh3k9me3     1700\nh3k4me1     1688\nh3k4me3     1688\nh3k27ac     1088\n\n\n\n\nGraph.\n\n\nCode\nplot_prediction_scores_distribution(\n    results_df=augmented_df,\n    group_by_column=ASSAY,\n    merge_assay_pairs=True,\n    min_y=0.79,\n    use_aggregate_vote=False,\n    title=\"Complete 11c assay classifier&lt;br&gt;inference on imputed data\",\n)\n\n\n                                                \n\n\n\n\n\n\nFor the code that produced the figures, see src/python/epiclass/utils/notebooks/paper/confidence_threshold.ipynb (permalink).\n\n\n\n\nSupplementary Figure 1H,I: Distribution of aggregated accuracy, F1-score and corresponding file subset size across varying prediction score thresholds, based on pooled predictions from all cross-validation folds for the Assay (H) and Biospecimen (I) classifiers.\n\n\n\n\nPerformance of EpiClass Assay and Biospecimen classifiers evaluated per training fold across various bin size resolutions and genomic feature sets.\n\n\nGraph assay/biospecimen metrics per 10fold, for reference (no assay breakdown)\n\nCode\nmetrics_supp2 = {name: all_metrics[name] for name in feature_sets_14}\n\ngraph_feature_set_metrics(\n    all_metrics=metrics_supp2,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=900,\n    height=600,\n)\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\nSupplementary Figure 2A,B: Distribution of accuracy over all files for the Assay (A) or Biospecimen (B) classifier.\n\n\n\nDefine function to compute metrics per assay: prepare_metric_sets_per_assay\n\n\nCode\ndef prepare_metric_sets_per_assay(\n    all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]], verbose: bool = False\n) -&gt; Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]:\n    \"\"\"Prepare metric sets per assay.\n\n    Args:\n        all_results (Dict[str, Dict[str, Dict[str, pd.DataFrame]]]): A dictionary containing all results for all feature sets.\n\n    Returns:\n        Dict[str, Dict[str, Dict[str, Dict[str, float]]]]: A dictionary containing all metrics per assay for all feature sets.\n            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n    \"\"\"\n    if verbose:\n        print(\"Loading metadata.\")\n    metadata = metadata_handler.load_metadata(\"v2\")\n    metadata.convert_classes(ASSAY, ASSAY_MERGE_DICT)\n    md5_per_assay = metadata.md5_per_class(ASSAY)\n    md5_per_assay = {k: set(v) for k, v in md5_per_assay.items()}\n\n    if verbose:\n        print(\"Getting results per assay.\")\n    results_per_assay = {}\n    for assay_label in ASSAY_ORDER:\n        if verbose:\n            print(assay_label)\n        results_per_assay[assay_label] = {}\n        for feature_set, task_dict in all_results.items():\n            if verbose:\n                print(feature_set)\n            results_per_assay[assay_label][feature_set] = {}\n            for task_name, split_dict in task_dict.items():\n                if verbose:\n                    print(task_name)\n                results_per_assay[assay_label][feature_set][task_name] = {}\n\n                # Only keep the relevant assay\n                for split_name, split_df in split_dict.items():\n                    if verbose:\n                        print(split_name)\n                    assay_df = split_df[split_df.index.isin(md5_per_assay[assay_label])]\n                    results_per_assay[assay_label][feature_set][task_name][\n                        split_name\n                    ] = assay_df\n\n    if verbose:\n        print(\"Finished getting results per assay. Now computing metrics.\")\n    metrics_per_assay = {}\n    for assay_label in ASSAY_ORDER:\n        if verbose:\n            print(assay_label)\n        metrics_per_assay[assay_label] = {}\n        for feature_set, task_dict in results_per_assay[assay_label].items():\n            if verbose:\n                print(feature_set)\n            assay_metrics = split_results_handler.compute_split_metrics(\n                task_dict, concat_first_level=True\n            )\n            inverted_dict = split_results_handler.invert_metrics_dict(assay_metrics)\n            metrics_per_assay[assay_label][feature_set] = inverted_dict\n\n    return metrics_per_assay\n\n\nDefine graphing function graph_feature_set_metrics_per_assay\n\n\nCode\ndef graph_feature_set_metrics_per_assay(\n    all_metrics_per_assay: Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]],\n    input_sizes: Dict[str, int],\n    logdir: Path | None = None,\n    sort_by_input_size: bool = False,\n    name: str | None = None,\n    y_range: Tuple[float, float] | None = None,\n    boxpoints: str = \"outliers\",\n) -&gt; None:\n    \"\"\"Graph the metrics for all feature sets, per assay, with separate plots for accuracy and F1-score.\n\n    Args:\n        all_metrics_per_assay (Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]): A dictionary containing all metrics per assay for all feature sets.\n            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n        logdir (Path): The directory where the figures will be saved. If None, the figures will only be displayed.\n        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n        name (str|None): The name of the figure.\n        y_range (Tuple[float, float]|None): The y-axis range for the plots.\n        boxpoints (str): The type of points to display in the box plots. Defaults to \"outliers\".\n    \"\"\"\n    valid_boxpoints = [\"all\", \"outliers\"]\n    if boxpoints not in valid_boxpoints:\n        raise ValueError(f\"Invalid boxpoints value. Choose from {valid_boxpoints}.\")\n\n    fig_assay_order = [\n        \"rna_seq\",\n        \"h3k27ac\",\n        \"h3k4me1\",\n        \"h3k4me3\",\n        \"h3k36me3\",\n        \"h3k27me3\",\n        \"h3k9me3\",\n        \"input\",\n        \"wgbs\",\n    ]\n\n    reference_assay = next(iter(all_metrics_per_assay))\n    reference_feature_set = next(iter(all_metrics_per_assay[reference_assay]))\n    metadata_categories = list(\n        all_metrics_per_assay[reference_assay][reference_feature_set].keys()\n    )\n\n    for _, category in enumerate(metadata_categories):\n        for metric, metric_name in [\n            (\"Accuracy\", \"Accuracy\"),\n            (\"F1_macro\", \"F1-score (macro)\"),\n        ]:\n            fig = go.Figure()\n\n            feature_sets = list(all_metrics_per_assay[reference_assay].keys())\n            unique_feature_sets = set(feature_sets)\n            for assay in fig_assay_order:\n                if set(all_metrics_per_assay[assay].keys()) != unique_feature_sets:\n                    raise ValueError(\"Different feature sets through assays.\")\n\n            feature_set_order = feature_sets\n            if sort_by_input_size:\n                feature_set_order = sorted(\n                    feature_set_order, key=lambda x: input_sizes[x]\n                )\n\n            # Adjust spacing so each assay group has dedicated space based on the number of feature sets\n            spacing_multiplier = (\n                1.1  # Increase this multiplier if needed to add more spacing\n            )\n            x_positions = {\n                assay: i * len(feature_set_order) * spacing_multiplier\n                for i, assay in enumerate(fig_assay_order)\n            }\n\n            for i, feature_set_name in enumerate(feature_set_order):\n                resolution = (\n                    feature_set_name.replace(\"_none\", \"\")\n                    .replace(\"hg38_\", \"\")\n                    .split(\"_\")[0]\n                )\n                color = resolution_colors[resolution]\n                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n\n                for assay in fig_assay_order:\n                    if feature_set_name not in all_metrics_per_assay[assay]:\n                        continue\n\n                    tasks_dicts = all_metrics_per_assay[assay][feature_set_name]\n\n                    if feature_set_name not in input_sizes:\n                        print(f\"Skipping {feature_set_name}, no input size found.\")\n                        continue\n\n                    task_name = category\n                    if \"split\" in task_name:\n                        raise ValueError(\"Split in task name. Wrong metrics dict.\")\n\n                    try:\n                        task_dict = tasks_dicts[task_name]\n                    except KeyError:\n                        print(\n                            f\"Skipping {feature_set_name}, {task_name} for assay {assay}\"\n                        )\n                        continue\n\n                    y_vals = [task_dict[split][metric] for split in task_dict]\n                    hovertext = [\n                        f\"{assay} - {display_name} - {split}: {metrics_dict[metric]:.4f}\"\n                        for split, metrics_dict in task_dict.items()\n                    ]\n\n                    x_position = x_positions[assay] + i\n                    fig.add_trace(\n                        go.Box(\n                            x=[x_position] * len(y_vals),\n                            y=y_vals,\n                            name=f\"{assay}|{display_name}\",\n                            boxmean=True,\n                            boxpoints=boxpoints,\n                            marker=dict(size=3, color=\"black\"),\n                            line=dict(width=1, color=\"black\"),\n                            fillcolor=color,\n                            hovertemplate=\"%{text}\",\n                            text=hovertext,\n                            showlegend=False,\n                            legendgroup=display_name,\n                        )\n                    )\n\n                    # separate box groups\n                    fig.add_vline(\n                        x=x_positions[assay] - 1, line_width=1, line_color=\"black\"\n                    )\n\n            # Add dummy traces for the legend\n            for feature_set_name in feature_set_order:\n                resolution = (\n                    feature_set_name.replace(\"_none\", \"\")\n                    .replace(\"hg38_\", \"\")\n                    .split(\"_\")[0]\n                )\n                color = resolution_colors[resolution]\n                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n                display_name = re.sub(r\"\\_[\\dmkb]+\\_coord\", \"\", display_name)\n\n                fig.add_trace(\n                    go.Scatter(\n                        name=display_name,\n                        x=[None],\n                        y=[None],\n                        mode=\"markers\",\n                        marker=dict(size=10, color=color),\n                        showlegend=True,\n                        legendgroup=display_name,\n                    )\n                )\n\n            title = f\"{category} - {metric_name} (per assay)\"\n            if name is not None:\n                title += f\" - {name}\"\n\n            fig.update_layout(\n                width=1250,\n                height=900,\n                title_text=title,\n                xaxis_title=\"Assay\",\n                yaxis_title=metric_name,\n                **main_title_settings\n            )\n\n            # Create x-axis labels\n            fig.update_xaxes(\n                tickmode=\"array\",\n                tickvals=[\n                    x_positions[assay] + len(feature_set_order) / 2\n                    for assay in fig_assay_order\n                ],\n                ticktext=list(x_positions.keys()),\n                title=\"Assay\",\n            )\n\n            fig.update_layout(\n                legend=dict(\n                    title=\"Feature Sets\", itemsizing=\"constant\", traceorder=\"normal\"\n                )\n            )\n            if y_range:\n                fig.update_yaxes(range=y_range)\n\n            if logdir:\n                base_name = f\"feature_set_metrics_{category}_{metric}_per_assay\"\n                if name is not None:\n                    base_name = base_name + f\"_{name}\"\n                fig.write_html(logdir / f\"{base_name}.html\")\n                fig.write_image(logdir / f\"{base_name}.svg\")\n                fig.write_image(logdir / f\"{base_name}.png\")\n\n            fig.show()\n\n\nGet prediction scores for multiple feature sets.\n\n\nCode\nset_selection_name = \"feature_sets_14\"\nall_results = split_results_handler.obtain_all_feature_set_data(\n    parent_folder=mixed_data_dir,\n    merge_assays=True,\n    return_type=\"split_results\",\n    include_categories=[CELL_TYPE],\n    include_sets=metric_orders_map[set_selection_name],\n    exclude_names=[\"16ct\", \"27ct\", \"7c\", \"chip-seq-only\"],\n)\n\n\nCompute metrics per assay\n\n\nCode\nmetrics_per_assay = prepare_metric_sets_per_assay(all_results)  # type: ignore\n\n\nReorder feature sets for graphing.\n\n\nCode\n# Reorder feature sets\nfeature_set_order = metric_orders_map[set_selection_name]\nfor assay, feature_sets in list(metrics_per_assay.items()):\n    metrics_per_assay[assay] = {\n        feature_set_name: metrics_per_assay[assay][feature_set_name]\n        for feature_set_name in feature_set_order\n    }\n\n\nGraph.\n\nCode\ngraph_feature_set_metrics_per_assay(\n    all_metrics_per_assay=metrics_per_assay,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    sort_by_input_size=False,\n    y_range=(0.1, 1.01)\n)\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\nSupplementary Figure 2C,D: Distribution of accuracy calculated per assay for the Biospecimen classifier. Bin sizes include 10 Mb, 1 Mb, 100 kb, and 10 kb, corresponding to 315, 3,044, 30,321, and 303,114 non-overlapping regions covering the whole-genome, respectively. Various numbers of random 100 kb, 10 kb and 1 kb regions were also used. Gene-based features include 19,864 gene regions, while cis-regulatory elements and methylation regions each comprise 30,320 and 303,114 regions, respectively. Dashed lines represent means, solid lines the medians, boxes the quartiles, whiskers the farthest points within 1.5× the interquartile range, and dots are outliers.\n\n\n\n\nMislabeled datasets identified with EpiClass.\n\nSupplementary Figure 3: Genome browser representation of the eight EpiATLAS originally mislabeled datasets identified by EpiClass in metadata freeze v1.0 that were discarded in following metadata freezes (purple), along with representative correct datasets. The observed tracks are shown as positive signal, while imputed tracks (where available) are shown as negative signal.\n\n\n\nExample of bad quality datasets identified using EpiClass\n\nSupplementary Figure 4: Genome browser representation of some of the EpiATLAS bad quality datasets identified by EpiClass in metadata freeze v1.0 that were discarded in following metadata freezes (purple), along with good quality ones from the same biospecimen. The observed tracks are shown as positive signal, while imputed tracks (where available) are shown as negative signal."
  },
  {
    "objectID": "figs/fig1.html#setup-code---imports-and-co.",
    "href": "figs/fig1.html#setup-code---imports-and-co.",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata",
    "section": "",
    "text": "Setup imports.\n\n\nCode\nfrom __future__ import annotations\n\nimport copy\nimport logging\nimport re\nimport tempfile\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Dict, Optional, Tuple\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom IPython.core.display import Image\nfrom IPython.display import display\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import auc, confusion_matrix as sk_cm, roc_curve\nfrom sklearn.preprocessing import label_binarize\n\nfrom epiclass.core.confusion_matrix import ConfusionMatrixWriter\nfrom epiclass.utils.notebooks.paper.metrics_per_assay import MetricsPerAssay\nfrom epiclass.utils.notebooks.paper.paper_utilities import (\n    ASSAY,\n    ASSAY_MERGE_DICT,\n    ASSAY_ORDER,\n    CELL_TYPE,\n    SEX,\n    IHECColorMap,\n    MetadataHandler,\n    SplitResultsHandler,\n    extract_input_sizes_from_output_files,\n    merge_similar_assays,\n    PathChecker\n)\n\n\nSetup paths.\n\n\nCode\n# Root path\nbase_dir = Path.home() / \"Projects/epiclass/output/paper\"\nPathChecker.check_directory(base_dir)\n\n# More precise\nbase_data_dir = base_dir / \"data\"\nbase_fig_dir = base_dir / \"figures\"\n\n# alias\npaper_dir = base_dir\n\n\nSetup colors.\n\n\nCode\nIHECColorMap = IHECColorMap(base_fig_dir)\nassay_colors = IHECColorMap.assay_color_map\ncell_type_colors = IHECColorMap.cell_type_color_map\n\n\nSetup metadata and prediction files handlers.\n\n\nCode\nsplit_results_handler = SplitResultsHandler()\n\nmetadata_handler = MetadataHandler(paper_dir)\nmetadata_v2 = metadata_handler.load_metadata(\"v2\")\nmetadata_v2_df = metadata_v2.to_df()\n\n\nSetup data directories.\n\n\nCode\ngen_data_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\nPathChecker.check_directory(gen_data_dir)\n\ndata_dir_100kb = gen_data_dir / \"hg38_100kb_all_none\"\nPathChecker.check_directory(data_dir_100kb)\n\n\nSetup figures general settings.\n\n\nCode\nmain_title_settings = {\n    \"title\":dict(\n        automargin=True,\n        x=0.5,\n        xanchor=\"center\",\n        yanchor=\"top\",\n        y=0.98\n        ),\n    \"margin\":dict(t=50, l=10, r=10)\n}"
  },
  {
    "objectID": "figs/fig1.html#figure-1",
    "href": "figs/fig1.html#figure-1",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata",
    "section": "",
    "text": "Performance of EpiClass Assay and Biospecimen classifiers.\n\n\n\nFig. 1A: Overview of the EpiClass training process for various classifiers and their inference on external data. Each classifier is trained independently.\n\n\n\nPath setup.\n\n\nCode\nmixed_data_dir = gen_data_dir / \"mixed\"\nPathChecker.check_directory(data_dir_100kb)\n\n\nFeature sets setup.\n\n\nCode\nfeature_sets_14 = [\n    \"hg38_10mb_all_none_1mb_coord\",\n    \"hg38_100kb_random_n316_none\",\n    \"hg38_1mb_all_none\",\n    \"hg38_100kb_random_n3044_none\",\n    \"hg38_100kb_all_none\",\n    \"hg38_gene_regions_100kb_coord_n19864\",\n    \"hg38_10kb_random_n30321_none\",\n    \"hg38_regulReg_allCorr_n30k\",\n    \"hg38_1kb_random_n30321_none\",\n    \"hg38_cpg_topvar_200bp_n30k_wrong_coordinates\",\n    \"hg38_10kb_all_none\",\n    \"hg38_regulReg_allCorr_n303k\",\n    \"hg38_1kb_random_n303114_none\",\n    \"hg38_cpg_topvar_200bp_n303k_wrong_coordinates\",\n]\nfig1_sets = [\n    \"hg38_10mb_all_none_1mb_coord\",\n    \"hg38_100kb_random_n316_none\",\n    \"hg38_1mb_all_none\",\n    \"hg38_100kb_random_n3044_none\",\n    \"hg38_100kb_all_none\",\n    \"hg38_10kb_random_n30321_none\",\n    \"hg38_1kb_random_n30321_none\",\n    \"hg38_10kb_all_none\",\n    \"hg38_1kb_random_n303114_none\",\n]\n\nmetric_orders_map = {\n    \"fig1_sets\": fig1_sets,\n    \"feature_sets_14\": feature_sets_14,\n}\n\n\nCompute input sizes for each feature set.\n\n\nCode\ninput_sizes = extract_input_sizes_from_output_files(mixed_data_dir)  # type: ignore\ninput_sizes: Dict[str, int] = {k: v.pop() for k, v in input_sizes.items() if len(v) == 1}  # type: ignore\n\n\nSet selection.\n\n\nCode\nset_selection_name = \"feature_sets_14\"\n\nlogdir = (\n    base_fig_dir\n    / \"fig2_EpiAtlas_other\"\n    / \"fig2--reduced_feature_sets\"\n    / \"test\"\n    / set_selection_name\n)\nlogdir.mkdir(parents=True, exist_ok=True)\n\n\nCompute metrics.\n\n\nCode\nall_metrics = split_results_handler.obtain_all_feature_set_data(\n    parent_folder=mixed_data_dir,\n    merge_assays=True,\n    return_type=\"metrics\",\n    include_categories=[ASSAY, CELL_TYPE],\n    include_sets=metric_orders_map[set_selection_name],\n    exclude_names=[\"16ct\", \"27ct\", \"7c\", \"chip-seq-only\"],\n)\n\n# Order the metrics\nall_metrics = {\n    name: all_metrics[name]  # type: ignore\n    for name in metric_orders_map[set_selection_name]\n    if name in all_metrics\n}\n\n\nLabel correction.\n\n\nCode\n# correct a name\ntry:\n    all_metrics[\"hg38_100kb_all_none\"][ASSAY] = all_metrics[\"hg38_100kb_all_none\"][  # type: ignore\n        f\"{ASSAY}_11c\"\n    ]\n    del all_metrics[\"hg38_100kb_all_none\"][f\"{ASSAY}_11c\"]\nexcept KeyError:\n    pass\n\n\nResolution/feature set –&gt; color mapping.\n\n\nCode\nresolution_colors = {\n    \"100kb\": px.colors.qualitative.Safe[0],\n    \"10kb\": px.colors.qualitative.Safe[1],\n    \"1kb\": px.colors.qualitative.Safe[2],\n    \"regulReg\": px.colors.qualitative.Safe[3],\n    \"gene\": px.colors.qualitative.Safe[4],\n    \"cpg\": px.colors.qualitative.Safe[5],\n    \"1mb\": px.colors.qualitative.Safe[6],\n    \"5mb\": px.colors.qualitative.Safe[7],\n    \"10mb\": px.colors.qualitative.Safe[8],\n}\n\n\nDefine graphing function graph_feature_set_metrics.\n\n\nCode\ndef graph_feature_set_metrics(\n    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n    input_sizes: Dict[str, int],\n    logdir: Path | None = None,\n    sort_by_input_size: bool = False,\n    name: str | None = None,\n    y_range: Tuple[float, float] | None = None,\n    boxpoints: str = \"all\",\n    width: int = 1200,\n    height: int = 1200,\n) -&gt; None:\n    \"\"\"Graph the metrics for all feature sets.\n\n    Args:\n        all_metrics (Dict[str, Dict[str, Dict[str, Dict[str, float]]]): A dictionary containing all metrics for all feature sets.\n            Format: {feature_set: {task_name: {split_name: metric_dict}}}\n        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n        logdir (Path): The directory where the figure will be saved. If None, the figure will only be displayed.\n        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n        name (str|None): The name of the figure.\n        y_range (Tuple[float, float]|None): The y-axis range for the figure.\n        boxpoints (str): The type of boxpoints to display. Can be \"all\" or \"outliers\". Defaults to \"all\".\n    \"\"\"\n    if boxpoints not in [\"all\", \"outliers\"]:\n        raise ValueError(\"Invalid boxpoints value.\")\n\n    reference_hdf5_type = \"hg38_100kb_all_none\"\n    metadata_categories = list(all_metrics[reference_hdf5_type].keys())\n\n    non_standard_names = {ASSAY: f\"{ASSAY}_11c\", SEX: f\"{SEX}_w-mixed\"}\n    non_standard_assay_task_names = [\"hg38_100kb_all_none\"]\n\n    used_resolutions = set()\n    for i in range(len(metadata_categories)):\n        category_idx = i\n        category_fig = make_subplots(\n            rows=1,\n            cols=2,\n            shared_yaxes=True,\n            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n            horizontal_spacing=0.01,\n        )\n\n        trace_names = []\n        order = list(all_metrics.keys())\n        if sort_by_input_size:\n            order = sorted(\n                all_metrics.keys(),\n                key=lambda x: input_sizes[x],\n            )\n        for feature_set_name in order:\n            # print(feature_set_name)\n            tasks_dicts = all_metrics[feature_set_name]\n            meta_categories = copy.deepcopy(metadata_categories)\n\n            if feature_set_name not in input_sizes:\n                print(f\"Skipping {feature_set_name}, no input size found.\")\n                continue\n\n            task_name = meta_categories[category_idx]\n            if \"split\" in task_name:\n                raise ValueError(\"Split in task name. Wrong metrics dict.\")\n\n            try:\n                task_dict = tasks_dicts[task_name]\n            except KeyError as err:\n                if SEX in str(err) and feature_set_name in non_standard_sex_task_name:\n                    task_dict = tasks_dicts[non_standard_names[SEX]]\n                elif (\n                    ASSAY in str(err)\n                    and feature_set_name in non_standard_assay_task_names\n                ):\n                    task_dict = tasks_dicts[non_standard_names[ASSAY]]\n                else:\n                    print(\"Skipping\", feature_set_name, task_name)\n                    continue\n\n            input_size = input_sizes[feature_set_name]\n\n            feature_set_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n            feature_set_name = re.sub(r\"\\_[\\dmkb]+\\_coord\", \"\", feature_set_name)\n\n            resolution = feature_set_name.split(\"_\")[0]\n            used_resolutions.add(resolution)\n\n            trace_name = f\"{input_size}|{feature_set_name}\"\n            trace_names.append(trace_name)\n\n            # Accuracy\n            metric = \"Accuracy\"\n            y_vals = [task_dict[split][metric] for split in task_dict]\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"\n                for split, metrics_dict in task_dict.items()\n            ]\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=boxpoints,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    fillcolor=resolution_colors[resolution],\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                    legendgroup=resolution,\n                    showlegend=False,\n                ),\n                row=1,\n                col=1,\n            )\n\n            metric = \"F1_macro\"\n            y_vals = [task_dict[split][metric] for split in task_dict]\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"\n                for split, metrics_dict in task_dict.items()\n            ]\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=boxpoints,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    fillcolor=resolution_colors[resolution],\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                    legendgroup=resolution,\n                    showlegend=False,\n                ),\n                row=1,\n                col=2,\n            )\n\n        title = f\"{metadata_categories[category_idx]} classification\"\n        title = title.replace(CELL_TYPE, \"biospecimen\")\n        if name is not None:\n            title += f\" - {name}\"\n        category_fig.update_layout(\n            width=width,\n            height=height,\n            title_text=title,\n            **main_title_settings\n        )\n\n        # dummy scatters for resolution colors\n        for resolution, color in resolution_colors.items():\n            if resolution not in used_resolutions:\n                continue\n            category_fig.add_trace(\n                go.Scatter(\n                    x=[None],\n                    y=[None],\n                    mode=\"markers\",\n                    name=resolution,\n                    marker=dict(color=color, size=5),\n                    showlegend=True,\n                    legendgroup=resolution,\n                )\n            )\n\n        category_fig.update_layout(legend=dict(itemsizing=\"constant\"))\n\n        # y-axis\n        if y_range:\n            category_fig.update_yaxes(range=y_range)\n        else:\n            if ASSAY in task_name:\n                category_fig.update_yaxes(range=[0.96, 1.001])\n            if CELL_TYPE in task_name:\n                category_fig.update_yaxes(range=[0.75, 1])\n\n        category_fig.update_layout(**main_title_settings)\n\n        # Save figure\n        if logdir:\n            base_name = f\"feature_set_metrics_{metadata_categories[category_idx]}\"\n            if name is not None:\n                base_name = base_name + f\"_{name}\"\n            category_fig.write_html(logdir / f\"{base_name}.html\")\n            category_fig.write_image(logdir / f\"{base_name}.svg\")\n            category_fig.write_image(logdir / f\"{base_name}.png\")\n\n        category_fig.show()\n\n\n\n\n\nGraph 100kb resolution MLP metrics.\n\nCode\nmetrics_fig1b = {name: all_metrics[name] for name in [\"hg38_100kb_all_none\"]}\n\nmetrics_fig1b_1 = {\n    \"hg38_100kb_all_none\": {ASSAY: metrics_fig1b[\"hg38_100kb_all_none\"][ASSAY]}\n}\ngraph_feature_set_metrics(\n    all_metrics=metrics_fig1b_1,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=425,\n    height=400,\n    y_range=(0.98, 1.001),\n)\n\nmetrics_fig1b_2 = {\n    \"hg38_100kb_all_none\": {CELL_TYPE: metrics_fig1b[\"hg38_100kb_all_none\"][CELL_TYPE]}\n}\ngraph_feature_set_metrics(\n    all_metrics=metrics_fig1b_2,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=425,\n    height=400,\n    y_range=(0.93, 1.001),\n)\n\n\n\n\n                                                \n\n\n                                                \n\n\n\nFig. 1B: Distribution of accuracy and F1-score for each of the ten training folds (dots) for the Assay and Biospecimen MLP classifiers.\n\n\n\nGraph.\n\nCode\nmetrics_fig1c = {name: all_metrics[name] for name in fig1_sets}\n\ngraph_feature_set_metrics(\n    all_metrics=metrics_fig1c,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=900,\n    height=600,\n)\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\nFig. 1C-alt: Distribution of accuracy per training fold for different bin resolutions for the Assay and Biospecimen classifiers.\n\n\nDefine function parse_bin_size to extract a numerical bin size in base pairs.\n\n\nCode\ndef parse_bin_size(feature_set_name: str) -&gt; Optional[float]:\n    \"\"\"\n    Parses the feature set name to extract a numerical bin size in base pairs.\n    Handles formats like '100kb', '5mb', 'regulReg', 'gene', 'cpg'.\n\n    Returns numerical size (float) or None if unparseable or non-numeric.\n    Assigns placeholder values for non-genomic-range types if needed,\n    but for a continuous axis, it's better to return None or filter later.\n    \"\"\"\n    name_parts = feature_set_name.replace(\"hg38_\", \"\").split(\"_\")\n    if not name_parts:\n        return None\n\n    resolution_str = name_parts[0].lower()\n\n    # Handle standard genomic ranges\n    match_kb = re.match(r\"(\\d+)kb\", resolution_str)\n    if match_kb:\n        return float(match_kb.group(1)) * 1_000\n    match_mb = re.match(r\"(\\d+)mb\", resolution_str)\n    if match_mb:\n        return float(match_mb.group(1)) * 1_000_000\n\n    # Handle non-range types - decide how to represent them.\n    # Option 1: Return None (they won't be plotted on the numeric axis)\n    # Option 2: Assign arbitrary numbers (might distort scale)\n    # Option 3: Could use different marker symbols later if needed\n    if resolution_str in [\"regulatory\", \"gene\", \"cpg\"]:\n        return None  # Returning None is cleaner for a pure numeric axis\n\n    # Fallback for unrecognised formats\n    try:\n        # Maybe it's just a number (e.g., representing window size)?\n        return float(resolution_str)\n    except ValueError:\n        return None\n\n\nDefine graphing function graph_feature_set_scatter to plot performance metrics as a scatter plot instead of bar plot.\n\n\nCode\ndef graph_feature_set_scatter(\n    all_metrics: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n    input_sizes: Dict[str, int],\n    logdir: Optional[Path] = None,\n    metric_to_plot: str = \"Accuracy\",\n    name: Optional[str] = None,\n    metric_range: Optional[Tuple[float, float]] = None,\n    assay_task_key: str = ASSAY,\n    sex_task_key: str = SEX,\n    cell_type_task_key: str = CELL_TYPE,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"\n    Graphs performance metrics as a scatter plot with modifications.\n\n    X-axis: Number of Features (log scale).\n    Y-axis: Average performance metric (e.g., Accuracy, F1_macro) across folds.\n            Vertical lines indicate the min/max range across folds.\n    Color: Bin Size (bp, log scale).\n\n    Args:\n        all_metrics: Nested dict {feature_set: {task_name: {split_name: metric_dict}}}.\n        input_sizes: Dict {feature_set: num_features}.\n        logdir: Directory to save figures. If None, display only.\n        metric_to_plot: The metric key to use for the Y-axis ('Accuracy', 'F1_macro').\n        name: Optional suffix for figure titles and filenames.\n        metric_range: Optional tuple (min, max) to set the Y-axis range.\n        assay_task_key: Key used for the assay prediction task.\n        sex_task_key: Key used for the sex prediction task.\n        cell_type_task_key: Key used for the cell type prediction task.\n    \"\"\"\n    if metric_to_plot not in [\"Accuracy\", \"F1_macro\"]:\n        raise ValueError(\"metric_to_plot must be 'Accuracy' or 'F1_macro'\")\n\n    # --- Standard Name Handling (simplified from original) ---\n    non_standard_names = {ASSAY: f\"{ASSAY}_11c\", SEX: f\"{SEX}_w-mixed\"}\n\n    # --- Find reference and task names ----\n    reference_hdf5_type = next(iter(all_metrics), None)\n    if reference_hdf5_type is None or not all_metrics.get(reference_hdf5_type):\n        print(\n            \"Warning: Could not determine tasks from all_metrics. Trying default tasks.\"\n        )\n        cleaned_metadata_categories = {assay_task_key, sex_task_key, cell_type_task_key}\n    else:\n        metadata_categories = list(all_metrics[reference_hdf5_type].keys())\n        cleaned_metadata_categories = set()\n        for cat in metadata_categories:\n            original_name = cat\n            for standard, non_standard in non_standard_names.items():\n                if cat == non_standard:\n                    original_name = standard\n                    break\n            cleaned_metadata_categories.add(original_name)\n\n    # --- Define Bin size categories and Colors ---\n    bin_category_names = [\"1Kb\", \"10Kb\", \"100Kb\", \"1Mb\", \"10Mb\"]\n    bin_category_values = [1000, 10000, 100 * 1000, 1000 * 1000, 10000 * 1000]\n    discrete_colors = px.colors.sequential.Viridis_r\n    color_map = {\n        name: discrete_colors[i * 2] for i, name in enumerate(bin_category_names)\n    }\n\n    if verbose:\n        print(f\"Plotting for tasks: {list(cleaned_metadata_categories)}\")\n\n    for category_name in cleaned_metadata_categories:\n        plot_data_points = []\n\n        for feature_set_name_orig in all_metrics.keys():\n            try:\n                num_features = input_sizes[feature_set_name_orig]\n            except KeyError as e:\n                raise ValueError(\n                    f\"Feature set '{feature_set_name_orig}' not found in input_sizes\"\n                ) from e\n\n            # Parse Bin Size\n            bin_size = parse_bin_size(feature_set_name_orig)\n            if bin_size is None:\n                print(\n                    f\"Skipping {feature_set_name_orig}, could not parse numeric bin size.\"\n                )\n                continue\n\n            # 3. Get Metric Values (Average, Min, Max)\n            tasks_dicts = all_metrics[feature_set_name_orig]\n\n            # --- Task Name Lookup ---\n            # 1. Try the standard category name first\n            # 2. If standard name not found, use non-standard name\n            task_dict = None\n            task_name = category_name\n            if category_name in tasks_dicts:\n                task_dict = tasks_dicts[category_name]\n            else:\n                non_standard_task_name = non_standard_names.get(category_name)\n                if non_standard_task_name and non_standard_task_name in tasks_dicts:\n                    task_name = non_standard_task_name\n                    task_dict = tasks_dicts[non_standard_task_name]\n\n                if task_dict is None:\n                    raise ValueError(\n                        f\"Task '{category_name}' not found in feature set '{feature_set_name_orig}'\"\n                    )\n            # --- End Task Name Lookup ---\n\n            # Calculate average, min, max metric value across splits\n            try:\n                metric_values = []\n                for split, split_data in task_dict.items():\n                    if metric_to_plot in split_data:\n                        metric_values.append(split_data[metric_to_plot])\n                    else:\n                        print(\n                            f\"Warning: Metric '{metric_to_plot}' not found in split '{split}' for {feature_set_name_orig} / {task_name}\"\n                        )\n\n                if not metric_values:\n                    print(\n                        f\"Warning: No metric values found for {feature_set_name_orig} / {task_name} / {metric_to_plot}\"\n                    )\n                    continue\n\n                avg_metric = np.mean(metric_values)\n                min_metric = np.min(metric_values)\n                max_metric = np.max(metric_values)\n\n            except Exception as e:  # pylint: disable=broad-except\n                raise ValueError(\n                    f\"Error calculating metrics for {feature_set_name_orig} / {task_name}: {e}\"\n                ) from e\n\n            # Clean feature set name for hover text\n            clean_name = feature_set_name_orig.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n            clean_name = re.sub(r\"\\_[\\dmkb]+\\_coord\", \"\", clean_name)\n\n            # Store data for this point\n            plot_data_points.append(\n                {\n                    \"bin_size\": bin_size,\n                    \"num_features\": num_features,\n                    \"metric_value\": avg_metric,\n                    \"min_metric\": min_metric,  # For error bar low\n                    \"max_metric\": max_metric,  # For error bar high\n                    \"name\": clean_name,\n                    \"raw_name\": feature_set_name_orig,\n                }\n            )\n\n        if not plot_data_points:\n            raise ValueError(\n                f\"No suitable data points found to plot for task: {category_name}\"\n            )\n\n        # --- Determine Marker Symbols ---\n        marker_symbols = []\n        default_symbol = \"circle\"\n        random_symbol = \"cross\"\n        for p in plot_data_points:\n            if \"random\" in p[\"raw_name\"]:\n                marker_symbols.append(random_symbol)\n            else:\n                marker_symbols.append(default_symbol)\n\n        # --- Group Data by Category ---\n        points_by_category = {name: [] for name in bin_category_names}\n        for i, point_data in enumerate(plot_data_points):\n            bin_size = point_data[\"bin_size\"]\n            assigned_category = None\n            for cat_name, cat_value in zip(bin_category_names, bin_category_values):\n                if bin_size == cat_value:\n                    assigned_category = cat_name\n                    break\n            else:\n                raise ValueError(f\"Could not find category for bin size: {bin_size}\")\n\n            points_by_category[assigned_category].append(\n                {\n                    \"x\": point_data[\"num_features\"],  # X is Num Features\n                    \"y\": point_data[\"metric_value\"],\n                    \"error_up\": point_data[\"max_metric\"] - point_data[\"metric_value\"],\n                    \"error_down\": point_data[\"metric_value\"] - point_data[\"min_metric\"],\n                    \"text\": point_data[\"name\"],\n                    \"customdata\": [\n                        point_data[\"min_metric\"],\n                        point_data[\"max_metric\"],\n                        point_data[\"bin_size\"],\n                    ],  # Keep bin size for hover\n                    \"symbol\": marker_symbols[i],  # Assign symbol determined earlier\n                }\n            )\n\n        # --- Create Figure and Add Traces PER CATEGORY ---\n        fig = go.Figure()\n        traces = []\n\n        for cat_name in bin_category_names:  # Iterate in defined order for legend\n            points_in_cat = points_by_category[cat_name]\n            if not points_in_cat:\n                continue\n\n            category_color = color_map[cat_name]\n\n            # Extract data for all points in this category\n            x_vals = [p[\"x\"] for p in points_in_cat]\n            y_vals = [p[\"y\"] for p in points_in_cat]\n            error_up_vals = [p[\"error_up\"] for p in points_in_cat]\n            error_down_vals = [p[\"error_down\"] for p in points_in_cat]\n            text_vals = [p[\"text\"] for p in points_in_cat]\n            customdata_vals = [p[\"customdata\"] for p in points_in_cat]\n            symbols_vals = [p[\"symbol\"] for p in points_in_cat]\n\n            trace = go.Scatter(\n                x=x_vals,\n                y=y_vals,\n                mode=\"markers\",\n                name=cat_name,\n                showlegend=False,\n                legendgroup=cat_name,  # Group legend entries\n                marker=dict(\n                    color=category_color,\n                    size=15,\n                    symbol=symbols_vals,\n                    line=dict(width=1, color=\"DarkSlateGrey\"),\n                ),\n                error_y=dict(\n                    type=\"data\",\n                    symmetric=False,\n                    array=error_up_vals,\n                    arrayminus=error_down_vals,\n                    visible=True,\n                    thickness=1.5,\n                    width=15,\n                    color=category_color,\n                ),\n                text=text_vals,\n                customdata=customdata_vals,\n                hovertemplate=(\n                    f\"&lt;b&gt;%{{text}}&lt;/b&gt;&lt;br&gt;&lt;br&gt;\"\n                    f\"Num Features: %{{x:,.0f}}&lt;br&gt;\"\n                    f\"{metric_to_plot}: %{{y:.4f}}&lt;br&gt;\"\n                    f\"Bin Size: %{{customdata:,.0f}} bp&lt;br&gt;\"\n                    f\"{metric_to_plot} Range (10-fold): %{{customdata:.4f}} - %{{customdata:.4f}}\"\n                    \"&lt;extra&gt;&lt;/extra&gt;\"\n                ),\n            )\n            traces.append(trace)\n\n        fig.add_traces(traces)\n\n        # --- Add Legend ---\n        # Add a hidden scatter trace with square markers for legend\n        for cat_name in bin_category_names:\n            category_color = color_map[cat_name]\n            legend_trace = go.Scatter(\n                x=[None],\n                y=[None],\n                mode=\"markers\",\n                name=cat_name,\n                marker=dict(\n                    color=category_color,\n                    size=15,\n                    symbol=\"square\",\n                    line=dict(width=1, color=\"DarkSlateGrey\"),\n                ),\n                legendgroup=cat_name,\n                showlegend=True,\n            )\n            fig.add_trace(legend_trace)\n\n        # --- Update layout ---\n        title_name = category_name.replace(CELL_TYPE, \"biospecimen\")\n\n        plot_title = f\"{metric_to_plot} vs Number of Features - {title_name}\"\n        if name:\n            plot_title += f\" - {name}\"\n        xaxis_title = \"Number of Features (log scale)\"\n        xaxis_type = \"log\"\n\n        yaxis_title = metric_to_plot.replace(\"_\", \" \").title()\n        yaxis_type = \"linear\"\n\n        fig.update_layout(\n            xaxis_title=xaxis_title,\n            yaxis_title=yaxis_title,\n            xaxis_type=xaxis_type,\n            yaxis_type=yaxis_type,\n            yaxis_range=metric_range,\n            width=500,\n            height=500,\n            hovermode=\"closest\",\n            legend_title_text=\"Bin Size\",\n            title_text=plot_title,\n            **main_title_settings\n        )\n\n        if category_name == CELL_TYPE:\n            fig.update_yaxes(range=[0.75, 1.005])\n        elif category_name == ASSAY:\n            fig.update_yaxes(range=[0.96, 1.001])\n\n        # --- Save or show figure ---\n        if logdir:\n            logdir.mkdir(parents=True, exist_ok=True)\n            # Include \"modified\" or similar in filename to distinguish\n            base_name = f\"feature_scatter_MODIFIED_v2_{category_name}_{metric_to_plot}\"\n            if name:\n                base_name += f\"_{name}\"\n            html_path = logdir / f\"{base_name}.html\"\n            svg_path = logdir / f\"{base_name}.svg\"\n            png_path = logdir / f\"{base_name}.png\"\n\n            print(f\"Saving modified plot for {category_name} to {html_path}\")\n            fig.write_html(html_path)\n            fig.write_image(svg_path)\n            fig.write_image(png_path)\n\n        fig.show()\n\n\nGraph\n\nCode\nfor metric in [\"Accuracy\", \"F1_macro\"]:\n    graph_feature_set_scatter(\n        all_metrics=metrics_fig1c,  # type: ignore\n        input_sizes=input_sizes,\n        metric_to_plot=metric,\n        verbose=False,\n    )\n\n\n\n\n                                                \n\n\n                                                \n\n\n\n\n                                                \n\n\n                                                \n\n\n\nFig. 1C: Distribution of accuracy per training fold for different bin resolutions for the Assay and Biospecimen classifiers. The circles represent the means and the whiskers the min and max values of the ten training folds.\n\n\n\nDefine function create_confusion_matrix to create and show a confusion matrix.\n\n\nCode\ndef create_confusion_matrix(\n    df: pd.DataFrame,\n    name: str = \"confusion_matrix\",\n    logdir: Path | None = None,\n    min_pred_score: float = 0,\n    majority: bool = False,\n    verbose:bool=False\n) -&gt; None:\n    \"\"\"Create a confusion matrix for the given DataFrame and save it to the logdir.\n\n    Args:\n        df (pd.DataFrame): The DataFrame containing the results.\n        logdir (Path): The directory path for saving the figures.\n        name (str): The name for the saved figures.\n        min_pred_score (float): The minimum prediction score to consider.\n        majority (bool): Whether to use majority vote (uuid-wise) for the predicted class.\n    \"\"\"\n    # Compute confusion matrix\n    classes = sorted(df[\"True class\"].unique())\n    if \"Max pred\" not in df.columns:\n        df[\"Max pred\"] = df[classes].max(axis=1)  # type: ignore\n    filtered_df = df[df[\"Max pred\"] &gt; min_pred_score]\n\n    if majority:\n        # Majority vote for predicted class\n        groupby_uuid = filtered_df.groupby([\"uuid\", \"True class\", \"Predicted class\"])[\n            \"Max pred\"\n        ].aggregate([\"size\", \"mean\"])\n\n        if groupby_uuid[\"size\"].max() &gt; 3:\n            raise ValueError(\"More than three predictions for the same uuid.\")\n\n        groupby_uuid = groupby_uuid.reset_index().sort_values(\n            [\"uuid\", \"True class\", \"size\"], ascending=[True, True, False]\n        )\n        groupby_uuid = groupby_uuid.drop_duplicates(\n            subset=[\"uuid\", \"True class\"], keep=\"first\"\n        )\n        filtered_df = groupby_uuid\n\n    confusion_mat = sk_cm(\n        filtered_df[\"True class\"], filtered_df[\"Predicted class\"], labels=classes\n    )\n\n    mat_writer = ConfusionMatrixWriter(labels=classes, confusion_matrix=confusion_mat)\n\n    if logdir is None:\n        logdir = Path(tempfile.gettempdir())\n\n    files = mat_writer.to_all_formats(logdir, name=f\"{name}_n{len(filtered_df)}\")\n\n    if verbose:\n        print(f\"Saved confusion matrix to {logdir}:\")\n        for file in files:\n            print(Path(file).name)\n\n    for file in files:\n        if \"png\" in file.name:\n            scale = 0.6\n            display(Image(filename=file, width=1250*scale, height=1000*scale))\n\n\nPrepare prediction data for confusion matrix.\n\n\nCode\nassay_split_dfs = split_results_handler.gather_split_results_across_methods(\n    results_dir=data_dir_100kb, label_category=ASSAY, only_NN=True\n)\nconcat_assay_df = split_results_handler.concatenate_split_results(assay_split_dfs)[\"NN\"]\n\ndf_with_meta = metadata_handler.join_metadata(concat_assay_df, metadata_v2)  # type: ignore\nif \"Predicted class\" not in df_with_meta.columns:\n    raise ValueError(\"`Predicted class` not in DataFrame\")\n\nclassifier_name = \"MLP\"\nmin_pred_score = 0\nmajority = False\n\nname = f\"{classifier_name}_pred&gt;{min_pred_score}\"\n\nlogdir = base_fig_dir / \"fig1_EpiAtlas_assay\" / \"fig1_supp_D-assay_c11_confusion_matrices\"\nif majority:\n    logdir = logdir / \"per_uuid\"\nelse:\n    logdir = logdir / \"per_file\"\nlogdir.mkdir(parents=True, exist_ok=True)\n\n\nGraph.\n\n\nCode\ncreate_confusion_matrix(\n    df=df_with_meta,\n    min_pred_score=min_pred_score,\n    majority=majority,\n)\n\n\n\n\n\n\n\n\n\nFig. 1D: Confusion matrix aggregating the cross-validation folds (therefore showing all files) without applying a prediction score threshold. RNA-seq and WGBS data were both separated according to two protocols during initial training (but combined thereafter to nine assays).\n\n\n\n\nFig. 1E: Genome browser representation showing in black the datasets swap between H3K4me3 and H3K27ac for IHECRE00001897 in the metadata freeze v1.0, along with typical correct datasets over a representative region."
  },
  {
    "objectID": "figs/fig1.html#supplementary-figure-1",
    "href": "figs/fig1.html#supplementary-figure-1",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata",
    "section": "",
    "text": "More detailled performance of EpiClass Assay and Biospecimen classifiers.\n\n\nFig. 1A,B data points are included in these two graphs (MLP data points).\n\n\n\nDefine graphing function plot_multiple_models_split_metrics.\n\n\nCode\ndef plot_multiple_models_split_metrics(\n    split_metrics: Dict[str, Dict[str, Dict[str, float]]],\n    label_category: str,\n    logdir: Path | None = None,\n    filename: str = \"fig1_all_classifiers_metrics\",\n) -&gt; None:\n    \"\"\"Render to box plots the metrics per classifier/models and split, each in its own subplot.\n\n    Args:\n        split_metrics: A dictionary containing metric scores for each classifier and split.\n        label_category: The label category for the classification task.\n        name: The name of the figure.\n        logdir: The directory to save the figure to. If None, the figure is only displayed.\n\n    Returns:\n        None: Displays the figure and saves it to the logdir if provided.\n    \"\"\"\n    metrics = [\"Accuracy\", \"F1_macro\", \"AUC_micro\", \"AUC_macro\"]\n    classifier_names = list(next(iter(split_metrics.values())).keys())\n    classifier_names = [\"NN\", \"LR\", \"LGBM\", \"LinearSVC\", \"RF\"]\n\n    # Create subplots, one row for each metric\n    fig = make_subplots(\n        rows=1,\n        cols=len(metrics),\n        subplot_titles=metrics,\n        horizontal_spacing=0.075,\n    )\n\n    for i, metric in enumerate(metrics):\n        for classifier in classifier_names:\n            values = [split_metrics[split][classifier][metric] for split in split_metrics]\n            if classifier == \"NN\":\n                classifier = \"MLP\"\n            fig.add_trace(\n                go.Box(\n                    y=values,\n                    name=classifier,\n                    line=dict(color=\"black\", width=1.5),\n                    marker=dict(size=3, color=\"black\"),\n                    boxmean=True,\n                    boxpoints=\"all\",  # or \"outliers\" to show only outliers\n                    pointpos=-1.4,\n                    showlegend=False,\n                    width=0.5,\n                    hovertemplate=\"%{text}\",\n                    text=[\n                        f\"{split}: {value:.4f}\"\n                        for split, value in zip(split_metrics, values)\n                    ],\n                ),\n                row=1,\n                col=i + 1,\n            )\n\n    fig.update_layout(\n        title_text=f\"{label_category} classification\",\n        boxmode=\"group\",\n        **main_title_settings,\n    )\n\n    # Adjust y-axis\n    if label_category == ASSAY:\n        range_acc = [0.95, 1.001]\n        range_AUC = [0.992, 1.0001]\n    elif label_category == CELL_TYPE:\n        range_acc = [0.81, 1]\n        range_AUC = [0.96, 1]\n    else:\n        range_acc = [0.6, 1.001]\n        range_AUC = [0.9, 1.0001]\n\n    fig.update_layout(\n        yaxis=dict(range=range_acc),\n        yaxis2=dict(range=range_acc),\n        yaxis3=dict(range=range_AUC),\n        yaxis4=dict(range=range_AUC),\n        height=450,\n    )\n\n    fig.update_layout(margin=dict(l=20, r=20))\n\n    # Save figure\n    if logdir:\n        fig.write_image(logdir / f\"{filename}.svg\")\n        fig.write_image(logdir / f\"{filename}.png\")\n        fig.write_html(logdir / f\"{filename}.html\")\n\n    fig.show()\n\n\nGraph.\n\nCode\nmerge_assays = True\n\nfor label_category in [ASSAY, CELL_TYPE]:\n    all_split_dfs = split_results_handler.gather_split_results_across_methods(\n        results_dir=data_dir_100kb,\n        label_category=label_category,\n        only_NN=False,\n    )\n\n    if merge_assays and label_category == ASSAY:\n        for split_name, split_dfs in all_split_dfs.items():\n            for classifier_type, df in split_dfs.items():\n                split_dfs[classifier_type] = merge_similar_assays(df)\n\n    split_metrics = split_results_handler.compute_split_metrics(all_split_dfs)\n\n    plot_multiple_models_split_metrics(\n        split_metrics,\n        label_category=label_category,\n    )\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\nSupplementary Figure 1A,B: Distribution of performance scores (accuracy, F1 as well as micro and macro AUROC) per training fold (dots) for each machine learning approach used for training on the Assay (A) and Biospecimen (B) metadata. Micro-averaging aggregates contributions from all classes (global true positive rate and false positive rate); macro-averaging averages the true positive rate from each class. Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5× the interquartile range.\n\n\nGoing forward, all results are for MLP classifiers.\n\n\n\nDefine graphing function plot_roc_curves. Computes macro-average ROC curves manually.\n\n\nCode\ndef plot_roc_curves(\n    results_df: pd.DataFrame,\n    label_category: str,\n    logdir: Path | None = None,\n    name: str = \"roc_curve\",\n    title: str | None = None,\n    colors_dict: Dict | None = None,  # Optional specific colors\n    verbose: bool = False,\n) -&gt; None:\n    \"\"\"\n    Generates and plots ROC curves for multi-class classification results using Plotly.\n\n    Calculates and plots individual class ROC curves, micro-average, and macro-average ROC curves.\n\n    Args:\n        results_df (pd.DataFrame): DataFrame with true labels and prediction probabilities for each class.\n                                   Must contain the `label_category` column (e.g., 'True class')\n                                   and probability columns named after each class.\n        label_category (str): The column name containing the true labels (e.g., 'True class', ASSAY, CELL_TYPE).\n        logdir (Path | None): Directory to save the figure. If None, only displays the figure.\n        name (str): Base name for saved files (e.g., \"supp_fig1e\").\n        title (str | None): Title suffix for the plot. If None, a default title based on label_category is used.\n        colors_dict (Dict | None): Optional dictionary mapping class names to colors. If None or a class\n                                   is missing, default Plotly colors are used.\n    \"\"\"\n    df = results_df.copy()\n    true_label_col = \"True class\"  # Assuming 'True class' holds the ground truth labels\n\n    if true_label_col not in df.columns:\n        raise ValueError(f\"True label column '{true_label_col}' not found in DataFrame.\")\n\n    classes = sorted(df[true_label_col].unique())\n    if verbose:\n        print(f\"Using classes: {classes}\")\n\n    n_classes = len(classes)\n    if n_classes &lt; 2:\n        print(\n            f\"Warning: Only {n_classes} class found after processing. Cannot generate ROC curve.\"\n        )\n        return\n\n    # Check if probability columns exist for all determined classes\n    missing_cols = [c for c in classes if c not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing probability columns for classes: {missing_cols}\")\n\n    # Binarize the true labels against the final set of classes\n    try:\n        y_true = label_binarize(df[true_label_col], classes=classes)\n    except ValueError as e:\n        raise ValueError(\n            f\"Error binarizing labels for classes {classes}. Check if all labels in '{true_label_col}' are included in 'classes'.\"\n        ) from e\n\n    if n_classes == 2 and y_true.shape[1] == 1:\n        # Adjust for binary case where label_binarize might return one column\n        y_true = np.hstack((1 - y_true, y_true))  # type: ignore\n    elif y_true.shape[1] != n_classes:\n        raise ValueError(\n            f\"Binarized labels shape {y_true.shape} does not match number of classes {n_classes}\"\n        )\n\n    # Get the predicted probabilities for each class\n    # Ensure columns are in the same order as 'classes'\n    y_score = df[classes].values\n\n    # --- Compute ROC curve and ROC area for each class ---\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i, class_name in enumerate(classes):\n        try:\n            fpr[class_name], tpr[class_name], _ = roc_curve(\n                y_true=y_true[:, i], y_score=y_score[:, i]  # type: ignore\n            )\n            roc_auc[class_name] = auc(fpr[class_name], tpr[class_name])\n        except ValueError as e:\n            raise ValueError(\"Could not compute ROC for class {class_name}.\") from e\n\n    # --- Compute micro-average ROC curve and ROC area ---\n    try:\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_score.ravel())  # type: ignore\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    except ValueError as e:\n        raise ValueError(\"Could not compute micro-average ROC.\") from e\n\n    # --- Compute macro-average ROC curve and ROC area ---\n    try:\n        # Aggregate all false positive rates\n        all_fpr = np.unique(\n            np.concatenate(\n                [fpr[class_name] for class_name in classes if class_name in fpr]\n            )\n        )\n        # Interpolate all ROC curves at these points\n        mean_tpr = np.zeros_like(all_fpr)\n        valid_classes_count = 0\n        for class_name in classes:\n            if class_name in fpr and class_name in tpr:\n                mean_tpr += np.interp(all_fpr, fpr[class_name], tpr[class_name])\n                valid_classes_count += 1\n\n        # Average it and compute AUC\n        if valid_classes_count &gt; 0:\n            mean_tpr /= valid_classes_count\n            fpr[\"macro\"] = all_fpr\n            tpr[\"macro\"] = mean_tpr\n            roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n        else:\n            raise ValueError(\"No valid classes found for macro averaging.\")\n\n    except ValueError as e:\n        raise ValueError(\"Could not compute macro-average ROC.\") from e\n\n    # --- Plot all ROC curves ---\n    fig = go.Figure()\n\n    # Plot diagonal line for reference\n    fig.add_shape(\n        type=\"line\", line=dict(dash=\"dash\", color=\"grey\", width=1), x0=0, x1=1, y0=0, y1=1\n    )\n\n    # Define colors for plotting\n    color_cycle = px.colors.qualitative.Plotly  # Default cycle\n    plot_colors = {}\n    for i, cls_name in enumerate(classes):\n        if colors_dict and cls_name in colors_dict:\n            plot_colors[cls_name] = colors_dict[cls_name]\n        else:\n            plot_colors[cls_name] = color_cycle[i % len(color_cycle)]\n\n    # Plot Micro-average ROC curve first (often plotted thicker/dashed)\n    fig.add_trace(\n        go.Scatter(\n            x=fpr[\"micro\"],\n            y=tpr[\"micro\"],\n            mode=\"lines\",\n            name=f'Micro-average ROC (AUC = {roc_auc[\"micro\"]:.5f})',\n            line=dict(color=\"deeppink\", width=3, dash=\"dash\"),\n            hoverinfo=\"skip\",  # Less important for hover usually\n        )\n    )\n\n    # Plot Macro-average ROC curve\n    fig.add_trace(\n        go.Scatter(\n            x=fpr[\"macro\"],\n            y=tpr[\"macro\"],\n            mode=\"lines\",\n            name=f'Macro-average ROC (AUC = {roc_auc[\"macro\"]:.5f})',\n            line=dict(color=\"navy\", width=3, dash=\"dash\"),\n            hoverinfo=\"skip\",\n        )\n    )\n\n    # Plot individual class ROC curves\n    for class_name in classes:\n        if class_name not in fpr or class_name not in tpr or class_name not in roc_auc:\n            continue  # Skip if calculation failed\n        fig.add_trace(\n            go.Scatter(\n                x=fpr[class_name],\n                y=tpr[class_name],\n                mode=\"lines\",\n                name=f\"{class_name} (AUC = {roc_auc[class_name]:.5f})\",\n                line=dict(width=1.5, color=plot_colors.get(class_name)),\n                hovertemplate=f\"&lt;b&gt;{class_name}&lt;/b&gt;&lt;br&gt;FPR=%{{x:.5f}}&lt;br&gt;TPR=%{{y:.5f}}&lt;extra&gt;&lt;/extra&gt;\",  # Show class name and values on hover\n            )\n        )\n\n    # --- Update layout ---\n    base_title = f\"ROC Curves&lt;br&gt;{label_category}\"\n    plot_title = f\"{base_title} - {title}\" if title else base_title\n\n    title_settings=dict(\n        yanchor=\"top\",\n        yref=\"paper\",\n        y=0.97,\n        xanchor=\"center\",\n        xref=\"paper\",\n        x=0.5,\n    )\n\n    fig.update_layout(\n        title=title_settings,\n        title_text=plot_title,\n        xaxis_title=\"False Positive Rate (1 - Specificity)\",\n        yaxis_title=\"True Positive Rate (Sensitivity)\",\n        xaxis=dict(range=[0.0, 1.0], constrain=\"domain\"),  # Ensure axes range 0-1\n        yaxis=dict(\n            range=[0.0, 1.01], scaleanchor=\"x\", scaleratio=1, constrain=\"domain\"\n        ),  # Make it square-ish, slight top margin\n        width=800,\n        height=650,\n        hovermode=\"closest\",\n        legend=dict(\n            traceorder=\"reversed\",  # Show averages first in legend\n            title=\"Classes & Averages\",\n            font=dict(size=9),\n            itemsizing=\"constant\",\n            y=0.8,\n            yref=\"paper\",\n        ),\n        margin=dict(l=60, r=30, t=0, b=0),\n    )\n\n    # --- Save figure if logdir is provided ---\n    if logdir:\n        logdir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n        filename_base = f\"{name}_{label_category}_roc\"\n        filepath_base = logdir / filename_base\n\n        fig.write_html(f\"{filepath_base}.html\")\n        fig.write_image(f\"{filepath_base}.svg\", width=800, height=750)\n        fig.write_image(f\"{filepath_base}.png\", width=800, height=750, scale=2)\n\n        print(f\"Saved ROC curve plots for {label_category} to {logdir}\")\n        print(f\" -&gt; {filename_base}.html / .svg / .png\")\n\n    fig.show()\n\n\nPrepare assay data for plotting.\n\n\nCode\ndata_dir = (\n    mixed_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{ASSAY}_1l_3000n\"\n    / \"11c\"\n    / \"10fold-oversampling\"\n)\nPathChecker.check_directory(data_dir)\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\n\ndf = merge_similar_assays(concat_df_w_meta.copy())\n\n\nGraph assay results.\n\n\nCode\nplot_roc_curves(\n    results_df=df.copy(),\n    label_category=ASSAY,\n    title=\"Aggregated 10fold\",  # Title suffix\n    colors_dict=assay_colors,\n    verbose=False,\n)\n\n\n                                                \n\n\nPrepare biospecimen data for plotting.\n\n\nCode\ndata_dir = (\n    mixed_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{CELL_TYPE}_1l_3000n\"\n    / \"10fold-oversampling\"\n)\nPathChecker.check_directory(data_dir)\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\n\n\nGraph biospecimen results.\n\n\nCode\nplot_roc_curves(\n    results_df=concat_df_w_meta,\n    label_category=CELL_TYPE,\n    title=\"Aggregated 10fold\",  # Title suffix\n    colors_dict=cell_type_colors,\n    verbose=False,\n)\n\n\n                                                \n\n\nSupplementary Figure 1C: ROC curves from aggregated cross-validation results for the Assay and Biospecimen classifiers. Curves for each class are computed in a one-vs-rest scheme.\n\n\n\nDefine graphing function create_blklst_graphs.\n\n\nCode\ndef create_blklst_graphs(\n    feature_set_metrics_dict: Dict[str, Dict[str, Dict[str, Dict[str, float]]]],\n    logdir: Path | None = None,\n) -&gt; List[go.Figure]:\n    \"\"\"Create boxplots for blacklisted related feature sets.\n\n    Args:\n        feature_set_metrics_dict (Dict[str, Dict[str, Dict[str, Dict[str, float]]]]): The dictionary containing all metrics for all blklst related feature sets.\n            format: {feature_set: {task_name: {split_name: metric_dict}}}\n        logdir (Path, Optional): The directory to save the figure to. If None, the figure is only displayed.\n    \"\"\"\n    figs = []\n\n    # Assume names exist in all feature sets\n    task_names = list(feature_set_metrics_dict.values())[0].keys()\n\n    traces_names_dict = {\n        \"hg38_100kb_all_none\": \"observed\",\n        \"hg38_100kb_all_none_0blklst\": \"0blklst\",\n        \"hg38_100kb_all_none_0blklst_winsorized\": \"0blklst_winsorized\",\n    }\n\n    for task_name in task_names:\n        category_fig = make_subplots(\n            rows=1,\n            cols=2,\n            shared_yaxes=False,\n            subplot_titles=[\"Accuracy\", \"F1-score (macro)\"],\n            horizontal_spacing=0.1,\n        )\n        for feature_set_name, tasks_dicts in feature_set_metrics_dict.items():\n            task_dict = tasks_dicts[task_name]\n            trace_name = traces_names_dict[feature_set_name]\n\n            # Accuracy\n            metric = \"Accuracy\"\n            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n                for split, metrics_dict in task_dict.items()\n            ]\n\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=\"all\",\n                    showlegend=False,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                ),\n                row=1,\n                col=1,\n            )\n\n            metric = \"F1_macro\"\n            y_vals = [task_dict[split][metric] for split in task_dict]  # type: ignore\n            hovertext = [\n                f\"{split}: {metrics_dict[metric]:.4f}\"  # type: ignore\n                for split, metrics_dict in task_dict.items()\n            ]\n            category_fig.add_trace(\n                go.Box(\n                    y=y_vals,\n                    name=trace_name,\n                    boxmean=True,\n                    boxpoints=\"all\",\n                    showlegend=False,\n                    marker=dict(size=3, color=\"black\"),\n                    line=dict(width=1, color=\"black\"),\n                    hovertemplate=\"%{text}\",\n                    text=hovertext,\n                ),\n                row=1,\n                col=2,\n            )\n\n        category_fig.update_xaxes(\n            categoryorder=\"array\",\n            categoryarray=list(traces_names_dict.values()),\n        )\n        category_fig.update_yaxes(range=[0.9, 1.001])\n\n        category_fig.update_layout(\n            title_text=task_name,\n            height=600,\n            width=500,\n            **main_title_settings\n        )\n\n        # Save figure\n        if logdir:\n            task_name = task_name.replace(\"_1l_3000n-10fold\", \"\")\n            base_name = f\"metrics_{task_name}\"\n\n            category_fig.write_html(logdir / f\"{base_name}.html\")\n            category_fig.write_image(logdir / f\"{base_name}.svg\")\n            category_fig.write_image(logdir / f\"{base_name}.png\")\n\n        figs.append(category_fig)\n\n    return figs\n\n\nPrepare paths.\n\n\nCode\ninclude_sets = [\n    \"hg38_100kb_all_none\",\n    \"hg38_100kb_all_none_0blklst\",\n    \"hg38_100kb_all_none_0blklst_winsorized\",\n]\n\nresults_folder_blklst = base_data_dir / \"training_results\" / \"2023-01-epiatlas-freeze\"\nPathChecker.check_directory(results_folder_blklst)\n\n\nCompute metrics.\n\n\nCode\n# Select 10-fold oversampling runs\n# expected result shape: {feature_set: {task_name: {split_name: metrics_dict}}}\nall_metrics_blklst: Dict[\n    str, Dict[str, Dict[str, Dict[str, float]]]\n] = split_results_handler.obtain_all_feature_set_data(\n    return_type=\"metrics\",\n    parent_folder=results_folder_blklst,\n    merge_assays=True,\n    include_categories=[ASSAY, CELL_TYPE],\n    include_sets=include_sets,\n    oversampled_only=False,\n    verbose=False,\n)  # type: ignore\n\n\nGraph.\n\nCode\nfigs = create_blklst_graphs(all_metrics_blklst)\n\nfigs[0].show()\nfigs[1].show()\n\n\n\n\n                                                \n\n\n                                                \n\n\n\nSupplementary Figure 1D: Distribution of accuracy and F1-score per training fold (dots) for the Assay and Biospecimen classifiers after removing signal from blacklisted regions and applying winsorization of 0.1%. Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5× the interquartile range.\n\n\n\n\nE: Assay training 10-fold cross-validation\nF: Assay complete training (mixed tracks), predictions on imputed data (all pval)\nG: Biospecimen 10-fold cross-validation\n\n\nDefine graphing function plot_prediction_scores_distribution.\nSupplementary Figure 1E-G: Distribution of average prediction score per file (dots) for the majority-vote class (up to three track type files) (E, F) or individual file (G), from the MLP approach for the Assay (E, G) and Biospecimen classifiers (F), using aggregated cross-validation results from observed data (E, F) or results from the classifier trained on all observed data and applied to imputed data from EpiATLAS (G). Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5× the interquartile range, with a violin representation on top.\n\n\nGather prediction scores.\n\n\nCode\ndata_dir = (\n    mixed_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{ASSAY}_1l_3000n\"\n    / \"11c\"\n    / \"10fold-oversampling\"\n)\nPathChecker.check_directory(data_dir)\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\n\n\nGraph.\n\n\nCode\nplot_prediction_scores_distribution(\n    results_df=concat_df_w_meta,\n    group_by_column=ASSAY,\n    merge_assay_pairs=True,\n    min_y=0.7,\n    title=\"11 classes assay training&lt;br&gt;Prediction scores for 10-fold cross-validation\",\n)\n\n\n                                                \n\n\n\n\n\nGather prediction scores.\n\n\nCode\ndata_dir = data_dir_100kb / f\"{CELL_TYPE}_1l_3000n\" / \"10fold-oversampling\"\nPathChecker.check_directory(data_dir)\n\ndfs = split_results_handler.read_split_results(data_dir)\nconcat_df: pd.DataFrame = split_results_handler.concatenate_split_results(dfs, depth=1)  # type: ignore\nconcat_df = split_results_handler.add_max_pred(concat_df)\nconcat_df_w_meta = metadata_handler.join_metadata(concat_df, metadata_v2)\nconcat_df_w_meta.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n\n\nGraph.\n\n\nCode\nplot_prediction_scores_distribution(\n    results_df=concat_df_w_meta,\n    group_by_column=ASSAY,\n    min_y=0,\n    title=\"Biospecimen training&lt;br&gt;Prediction scores for 10-fold cross-validation\",\n)\n\n\nSkipping assay merging: Wrong results dataframe, rna or wgbs columns missing.\n\n\n                                                \n\n\n\n\n\nGather imputed signal metadata.\n\n\nCode\nmetadata_path = (\n    paper_dir\n    / \"data\"\n    / \"metadata\"\n    / \"epiatlas\"\n    / \"imputed\"\n    / \"hg38_epiatlas_imputed_pval_chip_2024-02.json\"\n)\nmetadata_imputed: pd.DataFrame = metadata_handler.load_any_metadata(metadata_path, as_dataframe=True)  # type: ignore\n\n\nGather prediction scores.\n\n\nCode\ndata_dir = (\n    gen_data_dir\n    / \"hg38_100kb_all_none\"\n    / f\"{ASSAY}_1l_3000n\"\n    / \"11c\"\n    / \"complete_no_valid_oversample\"\n    / \"predictions\"\n    / \"epiatlas_imputed\"\n    / \"ChIP\"\n)\nPathChecker.check_directory(data_dir)\n\ndf_pred = pd.read_csv(\n    data_dir / \"complete_no_valid_oversample_prediction.csv\",\n    index_col=0,\n)\n\n\nPrepare dataframe for graphing.\n\n\nCode\nassay_classes = list(metadata_v2_df[ASSAY].unique())\ndf_pred = split_results_handler.add_max_pred(df_pred, expected_classes=assay_classes)\n\naugmented_df = pd.merge(df_pred, metadata_imputed, left_index=True, right_on=\"md5sum\")\naugmented_df[\"True class\"] = augmented_df[ASSAY]\nprint(\"Number of files per assay:\")\nprint(augmented_df[\"True class\"].value_counts(dropna=False).to_string())\n\n\nNumber of files per assay:\nh3k36me3    1703\nh3k27me3    1703\nh3k9me3     1700\nh3k4me1     1688\nh3k4me3     1688\nh3k27ac     1088\n\n\n\n\nGraph.\n\n\nCode\nplot_prediction_scores_distribution(\n    results_df=augmented_df,\n    group_by_column=ASSAY,\n    merge_assay_pairs=True,\n    min_y=0.79,\n    use_aggregate_vote=False,\n    title=\"Complete 11c assay classifier&lt;br&gt;inference on imputed data\",\n)\n\n\n                                                \n\n\n\n\n\n\nFor the code that produced the figures, see src/python/epiclass/utils/notebooks/paper/confidence_threshold.ipynb (permalink).\n\n\n\n\nSupplementary Figure 1H,I: Distribution of aggregated accuracy, F1-score and corresponding file subset size across varying prediction score thresholds, based on pooled predictions from all cross-validation folds for the Assay (H) and Biospecimen (I) classifiers."
  },
  {
    "objectID": "figs/fig1.html#supplementary-figure-2",
    "href": "figs/fig1.html#supplementary-figure-2",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata",
    "section": "",
    "text": "Performance of EpiClass Assay and Biospecimen classifiers evaluated per training fold across various bin size resolutions and genomic feature sets.\n\n\nGraph assay/biospecimen metrics per 10fold, for reference (no assay breakdown)\n\nCode\nmetrics_supp2 = {name: all_metrics[name] for name in feature_sets_14}\n\ngraph_feature_set_metrics(\n    all_metrics=metrics_supp2,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    width=900,\n    height=600,\n)\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\nSupplementary Figure 2A,B: Distribution of accuracy over all files for the Assay (A) or Biospecimen (B) classifier.\n\n\n\nDefine function to compute metrics per assay: prepare_metric_sets_per_assay\n\n\nCode\ndef prepare_metric_sets_per_assay(\n    all_results: Dict[str, Dict[str, Dict[str, pd.DataFrame]]], verbose: bool = False\n) -&gt; Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]:\n    \"\"\"Prepare metric sets per assay.\n\n    Args:\n        all_results (Dict[str, Dict[str, Dict[str, pd.DataFrame]]]): A dictionary containing all results for all feature sets.\n\n    Returns:\n        Dict[str, Dict[str, Dict[str, Dict[str, float]]]]: A dictionary containing all metrics per assay for all feature sets.\n            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n    \"\"\"\n    if verbose:\n        print(\"Loading metadata.\")\n    metadata = metadata_handler.load_metadata(\"v2\")\n    metadata.convert_classes(ASSAY, ASSAY_MERGE_DICT)\n    md5_per_assay = metadata.md5_per_class(ASSAY)\n    md5_per_assay = {k: set(v) for k, v in md5_per_assay.items()}\n\n    if verbose:\n        print(\"Getting results per assay.\")\n    results_per_assay = {}\n    for assay_label in ASSAY_ORDER:\n        if verbose:\n            print(assay_label)\n        results_per_assay[assay_label] = {}\n        for feature_set, task_dict in all_results.items():\n            if verbose:\n                print(feature_set)\n            results_per_assay[assay_label][feature_set] = {}\n            for task_name, split_dict in task_dict.items():\n                if verbose:\n                    print(task_name)\n                results_per_assay[assay_label][feature_set][task_name] = {}\n\n                # Only keep the relevant assay\n                for split_name, split_df in split_dict.items():\n                    if verbose:\n                        print(split_name)\n                    assay_df = split_df[split_df.index.isin(md5_per_assay[assay_label])]\n                    results_per_assay[assay_label][feature_set][task_name][\n                        split_name\n                    ] = assay_df\n\n    if verbose:\n        print(\"Finished getting results per assay. Now computing metrics.\")\n    metrics_per_assay = {}\n    for assay_label in ASSAY_ORDER:\n        if verbose:\n            print(assay_label)\n        metrics_per_assay[assay_label] = {}\n        for feature_set, task_dict in results_per_assay[assay_label].items():\n            if verbose:\n                print(feature_set)\n            assay_metrics = split_results_handler.compute_split_metrics(\n                task_dict, concat_first_level=True\n            )\n            inverted_dict = split_results_handler.invert_metrics_dict(assay_metrics)\n            metrics_per_assay[assay_label][feature_set] = inverted_dict\n\n    return metrics_per_assay\n\n\nDefine graphing function graph_feature_set_metrics_per_assay\n\n\nCode\ndef graph_feature_set_metrics_per_assay(\n    all_metrics_per_assay: Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]],\n    input_sizes: Dict[str, int],\n    logdir: Path | None = None,\n    sort_by_input_size: bool = False,\n    name: str | None = None,\n    y_range: Tuple[float, float] | None = None,\n    boxpoints: str = \"outliers\",\n) -&gt; None:\n    \"\"\"Graph the metrics for all feature sets, per assay, with separate plots for accuracy and F1-score.\n\n    Args:\n        all_metrics_per_assay (Dict[str, Dict[str, Dict[str, Dict[str, Dict[str, float]]]]]): A dictionary containing all metrics per assay for all feature sets.\n            Format: {assay: {feature_set: {task_name: {split_name: metric_dict}}}}\n        input_sizes (Dict[str, int]): A dictionary containing the input sizes for all feature sets.\n        logdir (Path): The directory where the figures will be saved. If None, the figures will only be displayed.\n        sort_by_input_size (bool): Whether to sort the feature sets by input size.\n        name (str|None): The name of the figure.\n        y_range (Tuple[float, float]|None): The y-axis range for the plots.\n        boxpoints (str): The type of points to display in the box plots. Defaults to \"outliers\".\n    \"\"\"\n    valid_boxpoints = [\"all\", \"outliers\"]\n    if boxpoints not in valid_boxpoints:\n        raise ValueError(f\"Invalid boxpoints value. Choose from {valid_boxpoints}.\")\n\n    fig_assay_order = [\n        \"rna_seq\",\n        \"h3k27ac\",\n        \"h3k4me1\",\n        \"h3k4me3\",\n        \"h3k36me3\",\n        \"h3k27me3\",\n        \"h3k9me3\",\n        \"input\",\n        \"wgbs\",\n    ]\n\n    reference_assay = next(iter(all_metrics_per_assay))\n    reference_feature_set = next(iter(all_metrics_per_assay[reference_assay]))\n    metadata_categories = list(\n        all_metrics_per_assay[reference_assay][reference_feature_set].keys()\n    )\n\n    for _, category in enumerate(metadata_categories):\n        for metric, metric_name in [\n            (\"Accuracy\", \"Accuracy\"),\n            (\"F1_macro\", \"F1-score (macro)\"),\n        ]:\n            fig = go.Figure()\n\n            feature_sets = list(all_metrics_per_assay[reference_assay].keys())\n            unique_feature_sets = set(feature_sets)\n            for assay in fig_assay_order:\n                if set(all_metrics_per_assay[assay].keys()) != unique_feature_sets:\n                    raise ValueError(\"Different feature sets through assays.\")\n\n            feature_set_order = feature_sets\n            if sort_by_input_size:\n                feature_set_order = sorted(\n                    feature_set_order, key=lambda x: input_sizes[x]\n                )\n\n            # Adjust spacing so each assay group has dedicated space based on the number of feature sets\n            spacing_multiplier = (\n                1.1  # Increase this multiplier if needed to add more spacing\n            )\n            x_positions = {\n                assay: i * len(feature_set_order) * spacing_multiplier\n                for i, assay in enumerate(fig_assay_order)\n            }\n\n            for i, feature_set_name in enumerate(feature_set_order):\n                resolution = (\n                    feature_set_name.replace(\"_none\", \"\")\n                    .replace(\"hg38_\", \"\")\n                    .split(\"_\")[0]\n                )\n                color = resolution_colors[resolution]\n                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n\n                for assay in fig_assay_order:\n                    if feature_set_name not in all_metrics_per_assay[assay]:\n                        continue\n\n                    tasks_dicts = all_metrics_per_assay[assay][feature_set_name]\n\n                    if feature_set_name not in input_sizes:\n                        print(f\"Skipping {feature_set_name}, no input size found.\")\n                        continue\n\n                    task_name = category\n                    if \"split\" in task_name:\n                        raise ValueError(\"Split in task name. Wrong metrics dict.\")\n\n                    try:\n                        task_dict = tasks_dicts[task_name]\n                    except KeyError:\n                        print(\n                            f\"Skipping {feature_set_name}, {task_name} for assay {assay}\"\n                        )\n                        continue\n\n                    y_vals = [task_dict[split][metric] for split in task_dict]\n                    hovertext = [\n                        f\"{assay} - {display_name} - {split}: {metrics_dict[metric]:.4f}\"\n                        for split, metrics_dict in task_dict.items()\n                    ]\n\n                    x_position = x_positions[assay] + i\n                    fig.add_trace(\n                        go.Box(\n                            x=[x_position] * len(y_vals),\n                            y=y_vals,\n                            name=f\"{assay}|{display_name}\",\n                            boxmean=True,\n                            boxpoints=boxpoints,\n                            marker=dict(size=3, color=\"black\"),\n                            line=dict(width=1, color=\"black\"),\n                            fillcolor=color,\n                            hovertemplate=\"%{text}\",\n                            text=hovertext,\n                            showlegend=False,\n                            legendgroup=display_name,\n                        )\n                    )\n\n                    # separate box groups\n                    fig.add_vline(\n                        x=x_positions[assay] - 1, line_width=1, line_color=\"black\"\n                    )\n\n            # Add dummy traces for the legend\n            for feature_set_name in feature_set_order:\n                resolution = (\n                    feature_set_name.replace(\"_none\", \"\")\n                    .replace(\"hg38_\", \"\")\n                    .split(\"_\")[0]\n                )\n                color = resolution_colors[resolution]\n                display_name = feature_set_name.replace(\"_none\", \"\").replace(\"hg38_\", \"\")\n                display_name = re.sub(r\"\\_[\\dmkb]+\\_coord\", \"\", display_name)\n\n                fig.add_trace(\n                    go.Scatter(\n                        name=display_name,\n                        x=[None],\n                        y=[None],\n                        mode=\"markers\",\n                        marker=dict(size=10, color=color),\n                        showlegend=True,\n                        legendgroup=display_name,\n                    )\n                )\n\n            title = f\"{category} - {metric_name} (per assay)\"\n            if name is not None:\n                title += f\" - {name}\"\n\n            fig.update_layout(\n                width=1250,\n                height=900,\n                title_text=title,\n                xaxis_title=\"Assay\",\n                yaxis_title=metric_name,\n                **main_title_settings\n            )\n\n            # Create x-axis labels\n            fig.update_xaxes(\n                tickmode=\"array\",\n                tickvals=[\n                    x_positions[assay] + len(feature_set_order) / 2\n                    for assay in fig_assay_order\n                ],\n                ticktext=list(x_positions.keys()),\n                title=\"Assay\",\n            )\n\n            fig.update_layout(\n                legend=dict(\n                    title=\"Feature Sets\", itemsizing=\"constant\", traceorder=\"normal\"\n                )\n            )\n            if y_range:\n                fig.update_yaxes(range=y_range)\n\n            if logdir:\n                base_name = f\"feature_set_metrics_{category}_{metric}_per_assay\"\n                if name is not None:\n                    base_name = base_name + f\"_{name}\"\n                fig.write_html(logdir / f\"{base_name}.html\")\n                fig.write_image(logdir / f\"{base_name}.svg\")\n                fig.write_image(logdir / f\"{base_name}.png\")\n\n            fig.show()\n\n\nGet prediction scores for multiple feature sets.\n\n\nCode\nset_selection_name = \"feature_sets_14\"\nall_results = split_results_handler.obtain_all_feature_set_data(\n    parent_folder=mixed_data_dir,\n    merge_assays=True,\n    return_type=\"split_results\",\n    include_categories=[CELL_TYPE],\n    include_sets=metric_orders_map[set_selection_name],\n    exclude_names=[\"16ct\", \"27ct\", \"7c\", \"chip-seq-only\"],\n)\n\n\nCompute metrics per assay\n\n\nCode\nmetrics_per_assay = prepare_metric_sets_per_assay(all_results)  # type: ignore\n\n\nReorder feature sets for graphing.\n\n\nCode\n# Reorder feature sets\nfeature_set_order = metric_orders_map[set_selection_name]\nfor assay, feature_sets in list(metrics_per_assay.items()):\n    metrics_per_assay[assay] = {\n        feature_set_name: metrics_per_assay[assay][feature_set_name]\n        for feature_set_name in feature_set_order\n    }\n\n\nGraph.\n\nCode\ngraph_feature_set_metrics_per_assay(\n    all_metrics_per_assay=metrics_per_assay,  # type: ignore\n    input_sizes=input_sizes,\n    boxpoints=\"all\",\n    sort_by_input_size=False,\n    y_range=(0.1, 1.01)\n)\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\nSupplementary Figure 2C,D: Distribution of accuracy calculated per assay for the Biospecimen classifier. Bin sizes include 10 Mb, 1 Mb, 100 kb, and 10 kb, corresponding to 315, 3,044, 30,321, and 303,114 non-overlapping regions covering the whole-genome, respectively. Various numbers of random 100 kb, 10 kb and 1 kb regions were also used. Gene-based features include 19,864 gene regions, while cis-regulatory elements and methylation regions each comprise 30,320 and 303,114 regions, respectively. Dashed lines represent means, solid lines the medians, boxes the quartiles, whiskers the farthest points within 1.5× the interquartile range, and dots are outliers."
  },
  {
    "objectID": "figs/fig1.html#supplementary-figure-3",
    "href": "figs/fig1.html#supplementary-figure-3",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata",
    "section": "",
    "text": "Mislabeled datasets identified with EpiClass.\n\nSupplementary Figure 3: Genome browser representation of the eight EpiATLAS originally mislabeled datasets identified by EpiClass in metadata freeze v1.0 that were discarded in following metadata freezes (purple), along with representative correct datasets. The observed tracks are shown as positive signal, while imputed tracks (where available) are shown as negative signal."
  },
  {
    "objectID": "figs/fig1.html#supplementary-figure-4",
    "href": "figs/fig1.html#supplementary-figure-4",
    "title": "EpiClass accurately predicts EpiATLAS assay and biospecimen metadata",
    "section": "",
    "text": "Example of bad quality datasets identified using EpiClass\n\nSupplementary Figure 4: Genome browser representation of some of the EpiATLAS bad quality datasets identified by EpiClass in metadata freeze v1.0 that were discarded in following metadata freezes (purple), along with good quality ones from the same biospecimen. The observed tracks are shown as positive signal, while imputed tracks (where available) are shown as negative signal."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EpiClass paper - Figures",
    "section": "",
    "text": "This is a Quarto website presenting figures from the EpiClass paper."
  },
  {
    "objectID": "index.html#figures",
    "href": "index.html#figures",
    "title": "EpiClass paper - Figures",
    "section": "Figures",
    "text": "Figures\nHere are the figure pages:\n\nSection 1: EpiClass accurately predicts EpiATLAS assay and biospecimen metadata\nSection 2: EpiClass reliably validates and augments other categories of EpiATLAS metadata\nSection 3: Application of EpiClass on public epigenomic data\nSupplementary figures 5 to 7"
  },
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "EpiClass paper - Figures",
    "section": "Quarto",
    "text": "Quarto\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "figs/fig3.html",
    "href": "figs/fig3.html",
    "title": "Application of EpiClass on public epigenomic data",
    "section": "",
    "text": "The formatting of the figures may differ slightly from those in the paper, but they display the same data points.\nAll code cells are folded by default. To view any cell, click “Code” to expand it, or use the code options near the main title above to unfold all at once.\nSome figures are not included here because:\n\nThe interactive versions would be too large (e.g., PCA figures)\nThey were created manually (no figure creation code available)\n\nThese figures can be viewed directly in the paper and the supplementary figures PDF.\n\n\nImports and paths setups.\n\n\nCode\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport skops.io as skio\nimport upsetplot\nfrom IPython.display import display\n\nfrom epiclass.utils.notebooks.paper.paper_utilities import (\n    ASSAY,\n    ASSAY_ORDER,\n    IHECColorMap,\n    MetadataHandler,\n    SplitResultsHandler,\n)\n\n\n\n\nCode\nCANCER = \"harmonized_sample_cancer_high\"\nCORE_ASSAYS = ASSAY_ORDER[0:7]\n\n\n\n\nCode\nbase_dir = Path.home() / \"Projects/epiclass/output/paper\"\npaper_dir = base_dir\nif not paper_dir.exists():\n    raise FileNotFoundError(f\"Directory {paper_dir} does not exist.\")\n\nbase_data_dir = base_dir / \"data\"\nbase_fig_dir = base_dir / \"figures\"\ntables_dir = base_dir / \"tables\"\n\nbase_metadata_dir = base_data_dir / \"metadata\"\n\n\n\n\nCode\nIHECColorMap = IHECColorMap(base_fig_dir)\nassay_colors = IHECColorMap.assay_color_map\ncell_type_colors = IHECColorMap.cell_type_color_map\nsex_colors = IHECColorMap.sex_color_map\n\n\n\n\nCode\nsplit_results_handler = SplitResultsHandler()\n\nmetadata_handler = MetadataHandler(paper_dir)\nmetadata_v2 = metadata_handler.load_metadata(\"v2\")\nmetadata_v2_df = metadata_handler.load_metadata_df(\"v2\")\n\n\n\n\nCode\nbase_pred_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"predictions\"\nif not base_pred_dir.exists():\n    raise FileNotFoundError(f\"Directory {base_pred_dir} does not exist.\")\n\n\n\n\n\nPredictions on external databases (ENCODE, ChIP-Atlas and Recount3) were all performed using the same classifiers trained on the labeled EpiAtlas samples.\n\n\n\n\n\n\n\n\n\nMetadata category\nNb classes\nExperiment Key (comet.com)\nTraining Files\n\n\n\n\nassay_epiclass\n7\n69488630801b4a05a53b5d9e572f0aaa\n16788\n\n\nassay_epiclass\n11\n0f8e5eb996114868a17057bebe64f87c\n20922\n\n\nharmonized_donor_sex\n3\n4b908b83e0ec45c3ab991e65aa27af0c\n18299\n\n\nharmonized_donor_life_stage\n5\n91214ed0b1664395b1826dc69a495ed4\n15372\n\n\nharmonized_sample_cancer_high\n2\n15da476b92f140eab818ece369248f4c\n20922\n\n\nharmonized_biomaterial_type\n4\nf42b6f4e147c4f1bbe378f3eed415099\n20922\n\n\n\nClasses:\n\nassay 7c: 6 h3k* histone marks + input\nassay 11c: assay7c + rna_seq + mrna_seq + wgbs_standard + wgbs_pbat\nharmonized_donor_sex: male, female, mixed\nharmonized_donor_life_stage: adult, child, newborn, fetal, embryonic\nharmonized_sample_cancer_high (modification of harmonized_sample_disease_high): cancer, non-cancer (healthy/None+disease)\nharmonized_biomaterial_type (biomat): cell line, primary cell, primary cell culture, primary tissue\n\nTraining metadata: Approximately IHEC_sample_metadata_harmonization.v1.1.extended.csv.\nSee data/metadata/epiatlas/README.md for metadata details, and training_metadata_vs_official_v1.1.json for exact difference of our training data and v1.1.\nAdditonally, for ENCODE, the harmonized_sample_ontology_intermediate model was used on a subset of files with known EpiATLAS biospecimens.\n\n\n\n\n\n\n\n\n\nMetadata category\nNb classes\nExperiment Key (comet.com)\nTraining Files\n\n\n\n\nharmonized_sample_ontology_intermediate\n16\nbb66b72ae83645d587e50b34aebb39c3\n16379\n\n\n\nThe metadata for ENCODE predictions was created using the FILE + EXPERIMENT + BIOSAMPLE accessions, starting from filenames. For the initial extraction process, see: src/python/epiclass/utils/notebooks/paper/encode_metadata_creation.ipynb (permalink). Final metadata file: encode_full_metadata_2025-02_no_revoked.freeze1.csv(.xz)\n\n\n\nChIP-Atlas, ENCODE and recount3 metrics at various prediction score thresholds were generated using the MetricsPerAssay class, from src/python/epiclass/utils/notebooks/paper/metrics_per_assay.py (permalink).\n(Folder permalink) - ChIP-Atlas: src/python/epiclass/utils/notebooks/paper/c-a_pred_analysis.ipynb (section Summary metrics by assay) - ENCODE: src/python/epiclass/utils/notebooks/paper/encode_pred_analysis.ipynb (section All 5 tasks metrics summary, run Setup section beforehand) - recount3: src/python/epiclass/utils/notebooks/paper/recount3_pred_analysis.ipynb\nResults were manually merged into Supplementary Table 9, and graphs were manually created using values from that table.\n\n\nThe following code shows how the various fractions presented in the graph are obtained.\n\nRead prediction file.\n\n\nCode\nca_preds_dir = tables_dir / \"dfreeze_v2\" / \"predictions\"\nca_preds_path = (\n    ca_preds_dir / \"ChIP-Atlas_predictions_20240606_merge_metadata_freeze1.csv.xz\"\n)\nca_preds_df = pd.read_csv(ca_preds_path, sep=\",\", low_memory=False, compression=\"xz\")\n\nprint(f\"ChIP-Atlas: {ca_preds_df.shape[0]} total files\")\n\n\nChIP-Atlas: 48669 total files\n\n\nRemoving ChIP-Atlas experiment overlap with EpiATLAS\n\n\nCode\nprint(f\"ChIP-Atlas: Initial {ca_preds_df.shape[0]} files\")\nno_epiatlas_df = ca_preds_df[ca_preds_df[\"is_EpiAtlas_EpiRR\"] == \"0\"]\n\ndiff = ca_preds_df.shape[0] - no_epiatlas_df.shape[0]\nprint(f\"ChIP-Atlas: {diff} files with EpiAtlas EpiRR removed\")\nprint(f\"ChIP-Atlas: {no_epiatlas_df.shape[0]} files without EpiAtlas EpiRR\")\n\n\nChIP-Atlas: Initial 48669 files\nChIP-Atlas: 1047 files with EpiAtlas EpiRR removed\nChIP-Atlas: 47622 files without EpiAtlas EpiRR\n\n\nIgnoring non-core consensus files.\n\n\nCode\nnon_core_labels = [\"non-core\", \"CTCF\"]\nca_core7_df = no_epiatlas_df[\n    ~no_epiatlas_df[\"target_majority_consensus\"].isin((non_core_labels))\n]\n\ndiff = no_epiatlas_df.shape[0] - ca_core7_df.shape[0]\nprint(f\"ChIP-Atlas: {diff} files with non-core consensus removed\")\nprint(f\"ChIP-Atlas: {ca_core7_df.shape[0]} files with core consensus\")\n\ndisplay(ca_core7_df[\"target_majority_consensus\"].value_counts(dropna=False))\n\n\nChIP-Atlas: 918 files with non-core consensus removed\nChIP-Atlas: 46704 files with core consensus\n\n\ninput           17930\nh3k27ac         11169\nh3k4me3          6328\nh3k27me3         4195\nh3k4me1          3034\nh3k9me3          2102\nh3k36me3         1421\nno_consensus      525\nName: target_majority_consensus, dtype: int64\n\n\n\nFigure 3B: Inference of the Assay classifier on the ChIP-Atlas datasets represented as a multi-layer donut chart where the internal layer is depicting the availability status of the metadata as being provided/extracted (yellow) vs missing (grey), the central layer depicting the classifier prediction matching (green) or not (red) the expected metadata label, and the external layer containing the confidence level where prediction scores above 0.6 correspond to high-confidence predictions (light to dark blue &gt;0.6, &gt;0.8 and &gt;0.9) vs low confidence predictions (brown). The high-confidence mismatching predictions were further divided between datasets predicted as Input (light orange) vs potential mislabels (dark red).\n\n\n\n\nCode\ntotal_N = ca_core7_df.shape[0]\nhigh_confidence_pred_df = ca_core7_df[ca_core7_df[\"Max_pred_assay7\"] &gt;= 0.6]\n\nhigh_confidence_N = high_confidence_pred_df.shape[0]\nN_percent = high_confidence_N / total_N\nprint(\n    f\"ChIP-Atlas: {high_confidence_N}/{total_N} files ({high_confidence_N/total_N:.2%}) with high confidence assay prediction\"\n)\n\n\nChIP-Atlas: 41796/46704 files (89.49%) with high confidence assay prediction\n\n\n\n\n\n\n\nCode\ntotal_N = high_confidence_pred_df.shape[0]\n\nmatch_rule = (\n    high_confidence_pred_df[\"target_majority_consensus\"]\n    == high_confidence_pred_df[\"Predicted_class_assay7\"]\n)\nmatch_df = high_confidence_pred_df[match_rule]\nmismatch_df = high_confidence_pred_df[~match_rule]\n\nagreement_N = match_df.shape[0]\n\nprint(\n    f\"ChIP-Atlas: {agreement_N}/{total_N} files ({agreement_N / total_N:.2%}) with agreement between consensus and predicted assay\"\n)\n\n\nChIP-Atlas: 39532/41796 files (94.58%) with agreement between consensus and predicted assay\n\n\n\n\n\n\n\nCode\ntotal_mismatch = mismatch_df.shape[0]\ninput_rule = mismatch_df[\"Predicted_class_assay7\"] == \"input\"\ninput_pred_N = input_rule.sum()\n\nprint(\n    f\"ChIP-Atlas: {input_pred_N}/{total_mismatch} files ({input_pred_N / total_mismatch:.2%}) with mismatch predicted as input\"\n)\nprint(\n    f\"ChIP-Atlas: {total_mismatch-input_pred_N}/{total_mismatch} files ({(total_mismatch-input_pred_N) / total_mismatch:.2%}) potential mislabels\"\n)\ndisplay(mismatch_df[~input_rule][\"core7_DBs_consensus\"].value_counts(dropna=False))\n\n\nChIP-Atlas: 1525/2264 files (67.36%) with mismatch predicted as input\nChIP-Atlas: 739/2264 files (32.64%) potential mislabels\n\n\nIdentical                       468\nDifferent                       158\nIgnored - Potential non-core     64\n1 source                         49\nName: core7_DBs_consensus, dtype: int64\n\n\n\n\n\n\n\nSee Supplementary Figures page\n\n\n\n\n\nSee previous section.\n\n\n\nLoad ENCODE predictions.\n\n\nCode\nencode_preds_dir = base_pred_dir / \"encode\"\nencode_preds_path = (\n    encode_preds_dir / \"complete_encode_predictions_augmented_2025-02_metadata.csv.gz\"\n)\nencode_preds_df = pd.read_csv(\n    encode_preds_path, sep=\",\", low_memory=False, compression=\"gzip\"\n)\nprint(f\"ENCODE: {encode_preds_df.shape[0]} total files\")\n\n\nENCODE: 11643 total files\n\n\nFilter out overlap with EpiATLAS EpiRRs.\n\n\nCode\nencode_preds_df = encode_preds_df[encode_preds_df[\"in_epiatlas\"].astype(str) == \"False\"]\nprint(f\"ENCODE: {encode_preds_df.shape[0]} total files with no EpiAtlas overlap\")\n\n\nENCODE: 8777 total files with no EpiAtlas overlap\n\n\nLoad ENCODE manually created assay categories, and merge them with ENCODE predictions data.\n\n\nCode\nencode_meta_dir = base_data_dir / \"metadata\" / \"encode\"\n\nnon_core_categories_path = (\n    encode_meta_dir / \"non-core_encode_assay_category_2024-08-29.csv\"\n)\n\nnon_core_categories_df = pd.read_csv(non_core_categories_path, sep=\",\", low_memory=False)\nprint(f\"ENCODE: {non_core_categories_df.shape[0]} non-core categories\")\n\nnon_core_map = non_core_categories_df.set_index(\"target\").to_dict()[\"Assay category\"]\n\nN_mapped = len([val for val in non_core_map.values() if val != \"not_looked\"])\nprint(f\"ENCODE: {N_mapped} non-core categories mapped to functional categories.\")\n\n\n# Select non-core samples\nencode_non_core_df = encode_preds_df[\n    encode_preds_df[ASSAY].isin([\"non-core\", \"ctcf\"])\n].copy()\n\n# Map assays to categories\nencode_non_core_df[\"assay_category\"] = (\n    encode_non_core_df[\"assay\"].str.lower().replace(non_core_map)\n)\n\n\nENCODE: 1170 non-core categories\nENCODE: 238 non-core categories mapped to functional categories.\n\n\nGraph.\n\n\nCode\nassay_categories_order = [\n    \"trx_reg\",\n    \"heterochrom\",\n    \"polycomb\",\n    \"splicing\",\n    \"insulator\",\n    \"other/mixed\",\n    \"not_looked\",\n]\n\nassay_epiclass_order = [\n    \"h3k27ac\",\n    \"h3k4me3\",\n    \"h3k4me1\",\n    \"h3k9me3\",\n    \"h3k27me3\",\n    \"h3k36me3\",\n    \"input\",\n]\nassay_epiclass_order = {assay: i for i, assay in enumerate(assay_epiclass_order)}\npred_col = f\"Predicted class ({ASSAY}_7c)\"\nmax_pred_col = f\"Max pred ({ASSAY}_7c)\"\n\nmin_pred = 0.6\nsub_df = encode_non_core_df[encode_non_core_df[max_pred_col] &gt;= min_pred]\ngroupby = (\n    sub_df.groupby([\"assay_category\", pred_col])\n    .size()\n    .reset_index(name=\"Count\")\n    .sort_values([\"assay_category\", \"Count\"], ascending=[True, False])\n)\ngroupby[\"Percentage\"] = groupby.groupby(\"assay_category\")[\"Count\"].transform(\n    lambda x: (x / x.sum()) * 100\n)\n\n# Add order for plotting\ngroupby[\"assay_order\"] = groupby[pred_col].map(assay_epiclass_order)\ngroupby = groupby.sort_values(\n    [\"assay_category\", \"assay_order\"], ascending=[False, True]\n)\n\n# Main plot\nfig = px.bar(\n    groupby,\n    x=\"assay_category\",\n    y=\"Percentage\",\n    color=pred_col,\n    barmode=\"stack\",\n    category_orders={\"assay_category\": assay_categories_order},\n    color_discrete_map=assay_colors,\n    title=f\"core7 predictions for non-core assays, predScore &gt;= {min_pred:.2f}\",\n    labels={\"Percentage\": \"Percentage (%)\", \"assay_category\": \"Assay Category\"},\n)\n\n# Modify x-axis labels\ntotal_counts = groupby.groupby(\"assay_category\")[\"Count\"].sum()\nticktext = [\n    f\"{assay_category} (N={total_counts[assay_category]})\"\n    for assay_category in assay_categories_order\n]\nfig.update_xaxes(tickvals=assay_categories_order, ticktext=ticktext)\nfig.show()\n\n\n                                                \n\n\nSupplementary Figure 8B: Proportion of non-core ChIP-Seq datasets from ENCODE classified with high-confidence (&gt;0.6) in the different functional categories predicted as one of the 7 core ChIP assays. Since none of the six histone modifications used for training are specifically associated with insulator factors, they were instead mostly predicted as Input.\n\n\n\nPCAs were computed via src/python/epiclass/utils/compute_pca.py (permalink), using IncrementalPCA from scikit-learn.\nGraphing was done using code similar to src/python/epiclass/utils/notebooks/paper/pca_plot.ipynb (permalink), which uses the output of compute_pca.py.\n\n\n\nThe following function gives a consensus label to each ChIP-Atlas sample, using the target values given by ChIP-Atlas, cistromeDB, NGS-QC and GEO.\nThe consensus description is based on the following rules:\n\n“Identical” if all labels are the same\n“Different” if at least one label is different\n“1 source” if only one DB has a label\n“Ignored - Potential non-core” if any label is not in the core assays\n\n\n\nCode\ndef create_4DB_consensus_description(\n    ca_core_df: pd.DataFrame, db_cols: List[str]\n) -&gt; pd.Series:\n    \"\"\"Create a description of the 4DB assay consensus labels.\n\n    Treat \"Unclassified\" from Chip-Atlas as absent samples for the target consensus evaluation.\n\n    The consensus description is based on the following rules:\n    - \"Identical\" if all labels are the same\n    - \"Different\" if at least one label is different\n    - \"1 source\" if only one DB has a label\n    - \"Ignored - Potential non-core\" if any label is not in the core assays\n\n    Args:\n        ca_core_df: ChIP-Atlas core7 DataFrame\n\n    Returns:\n        Series with the target consensus description\n    \"\"\"\n    id_db_target = []\n    tmp_df = ca_core_df.loc[:, db_cols].copy()\n    tmp_df[\"C-A\"].replace(\"unclassified\", \"----\", inplace=True)\n\n    for labels in tmp_df.values:\n        missing_N = sum(label == \"----\" for label in labels)\n        db_labels = set(labels)\n\n        try:\n            db_labels.remove(\"----\")\n        except KeyError:\n            pass\n        if any(label not in CORE_ASSAYS + [\"ctrl\"] for label in db_labels):\n            id_db_target.append(\"Ignored - Potential non-core\")\n        elif missing_N == 3:\n            id_db_target.append(\"1 source\")\n        elif len(db_labels) == 1:\n            id_db_target.append(\"Identical\")\n        else:\n            id_db_target.append(\"Different\")\n\n    return pd.Series(id_db_target, index=ca_core_df.index)\n\n\nDefine graphing function make_db_upsetplot.\n\n\nCode\ndef make_db_upsetplot(\n    df: pd.DataFrame, consensus_col: str, db_cols: List[str], title: str\n) -&gt; upsetplot.UpSet:\n    \"\"\"Make an upsetplot of the sample presence in the different databases.\"\"\"\n    df = df.copy()\n\n    # Create a new DataFrame with boolean columns for each database\n    upset_df = pd.DataFrame()\n    for col in db_cols:\n        upset_df[col] = df[col] != \"----\"\n    upset_df[consensus_col] = df[consensus_col]\n\n    # Set the index for the UpSet plot\n    upset_df = upset_df.set_index(db_cols)\n\n    # Create the UpSet plot\n    upset = upsetplot.UpSet(\n        upset_df,\n        intersection_plot_elements=0,  # disable the default bar chart\n        sort_by=\"cardinality\",\n        show_counts=True,  # type: ignore\n        orientation=\"horizontal\",\n    )\n\n    # Add stacked bars\n    upset.add_stacked_bars(by=consensus_col, elements=15)\n\n    # Plot and set title\n    axes = upset.plot()\n    plt.suptitle(title)\n    axes[\"totals\"].set_title(\"Total\")\n    plt.legend(loc=\"center left\")\n    plt.show()\n    return upset\n\n\nGraph.\n\n\nCode\nDB_COLS = [\"GEO_target_mod\", \"C-A_target\", \"Cistrome_target\", \"NGS_target_mod\"]\nconsensus_col = \"core7_DBs_consensus\"\ntitle = \"ChIP-Atlas core 7 samples presence in used DBs\\nTarget Consensus - No EpiAtlas overlap\"\nupset = make_db_upsetplot(\n    df=ca_core7_df, consensus_col=consensus_col, db_cols=DB_COLS, title=title\n)\n\n\n\n\n\n\n\n\n\nSupplementary Figure 8E: Comparison of the four public sources used to extract assay metadata after excluding ChIP-Atlas datasets present in EpiATLAS. The 1 source category corresponds to 5,115 datasets where the assay label was extracted only from GEO because they are unlabeled in ChIP-Atlas. A total of 706 datasets got different ChIP target names.\n\n\n\nGenome browser screenshots, using coordinates and samples specified in the image, see supplementary figures PDF.\n\n\n\nRead metadata for both ChIP-Atlas and imputed samples.\n\n\nCode\nimputed_metadata_path = (\n    base_metadata_dir\n    / \"epiatlas\"\n    / \"imputed\"\n    / \"hg38_epiatlas_imputed_pval_chip_2024-02.json\"\n)\nmetadata_imputed: pd.DataFrame = metadata_handler.load_any_metadata(imputed_metadata_path, as_dataframe=True)  # type: ignore\n\nca_metadata_path = base_metadata_dir / \"chip_atlas\" / \"CA.full_info_metadata.freeze1.tsv\"\nmetadata_ca = pd.read_csv(ca_metadata_path, sep=\"\\t\", low_memory=False)\n\n\nGather predictions from classifier training on observed core6 data (all pval, no input).\n\n\nCode\ndata_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\npred_dfs = {}  # Gather all 4 cases\n\nobserved_dir = (\n    data_dir\n    / \"hg38_100kb_all_none\"\n    / \"assay_epiclass_1l_3000n\"\n    / \"chip-seq-only\"\n    / \"complete_no_valid_oversample\"\n)\nobserved_inf_imputed_path = next((observed_dir / \"predict_imputed\").glob(\"*.csv\"))\nobserved_inf_CA_path = next((observed_dir / \"predict_C-A\").glob(\"*.csv\"))\n\nbasename = \"observed_core6_pval_inf\"\n\n# Imputed preds\ndf = pd.read_csv(observed_inf_imputed_path, header=0, index_col=0, low_memory=False)\ndf = pd.merge(df, metadata_imputed, left_index=True, right_on=\"md5sum\")\ndf[\"True class\"] = df[ASSAY]\n\nprint(f\"Imputed: {df.shape[0]} total files\")\npred_dfs[f\"{basename}_imputed\"] = df\n\n# C-A preds\ndf = pd.read_csv(observed_inf_CA_path, header=0, index_col=0, low_memory=False)\ndf = pd.merge(df, metadata_ca, left_index=True, right_on=\"ID\")\ndf[\"True class\"] = df[\"expected_assay\"]\nprint(f\"CA: {df.shape[0]} total files\")\n\ndisplay(df[\"expected_assay\"].value_counts(dropna=False))\npred_dfs[f\"{basename}_C-A\"] = df\n\n\nImputed: 9570 total files\nCA: 29105 total files\n\n\nh3k27ac     11316\nh3k4me3      6501\nh3k27me3     4319\nh3k4me1      3177\nh3k9me3      2228\nh3k36me3     1564\nName: expected_assay, dtype: int64\n\n\nGather predictions from classifier training on imputed data (all pval, no input).\n\n\nCode\nimputed_dir = (\n    data_dir\n    / \"hg38_100kb_all_none_imputed\"\n    / \"assay_epiclass_1l_3000n\"\n    / \"chip-seq-only\"\n    / \"complete_no_valid_oversample\"\n)\nimputed_inf_observed_path = next(\n    (imputed_dir / \"predict_epiatlas_pval_chip-seq\").glob(\"*.csv\")\n)\nimputed_inf_CA_path = next((imputed_dir / \"predict_C-A\").glob(\"*.csv\"))\n\nbasename = \"imputed_core6_pval_inf\"\n\n# Observed (non-imputed) data preds\ndf = pd.read_csv(imputed_inf_observed_path, header=0, index_col=0, low_memory=False)\ndf = pd.merge(df, metadata_v2_df, left_index=True, right_on=\"md5sum\")\ndf[\"True class\"] = df[ASSAY]\n\nprint(f\"EpiATLAS pval ChIP: {df.shape[0]} total files\")\npred_dfs[f\"{basename}_obs_core6_pval\"] = df\n\n# C-A preds\ndf = pd.read_csv(imputed_inf_CA_path, header=0, index_col=0, low_memory=False)\ndf = pd.merge(df, metadata_ca, left_index=True, right_on=\"ID\")\ndf[\"True class\"] = df[\"expected_assay\"]\n\nprint(f\"CA: {df.shape[0]} total files\")\npred_dfs[f\"{basename}_C-A\"] = df\n\n\nEpiATLAS pval ChIP: 5337 total files\nCA: 29105 total files\n\n\nCompute accuracy per assay.\n\n\nCode\ncore6_assays = ASSAY_ORDER[0:6]\nrows = []\n\nfor name, df in pred_dfs.items():\n    if \"Max pred\" not in df.columns:\n        df[\"Max pred\"] = df[core6_assays].max(axis=1)\n\n    task_name = f\"train_{name}\"\n\n    for label in core6_assays:\n        assay_df = df[df[\"True class\"] == label]\n\n        for min_pred in [\"0.0\", \"0.6\", \"0.9\"]:\n            sub_df = assay_df[assay_df[\"Max pred\"] &gt; float(min_pred)]\n            acc = (sub_df[\"True class\"] == sub_df[\"Predicted class\"]).mean()\n            rows.append([task_name, label, min_pred, acc, len(sub_df)])\n\ndf_acc_per_assay = pd.DataFrame(\n    rows, columns=[\"task_name\", \"assay\", \"min_predScore\", \"acc\", \"nb_samples\"]\n)\n\n\nGraphing results\n\n\nCode\n# Prepare data from graphing\ndf_acc_per_assay[\"scatter_name\"] = (\n    df_acc_per_assay[\"task_name\"]\n    .replace(\"train_\", \"\", regex=True)\n    .replace(\"imputed\", \"imp\", regex=True)\n    .replace(\"observed\", \"obs\", regex=True)\n)\ndf_acc_per_assay[\"inf_target\"] = df_acc_per_assay[\"scatter_name\"].str.split(\"_\").str[-1]\n\ndf_acc_per_assay = df_acc_per_assay.sort_values(\n    [\"assay\", \"min_predScore\", \"scatter_name\"]\n)\n\ngraph_df = df_acc_per_assay.copy()\ngraph_df = graph_df.sort_values([\"inf_target\", \"scatter_name\"])\n\n# Prepare boxplot data\ntick_group = [\n    \"obs_core6_pval_inf_imp\",\n    \"imp_core6_pval_inf_obs_core6_pval\",\n    \"obs_core6_pval_inf_C-A\",\n    \"imp_core6_pval_inf_C-A\",\n]\nscatter_name_to_position = {name: i for i, name in enumerate(tick_group)}\n\nmin_pred_values = [\"0.0\", \"0.6\", \"0.9\"]\noffset = [-0.25, 0, 0.25]  # Offset for each min_pred within a tick group\n\n# Define jitter magnitude (like 0.05 for left/right spacing)\njitter = 0.05\njitter_offsets = [-jitter, 0, jitter]\n\nmin_predScore_color_map = {\"0.0\": \"blue\", \"0.6\": \"orange\", \"0.9\": \"red\"}\n\nminY = 0.7\nmaxY = 1.005\n\n# Plot each trace\nfig = go.Figure()\nfor name in tick_group:\n    group = graph_df[graph_df[\"scatter_name\"] == name]\n\n    for i, min_pred in enumerate(min_pred_values):\n        df_subset = group[group[\"min_predScore\"] == min_pred]\n\n        x_position = scatter_name_to_position[name] + offset[i]\n        x_positions = [x_position] * len(df_subset)\n        y_values = df_subset[\"acc\"]\n        hover_texts = [\n            f\"{row['assay']}&lt;br&gt;Samples: {row['nb_samples']}\"\n            for _, row in df_subset.iterrows()\n        ]\n        colors = [assay_colors[assay] for assay in df_subset[\"assay\"]]\n\n        # Add box plot without points\n        fig.add_trace(\n            go.Box(\n                x=x_positions,\n                y=y_values,\n                name=f\"{name} - Min Pred Score: {min_pred}\",\n                line=dict(\n                    color=min_predScore_color_map[min_pred],\n                ),\n                boxpoints=\"all\",\n                marker=dict(\n                    opacity=0, size=1e-5\n                ),  # hide points, so whiskers don't go to min/max\n                boxmean=True,\n                showlegend=False,\n                hoverinfo=\"none\",\n            )\n        )\n        # Add scatter plot for individual points\n        x_jittered = [\n            x + jitter_offsets[i % len(jitter_offsets)] for i, x in enumerate(x_positions)\n        ]\n\n        # sort x/y together\n        x_jittered, y_values, hover_texts = zip(\n            *sorted(zip(x_jittered, y_values, hover_texts))\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=x_jittered,\n                y=y_values,\n                mode=\"markers\",\n                marker=dict(color=colors, size=8, line=dict(color=\"Black\", width=1)),\n                name=f\"{name} - Min Pred Score: {min_pred}\",\n                showlegend=False,\n                text=hover_texts,\n                hoverinfo=\"text+y\",\n            )\n        )\n\n# Update x-axis tick labels\nticktext = []\nfor tick in tick_group:\n    train, inf = tick.split(\"_inf_\")\n    ticktext.append(f\"&lt;b&gt;{train}&lt;/b&gt; \\u2192 &lt;b&gt;{inf}&lt;/b&gt;\")\n\nfig.update_xaxes(tickmode=\"array\", ticktext=ticktext, tickvals=list(range(len(ticktext))))\n\n# Update layout\nfig.update_layout(\n    title=\"Accuracy per Task (6 core assays)\",\n    xaxis_title=\"Task (training data \\u2192 inference data)\",\n    yaxis_title=\"Accuracy\",\n    showlegend=True,\n    height=600,\n    width=1000,\n    yaxis=dict(tickformat=\".2%\", range=[minY, maxY]),\n)\n\n# Add a legend for minPred colors\nfor val, color in min_predScore_color_map.items():\n    fig.add_trace(\n        go.Scatter(\n            x=[None],\n            y=[None],\n            mode=\"markers\",\n            marker=dict(size=10, color=color, symbol=\"square\"),\n            name=f\"Min Pred Score: {val}\",\n            showlegend=True,\n        )\n    )\n\n# Add a legend for assay colors\nfor assay in sorted(core6_assays):\n    color = assay_colors[assay]\n    fig.add_trace(\n        go.Scatter(\n            x=[None],\n            y=[None],\n            mode=\"markers\",\n            marker=dict(size=10, color=color),\n            name=assay,\n            legendgroup=\"assays\",\n            showlegend=True,\n        )\n    )\n\n# Add legend for obs and imp\nfig.add_annotation(\n    x=1.2,\n    y=0.2,\n    yref=\"paper\",\n    xref=\"paper\",\n    text=\"obs = observed&lt;br&gt;imp = imputed\",\n    showarrow=False,\n    font=dict(size=14),\n)\n\nfig.show()\n\n\n                                                \n\n\nThe number of samples associated with each point is visible on hover.\nSupplementary Figure 8H: Accuracy comparison per assay (dots) between Assay classifiers trained on observed vs imputed data from EpiATLAS and applied to either EpiATLAS or ChIP-Atlas without prediction score threshold (brown), or with a threshold of 0.6 (light blue) or 0.9 (dark blue). These classifiers were both trained only using the p-value datasets as this is the only track type available for imputed data (therefore no Input assay) (Supplementary Table 13). Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5× the interquartile range."
  },
  {
    "objectID": "figs/fig3.html#setup",
    "href": "figs/fig3.html#setup",
    "title": "Application of EpiClass on public epigenomic data",
    "section": "",
    "text": "Imports and paths setups.\n\n\nCode\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport skops.io as skio\nimport upsetplot\nfrom IPython.display import display\n\nfrom epiclass.utils.notebooks.paper.paper_utilities import (\n    ASSAY,\n    ASSAY_ORDER,\n    IHECColorMap,\n    MetadataHandler,\n    SplitResultsHandler,\n)\n\n\n\n\nCode\nCANCER = \"harmonized_sample_cancer_high\"\nCORE_ASSAYS = ASSAY_ORDER[0:7]\n\n\n\n\nCode\nbase_dir = Path.home() / \"Projects/epiclass/output/paper\"\npaper_dir = base_dir\nif not paper_dir.exists():\n    raise FileNotFoundError(f\"Directory {paper_dir} does not exist.\")\n\nbase_data_dir = base_dir / \"data\"\nbase_fig_dir = base_dir / \"figures\"\ntables_dir = base_dir / \"tables\"\n\nbase_metadata_dir = base_data_dir / \"metadata\"\n\n\n\n\nCode\nIHECColorMap = IHECColorMap(base_fig_dir)\nassay_colors = IHECColorMap.assay_color_map\ncell_type_colors = IHECColorMap.cell_type_color_map\nsex_colors = IHECColorMap.sex_color_map\n\n\n\n\nCode\nsplit_results_handler = SplitResultsHandler()\n\nmetadata_handler = MetadataHandler(paper_dir)\nmetadata_v2 = metadata_handler.load_metadata(\"v2\")\nmetadata_v2_df = metadata_handler.load_metadata_df(\"v2\")\n\n\n\n\nCode\nbase_pred_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\" / \"predictions\"\nif not base_pred_dir.exists():\n    raise FileNotFoundError(f\"Directory {base_pred_dir} does not exist.\")"
  },
  {
    "objectID": "figs/fig3.html#results-context",
    "href": "figs/fig3.html#results-context",
    "title": "Application of EpiClass on public epigenomic data",
    "section": "",
    "text": "Predictions on external databases (ENCODE, ChIP-Atlas and Recount3) were all performed using the same classifiers trained on the labeled EpiAtlas samples.\n\n\n\n\n\n\n\n\n\nMetadata category\nNb classes\nExperiment Key (comet.com)\nTraining Files\n\n\n\n\nassay_epiclass\n7\n69488630801b4a05a53b5d9e572f0aaa\n16788\n\n\nassay_epiclass\n11\n0f8e5eb996114868a17057bebe64f87c\n20922\n\n\nharmonized_donor_sex\n3\n4b908b83e0ec45c3ab991e65aa27af0c\n18299\n\n\nharmonized_donor_life_stage\n5\n91214ed0b1664395b1826dc69a495ed4\n15372\n\n\nharmonized_sample_cancer_high\n2\n15da476b92f140eab818ece369248f4c\n20922\n\n\nharmonized_biomaterial_type\n4\nf42b6f4e147c4f1bbe378f3eed415099\n20922\n\n\n\nClasses:\n\nassay 7c: 6 h3k* histone marks + input\nassay 11c: assay7c + rna_seq + mrna_seq + wgbs_standard + wgbs_pbat\nharmonized_donor_sex: male, female, mixed\nharmonized_donor_life_stage: adult, child, newborn, fetal, embryonic\nharmonized_sample_cancer_high (modification of harmonized_sample_disease_high): cancer, non-cancer (healthy/None+disease)\nharmonized_biomaterial_type (biomat): cell line, primary cell, primary cell culture, primary tissue\n\nTraining metadata: Approximately IHEC_sample_metadata_harmonization.v1.1.extended.csv.\nSee data/metadata/epiatlas/README.md for metadata details, and training_metadata_vs_official_v1.1.json for exact difference of our training data and v1.1.\nAdditonally, for ENCODE, the harmonized_sample_ontology_intermediate model was used on a subset of files with known EpiATLAS biospecimens.\n\n\n\n\n\n\n\n\n\nMetadata category\nNb classes\nExperiment Key (comet.com)\nTraining Files\n\n\n\n\nharmonized_sample_ontology_intermediate\n16\nbb66b72ae83645d587e50b34aebb39c3\n16379\n\n\n\nThe metadata for ENCODE predictions was created using the FILE + EXPERIMENT + BIOSAMPLE accessions, starting from filenames. For the initial extraction process, see: src/python/epiclass/utils/notebooks/paper/encode_metadata_creation.ipynb (permalink). Final metadata file: encode_full_metadata_2025-02_no_revoked.freeze1.csv(.xz)"
  },
  {
    "objectID": "figs/fig3.html#sec-all-DB-metrics",
    "href": "figs/fig3.html#sec-all-DB-metrics",
    "title": "Application of EpiClass on public epigenomic data",
    "section": "",
    "text": "ChIP-Atlas, ENCODE and recount3 metrics at various prediction score thresholds were generated using the MetricsPerAssay class, from src/python/epiclass/utils/notebooks/paper/metrics_per_assay.py (permalink).\n(Folder permalink) - ChIP-Atlas: src/python/epiclass/utils/notebooks/paper/c-a_pred_analysis.ipynb (section Summary metrics by assay) - ENCODE: src/python/epiclass/utils/notebooks/paper/encode_pred_analysis.ipynb (section All 5 tasks metrics summary, run Setup section beforehand) - recount3: src/python/epiclass/utils/notebooks/paper/recount3_pred_analysis.ipynb\nResults were manually merged into Supplementary Table 9, and graphs were manually created using values from that table.\n\n\nThe following code shows how the various fractions presented in the graph are obtained.\n\nRead prediction file.\n\n\nCode\nca_preds_dir = tables_dir / \"dfreeze_v2\" / \"predictions\"\nca_preds_path = (\n    ca_preds_dir / \"ChIP-Atlas_predictions_20240606_merge_metadata_freeze1.csv.xz\"\n)\nca_preds_df = pd.read_csv(ca_preds_path, sep=\",\", low_memory=False, compression=\"xz\")\n\nprint(f\"ChIP-Atlas: {ca_preds_df.shape[0]} total files\")\n\n\nChIP-Atlas: 48669 total files\n\n\nRemoving ChIP-Atlas experiment overlap with EpiATLAS\n\n\nCode\nprint(f\"ChIP-Atlas: Initial {ca_preds_df.shape[0]} files\")\nno_epiatlas_df = ca_preds_df[ca_preds_df[\"is_EpiAtlas_EpiRR\"] == \"0\"]\n\ndiff = ca_preds_df.shape[0] - no_epiatlas_df.shape[0]\nprint(f\"ChIP-Atlas: {diff} files with EpiAtlas EpiRR removed\")\nprint(f\"ChIP-Atlas: {no_epiatlas_df.shape[0]} files without EpiAtlas EpiRR\")\n\n\nChIP-Atlas: Initial 48669 files\nChIP-Atlas: 1047 files with EpiAtlas EpiRR removed\nChIP-Atlas: 47622 files without EpiAtlas EpiRR\n\n\nIgnoring non-core consensus files.\n\n\nCode\nnon_core_labels = [\"non-core\", \"CTCF\"]\nca_core7_df = no_epiatlas_df[\n    ~no_epiatlas_df[\"target_majority_consensus\"].isin((non_core_labels))\n]\n\ndiff = no_epiatlas_df.shape[0] - ca_core7_df.shape[0]\nprint(f\"ChIP-Atlas: {diff} files with non-core consensus removed\")\nprint(f\"ChIP-Atlas: {ca_core7_df.shape[0]} files with core consensus\")\n\ndisplay(ca_core7_df[\"target_majority_consensus\"].value_counts(dropna=False))\n\n\nChIP-Atlas: 918 files with non-core consensus removed\nChIP-Atlas: 46704 files with core consensus\n\n\ninput           17930\nh3k27ac         11169\nh3k4me3          6328\nh3k27me3         4195\nh3k4me1          3034\nh3k9me3          2102\nh3k36me3         1421\nno_consensus      525\nName: target_majority_consensus, dtype: int64\n\n\n\nFigure 3B: Inference of the Assay classifier on the ChIP-Atlas datasets represented as a multi-layer donut chart where the internal layer is depicting the availability status of the metadata as being provided/extracted (yellow) vs missing (grey), the central layer depicting the classifier prediction matching (green) or not (red) the expected metadata label, and the external layer containing the confidence level where prediction scores above 0.6 correspond to high-confidence predictions (light to dark blue &gt;0.6, &gt;0.8 and &gt;0.9) vs low confidence predictions (brown). The high-confidence mismatching predictions were further divided between datasets predicted as Input (light orange) vs potential mislabels (dark red).\n\n\n\n\nCode\ntotal_N = ca_core7_df.shape[0]\nhigh_confidence_pred_df = ca_core7_df[ca_core7_df[\"Max_pred_assay7\"] &gt;= 0.6]\n\nhigh_confidence_N = high_confidence_pred_df.shape[0]\nN_percent = high_confidence_N / total_N\nprint(\n    f\"ChIP-Atlas: {high_confidence_N}/{total_N} files ({high_confidence_N/total_N:.2%}) with high confidence assay prediction\"\n)\n\n\nChIP-Atlas: 41796/46704 files (89.49%) with high confidence assay prediction\n\n\n\n\n\n\n\nCode\ntotal_N = high_confidence_pred_df.shape[0]\n\nmatch_rule = (\n    high_confidence_pred_df[\"target_majority_consensus\"]\n    == high_confidence_pred_df[\"Predicted_class_assay7\"]\n)\nmatch_df = high_confidence_pred_df[match_rule]\nmismatch_df = high_confidence_pred_df[~match_rule]\n\nagreement_N = match_df.shape[0]\n\nprint(\n    f\"ChIP-Atlas: {agreement_N}/{total_N} files ({agreement_N / total_N:.2%}) with agreement between consensus and predicted assay\"\n)\n\n\nChIP-Atlas: 39532/41796 files (94.58%) with agreement between consensus and predicted assay\n\n\n\n\n\n\n\nCode\ntotal_mismatch = mismatch_df.shape[0]\ninput_rule = mismatch_df[\"Predicted_class_assay7\"] == \"input\"\ninput_pred_N = input_rule.sum()\n\nprint(\n    f\"ChIP-Atlas: {input_pred_N}/{total_mismatch} files ({input_pred_N / total_mismatch:.2%}) with mismatch predicted as input\"\n)\nprint(\n    f\"ChIP-Atlas: {total_mismatch-input_pred_N}/{total_mismatch} files ({(total_mismatch-input_pred_N) / total_mismatch:.2%}) potential mislabels\"\n)\ndisplay(mismatch_df[~input_rule][\"core7_DBs_consensus\"].value_counts(dropna=False))\n\n\nChIP-Atlas: 1525/2264 files (67.36%) with mismatch predicted as input\nChIP-Atlas: 739/2264 files (32.64%) potential mislabels\n\n\nIdentical                       468\nDifferent                       158\nIgnored - Potential non-core     64\n1 source                         49\nName: core7_DBs_consensus, dtype: int64"
  },
  {
    "objectID": "figs/fig3.html#supplementary-figure-6---prediction-score-threshold-impact",
    "href": "figs/fig3.html#supplementary-figure-6---prediction-score-threshold-impact",
    "title": "Application of EpiClass on public epigenomic data",
    "section": "",
    "text": "See Supplementary Figures page"
  },
  {
    "objectID": "figs/fig3.html#supplementary-figure-8",
    "href": "figs/fig3.html#supplementary-figure-8",
    "title": "Application of EpiClass on public epigenomic data",
    "section": "",
    "text": "See previous section.\n\n\n\nLoad ENCODE predictions.\n\n\nCode\nencode_preds_dir = base_pred_dir / \"encode\"\nencode_preds_path = (\n    encode_preds_dir / \"complete_encode_predictions_augmented_2025-02_metadata.csv.gz\"\n)\nencode_preds_df = pd.read_csv(\n    encode_preds_path, sep=\",\", low_memory=False, compression=\"gzip\"\n)\nprint(f\"ENCODE: {encode_preds_df.shape[0]} total files\")\n\n\nENCODE: 11643 total files\n\n\nFilter out overlap with EpiATLAS EpiRRs.\n\n\nCode\nencode_preds_df = encode_preds_df[encode_preds_df[\"in_epiatlas\"].astype(str) == \"False\"]\nprint(f\"ENCODE: {encode_preds_df.shape[0]} total files with no EpiAtlas overlap\")\n\n\nENCODE: 8777 total files with no EpiAtlas overlap\n\n\nLoad ENCODE manually created assay categories, and merge them with ENCODE predictions data.\n\n\nCode\nencode_meta_dir = base_data_dir / \"metadata\" / \"encode\"\n\nnon_core_categories_path = (\n    encode_meta_dir / \"non-core_encode_assay_category_2024-08-29.csv\"\n)\n\nnon_core_categories_df = pd.read_csv(non_core_categories_path, sep=\",\", low_memory=False)\nprint(f\"ENCODE: {non_core_categories_df.shape[0]} non-core categories\")\n\nnon_core_map = non_core_categories_df.set_index(\"target\").to_dict()[\"Assay category\"]\n\nN_mapped = len([val for val in non_core_map.values() if val != \"not_looked\"])\nprint(f\"ENCODE: {N_mapped} non-core categories mapped to functional categories.\")\n\n\n# Select non-core samples\nencode_non_core_df = encode_preds_df[\n    encode_preds_df[ASSAY].isin([\"non-core\", \"ctcf\"])\n].copy()\n\n# Map assays to categories\nencode_non_core_df[\"assay_category\"] = (\n    encode_non_core_df[\"assay\"].str.lower().replace(non_core_map)\n)\n\n\nENCODE: 1170 non-core categories\nENCODE: 238 non-core categories mapped to functional categories.\n\n\nGraph.\n\n\nCode\nassay_categories_order = [\n    \"trx_reg\",\n    \"heterochrom\",\n    \"polycomb\",\n    \"splicing\",\n    \"insulator\",\n    \"other/mixed\",\n    \"not_looked\",\n]\n\nassay_epiclass_order = [\n    \"h3k27ac\",\n    \"h3k4me3\",\n    \"h3k4me1\",\n    \"h3k9me3\",\n    \"h3k27me3\",\n    \"h3k36me3\",\n    \"input\",\n]\nassay_epiclass_order = {assay: i for i, assay in enumerate(assay_epiclass_order)}\npred_col = f\"Predicted class ({ASSAY}_7c)\"\nmax_pred_col = f\"Max pred ({ASSAY}_7c)\"\n\nmin_pred = 0.6\nsub_df = encode_non_core_df[encode_non_core_df[max_pred_col] &gt;= min_pred]\ngroupby = (\n    sub_df.groupby([\"assay_category\", pred_col])\n    .size()\n    .reset_index(name=\"Count\")\n    .sort_values([\"assay_category\", \"Count\"], ascending=[True, False])\n)\ngroupby[\"Percentage\"] = groupby.groupby(\"assay_category\")[\"Count\"].transform(\n    lambda x: (x / x.sum()) * 100\n)\n\n# Add order for plotting\ngroupby[\"assay_order\"] = groupby[pred_col].map(assay_epiclass_order)\ngroupby = groupby.sort_values(\n    [\"assay_category\", \"assay_order\"], ascending=[False, True]\n)\n\n# Main plot\nfig = px.bar(\n    groupby,\n    x=\"assay_category\",\n    y=\"Percentage\",\n    color=pred_col,\n    barmode=\"stack\",\n    category_orders={\"assay_category\": assay_categories_order},\n    color_discrete_map=assay_colors,\n    title=f\"core7 predictions for non-core assays, predScore &gt;= {min_pred:.2f}\",\n    labels={\"Percentage\": \"Percentage (%)\", \"assay_category\": \"Assay Category\"},\n)\n\n# Modify x-axis labels\ntotal_counts = groupby.groupby(\"assay_category\")[\"Count\"].sum()\nticktext = [\n    f\"{assay_category} (N={total_counts[assay_category]})\"\n    for assay_category in assay_categories_order\n]\nfig.update_xaxes(tickvals=assay_categories_order, ticktext=ticktext)\nfig.show()\n\n\n                                                \n\n\nSupplementary Figure 8B: Proportion of non-core ChIP-Seq datasets from ENCODE classified with high-confidence (&gt;0.6) in the different functional categories predicted as one of the 7 core ChIP assays. Since none of the six histone modifications used for training are specifically associated with insulator factors, they were instead mostly predicted as Input.\n\n\n\nPCAs were computed via src/python/epiclass/utils/compute_pca.py (permalink), using IncrementalPCA from scikit-learn.\nGraphing was done using code similar to src/python/epiclass/utils/notebooks/paper/pca_plot.ipynb (permalink), which uses the output of compute_pca.py.\n\n\n\nThe following function gives a consensus label to each ChIP-Atlas sample, using the target values given by ChIP-Atlas, cistromeDB, NGS-QC and GEO.\nThe consensus description is based on the following rules:\n\n“Identical” if all labels are the same\n“Different” if at least one label is different\n“1 source” if only one DB has a label\n“Ignored - Potential non-core” if any label is not in the core assays\n\n\n\nCode\ndef create_4DB_consensus_description(\n    ca_core_df: pd.DataFrame, db_cols: List[str]\n) -&gt; pd.Series:\n    \"\"\"Create a description of the 4DB assay consensus labels.\n\n    Treat \"Unclassified\" from Chip-Atlas as absent samples for the target consensus evaluation.\n\n    The consensus description is based on the following rules:\n    - \"Identical\" if all labels are the same\n    - \"Different\" if at least one label is different\n    - \"1 source\" if only one DB has a label\n    - \"Ignored - Potential non-core\" if any label is not in the core assays\n\n    Args:\n        ca_core_df: ChIP-Atlas core7 DataFrame\n\n    Returns:\n        Series with the target consensus description\n    \"\"\"\n    id_db_target = []\n    tmp_df = ca_core_df.loc[:, db_cols].copy()\n    tmp_df[\"C-A\"].replace(\"unclassified\", \"----\", inplace=True)\n\n    for labels in tmp_df.values:\n        missing_N = sum(label == \"----\" for label in labels)\n        db_labels = set(labels)\n\n        try:\n            db_labels.remove(\"----\")\n        except KeyError:\n            pass\n        if any(label not in CORE_ASSAYS + [\"ctrl\"] for label in db_labels):\n            id_db_target.append(\"Ignored - Potential non-core\")\n        elif missing_N == 3:\n            id_db_target.append(\"1 source\")\n        elif len(db_labels) == 1:\n            id_db_target.append(\"Identical\")\n        else:\n            id_db_target.append(\"Different\")\n\n    return pd.Series(id_db_target, index=ca_core_df.index)\n\n\nDefine graphing function make_db_upsetplot.\n\n\nCode\ndef make_db_upsetplot(\n    df: pd.DataFrame, consensus_col: str, db_cols: List[str], title: str\n) -&gt; upsetplot.UpSet:\n    \"\"\"Make an upsetplot of the sample presence in the different databases.\"\"\"\n    df = df.copy()\n\n    # Create a new DataFrame with boolean columns for each database\n    upset_df = pd.DataFrame()\n    for col in db_cols:\n        upset_df[col] = df[col] != \"----\"\n    upset_df[consensus_col] = df[consensus_col]\n\n    # Set the index for the UpSet plot\n    upset_df = upset_df.set_index(db_cols)\n\n    # Create the UpSet plot\n    upset = upsetplot.UpSet(\n        upset_df,\n        intersection_plot_elements=0,  # disable the default bar chart\n        sort_by=\"cardinality\",\n        show_counts=True,  # type: ignore\n        orientation=\"horizontal\",\n    )\n\n    # Add stacked bars\n    upset.add_stacked_bars(by=consensus_col, elements=15)\n\n    # Plot and set title\n    axes = upset.plot()\n    plt.suptitle(title)\n    axes[\"totals\"].set_title(\"Total\")\n    plt.legend(loc=\"center left\")\n    plt.show()\n    return upset\n\n\nGraph.\n\n\nCode\nDB_COLS = [\"GEO_target_mod\", \"C-A_target\", \"Cistrome_target\", \"NGS_target_mod\"]\nconsensus_col = \"core7_DBs_consensus\"\ntitle = \"ChIP-Atlas core 7 samples presence in used DBs\\nTarget Consensus - No EpiAtlas overlap\"\nupset = make_db_upsetplot(\n    df=ca_core7_df, consensus_col=consensus_col, db_cols=DB_COLS, title=title\n)\n\n\n\n\n\n\n\n\n\nSupplementary Figure 8E: Comparison of the four public sources used to extract assay metadata after excluding ChIP-Atlas datasets present in EpiATLAS. The 1 source category corresponds to 5,115 datasets where the assay label was extracted only from GEO because they are unlabeled in ChIP-Atlas. A total of 706 datasets got different ChIP target names.\n\n\n\nGenome browser screenshots, using coordinates and samples specified in the image, see supplementary figures PDF.\n\n\n\nRead metadata for both ChIP-Atlas and imputed samples.\n\n\nCode\nimputed_metadata_path = (\n    base_metadata_dir\n    / \"epiatlas\"\n    / \"imputed\"\n    / \"hg38_epiatlas_imputed_pval_chip_2024-02.json\"\n)\nmetadata_imputed: pd.DataFrame = metadata_handler.load_any_metadata(imputed_metadata_path, as_dataframe=True)  # type: ignore\n\nca_metadata_path = base_metadata_dir / \"chip_atlas\" / \"CA.full_info_metadata.freeze1.tsv\"\nmetadata_ca = pd.read_csv(ca_metadata_path, sep=\"\\t\", low_memory=False)\n\n\nGather predictions from classifier training on observed core6 data (all pval, no input).\n\n\nCode\ndata_dir = base_data_dir / \"training_results\" / \"dfreeze_v2\"\npred_dfs = {}  # Gather all 4 cases\n\nobserved_dir = (\n    data_dir\n    / \"hg38_100kb_all_none\"\n    / \"assay_epiclass_1l_3000n\"\n    / \"chip-seq-only\"\n    / \"complete_no_valid_oversample\"\n)\nobserved_inf_imputed_path = next((observed_dir / \"predict_imputed\").glob(\"*.csv\"))\nobserved_inf_CA_path = next((observed_dir / \"predict_C-A\").glob(\"*.csv\"))\n\nbasename = \"observed_core6_pval_inf\"\n\n# Imputed preds\ndf = pd.read_csv(observed_inf_imputed_path, header=0, index_col=0, low_memory=False)\ndf = pd.merge(df, metadata_imputed, left_index=True, right_on=\"md5sum\")\ndf[\"True class\"] = df[ASSAY]\n\nprint(f\"Imputed: {df.shape[0]} total files\")\npred_dfs[f\"{basename}_imputed\"] = df\n\n# C-A preds\ndf = pd.read_csv(observed_inf_CA_path, header=0, index_col=0, low_memory=False)\ndf = pd.merge(df, metadata_ca, left_index=True, right_on=\"ID\")\ndf[\"True class\"] = df[\"expected_assay\"]\nprint(f\"CA: {df.shape[0]} total files\")\n\ndisplay(df[\"expected_assay\"].value_counts(dropna=False))\npred_dfs[f\"{basename}_C-A\"] = df\n\n\nImputed: 9570 total files\nCA: 29105 total files\n\n\nh3k27ac     11316\nh3k4me3      6501\nh3k27me3     4319\nh3k4me1      3177\nh3k9me3      2228\nh3k36me3     1564\nName: expected_assay, dtype: int64\n\n\nGather predictions from classifier training on imputed data (all pval, no input).\n\n\nCode\nimputed_dir = (\n    data_dir\n    / \"hg38_100kb_all_none_imputed\"\n    / \"assay_epiclass_1l_3000n\"\n    / \"chip-seq-only\"\n    / \"complete_no_valid_oversample\"\n)\nimputed_inf_observed_path = next(\n    (imputed_dir / \"predict_epiatlas_pval_chip-seq\").glob(\"*.csv\")\n)\nimputed_inf_CA_path = next((imputed_dir / \"predict_C-A\").glob(\"*.csv\"))\n\nbasename = \"imputed_core6_pval_inf\"\n\n# Observed (non-imputed) data preds\ndf = pd.read_csv(imputed_inf_observed_path, header=0, index_col=0, low_memory=False)\ndf = pd.merge(df, metadata_v2_df, left_index=True, right_on=\"md5sum\")\ndf[\"True class\"] = df[ASSAY]\n\nprint(f\"EpiATLAS pval ChIP: {df.shape[0]} total files\")\npred_dfs[f\"{basename}_obs_core6_pval\"] = df\n\n# C-A preds\ndf = pd.read_csv(imputed_inf_CA_path, header=0, index_col=0, low_memory=False)\ndf = pd.merge(df, metadata_ca, left_index=True, right_on=\"ID\")\ndf[\"True class\"] = df[\"expected_assay\"]\n\nprint(f\"CA: {df.shape[0]} total files\")\npred_dfs[f\"{basename}_C-A\"] = df\n\n\nEpiATLAS pval ChIP: 5337 total files\nCA: 29105 total files\n\n\nCompute accuracy per assay.\n\n\nCode\ncore6_assays = ASSAY_ORDER[0:6]\nrows = []\n\nfor name, df in pred_dfs.items():\n    if \"Max pred\" not in df.columns:\n        df[\"Max pred\"] = df[core6_assays].max(axis=1)\n\n    task_name = f\"train_{name}\"\n\n    for label in core6_assays:\n        assay_df = df[df[\"True class\"] == label]\n\n        for min_pred in [\"0.0\", \"0.6\", \"0.9\"]:\n            sub_df = assay_df[assay_df[\"Max pred\"] &gt; float(min_pred)]\n            acc = (sub_df[\"True class\"] == sub_df[\"Predicted class\"]).mean()\n            rows.append([task_name, label, min_pred, acc, len(sub_df)])\n\ndf_acc_per_assay = pd.DataFrame(\n    rows, columns=[\"task_name\", \"assay\", \"min_predScore\", \"acc\", \"nb_samples\"]\n)\n\n\nGraphing results\n\n\nCode\n# Prepare data from graphing\ndf_acc_per_assay[\"scatter_name\"] = (\n    df_acc_per_assay[\"task_name\"]\n    .replace(\"train_\", \"\", regex=True)\n    .replace(\"imputed\", \"imp\", regex=True)\n    .replace(\"observed\", \"obs\", regex=True)\n)\ndf_acc_per_assay[\"inf_target\"] = df_acc_per_assay[\"scatter_name\"].str.split(\"_\").str[-1]\n\ndf_acc_per_assay = df_acc_per_assay.sort_values(\n    [\"assay\", \"min_predScore\", \"scatter_name\"]\n)\n\ngraph_df = df_acc_per_assay.copy()\ngraph_df = graph_df.sort_values([\"inf_target\", \"scatter_name\"])\n\n# Prepare boxplot data\ntick_group = [\n    \"obs_core6_pval_inf_imp\",\n    \"imp_core6_pval_inf_obs_core6_pval\",\n    \"obs_core6_pval_inf_C-A\",\n    \"imp_core6_pval_inf_C-A\",\n]\nscatter_name_to_position = {name: i for i, name in enumerate(tick_group)}\n\nmin_pred_values = [\"0.0\", \"0.6\", \"0.9\"]\noffset = [-0.25, 0, 0.25]  # Offset for each min_pred within a tick group\n\n# Define jitter magnitude (like 0.05 for left/right spacing)\njitter = 0.05\njitter_offsets = [-jitter, 0, jitter]\n\nmin_predScore_color_map = {\"0.0\": \"blue\", \"0.6\": \"orange\", \"0.9\": \"red\"}\n\nminY = 0.7\nmaxY = 1.005\n\n# Plot each trace\nfig = go.Figure()\nfor name in tick_group:\n    group = graph_df[graph_df[\"scatter_name\"] == name]\n\n    for i, min_pred in enumerate(min_pred_values):\n        df_subset = group[group[\"min_predScore\"] == min_pred]\n\n        x_position = scatter_name_to_position[name] + offset[i]\n        x_positions = [x_position] * len(df_subset)\n        y_values = df_subset[\"acc\"]\n        hover_texts = [\n            f\"{row['assay']}&lt;br&gt;Samples: {row['nb_samples']}\"\n            for _, row in df_subset.iterrows()\n        ]\n        colors = [assay_colors[assay] for assay in df_subset[\"assay\"]]\n\n        # Add box plot without points\n        fig.add_trace(\n            go.Box(\n                x=x_positions,\n                y=y_values,\n                name=f\"{name} - Min Pred Score: {min_pred}\",\n                line=dict(\n                    color=min_predScore_color_map[min_pred],\n                ),\n                boxpoints=\"all\",\n                marker=dict(\n                    opacity=0, size=1e-5\n                ),  # hide points, so whiskers don't go to min/max\n                boxmean=True,\n                showlegend=False,\n                hoverinfo=\"none\",\n            )\n        )\n        # Add scatter plot for individual points\n        x_jittered = [\n            x + jitter_offsets[i % len(jitter_offsets)] for i, x in enumerate(x_positions)\n        ]\n\n        # sort x/y together\n        x_jittered, y_values, hover_texts = zip(\n            *sorted(zip(x_jittered, y_values, hover_texts))\n        )\n        fig.add_trace(\n            go.Scatter(\n                x=x_jittered,\n                y=y_values,\n                mode=\"markers\",\n                marker=dict(color=colors, size=8, line=dict(color=\"Black\", width=1)),\n                name=f\"{name} - Min Pred Score: {min_pred}\",\n                showlegend=False,\n                text=hover_texts,\n                hoverinfo=\"text+y\",\n            )\n        )\n\n# Update x-axis tick labels\nticktext = []\nfor tick in tick_group:\n    train, inf = tick.split(\"_inf_\")\n    ticktext.append(f\"&lt;b&gt;{train}&lt;/b&gt; \\u2192 &lt;b&gt;{inf}&lt;/b&gt;\")\n\nfig.update_xaxes(tickmode=\"array\", ticktext=ticktext, tickvals=list(range(len(ticktext))))\n\n# Update layout\nfig.update_layout(\n    title=\"Accuracy per Task (6 core assays)\",\n    xaxis_title=\"Task (training data \\u2192 inference data)\",\n    yaxis_title=\"Accuracy\",\n    showlegend=True,\n    height=600,\n    width=1000,\n    yaxis=dict(tickformat=\".2%\", range=[minY, maxY]),\n)\n\n# Add a legend for minPred colors\nfor val, color in min_predScore_color_map.items():\n    fig.add_trace(\n        go.Scatter(\n            x=[None],\n            y=[None],\n            mode=\"markers\",\n            marker=dict(size=10, color=color, symbol=\"square\"),\n            name=f\"Min Pred Score: {val}\",\n            showlegend=True,\n        )\n    )\n\n# Add a legend for assay colors\nfor assay in sorted(core6_assays):\n    color = assay_colors[assay]\n    fig.add_trace(\n        go.Scatter(\n            x=[None],\n            y=[None],\n            mode=\"markers\",\n            marker=dict(size=10, color=color),\n            name=assay,\n            legendgroup=\"assays\",\n            showlegend=True,\n        )\n    )\n\n# Add legend for obs and imp\nfig.add_annotation(\n    x=1.2,\n    y=0.2,\n    yref=\"paper\",\n    xref=\"paper\",\n    text=\"obs = observed&lt;br&gt;imp = imputed\",\n    showarrow=False,\n    font=dict(size=14),\n)\n\nfig.show()\n\n\n                                                \n\n\nThe number of samples associated with each point is visible on hover.\nSupplementary Figure 8H: Accuracy comparison per assay (dots) between Assay classifiers trained on observed vs imputed data from EpiATLAS and applied to either EpiATLAS or ChIP-Atlas without prediction score threshold (brown), or with a threshold of 0.6 (light blue) or 0.9 (dark blue). These classifiers were both trained only using the p-value datasets as this is the only track type available for imputed data (therefore no Input assay) (Supplementary Table 13). Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5× the interquartile range."
  },
  {
    "objectID": "figs/fig2-supp.html",
    "href": "figs/fig2-supp.html",
    "title": "Supplementary Figure 5 to 7",
    "section": "",
    "text": "Supplementary Figures 5 and 7 correspond to Section 2. Supplementary Figure 6 is first mentioned in the second results section of the paper, but its content primarily relates to Section 3.\nThese figures are separate for page size reason, to keep a reasonable loading time.\nThe formatting of the figures may differ slightly from those in the paper, but they display the same data points.\nAll code cells are folded by default. To view any cell, click “Code” to expand it, or use the code options near the main title above to unfold all at once.\nSome code may be repeated, as the original Python notebook was designed for figures to be generated semi-independently."
  },
  {
    "objectID": "figs/fig2-supp.html#setup-code---imports-and-co.",
    "href": "figs/fig2-supp.html#setup-code---imports-and-co.",
    "title": "Supplementary Figure 5 to 7",
    "section": "Setup Code - Imports and co.",
    "text": "Setup Code - Imports and co.\nSetup imports.\n\n\nCode\nfrom __future__ import annotations\n\nimport itertools\nimport re\nimport tarfile\nfrom pathlib import Path\nfrom typing import Dict, List, Sequence\n\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom IPython.display import display\nfrom plotly.subplots import make_subplots\nfrom scipy.stats import zscore\n\nfrom epiclass.utils.notebooks.paper.paper_utilities import (\n    ASSAY,\n    ASSAY_MERGE_DICT,\n    ASSAY_ORDER,\n    BIOMATERIAL_TYPE,\n    CELL_TYPE,\n    LIFE_STAGE,\n    SEX,\n    IHECColorMap,\n    MetadataHandler,\n    SplitResultsHandler,\n    create_mislabel_corrector,\n    PathChecker\n)\n\nCORE7_ASSAYS = ASSAY_ORDER[0:7]\n\n\nSetup paths.\n\n\nCode\n# Root path\nbase_dir = Path.home() / \"Projects/epiclass/output/paper\"\nPathChecker.check_directory(base_dir)\n\n# More precise\nbase_data_dir = base_dir / \"data\"\nbase_fig_dir = base_dir / \"figures\"\ntable_dir = base_dir / \"tables\"\nmetadata_dir = base_data_dir / \"metadata\"\n\nofficial_metadata_dir = metadata_dir / \"epiatlas\" / \"official\"\nPathChecker.check_directory(official_metadata_dir)\n\n# alias\npaper_dir = base_dir\n\n\nSetup colors.\n\n\nCode\nIHECColorMap = IHECColorMap(base_fig_dir)\nassay_colors = IHECColorMap.assay_color_map\ncell_type_colors = IHECColorMap.cell_type_color_map\nsex_colors = IHECColorMap.sex_color_map\n\n\nSetup metadata and prediction files handlers.\n\n\nCode\nsplit_results_handler = SplitResultsHandler()\n\nmetadata_handler = MetadataHandler(paper_dir)\nmetadata_v2 = metadata_handler.load_metadata(\"v2\")\nmetadata_v2_df = metadata_v2.to_df()\n\n\nSetup figures general settings.\n\n\nCode\nmain_title_settings = {\n    \"title\":dict(\n        automargin=True,\n        x=0.5,\n        xanchor=\"center\",\n        yanchor=\"top\",\n        y=0.98\n        ),\n    \"margin\":dict(t=50, l=10, r=10)\n}"
  },
  {
    "objectID": "figs/fig2-supp.html#supplementary-figure-5",
    "href": "figs/fig2-supp.html#supplementary-figure-5",
    "title": "Supplementary Figure 5 to 7",
    "section": "Supplementary Figure 5",
    "text": "Supplementary Figure 5\n\nA - MLP AUROC on various classification tasks\nFor AUROC metrics, see Figure 2A,B\n\n\nB - chrY signal per sex: Average EpiRR z-score per assay\nLoad 10-fold cross-validation results.\n\n\nCode\nsex_10fold_dir = (\n    base_data_dir\n    / \"training_results\"\n    / \"dfreeze_v2\"\n    / \"hg38_100kb_all_none\"\n    / f\"{SEX}_1l_3000n\"\n    / \"10fold-oversampling\"\n)\nPathChecker.check_directory(sex_10fold_dir)\n\nsplit_results: Dict[str, pd.DataFrame] = split_results_handler.read_split_results(\n    sex_10fold_dir\n)\nconcat_results_10fold: pd.DataFrame = split_results_handler.concatenate_split_results(split_results, depth=1)  # type: ignore\nconcat_results_10fold = split_results_handler.add_max_pred(concat_results_10fold)\nconcat_results_10fold = metadata_handler.join_metadata(concat_results_10fold, metadata_v2)\n\n\nLoad chrY signal values.\n\n\nCode\nchrY_dir = base_data_dir / \"chrY\" / \"dfreeze_v2_stats\"\nPathChecker.check_directory(chrY_dir)\n\nchrY_df = pd.read_csv(chrY_dir / \"chrY_zscores.csv\")\n\n\nMerge previous dataframes.\n\n\nCode\ncross_val_analysis = pd.merge(\n    concat_results_10fold,\n    chrY_df,\n    left_index=True,\n    right_on=\"filename\",\n    suffixes=(\"\", \"_DROP\")\n    )\ncross_val_analysis.drop(\n    columns=[c for c in cross_val_analysis.columns if c.endswith(\"_DROP\")], inplace=True\n)\n\n\nDefine function zscore_per_assay to compute and graph the metric for each assay instead of globally.\n\n\nCode\ndef zscore_per_assay(\n    zscore_df: pd.DataFrame, logdir: Path | None = None, name: str | None = None\n) -&gt; None:\n    \"\"\"\n    Plot the z-score distributions per assay.\n\n    Does not include pval and raw tracks.\n\n    Args:\n        zscore_df: The dataframe with z-score data.\n    \"\"\"\n    zscore_df = zscore_df.copy(deep=True)\n\n    # Remove pval/raw tracks + rna unstranded\n    zscore_df = zscore_df[~zscore_df[\"track_type\"].isin([\"pval\", \"raw\", \"Unique_raw\"])]\n\n    # Merge rna protocols\n    zscore_df.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n\n    # Remove NAs\n    metric_label = \"chrY_zscore_vs_assay_w_track_exclusion\"\n    zscore_df = zscore_df[zscore_df[metric_label] != \"NA\"]\n\n    assay_sizes = zscore_df[ASSAY].value_counts()\n    assays = sorted(assay_sizes.index)\n\n    x_title = \"Sex z-score distributions per assay\"\n    fig = make_subplots(\n        rows=1,\n        cols=len(assays),\n        shared_yaxes=True,\n        x_title=x_title,\n        y_title=\"z-score\",\n        horizontal_spacing=0.02,\n        subplot_titles=[\n            f\"{assay_label} ({assay_sizes[assay_label]})\" for assay_label in assays\n        ],\n    )\n\n    for i, assay_label in enumerate(sorted(assays)):\n        sub_df = zscore_df[zscore_df[ASSAY] == assay_label]\n\n        hovertext = [\n            f\"{epirr}: z-score={z_score:.3f}, pred={pred:.3f}\"\n            for epirr, pred, z_score in zip(\n                sub_df[\"EpiRR\"],\n                sub_df[\"Max pred\"],\n                sub_df[metric_label],\n            )\n        ]\n        hovertext = np.array(hovertext)\n\n        sub_df.reset_index(drop=True, inplace=True)\n        y_values = sub_df[metric_label].values\n\n        female_idx = np.argwhere((sub_df[SEX] == \"female\").values).flatten()\n        male_idx = np.argwhere((sub_df[SEX] == \"male\").values).flatten()\n\n        fig.add_trace(\n            go.Box(\n                name=assay_label,\n                y=y_values[female_idx],\n                boxmean=True,\n                boxpoints=\"all\",\n                hovertemplate=\"%{text}\",\n                text=hovertext[female_idx],\n                marker=dict(\n                    size=2,\n                    color=sex_colors[\"female\"],\n                    line=dict(width=0.5, color=\"black\"),\n                ),\n                fillcolor=sex_colors[\"female\"],\n                line=dict(width=1, color=\"black\"),\n                showlegend=False,\n                legendgroup=\"Female\",\n            ),\n            row=1,\n            col=i + 1,\n        )\n\n        fig.add_trace(\n            go.Box(\n                name=assay_label,\n                y=y_values[male_idx],\n                boxmean=True,\n                boxpoints=\"all\",\n                hovertemplate=\"%{text}\",\n                text=hovertext[male_idx],\n                marker=dict(\n                    size=2, color=sex_colors[\"male\"], line=dict(width=0.5, color=\"black\")\n                ),\n                fillcolor=sex_colors[\"male\"],\n                line=dict(width=1, color=\"black\"),\n                showlegend=False,\n                legendgroup=\"Male\",\n            ),\n            row=1,\n            col=i + 1,\n        )\n\n    # Add a dummy scatter plot for legend\n    fig.add_trace(\n        go.Scatter(\n            x=[None],\n            y=[None],\n            mode=\"markers\",\n            name=\"Female\",\n            marker=dict(color=sex_colors[\"female\"], size=20),\n            showlegend=True,\n            legendgroup=\"Female\",\n        )\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=[None],\n            y=[None],\n            mode=\"markers\",\n            name=\"Male\",\n            marker=dict(color=sex_colors[\"male\"], size=20),\n            showlegend=True,\n            legendgroup=\"Male\",\n        )\n    )\n\n    fig.update_xaxes(showticklabels=False)\n    fig.update_yaxes(range=[-1.5, 3], showticklabels=True)\n\n    fig.update_layout(\n        width=1500,\n        height=750,\n    )\n\n    # Save figure\n    if logdir:\n        if name is None:\n            name = \"zscore_distributions_per_assay\"\n        fig.write_image(logdir / f\"{name}.svg\")\n        fig.write_image(logdir / f\"{name}.png\")\n        fig.write_html(logdir / f\"{name}.html\")\n\n    fig.show()\n\n\nGraph zscore per assay.\n\n\nCode\nzscore_per_assay(cross_val_analysis)\n\n\n                                                \n\n\nSupp. Fig. 5B: Distribution of average z-score signal of epigenomes (dots) over chrY per sex (female in red, male in blue) for each assay individually (showing only the fold change track type for the ChIP datasets, and the two types of WGBS and RNA-seq were merged). Dashed lines represent means, solid lines the medians, boxes the quartiles, and whiskers the farthest points within 1.5× the interquartile range.\nTo see the RNA-Seq and WGBS results, scroll to the right using the horizontal scrollbar at the bottom of the graph.\n\n\nC - Female/Male chrY signal z-score cluster separation\nDefine function merged_assays_separation_distance that computes and graphs the showing separation distance between male/female zscore clusters.\n\n\nCode\ndef merged_assays_separation_distance(\n    zscore_df: pd.DataFrame, logdir: Path | None = None, name: str | None = None\n) -&gt; None:\n    \"\"\"Complement to figure 2E, showing separation distance (mean, median)\n    between male/female zscore clusters, for ChIP-seq (core7). Grouped by EpiRR.\n\n    Args:\n        zscore_df (pd.DataFrame): The dataframe with z-score data.\n        logdir (Path): The directory path to save the output plots.\n        name (str): The base name for the output plot files.\n    \"\"\"\n    metric_label = \"chrY_zscore_vs_assay_track\"\n\n    # Preprocessing\n    zscore_df = zscore_df.copy(deep=True)\n    zscore_df.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)\n\n    zscore_df = zscore_df[zscore_df[ASSAY].isin(CORE7_ASSAYS)]  # type: ignore\n\n    # Remove pval/raw tracks\n    zscore_df = zscore_df[~zscore_df[\"track_type\"].isin([\"pval\", \"raw\"])]\n\n    # Average chrY z-score values\n    mean_chrY_values_df = zscore_df.groupby([\"EpiRR\", SEX]).agg(\n        {metric_label: \"mean\", \"Max pred\": \"mean\"}\n    )\n    mean_chrY_values_df.reset_index(inplace=True)\n    if not mean_chrY_values_df[\"EpiRR\"].is_unique:\n        raise ValueError(\"EpiRR is not unique.\")\n\n    mean_chrY_values_df.reset_index(drop=True, inplace=True)\n\n    distances = {\"mean\": [], \"median\": []}\n    min_preds = list(np.arange(0, 1.0, 0.01)) + [0.999]\n    sample_count = []\n    for min_pred in min_preds:\n        subset_chrY_values_df = mean_chrY_values_df[\n            mean_chrY_values_df[\"Max pred\"] &gt; min_pred\n        ]\n        sample_count.append(subset_chrY_values_df.shape[0])\n\n        # Compute separation distances\n        chrY_vals_female = subset_chrY_values_df[subset_chrY_values_df[SEX] == \"female\"][\n            metric_label\n        ]\n        chrY_vals_male = subset_chrY_values_df[subset_chrY_values_df[SEX] == \"male\"][\n            metric_label\n        ]\n\n        if not chrY_vals_female.empty and not chrY_vals_male.empty:\n            mean_distance = np.abs(chrY_vals_female.mean() - chrY_vals_male.mean())\n            median_distance = np.abs(chrY_vals_female.median() - chrY_vals_male.median())\n\n            distances[\"mean\"].append(mean_distance)\n            distances[\"median\"].append(median_distance)\n        else:\n            distances[\"mean\"].append(np.nan)\n            distances[\"median\"].append(np.nan)\n\n    # Plotting the results\n    fig = go.Figure()\n\n    # Add traces for mean and median distances\n    fig.add_trace(\n        go.Scatter(\n            x=min_preds,\n            y=distances[\"mean\"],\n            mode=\"lines+markers\",\n            name=\"Mean Distance (left)\",\n            line=dict(color=\"blue\"),\n        )\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=min_preds,\n            y=distances[\"median\"],\n            mode=\"lines+markers\",\n            name=\"Median Distance (left)\",\n            line=dict(color=\"green\"),\n        )\n    )\n\n    # Add trace for number of files\n    fig.add_trace(\n        go.Scatter(\n            x=min_preds,\n            y=np.array(sample_count) / max(sample_count),\n            mode=\"lines+markers\",\n            name=\"Proportion of samples (right)\",\n            line=dict(color=\"red\"),\n            yaxis=\"y2\",\n        )\n    )\n\n    fig.update_xaxes(range=[0.499, 1.0])\n\n    # Update layout for secondary y-axis\n    fig.update_layout(\n        title=\"Separation Distance of chrY z-scores male/female clusters - ChIP-Seq\",\n        xaxis_title=\"Average Prediction Score minimum threshold\",\n        yaxis_title=\"Z-score Distance\",\n        yaxis2=dict(title=\"Proportion of samples\", overlaying=\"y\", side=\"right\"),\n        yaxis2_range=[0, 1.001],\n        legend=dict(\n            x=1.08,\n        ),\n    )\n\n    # Save figure\n    if logdir:\n        if name is None:\n            name = \"zscore_cluster_separation_distance\"\n        fig.write_image(logdir / f\"{name}.svg\")\n        fig.write_image(logdir / f\"{name}.png\")\n        fig.write_html(logdir / f\"{name}.html\")\n\n    fig.show()\n\n\nGraph.\n\n\nCode\nmerged_assays_separation_distance(cross_val_analysis)\n\n\n                                                \n\n\nSupp. Fig. 5C: Effect of a prediction score threshold on the aggregated mean (blue) and median (green) sex z-score male/female cluster distances, as well as and corresponding file subset size (red) of ChIP-related assays from panel B.\n\n\nD - GP-Age, all samples\nSee Figure 2F section.\n\n\nE - Epilogos sex genes chromatin states\nImages extracted from Epilogos viewer, using specified coordinates (XIST and FIRRE positions), and:\n\nView mode: Paired\nDataset: IHEC\nPairwise: Male VS Female 100 samples\nSaliency Metric: S1\n\n\nSupp. Fig. 5E: Epilogos pairwise comparisons of male (top) vs female (bottom) showing portions of important regions for the Sex classifier, including the XIST (left) and FIRRE (right) genes.\nSee Annex A for a more detailled Epilogos color legend.\n\n\nF - Genome browser for biospecimen important regions\n\nSupp. Fig. 5F: Genome browser representation of the important regions shown in Figure 2I."
  },
  {
    "objectID": "figs/fig2-supp.html#sec-supp6",
    "href": "figs/fig2-supp.html#sec-supp6",
    "title": "Supplementary Figure 5 to 7",
    "section": "Supplementary Figure 6 - Prediction score threshold impact",
    "text": "Supplementary Figure 6 - Prediction score threshold impact\nFor full code, since the processing is more complex, see src/python/epiclass/utils/notebooks/paper/confidence_threshold.ipynb (permalink).\n\n\n\n\n\nSupplementary Figure 6: Impact of prediction score threshold on performance metrics. Impact of prediction score threshold on accuracy, F1-score and number of files for both EpiATLAS cross-validation performance and inference on datasets from other databases with provided or extracted labels. Performance for Assay, Sex, Cancer, Biomaterial type and Life stage classifiers are shown, for EpiATLAS, ENCODE core/non-core, ChIP-Atlas and Recount3 datasets. The number of classes (C) and the number of files analyzed (N) used to calculate the performances are shown at the bottom for each graph. The 11 classes of the Assay classifiers for EpiATLAS correspond to the six ChIP-Seq histone modifications, their control Input file, and two protocols of both RNA-Seq and WGBS, while for the 9 classes of ENCODE the two protocols were grouped, and indeed only the seven ChIP-related assays were used for ChIP-Atlas and RNA-Seq for Recount3. For the Sex classifier the third class corresponds to ‘mixed’, absent for ENCODE. The Cancer classifier is binary (where non-cancer is a mix of healthy and other diseases). For the Biomaterial classifier the ‘primary cell culture’ class is missing from all public sources (but ‘primary cell’, ‘primary tissue’ and ‘cell line’ are present), while the three classes (perinatal, pediatric, adult) were always used for the Life stage classifier."
  },
  {
    "objectID": "figs/fig2-supp.html#supplementary-figure-7---biospecimen-classifier---chromscore-for-high-shap-regions",
    "href": "figs/fig2-supp.html#supplementary-figure-7---biospecimen-classifier---chromscore-for-high-shap-regions",
    "title": "Supplementary Figure 5 to 7",
    "section": "Supplementary Figure 7 - Biospecimen classifier - ChromScore for high-SHAP regions",
    "text": "Supplementary Figure 7 - Biospecimen classifier - ChromScore for high-SHAP regions\nFor full code, since the processing is more complex, see src/python/epiclass/utils/notebooks/analyze_hdf5_vals.ipynb (permalink), particularly section “ChromScore hdf5 values”.\nThis is the breakdown per cell type of Figure 2G.\n\n\n\n\n\nSupplementary Figure 7: Class-Specific ChromScore Distributions in Biospecimen Classifier Regions Identified by SHAP. Distribution of the average ChromScore values over the important Biospecimen classifier regions according to SHAP (blue) compared to the global distribution (pink), for each biospecimen class sorted according to the EpiATLAS order. The number of files and regions analyzed to calculate the p-values are shown. ChromScore values for each region are averaged over files of each class (one average value per feature). Statistical significance was assessed using a two-sided Welch’s t-test where the number of regions was above 30, otherwise the reported p-value is the worst of Welch’s t-test and non-parametric Brunner-Munzel. Dashed lines represent means, solid lines the medians, boxes the quartiles."
  },
  {
    "objectID": "figs/fig2-supp.html#sec-annex-a",
    "href": "figs/fig2-supp.html#sec-annex-a",
    "title": "Supplementary Figure 5 to 7",
    "section": "Annex A - Epilogos color legend",
    "text": "Annex A - Epilogos color legend\n\n\nbiv = bivalent\nTx = Transcription\nRepr = Repressed\nPC = Polycomb"
  }
]