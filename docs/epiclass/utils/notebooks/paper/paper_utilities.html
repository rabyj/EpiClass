<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.0">
<title>epiclass.utils.notebooks.paper.paper_utilities API documentation</title>
<meta name="description" content="Utility functions for the paper notebooks.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>epiclass.utils.notebooks.paper.paper_utilities</code></h1>
</header>
<section id="section-intro">
<p>Utility functions for the paper notebooks.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.add_second_highest_prediction"><code class="name flex">
<span>def <span class="ident">add_second_highest_prediction</span></span>(<span>df: pd.DataFrame, pred_cols: List[str]) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Return the DataFrame with a columns for the second highest prediction class.</p>
<p>Adds columns:
- '2nd pred class': The class with the second highest prediction.
- '1rst/2nd prob diff': The difference between the highest and second highest prediction probabilities.
- '1rst/2nd prob ratio': The ratio of the highest to second highest prediction probabilities.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.check_label_coherence"><code class="name flex">
<span>def <span class="ident">check_label_coherence</span></span>(<span>dataframe, categories, column_templates: Dict[str, str] | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Check that the predicted and true labels for each category are consistent.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataframe</code></strong></dt>
<dd>DataFrame containing the predictions and true classes.</dd>
<dt><strong><code>categories</code></strong></dt>
<dd>List of categories to check.</dd>
<dt><strong><code>column_templates</code></strong></dt>
<dd>Dictionary mapping column types to template strings.
Requires "True", "Predicted"
If None, uses default templates.</dd>
</dl>
<p>Raises ValueError if the predicted and true labels are not consistent.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.create_mislabel_corrector"><code class="name flex">
<span>def <span class="ident">create_mislabel_corrector</span></span>(<span>paper_dir: Path) ‑> Tuple[Dict[str, str], Dict[str, Dict[str, str]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Obtain information necessary to correct sex and life_stage mislabels.</p>
<p>Returns:
- Dict[str, str]: {md5sum: EpiRR_no-v}
- Dict[str, Dict[str, str]]: {label_category: {EpiRR_no-v: corrected_label}}</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.display_perc"><code class="name flex">
<span>def <span class="ident">display_perc</span></span>(<span>df: pd.DataFrame | pd.Series)</span>
</code></dt>
<dd>
<div class="desc"><p>Display a DataFrame with percentages.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.extract_data_from_files"><code class="name flex">
<span>def <span class="ident">extract_data_from_files</span></span>(<span>parent_folder: Path, file_pattern: str, search_line: str, extract_pattern: str, type_cast: type = builtins.int, unique: bool = True) ‑> Dict[str, Set[Any]]</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts data from files matching a specific pattern within each directory of a parent folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>parent_folder</code></strong> :&ensp;<code>Path</code></dt>
<dd>The directory containing subdirectories to search.</dd>
<dt><strong><code>file_pattern</code></strong> :&ensp;<code>str</code></dt>
<dd>Glob pattern for files to search within each subdirectory.</dd>
<dt><strong><code>search_line</code></strong> :&ensp;<code>str</code></dt>
<dd>Line identifier to search for data extraction.</dd>
<dt><strong><code>extract_pattern</code></strong> :&ensp;<code>str</code></dt>
<dd>Regex pattern to extract the desired data from the identified line.</dd>
<dt><strong><code>type_cast</code></strong> :&ensp;<code>type</code></dt>
<dd>Type to cast the extracted data to.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, Set[type]]</code></dt>
<dd>A dictionary with subdirectory names as keys and sets of extracted data as values.</dd>
</dl></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.extract_experiment_keys_from_output_files"><code class="name flex">
<span>def <span class="ident">extract_experiment_keys_from_output_files</span></span>(<span>parent_folder: Path) ‑> Dict[str, Set[str]]</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts experiment keys from output (.o) files.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.extract_input_sizes_from_output_files"><code class="name flex">
<span>def <span class="ident">extract_input_sizes_from_output_files</span></span>(<span>parent_folder: Path) ‑> Dict[str, Set[int]]</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts model input sizes from output (.o) files.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.extract_node_jobs_from_error_files"><code class="name flex">
<span>def <span class="ident">extract_node_jobs_from_error_files</span></span>(<span>parent_folder: Path) ‑> Dict[str, Set[int]]</span>
</code></dt>
<dd>
<div class="desc"><p>Extracts SLURM job IDs from error (.e) files.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.filter_biomat_LS"><code class="name flex">
<span>def <span class="ident">filter_biomat_LS</span></span>(<span>df: pd.DataFrame, biomaterial_cat_name: str, col_templates: Dict[str, str], predScore_threshold: float = 0.8, verbose: bool = True) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Filter biomaterials for to enable more life stage predictions.</p>
<p>Remove samples expected to be from cell lines.</p>
<p>For unknown biomat, retain samples where biomat predictions are not
cell line with predScore &gt; predScore_threshold.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong></dt>
<dd>DataFrame with biomaterial type predictions.</dd>
<dt><strong><code>biomaterial_cat_name</code></strong></dt>
<dd>metadata category label (e.g. 'biomaterial_type')</dd>
<dt><strong><code>col_templates</code></strong></dt>
<dd>Column templates for 'true_col', 'pred_col', and 'max_pred' columns.</dd>
<dt><strong><code>predScore_threshold</code></strong></dt>
<dd>threshold for minimum prediction score</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>Print intermediate results</dd>
</dl></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.find_target_recall"><code class="name flex">
<span>def <span class="ident">find_target_recall</span></span>(<span>df: pd.DataFrame, category_name: str, col_templates: Dict[str, str], class_of_interest: str, target_recall: float = 0.9, verbose: bool = False, iterations: int = 250, minimum_threshold: float = 0.5)</span>
</code></dt>
<dd>
<div class="desc"><p>Find the first threshold such that recall ≥ target_recall
among predictions with prediction score ≥ threshold.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong></dt>
<dd>DataFrame</dd>
<dt><strong><code>category_name</code></strong></dt>
<dd>metadata category (e.g. 'biomaterial_type')</dd>
<dt><strong><code>col_templates</code></strong></dt>
<dd>Column templates for 'true_col', 'pred_col', and 'max_pred' columns.</dd>
<dt><strong><code>class_of_interest</code></strong></dt>
<dd>Label of class within category</dd>
<dt><strong><code>target_recall</code></strong></dt>
<dd>The target recall value</dd>
<dt><strong><code>verbose</code></strong></dt>
<dd>Print intermediate results</dd>
<dt><strong><code>iterations</code></strong></dt>
<dd>Number of thresholds to try in between minimum_threshold and 1</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>threshold, recall</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.format_labels"><code class="name flex">
<span>def <span class="ident">format_labels</span></span>(<span>df: pd.DataFrame, columns: List[str]) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>For given column label, format the labels by replacing spaces by underscores,
and converting to lowercase.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.merge_life_stages"><code class="name flex">
<span>def <span class="ident">merge_life_stages</span></span>(<span>df: pd.DataFrame, lifestage_column_name: str = 'harmonized_donor_life_stage', column_name_templates: List[str] | None = None, exact_replace: bool = False, verbose: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Merge perinatal stages into one category, for given column names.</p>
<p>New columns for f"{LIFE_STAGE}_merged" will be added for each column name template.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>DataFrame to merge columns in.</dd>
<dt><strong><code>lifestage_column_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the life stage category.</dd>
<dt>column_name_templates (List[str] | None): List of column name templates to merge.</dt>
<dt>ex: ["{}", "True class ({})", "Predicted class ({})"]</dt>
<dt><strong><code>exact_replace</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, raise error if column not found or replacement already exists.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, print the column names that are being merged or skipped.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>DataFrame with merged columns.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>KeyError</code></dt>
<dd>If exact_replace=True and column operations fail.</dd>
<dt><code>ValueError</code></dt>
<dd>If remapping fails due to unmapped values.</dd>
</dl></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.merge_similar_assays"><code class="name flex">
<span>def <span class="ident">merge_similar_assays</span></span>(<span>df: pd.DataFrame) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Attempt to merge rna-seq/wgbs categories, included prediction score.</p>
<p>Expects the dataframe to have the following columns:
- rna_seq
- mrna_seq
- wgbs-standard
- wgbs-pbat</p>
<p>The output dataframe will have the following columns:
- rna_seq
- wgbs</p>
<p>The dataframe is modified in place.</p>
<p>Expecteds "True class" or "Expected class", and "Predicted class" to be present.</p>
<p>A ValueError is raised if the columns are not present.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.print_column_content"><code class="name flex">
<span>def <span class="ident">print_column_content</span></span>(<span>df: pd.DataFrame, col: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Print absolute and relative count of a string column.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.rename_columns"><code class="name flex">
<span>def <span class="ident">rename_columns</span></span>(<span>df, remapper: Dict[str, str], exact_match: bool = False, verbose: bool = False) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Rename columns in a DataFrame using a dictionary of old to new column names.</p>
<p>If exact_match is True, the columns will be renamed exactly as specified in the dictionary.
If exact_match is False, the columns will be renamed using a regular expression to replace the all occurrences of a string to a new string.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.set_file_id"><code class="name flex">
<span>def <span class="ident">set_file_id</span></span>(<span>df: pd.DataFrame, input_col: str = 'Unnamed: 0', output_col: str = 'md5sum') ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Standardizes a filename column by extracting the prefix and ensuring it is the first column.</p>
<p>This function renames a given column (<code>input_col</code>) by extracting the prefix (before <code>_</code>)
and moves it to the first position in the DataFrame. If <code>input_col</code> and <code>output_col</code>
are the same, the column is updated in place and repositioned.</p>
<p>Handled filename cases:
- recount3: sra.base_sums.SRP076599_SRR3669968.ALL_[resolution]<em>[filters].hdf5 -&gt; SRR3669968
- ENCODE/ChIP-Atlas: [file_db_accession]</em>[resolution]<em>[filters].hdf5 -&gt; [file_db_accession]
- EpiATLAS: [md5sum]</em>[resolution]_[filters].hdf5 -&gt; [md5sum]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>The input DataFrame.</dd>
<dt><strong><code>input_col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The name of the column to process. Defaults to "Unnamed: 0".</dd>
<dt><strong><code>output_col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The name of the resulting column. Defaults to "md5sum".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>A modified DataFrame with the processed column as the first column.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>KeyError</code></dt>
<dd>If <code>input_col</code> is not found in the DataFrame.</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap"><code class="flex name class">
<span>class <span class="ident">IHECColorMap</span></span>
<span>(</span><span>base_fig_dir: Path)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to handle IHEC color map.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IHECColorMap:
    &#34;&#34;&#34;Class to handle IHEC color map.&#34;&#34;&#34;

    def __init__(self, base_fig_dir: Path):
        self.base_fig_dir = base_fig_dir
        self.ihec_colormap_name = &#34;IHEC_EpiATLAS_IA_colors_Apl01_2024.json&#34;
        self.ihec_color_map = self.get_IHEC_color_map(
            base_fig_dir, self.ihec_colormap_name
        )
        self.assay_color_map = self.create_assay_color_map()
        self.cell_type_color_map = self.create_cell_type_color_map()
        self.sex_color_map = self.create_sex_color_map()

    @staticmethod
    def get_IHEC_color_map(folder: Path, name: str) -&gt; List[Dict]:
        &#34;&#34;&#34;Get the IHEC color map.&#34;&#34;&#34;
        color_map_path = folder / name
        with open(color_map_path, &#34;r&#34;, encoding=&#34;utf8&#34;) as color_map_file:
            ihec_color_map = json.load(color_map_file)
        return ihec_color_map

    def _create_color_map(self, label_category: str) -&gt; Dict[str, str]:
        &#34;&#34;&#34;Create a rbg color map from IHEC rgb strings&#34;&#34;&#34;
        color_dict = [elem for elem in self.ihec_color_map if label_category in elem][0][
            label_category
        ][0]
        for name, color in list(color_dict.items()):
            rbg = color.split(&#34;,&#34;)
            color_dict[name] = f&#34;rgb({rbg[0]},{rbg[1]},{rbg[2]})&#34;
            color_dict[name.lower()] = color_dict[name]
            color_dict[name.lower().replace(&#34;-&#34;, &#34;_&#34;)] = color_dict[name]
        return color_dict

    def create_sex_color_map(self) -&gt; Dict[str, str]:
        &#34;&#34;&#34;Create a rbg color map for ihec sex label category.&#34;&#34;&#34;
        color_dict = self._create_color_map(SEX)
        return color_dict

    def create_assay_color_map(self) -&gt; Dict[str, str]:
        &#34;&#34;&#34;Create a rbg color map for ihec assays.&#34;&#34;&#34;
        category_label = &#34;experiment&#34;
        color_dict = self._create_color_map(category_label)
        color_dict[&#34;mrna_seq&#34;] = color_dict[&#34;rna_seq&#34;]
        for assay in [&#34;wgbs-pbat&#34;, &#34;wgbs-standard&#34;]:
            color_dict[assay] = color_dict[&#34;wgbs&#34;]
            color_dict[assay.replace(&#34;-&#34;, &#34;_&#34;)] = color_dict[&#34;wgbs&#34;]
        return color_dict

    def create_cell_type_color_map(self) -&gt; Dict[str, str]:
        &#34;&#34;&#34;Create the rbg color map for ihec cell types.&#34;&#34;&#34;
        return self._create_color_map(CELL_TYPE)</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap.get_IHEC_color_map"><code class="name flex">
<span>def <span class="ident">get_IHEC_color_map</span></span>(<span>folder: Path, name: str) ‑> List[Dict[~KT, ~VT]]</span>
</code></dt>
<dd>
<div class="desc"><p>Get the IHEC color map.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap.create_assay_color_map"><code class="name flex">
<span>def <span class="ident">create_assay_color_map</span></span>(<span>self) ‑> Dict[str, str]</span>
</code></dt>
<dd>
<div class="desc"><p>Create a rbg color map for ihec assays.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap.create_cell_type_color_map"><code class="name flex">
<span>def <span class="ident">create_cell_type_color_map</span></span>(<span>self) ‑> Dict[str, str]</span>
</code></dt>
<dd>
<div class="desc"><p>Create the rbg color map for ihec cell types.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap.create_sex_color_map"><code class="name flex">
<span>def <span class="ident">create_sex_color_map</span></span>(<span>self) ‑> Dict[str, str]</span>
</code></dt>
<dd>
<div class="desc"><p>Create a rbg color map for ihec sex label category.</p></div>
</dd>
</dl>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler"><code class="flex name class">
<span>class <span class="ident">MetadataHandler</span></span>
<span>(</span><span>paper_dir: Path | str)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to handle Metadata objects.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MetadataHandler:
    &#34;&#34;&#34;Class to handle Metadata objects.&#34;&#34;&#34;

    def __init__(self, paper_dir: Path | str):
        self.paper_dir = Path(paper_dir)
        self.data_dir = self.paper_dir / &#34;data&#34;

        self.version_names = {
            &#34;v1&#34;: &#34;hg38_2023-epiatlas_dfreeze_formatted_JR.json&#34;,
            &#34;v2&#34;: &#34;hg38_2023-epiatlas-dfreeze-pospurge-nodup_filterCtl.json&#34;,
            &#34;v2-encode&#34;: &#34;hg38_2023-epiatlas-dfreeze_v2.1_w_encode_noncore_2.json&#34;,
        }

    def load_metadata(self, version: str) -&gt; Metadata:
        &#34;&#34;&#34;Return metadata for a specific version.

        Args:
            version (str): The version of the metadata to load.
            One of &#39;v1&#39;, &#39;v2&#39;, &#39;v2-encode&#39;.

        Example of epiRR unique to v1: IHECRE00003355.2
        &#34;&#34;&#34;
        if version not in self.version_names:
            raise ValueError(f&#34;Version must be one of {self.version_names.keys()}&#34;)

        metadata = Metadata(
            self.data_dir / &#34;metadata&#34; / &#34;epiatlas&#34; / self.version_names[version]
        )
        return metadata

    def load_any_metadata(
        self, path: Path | str, as_dataframe: bool = False
    ) -&gt; Metadata | pd.DataFrame:
        &#34;&#34;&#34;Return metadata for a specific file.&#34;&#34;&#34;
        metadata = Metadata(Path(path))
        if as_dataframe:
            metadata = metadata.to_df()
        return metadata

    def load_metadata_df(self, version: str, merge_assays: bool = True) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Load a metadata dataframe for a given version.

        merge_assays: Merge similar assays (rna 2x / wgb 2x)
        &#34;&#34;&#34;
        metadata = self.load_metadata(version)
        metadata_df = self.metadata_to_df(metadata, merge_assays)
        return metadata_df

    @staticmethod
    def metadata_to_df(metadata: Metadata, merge_assays: bool = True) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Convert the metadata to a dataframe.

        merge_assays: Merge similar assays (rna 2x / wgb 2x)
        &#34;&#34;&#34;
        metadata_df = pd.DataFrame.from_records(list(metadata.datasets))
        metadata_df.set_index(&#34;md5sum&#34;, inplace=True)
        if merge_assays:
            metadata_df.replace({ASSAY: ASSAY_MERGE_DICT}, inplace=True)
        return metadata_df

    @staticmethod
    def join_metadata(df: pd.DataFrame, metadata: Metadata) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Join the metadata to the results dataframe.&#34;&#34;&#34;
        metadata_df = pd.DataFrame(metadata.datasets)
        metadata_df.set_index(&#34;md5sum&#34;, inplace=True)
        metadata_df[&#34;md5sum&#34;] = metadata_df.index

        diff_set = set(df.index) - set(metadata_df.index)
        if diff_set:
            err_df = pd.DataFrame(diff_set, columns=[&#34;md5sum&#34;])
            print(err_df.to_string(), file=sys.stderr)
            raise AssertionError(
                f&#34;{len(diff_set)} md5sums in the results dataframe are not present in the metadata dataframe. Saved error md5sums to join_missing_md5sums.csv.&#34;
            )

        merged_df = df.merge(
            metadata_df,
            how=&#34;left&#34;,
            left_index=True,
            right_index=True,
            suffixes=(None, &#34;_delete&#34;),
        )
        if len(merged_df) != len(df):
            raise AssertionError(
                &#34;Merged dataframe has different length than original dataframe&#34;
            )
        to_drop = [col for col in merged_df.columns if &#34;_delete&#34; in col]
        merged_df.drop(columns=to_drop, inplace=True)

        return merged_df

    @staticmethod
    def uniformize_metadata_for_plotting(
        epiatlas_metadata: Metadata,
        ca_pred_df: pd.DataFrame | None = None,
        enc_pred_df: pd.DataFrame | None = None,
        recount3_metadata: pd.DataFrame | None = None,
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Simplify metadata with chip-atlas metadata for plotting.&#34;&#34;&#34;
        columns_to_keep = [&#34;id&#34;, ASSAY, &#34;track_type&#34;, &#34;source&#34;, &#34;plot_label&#34;]
        to_concat = []

        epiatlas_df = pd.DataFrame.from_records(list(epiatlas_metadata.datasets))
        epiatlas_df[&#34;source&#34;] = [&#34;epiatlas&#34;] * len(epiatlas_df)
        epiatlas_df[&#34;id&#34;] = epiatlas_df[&#34;md5sum&#34;]
        if &#34;plot_label&#34; not in epiatlas_df.columns:
            epiatlas_df[&#34;plot_label&#34;] = [None] * len(epiatlas_df)
        epiatlas_df = epiatlas_df[columns_to_keep]

        to_concat.append(epiatlas_df)

        if ca_pred_df is not None:
            ca_df = ca_pred_df.copy(deep=True)
            ca_df[&#34;source&#34;] = [&#34;C-A&#34;] * len(ca_df)
            ca_df[ASSAY] = ca_df[&#34;manual_target_consensus&#34;].str.lower()
            ca_df[&#34;track_type&#34;] = [&#34;raw&#34;] * len(ca_df)
            ca_df[&#34;id&#34;] = ca_df[&#34;Experimental-id&#34;]
            if &#34;plot_label&#34; not in ca_df.columns:
                ca_df[&#34;plot_label&#34;] = [None] * len(ca_df)

            ca_df = ca_df[columns_to_keep]
            to_concat.append(ca_df)

        if enc_pred_df is not None:
            enc_df = enc_pred_df.copy(deep=True)
            enc_df[&#34;source&#34;] = [&#34;encode&#34;] * len(enc_df)
            enc_df[ASSAY] = enc_df[ASSAY].str.lower()
            enc_df[&#34;track_type&#34;] = [&#34;pval&#34;] * len(enc_df)
            enc_df[&#34;id&#34;] = enc_df[&#34;FILE_accession&#34;]
            if &#34;plot_label&#34; not in enc_df.columns:
                enc_df[&#34;plot_label&#34;] = [None] * len(enc_df)

            enc_df = enc_df[columns_to_keep]
            to_concat.append(enc_df)

        if recount3_metadata is not None:
            recount3_df = recount3_metadata.copy(deep=True)
            recount3_df[&#34;source&#34;] = [&#34;recount3&#34;] * len(recount3_df)
            recount3_df[ASSAY] = recount3_df[&#34;harmonized_assay&#34;].str.lower()
            recount3_df[&#34;track_type&#34;] = [&#34;unique_raw&#34;] * len(recount3_df)
            recount3_df[&#34;id&#34;] = recount3_df[&#34;ID&#34;]
            if &#34;plot_label&#34; not in recount3_df.columns:
                recount3_df[&#34;plot_label&#34;] = [None] * len(recount3_df)

            recount3_df = recount3_df[columns_to_keep]
            to_concat.append(recount3_df)

        return pd.concat(to_concat)</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.join_metadata"><code class="name flex">
<span>def <span class="ident">join_metadata</span></span>(<span>df: pd.DataFrame, metadata: Metadata) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Join the metadata to the results dataframe.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.metadata_to_df"><code class="name flex">
<span>def <span class="ident">metadata_to_df</span></span>(<span>metadata: Metadata, merge_assays: bool = True) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Convert the metadata to a dataframe.</p>
<p>merge_assays: Merge similar assays (rna 2x / wgb 2x)</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.uniformize_metadata_for_plotting"><code class="name flex">
<span>def <span class="ident">uniformize_metadata_for_plotting</span></span>(<span>epiatlas_metadata: Metadata, ca_pred_df: pd.DataFrame | None = None, enc_pred_df: pd.DataFrame | None = None, recount3_metadata: pd.DataFrame | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Simplify metadata with chip-atlas metadata for plotting.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.load_any_metadata"><code class="name flex">
<span>def <span class="ident">load_any_metadata</span></span>(<span>self, path: Path | str, as_dataframe: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Return metadata for a specific file.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.load_metadata"><code class="name flex">
<span>def <span class="ident">load_metadata</span></span>(<span>self, version: str) ‑> <a title="epiclass.core.metadata.Metadata" href="../../../core/metadata.html#epiclass.core.metadata.Metadata">Metadata</a></span>
</code></dt>
<dd>
<div class="desc"><p>Return metadata for a specific version.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>version</code></strong> :&ensp;<code>str</code></dt>
<dd>The version of the metadata to load.</dd>
</dl>
<p>One of 'v1', 'v2', 'v2-encode'.
Example of epiRR unique to v1: IHECRE00003355.2</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.load_metadata_df"><code class="name flex">
<span>def <span class="ident">load_metadata_df</span></span>(<span>self, version: str, merge_assays: bool = True) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Load a metadata dataframe for a given version.</p>
<p>merge_assays: Merge similar assays (rna 2x / wgb 2x)</p></div>
</dd>
</dl>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler"><code class="flex name class">
<span>class <span class="ident">SplitResultsHandler</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class to handle split results.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SplitResultsHandler:
    &#34;&#34;&#34;Class to handle split results.&#34;&#34;&#34;

    @staticmethod
    def add_max_pred(
        df: pd.DataFrame,
        target_label: str = &#34;True class&#34;,
        expected_classes: List[str] | None = None,
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Add the max prediction column (&#34;Max pred&#34;) to the results dataframe.

        The dataframe needs to not contain extra metadata columns.
        target_label: Column to ascertain output classes columns
        &#34;&#34;&#34;
        if &#34;Max pred&#34; not in df.columns:
            df = df.copy(deep=True)
            classes_test = (
                df[target_label].astype(str).unique().tolist()
                + df[&#34;Predicted class&#34;].astype(str).unique().tolist()
            )
            classes_test = list(set(classes_test))

            if expected_classes:
                classes_test = expected_classes

            classes = list(df.columns[2:])
            if any(label in classes for label in [&#34;TRUE&#34;, &#34;FALSE&#34;]):
                print(
                    &#34;WARNING: Found TRUE or FALSE in pred vector columns. Changing column names.&#34;
                )
                df.rename(columns={&#34;TRUE&#34;: &#34;True&#34;, &#34;FALSE&#34;: &#34;False&#34;}, inplace=True)
                classes = list(df.columns[2:])

            for col in [&#34;md5sum&#34;, &#34;split&#34;, &#34;Same?&#34;, &#34;Predicted class&#34;, &#34;True class&#34;]:
                try:
                    classes.remove(col)
                except ValueError:
                    pass
            for class_label in classes:
                if class_label not in classes_test:
                    raise ValueError(
                        f&#34;&#34;&#34;Dataframe contains extra metadata columns, cannot ascertain classes: {classes}.
                        Column &#39;{class_label}&#39; is not in &#39;{classes_test}&#39;&#34;&#34;&#34;
                    )
            df[&#34;Max pred&#34;] = df[classes].max(axis=1)
        return df

    @staticmethod
    def verify_md5_consistency(dfs: Dict[str, pd.DataFrame]) -&gt; None:
        &#34;&#34;&#34;Verify that all dataframes have the same md5sums.

        Used to compare the md5sums of the same split across different classifiers methods.

        Args:
            dfs (Dict[str, pd.DataFrame]): {classifier_name: results_df}
                Results dataframes need to have md5sums as index.
        &#34;&#34;&#34;
        md5s = {}
        for key, df in dfs.items():
            md5s[key] = set(df.index)

        first_key = list(dfs.keys())[0]
        base_md5s = set(dfs[first_key].index)
        if not base_md5s.intersection(*list(md5s.values())) == base_md5s:
            raise AssertionError(&#34;Not all dataframes have the same md5sums&#34;)

    @staticmethod
    def compute_acc_per_assay(
        split_results: Dict[str, pd.DataFrame], metadata_df: pd.DataFrame
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Compute accuracy per assay for each split.

        Args:
        - split_results: {split_name: results_df}.
        - metadata_df: The metadata dataframe.
        &#34;&#34;&#34;

        assay_acc = defaultdict(dict)
        for split_name, split_result_df in split_results.items():
            # Merge metadata
            split_result_df = split_result_df.merge(
                metadata_df, left_index=True, right_index=True
            )

            # Compute accuracy per assay
            assay_groupby = split_result_df.groupby(ASSAY)
            for assay, assay_df in assay_groupby:
                assay_acc[assay][split_name] = np.mean(
                    assay_df[&#34;True class&#34;].astype(str).str.lower()
                    == assay_df[&#34;Predicted class&#34;].astype(str).str.lower()
                )

        return pd.DataFrame(assay_acc)

    @staticmethod
    def gather_split_results_across_methods(
        results_dir: Path, label_category: str, only_NN: bool = False
    ) -&gt; Dict[str, Dict[str, pd.DataFrame]]:
        &#34;&#34;&#34;Gather split results for each classifier type.

        Args:
            results_dir: The directory containing the results. Child directories should be task/classifer names.
            label_category: The label category for the results.
            only_NN: A boolean flag to only gather the NN results.

        Returns:
            Dict[str, Dict[str, pd.DataFrame]]: {split_name:{classifier_name: results_df}}
        &#34;&#34;&#34;
        base_NN_path = results_dir / f&#34;{label_category}_1l_3000n&#34;
        if label_category == ASSAY:
            NN_csv_path_template = base_NN_path / &#34;11c&#34;
            if not NN_csv_path_template.exists():
                NN_csv_path_template = base_NN_path
        elif label_category == CELL_TYPE:
            NN_csv_path_template = base_NN_path
        else:
            raise ValueError(&#34;Label category not supported.&#34;)

        NN_csv_path_template = str(
            NN_csv_path_template
            / &#34;10fold-oversampling&#34;
            / &#34;{split}&#34;
            / &#34;validation_prediction.csv&#34;
        )
        all_split_dfs = {}
        for split in [f&#34;split{i}&#34; for i in range(10)]:
            # Get the csv paths
            NN_csv_path = Path(NN_csv_path_template.format(split=split))  # type: ignore
            other_csv_root = results_dir / f&#34;{label_category}&#34; / &#34;predict-10fold&#34;

            if not only_NN:
                if not other_csv_root.exists():
                    raise FileNotFoundError(f&#34;Could not find {other_csv_root}&#34;)
                other_csv_paths = other_csv_root.glob(
                    f&#34;*/*_{split}_validation_prediction.csv&#34;
                )

                other_csv_paths = list(other_csv_paths)
                if len(other_csv_paths) != 4:
                    raise AssertionError(
                        f&#34;Expected 4 other_csv_paths, got {len(other_csv_paths)}&#34;
                    )

            # Load the dataframes
            dfs: Dict[str, pd.DataFrame] = {}
            dfs[&#34;NN&#34;] = pd.read_csv(NN_csv_path, header=0, index_col=0, low_memory=False)

            if not only_NN:
                for path in other_csv_paths:
                    category = path.name.split(&#34;_&#34;, maxsplit=1)[0]
                    dfs[category] = pd.read_csv(
                        path, header=0, index_col=0, low_memory=False
                    )

                # Verify md5sum consistency
                SplitResultsHandler.verify_md5_consistency(dfs)

            all_split_dfs[split] = dfs

        return all_split_dfs

    @staticmethod
    def read_split_results(parent_dir: Path) -&gt; Dict[str, pd.DataFrame]:
        &#34;&#34;&#34;Read split results from the given parent directory.

        Args:
            parent_dir: The parent directory containing the split results.

        Returns:
            Dict[str, pd.DataFrame]: {split_name: results_df}
        &#34;&#34;&#34;
        csv_path_template = &#34;split*/validation_prediction.csv&#34;
        experiment_dict = {}
        for split_result_csv in parent_dir.glob(csv_path_template):
            split_name = split_result_csv.parent.name
            df = pd.read_csv(split_result_csv, header=0, index_col=0, low_memory=False)
            experiment_dict[split_name] = df
        return experiment_dict

    @staticmethod
    def gather_split_results_across_categories(
        parent_results_dir: Path, verbose: bool = False
    ) -&gt; Dict[str, Dict[str, pd.DataFrame]]:
        &#34;&#34;&#34;Gather NN split results for each classification task in the given folder children.

        Returns:
            Dict[str, Dict[str, pd.DataFrame]]: {general_name:{split_name: results_df}}
        &#34;&#34;&#34;
        all_dfs = {}

        for category_dir in parent_results_dir.iterdir():
            if not category_dir.is_dir():
                continue
            for experiment_dir in category_dir.iterdir():
                if not experiment_dir.is_dir():
                    continue
                experiment_name = experiment_dir.name
                category_name = category_dir.name
                general_name = f&#34;{category_name}_{experiment_name}&#34;
                if verbose:
                    print(f&#34;Reading {general_name} from {experiment_dir}&#34;)

                experiment_dict = SplitResultsHandler.read_split_results(experiment_dir)
                if verbose:
                    print(
                        f&#34;Found {len(experiment_dict)} split results for {general_name}&#34;
                    )

                all_dfs[general_name] = experiment_dict

        return all_dfs

    @staticmethod
    def concatenate_split_results(
        split_dfs: Dict[str, Dict[str, pd.DataFrame]] | Dict[str, pd.DataFrame],
        concat_first_level: bool = False,
        depth: int = 2,
    ) -&gt; Dict[str, pd.DataFrame] | pd.DataFrame:
        &#34;&#34;&#34;Concatenate split results for each different classifier or split name based on the order.

        This method supports two structures of input dictionaries:
        1. {split_name: {classifier_name: results_df}} when concat_first_level is False.
        2. {classifier_name: {split_name: results_df}} when concat_first_level is True.

        Args:
            split_dfs: A nested dictionary of DataFrames to be concatenated. The structure
                       depends on the concat_first_level flag.
            concat_first_level: A boolean flag that indicates the structure of the split_dfs dictionary.
                                If True, the first level is the classifier name. Otherwise, the first
                                level is the split name.
            depth: The depth of the dictionary structure. Must be 1 or 2.

        Returns:
            Dict[str, pd.DataFrame] : {classifier_name: concatenated_dataframe}

        Raises:
            AssertionError: If the index of any concatenated DataFrame is not of type str, indicating
                            an unexpected index format.
        &#34;&#34;&#34;
        if depth not in [1, 2]:
            raise ValueError(f&#34;Depth must be 1 or 2, got {depth}&#34;)

        # Handle basic case, code wasn&#39;t made for that, so a hack is done
        if depth == 1:
            to_concat = {&#34;classifier&#34;: split_dfs}
            return SplitResultsHandler.concatenate_split_results(to_concat, concat_first_level=True, depth=2)[&#34;classifier&#34;]  # type: ignore

        # Check for user error
        if depth == 2 and isinstance(next(iter(split_dfs.values())), pd.DataFrame):
            raise ValueError(
                &#34;Depth given is two, but the input is not a nested dictionary.&#34;
            )

        if concat_first_level:
            # Reverse the nesting of the dictionary if concatenating by the first level.
            reversed_dfs = defaultdict(dict)
            for outer_key, inner_dict in split_dfs.items():
                for inner_key, df in inner_dict.items():
                    reversed_dfs[inner_key][outer_key] = df
            split_dfs = reversed_dfs

        to_concat_dfs = defaultdict(list)
        for split_name, dfs in split_dfs.items():
            for classifier, df in dfs.items():
                try:
                    df = df.assign(split=int(split_name.split(&#34;split&#34;)[-1]))
                except ValueError as e:
                    if &#34;base 10&#34; in str(e):
                        raise ValueError(
                            &#34;Wrong concat_first_level value was probably used&#34;
                        ) from e
                    raise e
                to_concat_dfs[classifier].append(df)

        concatenated_dfs = {
            classifier: pd.concat(dfs, axis=0)
            for classifier, dfs in to_concat_dfs.items()
        }

        # Verify index is still md5sum
        for df in concatenated_dfs.values():
            if not isinstance(df.index[0], str):
                raise AssertionError(&#34;Index is not md5sum&#34;)
            df[&#34;md5sum&#34;] = df.index
            df.index.name = &#34;md5sum&#34;

        return concatenated_dfs

    @staticmethod
    def calculate_metrics_for_single_df(
        df: pd.DataFrame,
        task_name: str = &#34;unknown task&#34;,
        logging_name: str = &#34;df&#34;,
    ) -&gt; Dict[str, float]:
        &#34;&#34;&#34;Compute metrics for a single task&#39;s DataFrame.

        Expects a DataFrame with the following columns:
            md5sum, True class, predicted class, predicted probabilities

        Output is a dictionary of metrics with the following the following keys:
            Accuracy, F1_macro, AUC_micro, AUC_macro, count
        &#34;&#34;&#34;
        df = df.copy()

        # Ensure &#39;True class&#39; labels match (e.g. int or bool class labels)
        df[&#34;True class&#34;] = df[&#34;True class&#34;].astype(str).str.lower()
        df.columns = list(df.columns[:2]) + [
            label.lower() for i, label in enumerate(df.columns.str.lower()) if i &gt; 1
        ]
        # One hot encode true class and get predicted probabilities
        # Reindex to ensure that the order of classes is consistent
        classes_order = df.columns[2:]
        onehot_true = (
            pd.get_dummies(df[&#34;True class&#34;], dtype=int)
            .reindex(columns=classes_order, fill_value=0)
            .values
        )

        pred_probs = df[classes_order].values

        # Argmax encodings for accuracy/F1
        ravel_true = np.argmax(onehot_true, axis=1)
        ravel_pred = np.argmax(pred_probs, axis=1)

        # fmt: off
        # Core metrics
        task_metrics_dict: Dict[str, float|int] = {
            &#34;Accuracy&#34;: accuracy_score(ravel_true, ravel_pred),
            &#34;F1_macro&#34;: f1_score(ravel_true, ravel_pred, average=&#34;macro&#34;, zero_division=&#34;warn&#34;), # type: ignore
            &#34;count&#34;: len(df),
            }

        # --- AUC Handling ---
        unique_classes = np.unique(ravel_true)
        if unique_classes.shape[0] &lt; 2:
            # Not enough classes -&gt;- AUC undefined
            logging.warning(
                &#34;Cannot compute ROC AUC: only one class present in %s for %s (%s)&#34;,
                logging_name,
                task_name,
                df[&#34;True class&#34;].unique(),
            )
            task_metrics_dict.update({&#34;AUC_micro&#34;: np.nan, &#34;AUC_macro&#34;: np.nan})
            return task_metrics_dict

        # Try to compute AUC safely
        try:
            # Suppress sklearn warnings explicitly inside block
            with warnings.catch_warnings():
                warnings.filterwarnings(&#34;ignore&#34;, category=UndefinedMetricWarning)
                task_metrics_dict.update(
                    {
                        &#34;AUC_micro&#34;: roc_auc_score(
                            onehot_true, pred_probs, multi_class=&#34;ovr&#34;, average=&#34;micro&#34; # type: ignore
                        ),
                        &#34;AUC_macro&#34;: roc_auc_score(
                            onehot_true, pred_probs, multi_class=&#34;ovr&#34;, average=&#34;macro&#34;
                        ),
                    }
                )
        except ValueError as err:
            if &#34;multiclass&#34; in str(err) or &#34;Only one class&#34; in str(err):
                logging.warning(
                    &#34;Cannot compute ROC AUC for %s/%s: %s&#34;,
                    logging_name,
                    task_name,
                    err,
                )
                task_metrics_dict.update({&#34;AUC_micro&#34;: np.nan, &#34;AUC_macro&#34;: np.nan})
            else:
                err_msg = f&#34;Unexpected AUC error in {logging_name} for {task_name}.&#34;
                logging.error(err_msg)
                logging.debug(&#34;columns: %s&#34;, df.columns)
                logging.debug(&#34;True class values: %s\n&#34;, df[&#34;True class&#34;].value_counts())
                raise ValueError(err_msg) from err

        return task_metrics_dict

    @staticmethod
    def compute_split_metrics(
        all_split_dfs: Dict[str, Dict[str, pd.DataFrame]],
        concat_first_level: bool = False,
    ) -&gt; Dict[str, Dict[str, Dict[str, float]]]:
        &#34;&#34;&#34;Compute desired metrics for each split and classifier, accommodating different dictionary structures.

        This method supports two structures of input dictionaries:
        1. {split_name: {classifier_name: results_df}} when concat_first_level is False.
        2. {classifier_name: {split_name: results_df}} when concat_first_level is True.

        Args:
            all_split_dfs: A nested dictionary of DataFrames. The structure
                           depends on the concat_first_level flag. (Docstring corrected to use all_split_dfs)
            concat_first_level: A boolean flag that indicates the structure of the all_split_dfs dictionary.
                                If True, the first level is the classifier name. Otherwise, the first
                                level is the split name.

        Returns:
            A nested dictionary with metrics computed for each classifier and split. The structure is
            {split_name: {classifier_name: {metric_name: value}}}.
        &#34;&#34;&#34;
        all_split_dfs = copy.deepcopy(all_split_dfs)
        split_metrics: Dict[str, Dict[str, Dict[str, float]]] = {}

        # Reorganize the dictionary to always work with {split_name: {classifier_name: DataFrame}}
        if concat_first_level:
            temp_dict: Dict[str, Dict[str, pd.DataFrame]] = {}
            for classifier, splits in all_split_dfs.items():
                for split_val, df_val in splits.items():
                    if split_val not in temp_dict:
                        temp_dict[split_val] = {}
                    temp_dict[split_val][classifier] = df_val

            all_split_dfs = temp_dict

        for split in [f&#34;split{i}&#34; for i in range(10)]:
            dfs = all_split_dfs[split]

            metrics: Dict[str, Dict[str, float]] = {}

            for task_name, df in dfs.items():
                metrics[
                    task_name
                ] = SplitResultsHandler().calculate_metrics_for_single_df(
                    df=df, task_name=task_name, logging_name=split
                )
                # --- END OF REFACTORED PART ---

            split_metrics[split] = metrics

        return split_metrics

    @staticmethod
    def invert_metrics_dict(
        metrics: Dict[str, Dict[str, Dict[str, float]]]
    ) -&gt; Dict[str, Dict[str, Dict[str, float]]]:
        &#34;&#34;&#34;Invert metrics dict so classifier name is parent key.
        Before: {split_name:{classifier_name:{metric_name:val}}}
        After: {classifier_name:{split_name:{metric_name:val}}}
        &#34;&#34;&#34;
        # Check for correct input
        first_keys = list(metrics.keys())
        if not all(&#34;split&#34; in key for key in first_keys):
            raise ValueError(&#34;Wrong input dict: first level keys don&#39;t contain &#39;split&#39;.&#34;)

        # Invert
        new_metrics = defaultdict(dict)
        for split_name, split_metrics in metrics.items():
            for classifier_name, classifier_metrics in split_metrics.items():
                new_metrics[classifier_name][split_name] = classifier_metrics
        return dict(new_metrics)

    @staticmethod
    def extract_count_from_metrics(metrics) -&gt; Dict[str, int]:
        &#34;&#34;&#34;Extract total file count from metrics dict (sums on each split).

        Returns a dict {classifier_name: count}
        &#34;&#34;&#34;
        new_metrics = copy.deepcopy(metrics)

        # Check for correct input
        first_keys = list(new_metrics.keys())
        if all(&#34;split&#34; in key for key in first_keys):
            new_metrics = SplitResultsHandler.invert_metrics_dict(new_metrics)

        counts = defaultdict(int)
        for classifier_name, all_split_metrics in new_metrics.items():
            for _, split_metrics in all_split_metrics.items():
                counts[classifier_name] += split_metrics[&#34;count&#34;]  # type: ignore

        return dict(counts)

    @staticmethod
    def general_split_metrics(
        results_dir: Path,
        merge_assays: bool,
        exclude_categories: List[str] | None = None,
        exclude_names: List[str] | None = None,
        include_categories: List[str] | None = None,
        include_names: List[str] | None = None,
        return_type: str = &#34;both&#34;,
        mislabel_corrections: Tuple[Dict[str, str], Dict[str, Dict[str, str]]]
        | None = None,
        oversampled_only: bool | None = True,
        verbose: bool | None = False,
        min_pred_score: float | None = None,
    ) -&gt; (
        Dict[str, Dict[str, Dict[str, float]]]
        | Dict[str, Dict[str, pd.DataFrame]]
        | Tuple[
            Dict[str, Dict[str, Dict[str, float]]], Dict[str, Dict[str, pd.DataFrame]]
        ]
    ):
        &#34;&#34;&#34;Create the content data for figure 2a. (get metrics for each task)

        Args:
            results_dir (Path): Directory containing the results. Needs to be parent over category folders.
            merge_assays (bool): Merge similar assays (rna-seq x2, wgbs x2)
            exclude_categories (List[str]): Task categories to exclude (first level directory names).
            exclude_names (List[str]): Names of folders to exclude (ex: 7c or no-mix).
            include_categories (List[str]): Task categories to include (first level directory names).
            include_names (List[str]): Names of folders to include (ex: 7c or no-mix).
            return_type (str): Type of data to return (&#39;metrics&#39;, &#39;split_results&#39;, &#39;both&#39;).
            mislabel_corrections (Tuple[Dict[str, str], Dict[str, Dict[str, str]]]): ({md5sum: EpiRR_no-v},{label_category: {EpiRR_no-v: corrected_label}})
            oversampled_only (bool): Only include oversampled runs.
            verbose (bool): Print additional information.
            min_pred_score (float): Minimum prediction score to consider. Affects the metrics. Defaults to None.

        Returns:
            Union[Dict[str, Dict[str, Dict[str, float]]],
                Dict[str, Dict[str, pd.DataFrame]],
                Tuple[Dict[str, Dict[str, Dict[str, float]]], Dict[str, Dict[str, pd.DataFrame]]]]
                Depending on return_type, it returns:
                - &#39;metrics&#39;: A metrics dictionary with the structure {split_name: {task_name: metrics_dict}}
                - &#39;split_results&#39;: A split results dictionary with the structure {task_name: {split_name: split_results_df}}
                - &#39;both&#39;: A tuple with both dictionaries described above
        &#34;&#34;&#34;
        if return_type not in [&#34;metrics&#34;, &#34;split_results&#34;, &#34;both&#34;]:
            raise ValueError(
                f&#34;Invalid return_type: {return_type}. Choose from &#39;metrics&#39;, &#39;split_results&#39;, or &#39;both&#39;.&#34;
            )

        all_split_results = {}
        split_results_handler = SplitResultsHandler()

        if mislabel_corrections:
            md5sum_to_epirr, epirr_to_corrections = mislabel_corrections
        else:
            md5sum_to_epirr = {}
            epirr_to_corrections = {}

        for parent, _, _ in os.walk(results_dir, followlinks=True):
            # Looking for oversampling only results
            parent = Path(parent)
            if &#34;10fold&#34; not in parent.name:
                if verbose:
                    print(f&#34;Skipping {parent}: not 10fold&#34;)
                continue
            if parent.name != &#34;10fold-oversampling&#34; and oversampled_only:
                if verbose:
                    print(f&#34;Skipping {parent}: not oversampled&#34;)
                continue

            if verbose:
                print(f&#34;Checking {parent}&#34;)

            # Get the category + filter
            relpath = parent.relative_to(results_dir)
            category = relpath.parts[0].replace(&#34;_1l_3000n&#34;, &#34;&#34;)
            if verbose:
                print(f&#34;Checking category: {category}&#34;)
            if include_categories is not None:
                if not any(include_str in category for include_str in include_categories):
                    if verbose:
                        print(f&#34;Skipping {category}: not in {include_categories}&#34;)
                    continue
            if exclude_categories is not None:
                if any(exclude_str in category for exclude_str in exclude_categories):
                    if verbose:
                        print(f&#34;Skipping {category}: in {exclude_categories}&#34;)
                    continue

            # Get the rest of the name, ignore certain runs
            rest_of_name = list(relpath.parts[1:])
            for dir_name in [&#34;10fold&#34;, &#34;10fold-oversampling&#34;]:
                try:
                    rest_of_name.remove(dir_name)
                except ValueError:
                    pass

            if len(rest_of_name) &gt; 1:
                raise ValueError(
                    f&#34;Too many parts in the name: {rest_of_name}. Path: {relpath}&#34;
                )
            if rest_of_name:
                rest_of_name = rest_of_name[0]
            if verbose:
                print(f&#34;Rest of name: {rest_of_name}&#34;)

            # Filter out certain runs
            if include_names is not None:
                if not any(name in rest_of_name for name in include_names):
                    if verbose:
                        print(
                            f&#34;Skipping {category} {rest_of_name}: not in {include_names}&#34;
                        )
                    continue
            if exclude_names is not None:
                if any(name in rest_of_name for name in exclude_names):
                    if verbose:
                        print(f&#34;Skipping {category} {rest_of_name}: in {exclude_names}&#34;)
                    continue

            full_task_name = category
            if rest_of_name:
                full_task_name += f&#34;_{rest_of_name}&#34;

            # Get the split results
            if verbose:
                print(f&#34;Getting split results for {full_task_name}&#34;)
            split_results = split_results_handler.read_split_results(parent)
            if not split_results:
                raise ValueError(f&#34;No split results found in {parent}&#34;)

            if (
                (&#34;sex&#34; in full_task_name) or (&#34;life_stage&#34; in full_task_name)
            ) and mislabel_corrections:
                corrections = epirr_to_corrections[category]
                for split_name, results_df in list(split_results.items()):
                    current_true_class = results_df[&#34;True class&#34;].to_dict()
                    new_true_class = {
                        k: corrections.get(md5sum_to_epirr[k], v)
                        for k, v in current_true_class.items()
                    }
                    results_df[&#34;True class&#34;] = new_true_class.values()

                    split_results[split_name] = results_df

            if (&#34;assay&#34; in full_task_name) and merge_assays:
                for split_name, df in split_results.items():
                    try:
                        split_result_df = merge_similar_assays(df)
                    except ValueError as e:
                        print(f&#34;Skipping {full_task_name} assay merging: {e}&#34;)
                        break
                    split_results[split_name] = split_result_df

            if min_pred_score:
                for split_name, df in split_results.items():
                    tmp_df = df.copy()
                    tmp_df = SplitResultsHandler.add_max_pred(tmp_df)
                    tmp_df = tmp_df[tmp_df[&#34;Max pred&#34;] &gt;= min_pred_score]
                    tmp_df = tmp_df.drop(columns=[&#34;Max pred&#34;])
                    split_results[split_name] = tmp_df

            all_split_results[full_task_name] = split_results

        if return_type in [&#34;metrics&#34;, &#34;both&#34;]:
            try:
                split_results_metrics = split_results_handler.compute_split_metrics(
                    all_split_results, concat_first_level=True
                )
            except KeyError as e:
                logging.error(&#34;KeyError: %s&#34;, e)
                logging.error(&#34;all_split_results: %s&#34;, all_split_results)
                logging.error(&#34;check folder: %s&#34;, results_dir)
                raise e

        if return_type == &#34;metrics&#34;:
            # pylint: disable=possibly-used-before-assignment
            return split_results_metrics
        if return_type == &#34;split_results&#34;:
            return all_split_results

        # the default return type is &#39;both&#39;
        return split_results_metrics, all_split_results

    def obtain_all_feature_set_data(
        self,
        parent_folder: Path,
        merge_assays: bool,
        return_type: str,
        include_sets: List[str] | None = None,
        include_categories: List[str] | None = None,
        exclude_names: List[str] | None = None,
        oversampled_only: bool | None = True,
        verbose: bool | None = False,
    ) -&gt; (
        Dict[str, Dict[str, Dict[str, Dict[str, float]]]]
        | Dict[str, Dict[str, Dict[str, pd.DataFrame]]]
    ):
        &#34;&#34;&#34;
        Obtain either metrics or split results for all feature sets based on the specified return_type.

        Args:
            parent_folder (Path): The parent folder containing all feature set folders.
                Needs to be the parent of feature set folders.
            merge_assays (bool): Whether to merge similar assays (e.g., RNA-seq x2, WGBS x2).
            return_type (str): Type of data to return, either &#34;metrics&#34; or &#34;split_results&#34;.
            include_sets (List[str] | None): Feature sets to include.
            include_categories (List[str] | None): Task categories to include.
            exclude_names (List[str] | None): Names of folders to exclude (e.g., 7c or no-mix).
            oversampled_only (bool): Only include oversampled runs.
            verbose (bool): Print additional information.

        Returns:
            Union[Dict[str, Dict[str, Dict[str, Dict[str, float]]]], Dict[str, Dict[str, Dict[str, pd.DataFrame]]]]:
            A dictionary containing either metrics or results for all feature sets.
            Format for metrics: {feature_set: {task_name: {split_name: metric_dict}}}
            Format for split results: {feature_set: {task_name: {split_name: results_dataframe}}}
        &#34;&#34;&#34;
        valid_return_types = [&#34;metrics&#34;, &#34;split_results&#34;]
        if return_type not in valid_return_types:
            raise ValueError(
                f&#34;Invalid return_type: {return_type}. Choose from {valid_return_types}.&#34;
            )

        all_data = {}
        if verbose:
            print(f&#34;Checking folder: {parent_folder}&#34;)
        for folder in parent_folder.iterdir():
            if not folder.is_dir():
                continue

            feature_set = folder.name
            if include_sets is not None and feature_set not in include_sets:
                if verbose:
                    print(f&#34;Skipping {feature_set}: not in &#39;include_sets&#39; list.&#34;)
                continue

            try:
                # Fetch either metrics or split results based on the return_type
                split_data = self.general_split_metrics(
                    results_dir=folder,
                    merge_assays=merge_assays,
                    return_type=return_type,
                    include_categories=include_categories,
                    exclude_names=exclude_names,
                    oversampled_only=oversampled_only,
                    verbose=verbose,
                )

                # Invert if metrics, otherwise keep as is
                if return_type == &#34;metrics&#34;:
                    inverted_data = self.invert_metrics_dict(split_data)  # type: ignore
                    all_data[feature_set] = inverted_data
                elif return_type == &#34;split_results&#34;:
                    all_data[feature_set] = split_data  # type: ignore

            except ValueError as err:
                raise ValueError(f&#34;Problem with {feature_set}&#34;) from err

        return all_data</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.add_max_pred"><code class="name flex">
<span>def <span class="ident">add_max_pred</span></span>(<span>df: pd.DataFrame, target_label: str = 'True class', expected_classes: List[str] | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Add the max prediction column ("Max pred") to the results dataframe.</p>
<p>The dataframe needs to not contain extra metadata columns.
target_label: Column to ascertain output classes columns</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.calculate_metrics_for_single_df"><code class="name flex">
<span>def <span class="ident">calculate_metrics_for_single_df</span></span>(<span>df: pd.DataFrame, task_name: str = 'unknown task', logging_name: str = 'df') ‑> Dict[str, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Compute metrics for a single task's DataFrame.</p>
<p>Expects a DataFrame with the following columns:
md5sum, True class, predicted class, predicted probabilities</p>
<p>Output is a dictionary of metrics with the following the following keys:
Accuracy, F1_macro, AUC_micro, AUC_macro, count</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.compute_acc_per_assay"><code class="name flex">
<span>def <span class="ident">compute_acc_per_assay</span></span>(<span>split_results: Dict[str, pd.DataFrame], metadata_df: pd.DataFrame) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Compute accuracy per assay for each split.</p>
<p>Args:
- split_results: {split_name: results_df}.
- metadata_df: The metadata dataframe.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.compute_split_metrics"><code class="name flex">
<span>def <span class="ident">compute_split_metrics</span></span>(<span>all_split_dfs: Dict[str, Dict[str, pd.DataFrame]], concat_first_level: bool = False) ‑> Dict[str, Dict[str, Dict[str, float]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Compute desired metrics for each split and classifier, accommodating different dictionary structures.</p>
<p>This method supports two structures of input dictionaries:
1. {split_name: {classifier_name: results_df}} when concat_first_level is False.
2. {classifier_name: {split_name: results_df}} when concat_first_level is True.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>all_split_dfs</code></strong></dt>
<dd>A nested dictionary of DataFrames. The structure
depends on the concat_first_level flag. (Docstring corrected to use all_split_dfs)</dd>
<dt><strong><code>concat_first_level</code></strong></dt>
<dd>A boolean flag that indicates the structure of the all_split_dfs dictionary.
If True, the first level is the classifier name. Otherwise, the first
level is the split name.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A nested dictionary with metrics computed for each classifier and split. The structure is
{split_name: {classifier_name: {metric_name: value}}}.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.concatenate_split_results"><code class="name flex">
<span>def <span class="ident">concatenate_split_results</span></span>(<span>split_dfs: Dict[str, Dict[str, pd.DataFrame]] | Dict[str, pd.DataFrame], concat_first_level: bool = False, depth: int = 2)</span>
</code></dt>
<dd>
<div class="desc"><p>Concatenate split results for each different classifier or split name based on the order.</p>
<p>This method supports two structures of input dictionaries:
1. {split_name: {classifier_name: results_df}} when concat_first_level is False.
2. {classifier_name: {split_name: results_df}} when concat_first_level is True.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>split_dfs</code></strong></dt>
<dd>A nested dictionary of DataFrames to be concatenated. The structure
depends on the concat_first_level flag.</dd>
<dt><strong><code>concat_first_level</code></strong></dt>
<dd>A boolean flag that indicates the structure of the split_dfs dictionary.
If True, the first level is the classifier name. Otherwise, the first
level is the split name.</dd>
<dt><strong><code>depth</code></strong></dt>
<dd>The depth of the dictionary structure. Must be 1 or 2.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, pd.DataFrame] </code></dt>
<dd>{classifier_name: concatenated_dataframe}</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AssertionError</code></dt>
<dd>If the index of any concatenated DataFrame is not of type str, indicating
an unexpected index format.</dd>
</dl></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.extract_count_from_metrics"><code class="name flex">
<span>def <span class="ident">extract_count_from_metrics</span></span>(<span>metrics) ‑> Dict[str, int]</span>
</code></dt>
<dd>
<div class="desc"><p>Extract total file count from metrics dict (sums on each split).</p>
<p>Returns a dict {classifier_name: count}</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.gather_split_results_across_categories"><code class="name flex">
<span>def <span class="ident">gather_split_results_across_categories</span></span>(<span>parent_results_dir: Path, verbose: bool = False) ‑> Dict[str, Dict[str, pandas.core.frame.DataFrame]]</span>
</code></dt>
<dd>
<div class="desc"><p>Gather NN split results for each classification task in the given folder children.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, Dict[str, pd.DataFrame]]</code></dt>
<dd>{general_name:{split_name: results_df}}</dd>
</dl></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.gather_split_results_across_methods"><code class="name flex">
<span>def <span class="ident">gather_split_results_across_methods</span></span>(<span>results_dir: Path, label_category: str, only_NN: bool = False) ‑> Dict[str, Dict[str, pandas.core.frame.DataFrame]]</span>
</code></dt>
<dd>
<div class="desc"><p>Gather split results for each classifier type.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>results_dir</code></strong></dt>
<dd>The directory containing the results. Child directories should be task/classifer names.</dd>
<dt><strong><code>label_category</code></strong></dt>
<dd>The label category for the results.</dd>
<dt><strong><code>only_NN</code></strong></dt>
<dd>A boolean flag to only gather the NN results.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, Dict[str, pd.DataFrame]]</code></dt>
<dd>{split_name:{classifier_name: results_df}}</dd>
</dl></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.general_split_metrics"><code class="name flex">
<span>def <span class="ident">general_split_metrics</span></span>(<span>results_dir: Path, merge_assays: bool, exclude_categories: List[str] | None = None, exclude_names: List[str] | None = None, include_categories: List[str] | None = None, include_names: List[str] | None = None, return_type: str = 'both', mislabel_corrections: Tuple[Dict[str, str], Dict[str, Dict[str, str]]] | None = None, oversampled_only: bool | None = True, verbose: bool | None = False, min_pred_score: float | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Create the content data for figure 2a. (get metrics for each task)</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>results_dir</code></strong> :&ensp;<code>Path</code></dt>
<dd>Directory containing the results. Needs to be parent over category folders.</dd>
<dt><strong><code>merge_assays</code></strong> :&ensp;<code>bool</code></dt>
<dd>Merge similar assays (rna-seq x2, wgbs x2)</dd>
<dt><strong><code>exclude_categories</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>Task categories to exclude (first level directory names).</dd>
<dt><strong><code>exclude_names</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>Names of folders to exclude (ex: 7c or no-mix).</dd>
<dt><strong><code>include_categories</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>Task categories to include (first level directory names).</dd>
<dt><strong><code>include_names</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>Names of folders to include (ex: 7c or no-mix).</dd>
<dt><strong><code>return_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Type of data to return ('metrics', 'split_results', 'both').</dd>
<dt><strong><code>mislabel_corrections</code></strong> :&ensp;<code>Tuple[Dict[str, str], Dict[str, Dict[str, str]]]</code></dt>
<dd>({md5sum: EpiRR_no-v},{label_category: {EpiRR_no-v: corrected_label}})</dd>
<dt><strong><code>oversampled_only</code></strong> :&ensp;<code>bool</code></dt>
<dd>Only include oversampled runs.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Print additional information.</dd>
<dt><strong><code>min_pred_score</code></strong> :&ensp;<code>float</code></dt>
<dd>Minimum prediction score to consider. Affects the metrics. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Union[Dict[str, Dict[str, Dict[str, float]]],
Dict[str, Dict[str, pd.DataFrame]],
Tuple[Dict[str, Dict[str, Dict[str, float]]], Dict[str, Dict[str, pd.DataFrame]]]]
Depending on return_type, it returns:
- 'metrics': A metrics dictionary with the structure {split_name: {task_name: metrics_dict}}
- 'split_results': A split results dictionary with the structure {task_name: {split_name: split_results_df}}
- 'both': A tuple with both dictionaries described above</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.invert_metrics_dict"><code class="name flex">
<span>def <span class="ident">invert_metrics_dict</span></span>(<span>metrics: Dict[str, Dict[str, Dict[str, float]]]) ‑> Dict[str, Dict[str, Dict[str, float]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Invert metrics dict so classifier name is parent key.
Before: {split_name:{classifier_name:{metric_name:val}}}
After: {classifier_name:{split_name:{metric_name:val}}}</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.read_split_results"><code class="name flex">
<span>def <span class="ident">read_split_results</span></span>(<span>parent_dir: Path) ‑> Dict[str, pandas.core.frame.DataFrame]</span>
</code></dt>
<dd>
<div class="desc"><p>Read split results from the given parent directory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>parent_dir</code></strong></dt>
<dd>The parent directory containing the split results.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, pd.DataFrame]</code></dt>
<dd>{split_name: results_df}</dd>
</dl></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.verify_md5_consistency"><code class="name flex">
<span>def <span class="ident">verify_md5_consistency</span></span>(<span>dfs: Dict[str, pd.DataFrame]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Verify that all dataframes have the same md5sums.</p>
<p>Used to compare the md5sums of the same split across different classifiers methods.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dfs</code></strong> :&ensp;<code>Dict[str, pd.DataFrame]</code></dt>
<dd>{classifier_name: results_df}
Results dataframes need to have md5sums as index.</dd>
</dl></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.obtain_all_feature_set_data"><code class="name flex">
<span>def <span class="ident">obtain_all_feature_set_data</span></span>(<span>self, parent_folder: Path, merge_assays: bool, return_type: str, include_sets: List[str] | None = None, include_categories: List[str] | None = None, exclude_names: List[str] | None = None, oversampled_only: bool | None = True, verbose: bool | None = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Obtain either metrics or split results for all feature sets based on the specified return_type.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>parent_folder</code></strong> :&ensp;<code>Path</code></dt>
<dd>The parent folder containing all feature set folders.
Needs to be the parent of feature set folders.</dd>
<dt><strong><code>merge_assays</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to merge similar assays (e.g., RNA-seq x2, WGBS x2).</dd>
<dt><strong><code>return_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Type of data to return, either "metrics" or "split_results".</dd>
<dt>include_sets (List[str] | None): Feature sets to include.</dt>
<dt>include_categories (List[str] | None): Task categories to include.</dt>
<dt>exclude_names (List[str] | None): Names of folders to exclude (e.g., 7c or no-mix).</dt>
<dt><strong><code>oversampled_only</code></strong> :&ensp;<code>bool</code></dt>
<dd>Only include oversampled runs.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Print additional information.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt>Union[Dict[str, Dict[str, Dict[str, Dict[str, float]]]], Dict[str, Dict[str, Dict[str, pd.DataFrame]]]]:</dt>
<dt>A dictionary containing either metrics or results for all feature sets.</dt>
<dt><code>Format for metrics</code></dt>
<dd>{feature_set: {task_name: {split_name: metric_dict}}}</dd>
<dt><code>Format for split results</code></dt>
<dd>{feature_set: {task_name: {split_name: results_dataframe}}}</dd>
</dl></div>
</dd>
</dl>
</dd>
<dt id="epiclass.utils.notebooks.paper.paper_utilities.TemporaryLogFilter"><code class="flex name class">
<span>class <span class="ident">TemporaryLogFilter</span></span>
<span>(</span><span>filter_obj, logger=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Context manager for adding a filter to a logger.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TemporaryLogFilter:
    &#34;&#34;&#34;Context manager for adding a filter to a logger.&#34;&#34;&#34;

    def __init__(self, filter_obj, logger=None):
        self.logger = logger or logging.getLogger()
        self.filter = filter_obj

    def __enter__(self):
        self.logger.addFilter(self.filter)

    def __exit__(self, exc_type, exc_value, traceback):
        self.logger.removeFilter(self.filter)</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="epiclass.utils.notebooks.paper" href="index.html">epiclass.utils.notebooks.paper</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.add_second_highest_prediction" href="#epiclass.utils.notebooks.paper.paper_utilities.add_second_highest_prediction">add_second_highest_prediction</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.check_label_coherence" href="#epiclass.utils.notebooks.paper.paper_utilities.check_label_coherence">check_label_coherence</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.create_mislabel_corrector" href="#epiclass.utils.notebooks.paper.paper_utilities.create_mislabel_corrector">create_mislabel_corrector</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.display_perc" href="#epiclass.utils.notebooks.paper.paper_utilities.display_perc">display_perc</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.extract_data_from_files" href="#epiclass.utils.notebooks.paper.paper_utilities.extract_data_from_files">extract_data_from_files</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.extract_experiment_keys_from_output_files" href="#epiclass.utils.notebooks.paper.paper_utilities.extract_experiment_keys_from_output_files">extract_experiment_keys_from_output_files</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.extract_input_sizes_from_output_files" href="#epiclass.utils.notebooks.paper.paper_utilities.extract_input_sizes_from_output_files">extract_input_sizes_from_output_files</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.extract_node_jobs_from_error_files" href="#epiclass.utils.notebooks.paper.paper_utilities.extract_node_jobs_from_error_files">extract_node_jobs_from_error_files</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.filter_biomat_LS" href="#epiclass.utils.notebooks.paper.paper_utilities.filter_biomat_LS">filter_biomat_LS</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.find_target_recall" href="#epiclass.utils.notebooks.paper.paper_utilities.find_target_recall">find_target_recall</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.format_labels" href="#epiclass.utils.notebooks.paper.paper_utilities.format_labels">format_labels</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.merge_life_stages" href="#epiclass.utils.notebooks.paper.paper_utilities.merge_life_stages">merge_life_stages</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.merge_similar_assays" href="#epiclass.utils.notebooks.paper.paper_utilities.merge_similar_assays">merge_similar_assays</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.print_column_content" href="#epiclass.utils.notebooks.paper.paper_utilities.print_column_content">print_column_content</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.rename_columns" href="#epiclass.utils.notebooks.paper.paper_utilities.rename_columns">rename_columns</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.set_file_id" href="#epiclass.utils.notebooks.paper.paper_utilities.set_file_id">set_file_id</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap" href="#epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap">IHECColorMap</a></code></h4>
<ul class="">
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap.create_assay_color_map" href="#epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap.create_assay_color_map">create_assay_color_map</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap.create_cell_type_color_map" href="#epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap.create_cell_type_color_map">create_cell_type_color_map</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap.create_sex_color_map" href="#epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap.create_sex_color_map">create_sex_color_map</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap.get_IHEC_color_map" href="#epiclass.utils.notebooks.paper.paper_utilities.IHECColorMap.get_IHEC_color_map">get_IHEC_color_map</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler" href="#epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler">MetadataHandler</a></code></h4>
<ul class="">
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.join_metadata" href="#epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.join_metadata">join_metadata</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.load_any_metadata" href="#epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.load_any_metadata">load_any_metadata</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.load_metadata" href="#epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.load_metadata">load_metadata</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.load_metadata_df" href="#epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.load_metadata_df">load_metadata_df</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.metadata_to_df" href="#epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.metadata_to_df">metadata_to_df</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.uniformize_metadata_for_plotting" href="#epiclass.utils.notebooks.paper.paper_utilities.MetadataHandler.uniformize_metadata_for_plotting">uniformize_metadata_for_plotting</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler">SplitResultsHandler</a></code></h4>
<ul class="">
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.add_max_pred" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.add_max_pred">add_max_pred</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.calculate_metrics_for_single_df" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.calculate_metrics_for_single_df">calculate_metrics_for_single_df</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.compute_acc_per_assay" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.compute_acc_per_assay">compute_acc_per_assay</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.compute_split_metrics" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.compute_split_metrics">compute_split_metrics</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.concatenate_split_results" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.concatenate_split_results">concatenate_split_results</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.extract_count_from_metrics" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.extract_count_from_metrics">extract_count_from_metrics</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.gather_split_results_across_categories" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.gather_split_results_across_categories">gather_split_results_across_categories</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.gather_split_results_across_methods" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.gather_split_results_across_methods">gather_split_results_across_methods</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.general_split_metrics" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.general_split_metrics">general_split_metrics</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.invert_metrics_dict" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.invert_metrics_dict">invert_metrics_dict</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.obtain_all_feature_set_data" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.obtain_all_feature_set_data">obtain_all_feature_set_data</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.read_split_results" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.read_split_results">read_split_results</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.verify_md5_consistency" href="#epiclass.utils.notebooks.paper.paper_utilities.SplitResultsHandler.verify_md5_consistency">verify_md5_consistency</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="epiclass.utils.notebooks.paper.paper_utilities.TemporaryLogFilter" href="#epiclass.utils.notebooks.paper.paper_utilities.TemporaryLogFilter">TemporaryLogFilter</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.0</a>.</p>
</footer>
</body>
</html>
