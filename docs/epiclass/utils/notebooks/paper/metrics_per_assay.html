<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.0">
<title>epiclass.utils.notebooks.paper.metrics_per_assay API documentation</title>
<meta name="description" content="Module that defines class for computing metrics (acc, f1) per assay.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>epiclass.utils.notebooks.paper.metrics_per_assay</code></h1>
</header>
<section id="section-intro">
<p>Module that defines class for computing metrics (acc, f1) per assay.</p>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay"><code class="flex name class">
<span>class <span class="ident">MetricsPerAssay</span></span>
</code></dt>
<dd>
<div class="desc"><p>Class for computing metrics (acc, f1) per assay and saving the results.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MetricsPerAssay:
    &#34;&#34;&#34;Class for computing metrics (acc, f1) per assay and saving the results.&#34;&#34;&#34;

    @staticmethod
    def compute_metrics(
        df: pd.DataFrame,
        cat_label: str | None = None,
        column_templates: Dict[str, str] | None = None,
        min_pred: float | None = None,
    ) -&gt; Tuple[float, float, int]:
        &#34;&#34;&#34;Compute the accuracy and f1 of the predictions on a DataFrame that
        has columns of the format &#39;True/Predicted class ([category])&#39;.

        F1 score is computed against known true classes.

        If min_pred is not None, only consider predictions with a score
        greater than or equal to min_pred.

        If min_pred is higher than the maximum prediction score in the
        DataFrame, return 0.0, 0.0, 0

        Args:
            df: DataFrame containing the predictions and true classes.
            cat_label: Label for the category being evaluated, for
                labels of the form &#34;True class (category)&#34;.
            column_templates: Dictionary mapping column types to template strings.
                Requires &#34;True&#34;, &#34;Predicted&#34; and &#34;Max pred&#34; keys.
                If None, uses default templates.
            min_pred: Minimum prediction score to consider.

        Returns:
            Tuple of accuracy, f1 and number of samples.
        &#34;&#34;&#34;
        if column_templates is None:
            column_templates = {
                &#34;True&#34;: &#34;True class ({})&#34;,
                &#34;Predicted&#34;: &#34;Predicted class ({})&#34;,
                &#34;Max pred&#34;: &#34;Max pred ({})&#34;,
            }

        true_label = &#34;True class&#34;
        pred_label = &#34;Predicted class&#34;
        max_pred_label = &#34;Max pred&#34;
        if cat_label:
            true_label = column_templates[&#34;True&#34;].format(cat_label)
            pred_label = column_templates[&#34;Predicted&#34;].format(cat_label)
            max_pred_label = column_templates[&#34;Max pred&#34;].format(cat_label)

        for label in [true_label, pred_label, max_pred_label]:
            if label not in df.columns:
                raise KeyError(f&#34;Column &#39;{label}&#39; not found in DataFrame.&#34;)

        sub_df = df.copy()
        if min_pred:
            sub_df = sub_df[sub_df[max_pred_label] &gt;= min_pred]

        if sub_df.shape[0] == 0:
            return 0.0, 0.0, 0

        y_true = sub_df[true_label]
        y_pred = sub_df[pred_label]

        acc = (y_true == y_pred).mean()

        f1: float = f1_score(  # type: ignore
            y_true,
            y_pred,
            labels=y_true.unique(),
            average=&#34;macro&#34;,
        )
        return acc, f1, sub_df.shape[0]

    def compute_chunked_metrics(
        self,
        df: pd.DataFrame,
        cat_label: str,
        interval: float = 0.1,
        min_pred_column: str | None = None,
        column_templates: Dict[str, str] | None = None,
    ) -&gt; List[Tuple[float, float, float, float, int]]:
        &#34;&#34;&#34;
        Compute metrics chunked by prediction score intervals.

        Args:
            df: DataFrame containing predictions
            cat_label: Category label to compute metrics for
            interval: Size of prediction score interval (default: 0.1)
            min_pred_column: Column name containing prediction scores (if None, uses column template)
            column_templates: Dictionary mapping column types to template strings

        Returns:
            List of tuples: (lower_bound, upper_bound, accuracy, f1_score, sample_count)
        &#34;&#34;&#34;
        if column_templates is None:
            column_templates = {
                &#34;True&#34;: &#34;True class ({})&#34;,
                &#34;Predicted&#34;: &#34;Predicted class ({})&#34;,
                &#34;Max pred&#34;: &#34;Max pred ({})&#34;,
            }

        if min_pred_column is None:
            min_pred_column = column_templates[&#34;Max pred&#34;].format(cat_label)

        if min_pred_column not in df.columns:
            raise KeyError(f&#34;Column &#39;{min_pred_column}&#39; not found in DataFrame.&#34;)

        # Create chunks based on interval
        chunk_bounds = np.arange(0, 1, interval)
        chunk_bounds = np.append(
            chunk_bounds, 1.0001
        )  # last upper bound needs to be inclusive
        chunked_results = []

        for i in range(len(chunk_bounds) - 1):
            lower_bound = chunk_bounds[i]
            upper_bound = chunk_bounds[i + 1]

            # Filter by prediction score range
            chunk_df = df[
                (df[min_pred_column] &gt;= lower_bound) &amp; (df[min_pred_column] &lt; upper_bound)
            ]

            if not chunk_df.empty:
                acc, f1, n_samples = self.compute_metrics(
                    chunk_df,
                    cat_label=cat_label,
                    min_pred=lower_bound,
                    column_templates=column_templates,
                )
                chunked_results.append((lower_bound, upper_bound, acc, f1, n_samples))
            else:
                # Include empty chunks with zeros
                chunked_results.append((lower_bound, upper_bound, 0.0, 0.0, 0))

        return chunked_results

    @staticmethod
    def _count_unknown(
        df_subset: pd.DataFrame, max_pred_col_name: str, chunked: bool, interval: float
    ) -&gt; List[Tuple[Any, ...]]:
        &#34;&#34;&#34;
        Helper function to calculate counts of unknown samples,
        either chunked by prediction score or by min_pred thresholds.
        Sets acc/f1 to np.nan as they are not applicable for unknown counts.
        &#34;&#34;&#34;
        # (lower_bound, upper_bound, acc, f1, n_samples) or (min_pred, acc, f1, n_samples)
        counts_list: List[Tuple[Any, ...]] = []

        if df_subset.empty:
            # Populate with zero counts if the subset DataFrame is empty
            if chunked:
                # Use robust chunk bounds consistent with compute_chunked_metrics
                current_chunk_bounds = np.arange(0, 1, interval)
                current_chunk_bounds = np.append(
                    current_chunk_bounds, 1.0001
                )  # Ensures last bin includes 1.0
                for i in range(len(current_chunk_bounds) - 1):
                    lower_bound = current_chunk_bounds[i]
                    upper_bound = current_chunk_bounds[i + 1]
                    counts_list.append((lower_bound, upper_bound, np.nan, np.nan, 0))
            else:  # chunked=False
                for min_pred_str in [&#34;0.0&#34;, &#34;0.6&#34;, &#34;0.8&#34;, &#34;0.9&#34;]:
                    counts_list.append((min_pred_str, np.nan, np.nan, 0))
            return counts_list

        if chunked:
            current_chunk_bounds = np.arange(0, 1, interval)
            current_chunk_bounds = np.append(current_chunk_bounds, 1.0001)

            for i in range(len(current_chunk_bounds) - 1):
                lower_bound = current_chunk_bounds[i]
                upper_bound = current_chunk_bounds[i + 1]
                count = (
                    (df_subset[max_pred_col_name] &gt;= lower_bound)
                    &amp; (df_subset[max_pred_col_name] &lt; upper_bound)
                ).sum()
                counts_list.append((lower_bound, upper_bound, np.nan, np.nan, count))
        else:  # chunked=False
            for min_pred_str in [&#34;0.0&#34;, &#34;0.6&#34;, &#34;0.8&#34;, &#34;0.9&#34;]:
                min_pred_float = float(min_pred_str)
                high_pred_count = (df_subset[max_pred_col_name] &gt;= min_pred_float).sum()
                counts_list.append((min_pred_str, np.nan, np.nan, high_pred_count))
        return counts_list

    def _compute_metrics_per_assay(
        self,
        all_preds: pd.DataFrame,
        categories: List[str] | None = None,
        verbose: bool = True,
        no_epiatlas: bool = True,
        column_templates: Dict[str, str] | None = None,
        merge_assays: bool = True,
        assay_label: str = ASSAY,
        chunked: bool = False,
        interval: float = 0.1,
        core_assays: List[str] | None = None,
        non_core_assays: List[str] | None = None,
        metric_function: Callable | None = None,
        metric_args: Dict[str, Any] | None = None,
    ) -&gt; Dict[str, Dict]:
        &#34;&#34;&#34;Compute metrics for each assay.

        Args:
            all_preds (pd.DataFrame): Dataframe containing predictions.
            categories (List[str] | None): List of categories to compute accuracy/F1 for. If None, uses default categories.
                Default: [f&#34;{ASSAY}_7c&#34;, f&#34;{ASSAY}_11c&#34;, CELL_TYPE, SEX, LIFE_STAGE, f&#34;{LIFE_STAGE}_merged&#34;, CANCER, f&#34;{CANCER}_merged&#34;]
            verbose (bool): Whether to print the results.
            no_epiatlas (bool): Whether to exclude EpiAtlas samples.
            column_templates (Dict[str, str] | None): Column name templates. If None, uses default templates.
                Default: {&#39;True&#39;: &#39;True class ({})&#39;, &#39;Predicted&#39;: &#39;Predicted class ({})&#39;, &#39;Max pred&#39;: &#39;Max pred ({})&#39;}
            merge_assays (bool): Whether to merge similar assays.
            assay_label (str): Label to use for the assay column.
                Default: &#34;assay_epiclass
            chunked (bool): Whether to compute chunked metrics.
            interval (float): Prediction score interval (only if chunked=True).
            core_assays (List[str] | None): List of core assays.
                Default: core11 assays (6 histones + input + 2 rna + 2 wgb)
            non_core_assays (List[str] | None): List of non-core assays.
                Default: [&#34;ctcf&#34;, &#34;non-core&#34;]
            metric_function (Callable | None): Function for computing metrics.
            metric_args (Dict[str, Any] | None): Additional arguments for metric function.


        Returns:
            Dict[str, Dict]: Metrics for each assay.
        &#34;&#34;&#34;
        if categories is None:
            categories = [
                f&#34;{ASSAY}_7c&#34;,
                f&#34;{ASSAY}_11c&#34;,
                CELL_TYPE,
                SEX,
                LIFE_STAGE,
                f&#34;{LIFE_STAGE}_merged&#34;,
                CANCER,
                BIOMATERIAL_TYPE,
            ]
        if column_templates is None:
            column_templates = {
                &#34;True&#34;: &#34;True class ({})&#34;,
                &#34;Predicted&#34;: &#34;Predicted class ({})&#34;,
                &#34;Max pred&#34;: &#34;Max pred ({})&#34;,
            }
        if metric_args is None:
            metric_args = {}

        # Set the default metric function if not provided
        if metric_function is None:
            if chunked:
                metric_function = self.compute_chunked_metrics
                metric_args[&#34;interval&#34;] = interval
            else:
                metric_function = self.compute_metrics

        df = all_preds.copy(deep=True)
        if no_epiatlas and not (all_preds[&#34;in_epiatlas&#34;].astype(str) == &#34;False&#34;).all():
            df = df[df[&#34;in_epiatlas&#34;].astype(str) == &#34;False&#34;]

        all_df_assays = df[assay_label].unique()

        # assay target classification
        df = df.fillna(&#34;unknown&#34;)
        if core_assays is None:
            core_assays = ASSAY_ORDER.copy()

        # only accepting specific labels, all labels not in core_assays
        if non_core_assays is None:
            accepted_nc_labels = [&#34;ctcf&#34;, &#34;non-core&#34;, &#34;other&#34;]
            non_core_assays = list(set(all_df_assays) &amp; set(accepted_nc_labels))

        all_assays = core_assays + non_core_assays

        # Define &#39;unknown&#39; for expected labels (not assay targets)
        unknown_labels = [&#34;unknown&#34;, &#34;other&#34;, &#34;no_consensus&#34;]

        # merging rna / wgbs assays
        if merge_assays:
            assay_cols = [
                ASSAY,
                column_templates[&#34;True&#34;].format(f&#34;{ASSAY}_11c&#34;),
                column_templates[&#34;Predicted&#34;].format(f&#34;{ASSAY}_11c&#34;),
                column_templates[&#34;True&#34;].format(f&#34;{ASSAY}_7c&#34;),
                column_templates[&#34;Predicted&#34;].format(f&#34;{ASSAY}_7c&#34;),
            ]
            for col in assay_cols:
                if col in df.columns:  # Check if column exists
                    for pair in (
                        (&#34;mrna_seq&#34;, &#34;rna_seq&#34;),
                        (&#34;wgbs-pbat&#34;, &#34;wgbs&#34;),
                        (&#34;wgbs-standard&#34;, &#34;wgbs&#34;),
                    ):
                        df[col] = df[col].str.replace(
                            pat=pair[0], repl=pair[1], regex=False
                        )
                else:
                    if verbose:
                        print(f&#34;Warning: Column &#39;{col}&#39; not found for merging assays.&#34;)

        filter_dict = {
            &#34;avg-all&#34;: lambda df: pd.Series(True, index=df.index),
            &#34;avg-core&#34;: lambda df: df[assay_label].isin(core_assays),
            &#34;avg-non-core&#34;: lambda df: df[assay_label].isin(non_core_assays),
        }

        all_metrics_per_assay = {}
        for category_name in categories:
            metric_name = &#34;chunked metrics&#34; if chunked else &#34;metrics&#34;
            if verbose:
                print(f&#34;Computing {metric_name} for {category_name}&#34;)

            task_df = df.copy(deep=True)
            y_true_col = column_templates[&#34;True&#34;].format(category_name)
            y_pred_col = column_templates[&#34;Predicted&#34;].format(category_name)
            max_pred_label = column_templates[&#34;Max pred&#34;].format(category_name)

            if max_pred_label not in df.columns:
                raise ValueError(f&#34;Column &#39;{max_pred_label}&#39; not found.&#34;)

            for label in [y_true_col, y_pred_col]:
                task_df[label] = task_df[label].str.lower()

            # Get unknown samples
            unknown_mask = task_df[y_true_col].isin(unknown_labels)
            unknown_df = task_df[unknown_mask]

            # Remove unknown samples, if any
            known_df = task_df[~unknown_mask]
            known_df = known_df[known_df[y_pred_col] != &#34;unknown&#34;]

            if category_name == CELL_TYPE:
                known_df = known_df[known_df[CELL_TYPE].isin(EPIATLAS_16_CT)]

            # assumed to be ASSAY_11c/ASSAY_7c, non-core assays are removed (+ no unknown)
            cat_core_assays = core_assays.copy()
            if ASSAY in category_name:
                cat_core_assays = set(cat_core_assays) &amp; set(ASSAY_ORDER)
                known_df = known_df[known_df[assay_label].isin(cat_core_assays)]

            if verbose:
                print(category_name, known_df.shape)
                print(known_df[y_true_col].value_counts(dropna=False))
                if not unknown_df.empty:
                    print(f&#34;Unknown {category_name} samples: {unknown_df.shape[0]}&#34;)
                print()

            # Metrics per assay
            metrics_per_assay = {}

            # Process individual assays
            for label in all_assays:
                if verbose:
                    print(f&#34;Processing assay target {label}&#34;)

                # Process known labels
                known_assay_df = known_df[known_df[assay_label] == label]
                if verbose:
                    print(f&#34;Known {label} samples: {known_assay_df.shape[0]}&#34;)
                    print(known_assay_df[y_true_col].value_counts(dropna=False), &#34;\n&#34;)
                    print(known_assay_df[y_pred_col].value_counts(dropna=False), &#34;\n&#34;)

                if not known_assay_df.empty or label in known_df[assay_label].unique():
                    if chunked:
                        # Compute chunked metrics for this assay
                        metrics_per_assay[label] = metric_function(
                            known_assay_df,
                            cat_label=category_name,
                            column_templates=column_templates,
                            **metric_args,
                        )
                    else:
                        # Compute standard metrics for this assay with different thresholds
                        metrics_per_assay[label] = []
                        for min_pred in [&#34;0.0&#34;, &#34;0.6&#34;, &#34;0.8&#34;, &#34;0.9&#34;]:
                            metric_args[&#34;min_pred&#34;] = float(min_pred)
                            result = metric_function(
                                known_assay_df,
                                cat_label=category_name,
                                column_templates=column_templates,
                                **metric_args,
                            )
                            metrics_per_assay[label].append((min_pred, *result))

            # --- Calculate global metrics for KNOWN expected class ---
            set_labels = [&#34;avg-all&#34;, &#34;avg-core&#34;, &#34;avg-non-core&#34;]
            if all_assays in (cat_core_assays, non_core_assays):
                set_labels = [&#34;avg-all&#34;]

            if ASSAY in category_name:
                set_labels = [&#34;avg-core&#34;]

            for set_label in set_labels:
                metrics_per_assay[set_label] = []

            for set_label in set_labels:
                filter_condition = filter_dict[set_label]
                filtered_df = known_df[filter_condition(known_df)]
                if filtered_df.empty:
                    continue

                if chunked:
                    metrics_per_assay[set_label] = metric_function(
                        df=filtered_df,
                        cat_label=category_name,
                        column_templates=column_templates,
                        **metric_args,
                    )
                else:
                    # Standard global metrics with different thresholds
                    for min_pred in [&#34;0.0&#34;, &#34;0.6&#34;, &#34;0.8&#34;, &#34;0.9&#34;]:
                        metric_args[&#34;min_pred&#34;] = float(min_pred)
                        result = metric_function(
                            df=filtered_df,
                            cat_label=category_name,
                            column_templates=column_templates,
                            **metric_args,
                        )
                        metrics_per_assay[set_label].append((min_pred, *result))

            # --- Calculate metrics for UNKNOWN expected class ---

            # Total
            label = &#34;count-unknown&#34;
            metrics_per_assay[label] = MetricsPerAssay._count_unknown(
                unknown_df, max_pred_label, chunked, interval
            )
            N_all = metrics_per_assay[label][0][-1]

            # Core
            label = &#34;count-unknown-core&#34;
            unknown_core_df = unknown_df[unknown_df[assay_label].isin(core_assays)]
            metrics_per_assay[label] = MetricsPerAssay._count_unknown(
                unknown_core_df, max_pred_label, chunked, interval
            )
            N_core = metrics_per_assay[label][0][-1]

            # non-core
            label = &#34;count-unknown-non_core&#34;
            unknown_non_core_df = unknown_df[
                unknown_df[assay_label].isin(non_core_assays)
            ]
            metrics_per_assay[label] = MetricsPerAssay._count_unknown(
                unknown_non_core_df, max_pred_label, chunked, interval
            )
            N_non_core = metrics_per_assay[label][0][-1]

            # Check that all unknown samples are accounted for
            if N_all != N_core + N_non_core:
                raise ValueError(
                    f&#34;Unknown sample core/non-core not complementary: N_all ({N_all}) != N_core ({N_core}) + N_non_core ({N_non_core})&#34;
                )

            all_metrics_per_assay[category_name] = metrics_per_assay

        if verbose:
            print(f&#34;Computed metrics for {len(categories)} categories&#34;)
            if chunked:
                print(f&#34;Used interval size: {interval}&#34;)

        return all_metrics_per_assay

    @wraps(_compute_metrics_per_assay)
    def compute_all_chunked_acc_per_assay(self, *args, **kwargs):
        &#34;&#34;&#34;Compute metrics for all assays using chunked evaluation.&#34;&#34;&#34;
        result = self._compute_metrics_per_assay(*args, chunked=True, **kwargs)
        return result

    @wraps(_compute_metrics_per_assay)
    def compute_all_acc_per_assay(self, *args, **kwargs):
        &#34;&#34;&#34;Compute metrics for all assays using threshold-based evaluation.&#34;&#34;&#34;
        result = self._compute_metrics_per_assay(*args, chunked=False, **kwargs)
        return result

    def create_metrics_dataframe(
        self,
        input_metrics: Dict[str, Dict],
        chunked: bool = False,
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        Takes a metrics dictionary and converts it into a structured Pandas DataFrame.
        Applies necessary formatting, type conversions, and post-processing.

        FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value &#39;NA&#39; has dtype incompatible with float64, please explicitly cast to a compatible dtype first.
        Currently tested for pandas 2.2.3 (py3.12) and 1.4.1 (py3.8).

        Args:
        - input_metrics: Output of compute_all_acc_per_assay or compute_all_chunked_acc_per_assay.
        - chunked: Whether the input contains chunked metrics (True) or standard metrics (False).
        - ASSAY: The name of the assay column in the data.

        Returns:
        - A processed Pandas DataFrame containing the metrics. Contains float columns with &#39;NA&#39; values.
        &#34;&#34;&#34;
        rows = []
        for name, metrics_per_assay in input_metrics.items():
            for assay, values in metrics_per_assay.items():
                # Handle potential empty values list gracefully
                if not values:
                    continue
                for value_tuple in values:
                    if chunked:
                        # Chunked format: (lower_bound, upper_bound, acc, f1, nb_samples)
                        if len(value_tuple) != 5:
                            # Add some basic validation if needed
                            print(
                                f&#34;Warning: Skipping malformed chunked tuple for {name}/{assay}: {value_tuple}&#34;
                            )
                            continue
                        lower_bound, upper_bound, acc, f1, nb_samples = value_tuple
                        rows.append(
                            [name, assay, lower_bound, upper_bound, acc, f1, nb_samples]
                        )
                    else:
                        # Standard format: (min_pred, acc, f1, nb_samples)
                        if len(value_tuple) != 4:
                            print(
                                f&#34;Warning: Skipping malformed standard tuple for {name}/{assay}: {value_tuple}&#34;
                            )
                            continue
                        min_pred, acc, f1, nb_samples = value_tuple
                        rows.append([name, assay, min_pred, acc, f1, nb_samples])

        # Return empty DataFrame if no valid rows were generated
        if not rows:
            raise ValueError(&#34;Warning: No valid rows generated from input_metrics.&#34;)

        # Create DataFrame with appropriate columns
        if chunked:
            columns = [
                &#34;task_name&#34;,
                ASSAY,
                &#34;pred_score_min&#34;,
                &#34;pred_score_max&#34;,
                &#34;acc&#34;,
                &#34;f1-score&#34;,
                &#34;nb_samples&#34;,
            ]
            df_metrics = pd.DataFrame(rows, columns=columns)
            df_metrics = df_metrics.astype(
                {
                    &#34;task_name&#34;: &#34;str&#34;,
                    ASSAY: &#34;str&#34;,
                    &#34;pred_score_min&#34;: &#34;float&#34;,
                    &#34;pred_score_max&#34;: &#34;float&#34;,
                    &#34;acc&#34;: &#34;float&#34;,
                    &#34;f1-score&#34;: &#34;float&#34;,
                    &#34;nb_samples&#34;: &#34;int&#34;,
                }
            )
            df_metrics = df_metrics.sort_values(
                [&#34;task_name&#34;, ASSAY, &#34;pred_score_min&#34;], ignore_index=True
            )
        else:
            columns = [
                &#34;task_name&#34;,
                ASSAY,
                &#34;min_predScore&#34;,
                &#34;acc&#34;,
                &#34;f1-score&#34;,
                &#34;nb_samples&#34;,
            ]
            df_metrics = pd.DataFrame(rows, columns=columns)
            df_metrics = df_metrics.astype(
                {
                    &#34;task_name&#34;: &#34;str&#34;,
                    ASSAY: &#34;str&#34;,
                    &#34;min_predScore&#34;: &#34;float&#34;,
                    &#34;acc&#34;: &#34;float&#34;,
                    &#34;f1-score&#34;: &#34;float&#34;,
                    &#34;nb_samples&#34;: &#34;int&#34;,
                }
            )
            df_metrics = df_metrics.sort_values(
                [&#34;task_name&#34;, ASSAY, &#34;min_predScore&#34;], ignore_index=True
            )

        # Round float columns - Use .round() directly on columns
        # Convert to numeric first, coercing errors, then round, then handle NAs if needed
        float_cols = [&#34;acc&#34;, &#34;f1-score&#34;]
        for col in float_cols:
            # Ensure column is numeric, making non-numeric values NaN
            df_metrics[col] = pd.to_numeric(df_metrics[col], errors=&#34;coerce&#34;)
        df_metrics[float_cols] = df_metrics[float_cols].fillna(pd.NA).round(4)

        # --- Apply post-processing rules ---

        # f1-score on ASSAY task, per assay, doesn&#39;t make sense
        df_metrics.loc[df_metrics[&#34;task_name&#34;] == ASSAY, &#34;f1-score&#34;] = pd.NA

        # metrics for unknown expected class are not defined
        unknown_count_keys = [
            &#34;count-unknown&#34;,
            &#34;count-unknown-core&#34;,
            &#34;count-unknown-non_core&#34;,
        ]
        df_metrics.loc[
            df_metrics[ASSAY].isin(unknown_count_keys), [&#34;acc&#34;, &#34;f1-score&#34;]
        ] = pd.NA

        # acc / f1 for 0 samples is not defined
        df_metrics.loc[
            df_metrics[&#34;nb_samples&#34;].astype(int) == 0, [&#34;acc&#34;, &#34;f1-score&#34;]
        ] = pd.NA

        return df_metrics

    def save_dataframe_to_tsv(
        self,
        df_to_save: pd.DataFrame,
        folders: List[Path] | Path,
        filename: str,
        verbose: bool = True,
    ) -&gt; None:
        &#34;&#34;&#34;
        Saves a Pandas DataFrame to one or more TSV files.

        Args:
        - df_to_save: The Pandas DataFrame to save.
        - folders: A single Path object or a list of Path objects indicating the directories to save the file in.
        - filename: The name of the file (e.g., &#39;metrics.tsv&#39;).
        - verbose: Whether to print status messages.
        &#34;&#34;&#34;
        if df_to_save.empty:
            if verbose:
                print(f&#34;DataFrame is empty. Skipping save for &#39;{filename}&#39;.&#34;)
            return

        if verbose:
            print(f&#34;Preparing to save {df_to_save.shape[0]} rows to &#39;{filename}&#39;...&#34;)

        if isinstance(folders, Path):
            folders = [folders]
        elif not isinstance(folders, list):
            raise TypeError(
                f&#34;`folders` must be a Path or a list of Paths, got {type(folders)}&#34;
            )

        saved_count = 0
        for folder in folders:
            if not isinstance(folder, Path):
                print(
                    f&#34;Warning: Skipping invalid folder type: {type(folder)}. Expected Path.&#34;
                )
                continue

            try:
                folder.mkdir(parents=True, exist_ok=True)
                path = folder / filename
                df_to_save.to_csv(
                    path,
                    sep=&#34;\t&#34;,
                    index=False,
                    na_rep=&#34;NA&#34;,  # Represent missing values as &#39;NA&#39; in the TSV
                )
                if verbose:
                    print(f&#34;Successfully saved to {path}&#34;)
                saved_count += 1
            except OSError as e:
                print(
                    f&#34;Error: Could not create directory or save file at {folder / filename}: {e}&#34;
                )
            except Exception as e:  # pylint: disable=broad-except
                print(
                    f&#34;Error: An unexpected error occurred while saving to {folder / filename}: {e}&#34;
                )

        if verbose and saved_count &gt; 0:
            print(f&#34;Finished saving. &#39;{filename}&#39; saved in {saved_count} location(s).&#34;)
        elif verbose and saved_count == 0:
            print(
                f&#34;Warning: &#39;{filename}&#39; was not saved to any locations due to errors or invalid folder paths.&#34;
            )

    def save_metrics_per_assay(
        self,
        input_metrics: Dict[str, Dict],
        folders: List[Path] | Path,
        filename: str,
        chunked: bool = False,
        verbose: bool = True,
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        Take metrics dictionary and save it to TSV files.
        Works with both standard metrics and chunked metrics.

        Args:
        - metrics: Output of compute_all_acc_per_assay or compute_all_chunked_acc_per_assay.
        - folders: A list of folders to save the results to.
        - filename: The name of the file to save the results to.
        - chunked: Whether the input contains chunked metrics (True) or standard metrics (False).
        - verbose: Whether to print verbose output.

        Returns:
        - A restructured dataframe with metrics for each assay
        &#34;&#34;&#34;

        df_metrics = self.create_metrics_dataframe(
            input_metrics=input_metrics,
            chunked=chunked,
        )

        self.save_dataframe_to_tsv(
            df_to_save=df_metrics,
            folders=folders,
            filename=filename,
            verbose=verbose,
        )

        return df_metrics

    def save_acc_per_assay(
        self,
        metrics: Dict[str, Dict],
        folders: List[Path] | Path,
        filename: str,
        verbose: bool = True,
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        Take a dictionary containing standard metrics for multiple tasks and save to files.
        This is a wrapper around save_metrics_per_assay for backward compatibility.

        Args:
        - metrics: Output of the compute_all_acc_per_assay function.
        - folders: A list of folders to save the results to.
        - filename: The name of the file to save the results to.
        - verbose: Whether to print verbose output.

        Returns:
        - A restructured dataframe with (accuracy, f1, N) for each assay
        &#34;&#34;&#34;
        return self.save_metrics_per_assay(
            input_metrics=metrics,
            folders=folders,
            filename=filename,
            chunked=False,
            verbose=verbose,
        )

    def save_chunked_acc_per_assay(
        self,
        metrics: Dict[str, Dict],
        folders: List[Path] | Path,
        filename: str,
        verbose: bool = True,
    ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        Take a dictionary containing chunked metrics for multiple tasks and save to files.
        This is a wrapper around save_metrics_per_assay for backward compatibility.

        Args:
        - metrics: Output of the compute_all_chunked_acc_per_assay function.
        - folders: A list of folders to save the results to.
        - filename: The name of the file to save the results to.
        - verbose: Whether to print verbose output.

        Returns:
        - A restructured dataframe with chunked metrics for each assay
        &#34;&#34;&#34;
        return self.save_metrics_per_assay(
            input_metrics=metrics,
            folders=folders,
            filename=filename,
            chunked=True,
            verbose=verbose,
        )

    def compute_multiple_metric_formats(
        self,
        preds: pd.DataFrame,
        folders_to_save: List[Path],
        general_filename: str,
        verbose: bool = True,
        return_df: bool = False,
        compute_fct_kwargs: Dict[str, Any] | None = None,
    ) -&gt; None | Dict[str, pd.DataFrame]:
        &#34;&#34;&#34;Compute and save metrics in different formats.

        Args:
            preds (pd.DataFrame): Dataframe containing predictions.
            folders_to_save (List[Path]): List of folders to save the results to.
            general_filename (str): The filename stem to use for the output files.
                Saves files will will be &#34;&lt;general_filename&gt;.tsv&#34; and &#34;&lt;general_filename&gt;_chunked.tsv&#34;
            verbose (bool, optional): Whether to print verbose output. Defaults to True.
            return_df (bool, optional): Whether to return the metrics dataframes. Defaults to False.
            compute_fct_kwargs (Dict[str, Any], optional): Keyword arguments to pass to the compute function. Defaults to None.
        &#34;&#34;&#34;
        if return_df:
            return_dict = {}

        for filename in [f&#34;{general_filename}.tsv&#34;, f&#34;{general_filename}_chunked.tsv&#34;]:
            if &#34;chunked&#34; in filename:
                compute_fct = self.compute_all_chunked_acc_per_assay
                save_fct = self.save_chunked_acc_per_assay
            else:
                compute_fct = self.compute_all_acc_per_assay
                save_fct = self.save_acc_per_assay

            metrics = compute_fct(  # type: ignore
                all_preds=preds,
                verbose=verbose,
                **(compute_fct_kwargs or {}),
            )
            metrics_df = save_fct(
                metrics=metrics,  # type: ignore
                folders=folders_to_save,
                filename=filename,
                verbose=verbose,
            )

            if return_df:
                return_dict[filename] = metrics_df

        if return_df:
            return return_dict

        return None</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_metrics"><code class="name flex">
<span>def <span class="ident">compute_metrics</span></span>(<span>df: pd.DataFrame, cat_label: str | None = None, column_templates: Dict[str, str] | None = None, min_pred: float | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the accuracy and f1 of the predictions on a DataFrame that
has columns of the format 'True/Predicted class ([category])'.</p>
<p>F1 score is computed against known true classes.</p>
<p>If min_pred is not None, only consider predictions with a score
greater than or equal to min_pred.</p>
<p>If min_pred is higher than the maximum prediction score in the
DataFrame, return 0.0, 0.0, 0</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong></dt>
<dd>DataFrame containing the predictions and true classes.</dd>
<dt><strong><code>cat_label</code></strong></dt>
<dd>Label for the category being evaluated, for
labels of the form "True class (category)".</dd>
<dt><strong><code>column_templates</code></strong></dt>
<dd>Dictionary mapping column types to template strings.
Requires "True", "Predicted" and "Max pred" keys.
If None, uses default templates.</dd>
<dt><strong><code>min_pred</code></strong></dt>
<dd>Minimum prediction score to consider.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tuple of accuracy, f1 and number of samples.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_all_acc_per_assay"><code class="name flex">
<span>def <span class="ident">compute_all_acc_per_assay</span></span>(<span>self, all_preds: pd.DataFrame, categories: List[str] | None = None, verbose: bool = True, no_epiatlas: bool = True, column_templates: Dict[str, str] | None = None, merge_assays: bool = True, assay_label: str = 'assay_epiclass', chunked: bool = False, interval: float = 0.1, core_assays: List[str] | None = None, non_core_assays: List[str] | None = None, metric_function: Callable | None = None, metric_args: Dict[str, Any] | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute metrics for each assay.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>all_preds</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Dataframe containing predictions.</dd>
<dt>categories (List[str] | None): List of categories to compute accuracy/F1 for. If None, uses default categories.</dt>
<dt>Default: [f"{ASSAY}_7c", f"{ASSAY}_11c", CELL_TYPE, SEX, LIFE_STAGE, f"{LIFE_STAGE}_merged", CANCER, f"{CANCER}_merged"]</dt>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to print the results.</dd>
<dt><strong><code>no_epiatlas</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to exclude EpiAtlas samples.</dd>
<dt>column_templates (Dict[str, str] | None): Column name templates. If None, uses default templates.</dt>
<dt>Default: {'True': 'True class ({})', 'Predicted': 'Predicted class ({})', 'Max pred': 'Max pred ({})'}</dt>
<dt><strong><code>merge_assays</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to merge similar assays.</dd>
<dt><strong><code>assay_label</code></strong> :&ensp;<code>str</code></dt>
<dd>Label to use for the assay column.
Default: "assay_epiclass</dd>
<dt><strong><code>chunked</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to compute chunked metrics.</dd>
<dt><strong><code>interval</code></strong> :&ensp;<code>float</code></dt>
<dd>Prediction score interval (only if chunked=True).</dd>
</dl>
<p>core_assays (List[str] | None): List of core assays.
Default: core11 assays (6 histones + input + 2 rna + 2 wgb)
non_core_assays (List[str] | None): List of non-core assays.
Default: ["ctcf", "non-core"]
metric_function (Callable | None): Function for computing metrics.
metric_args (Dict[str, Any] | None): Additional arguments for metric function.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, Dict]</code></dt>
<dd>Metrics for each assay.</dd>
</dl></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_all_chunked_acc_per_assay"><code class="name flex">
<span>def <span class="ident">compute_all_chunked_acc_per_assay</span></span>(<span>self, all_preds: pd.DataFrame, categories: List[str] | None = None, verbose: bool = True, no_epiatlas: bool = True, column_templates: Dict[str, str] | None = None, merge_assays: bool = True, assay_label: str = 'assay_epiclass', chunked: bool = False, interval: float = 0.1, core_assays: List[str] | None = None, non_core_assays: List[str] | None = None, metric_function: Callable | None = None, metric_args: Dict[str, Any] | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute metrics for each assay.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>all_preds</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Dataframe containing predictions.</dd>
<dt>categories (List[str] | None): List of categories to compute accuracy/F1 for. If None, uses default categories.</dt>
<dt>Default: [f"{ASSAY}_7c", f"{ASSAY}_11c", CELL_TYPE, SEX, LIFE_STAGE, f"{LIFE_STAGE}_merged", CANCER, f"{CANCER}_merged"]</dt>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to print the results.</dd>
<dt><strong><code>no_epiatlas</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to exclude EpiAtlas samples.</dd>
<dt>column_templates (Dict[str, str] | None): Column name templates. If None, uses default templates.</dt>
<dt>Default: {'True': 'True class ({})', 'Predicted': 'Predicted class ({})', 'Max pred': 'Max pred ({})'}</dt>
<dt><strong><code>merge_assays</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to merge similar assays.</dd>
<dt><strong><code>assay_label</code></strong> :&ensp;<code>str</code></dt>
<dd>Label to use for the assay column.
Default: "assay_epiclass</dd>
<dt><strong><code>chunked</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to compute chunked metrics.</dd>
<dt><strong><code>interval</code></strong> :&ensp;<code>float</code></dt>
<dd>Prediction score interval (only if chunked=True).</dd>
</dl>
<p>core_assays (List[str] | None): List of core assays.
Default: core11 assays (6 histones + input + 2 rna + 2 wgb)
non_core_assays (List[str] | None): List of non-core assays.
Default: ["ctcf", "non-core"]
metric_function (Callable | None): Function for computing metrics.
metric_args (Dict[str, Any] | None): Additional arguments for metric function.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict[str, Dict]</code></dt>
<dd>Metrics for each assay.</dd>
</dl></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_chunked_metrics"><code class="name flex">
<span>def <span class="ident">compute_chunked_metrics</span></span>(<span>self, df: pd.DataFrame, cat_label: str, interval: float = 0.1, min_pred_column: str | None = None, column_templates: Dict[str, str] | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute metrics chunked by prediction score intervals.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong></dt>
<dd>DataFrame containing predictions</dd>
<dt><strong><code>cat_label</code></strong></dt>
<dd>Category label to compute metrics for</dd>
<dt><strong><code>interval</code></strong></dt>
<dd>Size of prediction score interval (default: 0.1)</dd>
<dt><strong><code>min_pred_column</code></strong></dt>
<dd>Column name containing prediction scores (if None, uses column template)</dd>
<dt><strong><code>column_templates</code></strong></dt>
<dd>Dictionary mapping column types to template strings</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List</code> of <code>tuples</code></dt>
<dd>(lower_bound, upper_bound, accuracy, f1_score, sample_count)</dd>
</dl></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_multiple_metric_formats"><code class="name flex">
<span>def <span class="ident">compute_multiple_metric_formats</span></span>(<span>self, preds: pd.DataFrame, folders_to_save: List[Path], general_filename: str, verbose: bool = True, return_df: bool = False, compute_fct_kwargs: Dict[str, Any] | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute and save metrics in different formats.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>preds</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>Dataframe containing predictions.</dd>
<dt><strong><code>folders_to_save</code></strong> :&ensp;<code>List[Path]</code></dt>
<dd>List of folders to save the results to.</dd>
<dt><strong><code>general_filename</code></strong> :&ensp;<code>str</code></dt>
<dd>The filename stem to use for the output files.
Saves files will will be "<general_filename>.tsv" and "<general_filename>_chunked.tsv"</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to print verbose output. Defaults to True.</dd>
<dt><strong><code>return_df</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to return the metrics dataframes. Defaults to False.</dd>
<dt><strong><code>compute_fct_kwargs</code></strong> :&ensp;<code>Dict[str, Any]</code>, optional</dt>
<dd>Keyword arguments to pass to the compute function. Defaults to None.</dd>
</dl></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.create_metrics_dataframe"><code class="name flex">
<span>def <span class="ident">create_metrics_dataframe</span></span>(<span>self, input_metrics: Dict[str, Dict], chunked: bool = False) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Takes a metrics dictionary and converts it into a structured Pandas DataFrame.
Applies necessary formatting, type conversions, and post-processing.</p>
<p>FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'NA' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.
Currently tested for pandas 2.2.3 (py3.12) and 1.4.1 (py3.8).</p>
<p>Args:
- input_metrics: Output of compute_all_acc_per_assay or compute_all_chunked_acc_per_assay.
- chunked: Whether the input contains chunked metrics (True) or standard metrics (False).
- ASSAY: The name of the assay column in the data.</p>
<p>Returns:
- A processed Pandas DataFrame containing the metrics. Contains float columns with 'NA' values.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.save_acc_per_assay"><code class="name flex">
<span>def <span class="ident">save_acc_per_assay</span></span>(<span>self, metrics: Dict[str, Dict], folders: List[Path] | Path, filename: str, verbose: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Take a dictionary containing standard metrics for multiple tasks and save to files.
This is a wrapper around save_metrics_per_assay for backward compatibility.</p>
<p>Args:
- metrics: Output of the compute_all_acc_per_assay function.
- folders: A list of folders to save the results to.
- filename: The name of the file to save the results to.
- verbose: Whether to print verbose output.</p>
<p>Returns:
- A restructured dataframe with (accuracy, f1, N) for each assay</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.save_chunked_acc_per_assay"><code class="name flex">
<span>def <span class="ident">save_chunked_acc_per_assay</span></span>(<span>self, metrics: Dict[str, Dict], folders: List[Path] | Path, filename: str, verbose: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Take a dictionary containing chunked metrics for multiple tasks and save to files.
This is a wrapper around save_metrics_per_assay for backward compatibility.</p>
<p>Args:
- metrics: Output of the compute_all_chunked_acc_per_assay function.
- folders: A list of folders to save the results to.
- filename: The name of the file to save the results to.
- verbose: Whether to print verbose output.</p>
<p>Returns:
- A restructured dataframe with chunked metrics for each assay</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.save_dataframe_to_tsv"><code class="name flex">
<span>def <span class="ident">save_dataframe_to_tsv</span></span>(<span>self, df_to_save: pd.DataFrame, folders: List[Path] | Path, filename: str, verbose: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a Pandas DataFrame to one or more TSV files.</p>
<p>Args:
- df_to_save: The Pandas DataFrame to save.
- folders: A single Path object or a list of Path objects indicating the directories to save the file in.
- filename: The name of the file (e.g., 'metrics.tsv').
- verbose: Whether to print status messages.</p></div>
</dd>
<dt id="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.save_metrics_per_assay"><code class="name flex">
<span>def <span class="ident">save_metrics_per_assay</span></span>(<span>self, input_metrics: Dict[str, Dict], folders: List[Path] | Path, filename: str, chunked: bool = False, verbose: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Take metrics dictionary and save it to TSV files.
Works with both standard metrics and chunked metrics.</p>
<p>Args:
- metrics: Output of compute_all_acc_per_assay or compute_all_chunked_acc_per_assay.
- folders: A list of folders to save the results to.
- filename: The name of the file to save the results to.
- chunked: Whether the input contains chunked metrics (True) or standard metrics (False).
- verbose: Whether to print verbose output.</p>
<p>Returns:
- A restructured dataframe with metrics for each assay</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="epiclass.utils.notebooks.paper" href="index.html">epiclass.utils.notebooks.paper</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay" href="#epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay">MetricsPerAssay</a></code></h4>
<ul class="">
<li><code><a title="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_all_acc_per_assay" href="#epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_all_acc_per_assay">compute_all_acc_per_assay</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_all_chunked_acc_per_assay" href="#epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_all_chunked_acc_per_assay">compute_all_chunked_acc_per_assay</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_chunked_metrics" href="#epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_chunked_metrics">compute_chunked_metrics</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_metrics" href="#epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_metrics">compute_metrics</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_multiple_metric_formats" href="#epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.compute_multiple_metric_formats">compute_multiple_metric_formats</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.create_metrics_dataframe" href="#epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.create_metrics_dataframe">create_metrics_dataframe</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.save_acc_per_assay" href="#epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.save_acc_per_assay">save_acc_per_assay</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.save_chunked_acc_per_assay" href="#epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.save_chunked_acc_per_assay">save_chunked_acc_per_assay</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.save_dataframe_to_tsv" href="#epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.save_dataframe_to_tsv">save_dataframe_to_tsv</a></code></li>
<li><code><a title="epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.save_metrics_per_assay" href="#epiclass.utils.notebooks.paper.metrics_per_assay.MetricsPerAssay.save_metrics_per_assay">save_metrics_per_assay</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.0</a>.</p>
</footer>
</body>
</html>
